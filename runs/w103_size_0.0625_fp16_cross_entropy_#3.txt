Sender: LSF System <lsfadmin@eu-g2-08>
Subject: Job 207133001: <w103_size_0.0625_fp16_cross_entropy_#3> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_cross_entropy_#3> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:35:30 2022
Job was executed on host(s) <eu-g2-08>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:35:42 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:35:42 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575613 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   97859.85 sec.
    Max Memory :                                 8454 MB
    Average Memory :                             2214.29 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11546.00 MB
    Max Swap :                                   2527 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   92980 sec.
    Turnaround time :                            92992 sec.

The output (if any) follows:

2022-03-04 09:35:55 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575613, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:35:56 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 09:35:58 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 09:35:58 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:35:58 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:35:58 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-04 09:35:58 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 09:35:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:35:58 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 09:36:07 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:36:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:36:07 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-03-04 09:36:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:36:07 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:36:07 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:36:07 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 09:36:07 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 09:36:07 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:36:07 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 09:36:07 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:36:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:36:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:36:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:36:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:36:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:36:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:41:08 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.283 | ppl 19940.7 | wps 38052.5 | wpb 510.9 | bsz 1 | num_updates 92
2022-03-04 09:41:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 92 updates
2022-03-04 09:41:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:41:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:41:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 1 @ 92 updates, score 14.283) (writing took 6.178776860237122 seconds)
2022-03-04 09:41:14 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:41:14 | INFO | train | epoch 001 | loss 16.047 | ppl 67722.4 | wps 21969.6 | ups 0.34 | wpb 65489.2 | bsz 127.9 | num_updates 92 | lr 1.15977e-05 | gnorm 3.818 | loss_scale 4 | train_wall 271 | gb_free 8.2 | wall 308
2022-03-04 09:41:15 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:41:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:41:37 | INFO | train_inner | epoch 002:      8 / 97 loss=15.913, ppl=61698.6, wps=22062.9, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.647, loss_scale=4, train_wall=291, gb_free=8.2, wall=330
2022-03-04 09:45:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:45:57 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.708 | ppl 6690 | wps 40225.9 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 12.708
2022-03-04 09:45:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 09:45:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:46:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 2 @ 189 updates, score 12.708) (writing took 6.498649777844548 seconds)
2022-03-04 09:46:03 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 09:46:03 | INFO | train | epoch 002 | loss 13.69 | ppl 13217.2 | wps 21989.5 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.49 | loss_scale 8 | train_wall 252 | gb_free 8.2 | wall 597
2022-03-04 09:46:03 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 09:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:46:35 | INFO | train_inner | epoch 003:     11 / 97 loss=13.527, ppl=11802.1, wps=22014.6, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.448, loss_scale=8, train_wall=260, gb_free=8.2, wall=628
2022-03-04 09:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:50:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.027 | ppl 2086.14 | wps 40947.7 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.027
2022-03-04 09:50:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 09:50:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:50:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:50:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.027) (writing took 6.414618914946914 seconds)
2022-03-04 09:50:52 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 09:50:52 | INFO | train | epoch 003 | loss 11.909 | ppl 3844.51 | wps 22012.4 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 1.012 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 885
2022-03-04 09:50:52 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 09:50:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:51:32 | INFO | train_inner | epoch 004:     14 / 97 loss=11.694, ppl=3312.69, wps=22034.6, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=0.946, loss_scale=16, train_wall=260, gb_free=8.2, wall=925
2022-03-04 09:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:55:34 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.207 | ppl 1182.14 | wps 38361.7 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 10.207
2022-03-04 09:55:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 09:55:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:55:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 09:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 4 @ 383 updates, score 10.207) (writing took 41.12409086152911 seconds)
2022-03-04 09:56:15 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 09:56:15 | INFO | train | epoch 004 | loss 10.59 | ppl 1540.9 | wps 19650.4 | ups 0.3 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.59 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 1209
2022-03-04 09:56:15 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 09:56:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:57:03 | INFO | train_inner | epoch 005:     17 / 97 loss=10.463, ppl=1411.6, wps=19761.4, ups=0.3, wpb=65495, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.563, loss_scale=32, train_wall=259, gb_free=8.2, wall=1257
2022-03-04 10:00:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:00:57 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.83 | ppl 910.34 | wps 39474.9 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 9.83
2022-03-04 10:00:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 10:00:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:01:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:01:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 5 @ 480 updates, score 9.83) (writing took 6.614102512598038 seconds)
2022-03-04 10:01:04 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:01:04 | INFO | train | epoch 005 | loss 10.02 | ppl 1038.61 | wps 22023.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.525 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 1497
2022-03-04 10:01:04 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:01:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:02:01 | INFO | train_inner | epoch 006:     20 / 97 loss=9.947, ppl=986.81, wps=21981.3, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.509, loss_scale=32, train_wall=260, gb_free=8.2, wall=1555
2022-03-04 10:03:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:05:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:05:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.54 | ppl 744.68 | wps 40761.1 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 9.54
2022-03-04 10:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 10:05:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:05:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 6 @ 576 updates, score 9.54) (writing took 7.098665311932564 seconds)
2022-03-04 10:05:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 10:05:54 | INFO | train | epoch 006 | loss 9.67 | ppl 814.66 | wps 21645.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.562 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 1788
2022-03-04 10:05:54 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 10:05:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:07:02 | INFO | train_inner | epoch 007:     24 / 97 loss=9.598, ppl=775.17, wps=21781.8, ups=0.33, wpb=65495, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.608, loss_scale=32, train_wall=262, gb_free=8.2, wall=1855
2022-03-04 10:09:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:10:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:10:36 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.29 | ppl 626.19 | wps 39619.3 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 9.29
2022-03-04 10:10:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 10:10:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:10:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:11:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 7 @ 672 updates, score 9.29) (writing took 74.33445734716952 seconds)
2022-03-04 10:11:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 10:11:51 | INFO | train | epoch 007 | loss 9.374 | ppl 663.31 | wps 17641 | ups 0.27 | wpb 65491.1 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.647 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 2144
2022-03-04 10:11:51 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 10:11:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:13:09 | INFO | train_inner | epoch 008:     28 / 97 loss=9.297, ppl=628.86, wps=17815.8, ups=0.27, wpb=65492.9, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.658, loss_scale=32, train_wall=262, gb_free=8.2, wall=2223
2022-03-04 10:16:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:16:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.074 | ppl 539.03 | wps 38123.1 | wpb 510.9 | bsz 1 | num_updates 769 | best_loss 9.074
2022-03-04 10:16:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 769 updates
2022-03-04 10:16:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:16:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 8 @ 769 updates, score 9.074) (writing took 7.08925323933363 seconds)
2022-03-04 10:16:40 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 10:16:40 | INFO | train | epoch 008 | loss 9.105 | ppl 550.49 | wps 21983.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 769 | lr 9.62058e-05 | gnorm 0.738 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 2433
2022-03-04 10:16:40 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 10:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:17:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:18:10 | INFO | train_inner | epoch 009:     32 / 97 loss=9.025, ppl=521.07, wps=21811.1, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.74, loss_scale=32, train_wall=261, gb_free=8.2, wall=2523
2022-03-04 10:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:21:22 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.879 | ppl 470.86 | wps 38370.1 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 8.879
2022-03-04 10:21:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 10:21:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:21:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 9 @ 865 updates, score 8.879) (writing took 6.973674055188894 seconds)
2022-03-04 10:21:29 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 10:21:29 | INFO | train | epoch 009 | loss 8.854 | ppl 462.67 | wps 21706.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.805 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 2723
2022-03-04 10:21:29 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 10:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:23:08 | INFO | train_inner | epoch 010:     35 / 97 loss=8.775, ppl=438.01, wps=21936.2, ups=0.33, wpb=65495, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.878, loss_scale=32, train_wall=260, gb_free=8.2, wall=2822
2022-03-04 10:24:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:26:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:26:12 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.695 | ppl 414.29 | wps 39195.7 | wpb 510.9 | bsz 1 | num_updates 961 | best_loss 8.695
2022-03-04 10:26:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 961 updates
2022-03-04 10:26:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:26:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 10 @ 961 updates, score 8.695) (writing took 7.003568284213543 seconds)
2022-03-04 10:26:19 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 10:26:19 | INFO | train | epoch 010 | loss 8.623 | ppl 394.28 | wps 21697.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 961 | lr 0.000120201 | gnorm 0.9 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 3012
2022-03-04 10:26:19 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 10:26:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:28:09 | INFO | train_inner | epoch 011:     39 / 97 loss=8.537, ppl=371.49, wps=21768.3, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.867, loss_scale=32, train_wall=262, gb_free=8.2, wall=3123
2022-03-04 10:30:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:31:01 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.545 | ppl 373.55 | wps 39825.4 | wpb 510.9 | bsz 1 | num_updates 1057 | best_loss 8.545
2022-03-04 10:31:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1057 updates
2022-03-04 10:31:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:31:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 11 @ 1057 updates, score 8.545) (writing took 34.24065889790654 seconds)
2022-03-04 10:31:35 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 10:31:35 | INFO | train | epoch 011 | loss 8.411 | ppl 340.32 | wps 19876.3 | ups 0.3 | wpb 65491.1 | bsz 127.9 | num_updates 1057 | lr 0.000132199 | gnorm 0.895 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 3329
2022-03-04 10:31:35 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 10:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:33:37 | INFO | train_inner | epoch 012:     43 / 97 loss=8.321, ppl=319.79, wps=19990.4, ups=0.31, wpb=65495, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.922, loss_scale=32, train_wall=262, gb_free=8.2, wall=3450
2022-03-04 10:36:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:36:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.415 | ppl 341.22 | wps 40905.6 | wpb 510.9 | bsz 1 | num_updates 1154 | best_loss 8.415
2022-03-04 10:36:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1154 updates
2022-03-04 10:36:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:36:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 12 @ 1154 updates, score 8.415) (writing took 7.264901407063007 seconds)
2022-03-04 10:36:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 10:36:23 | INFO | train | epoch 012 | loss 8.214 | ppl 296.98 | wps 22059.1 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 1154 | lr 0.000144321 | gnorm 0.928 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 3617
2022-03-04 10:36:23 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 10:36:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:37:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:38:35 | INFO | train_inner | epoch 013:     47 / 97 loss=8.128, ppl=279.7, wps=21970.5, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.939, loss_scale=32, train_wall=260, gb_free=8.2, wall=3748
2022-03-04 10:40:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:41:03 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.315 | ppl 318.36 | wps 41842.9 | wpb 510.9 | bsz 1 | num_updates 1250 | best_loss 8.315
2022-03-04 10:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1250 updates
2022-03-04 10:41:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:41:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 13 @ 1250 updates, score 8.315) (writing took 6.345567844808102 seconds)
2022-03-04 10:41:09 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 10:41:09 | INFO | train | epoch 013 | loss 8.031 | ppl 261.6 | wps 21968.5 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 1250 | lr 0.000156319 | gnorm 0.944 | loss_scale 32 | train_wall 250 | gb_free 8.2 | wall 3903
2022-03-04 10:41:09 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 10:41:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:43:35 | INFO | train_inner | epoch 014:     50 / 97 loss=7.947, ppl=246.68, wps=21847.5, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.971, loss_scale=32, train_wall=262, gb_free=8.2, wall=4048
2022-03-04 10:44:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:45:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:45:59 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.209 | ppl 295.92 | wps 37163.9 | wpb 510.9 | bsz 1 | num_updates 1346 | best_loss 8.209
2022-03-04 10:45:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1346 updates
2022-03-04 10:45:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:46:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:46:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 14 @ 1346 updates, score 8.209) (writing took 6.108758205547929 seconds)
2022-03-04 10:46:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 10:46:05 | INFO | train | epoch 014 | loss 7.858 | ppl 231.98 | wps 21237.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 1346 | lr 0.000168316 | gnorm 0.968 | loss_scale 32 | train_wall 258 | gb_free 8.2 | wall 4199
2022-03-04 10:46:06 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 10:46:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:48:44 | INFO | train_inner | epoch 015:     54 / 97 loss=7.765, ppl=217.49, wps=21172.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=0.962, loss_scale=32, train_wall=270, gb_free=8.2, wall=4357
2022-03-04 10:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:50:57 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.096 | ppl 273.59 | wps 37482 | wpb 510.9 | bsz 1 | num_updates 1443 | best_loss 8.096
2022-03-04 10:50:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1443 updates
2022-03-04 10:50:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:51:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 15 @ 1443 updates, score 8.096) (writing took 6.052793100476265 seconds)
2022-03-04 10:51:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 10:51:03 | INFO | train | epoch 015 | loss 7.687 | ppl 206.01 | wps 21380.9 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 1443 | lr 0.000180439 | gnorm 0.973 | loss_scale 64 | train_wall 259 | gb_free 8.2 | wall 4496
2022-03-04 10:51:03 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 10:51:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:53:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:53:53 | INFO | train_inner | epoch 016:     58 / 97 loss=7.588, ppl=192.43, wps=21164.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=0.965, loss_scale=32, train_wall=270, gb_free=8.2, wall=4667
2022-03-04 10:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:55:54 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.028 | ppl 261.03 | wps 37084.8 | wpb 510.9 | bsz 1 | num_updates 1539 | best_loss 8.028
2022-03-04 10:55:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1539 updates
2022-03-04 10:55:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:55:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 10:56:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 16 @ 1539 updates, score 8.028) (writing took 5.559940882027149 seconds)
2022-03-04 10:56:00 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 10:56:00 | INFO | train | epoch 016 | loss 7.521 | ppl 183.63 | wps 21146.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 1539 | lr 0.000192437 | gnorm 0.994 | loss_scale 32 | train_wall 260 | gb_free 8.2 | wall 4793
2022-03-04 10:56:00 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 10:56:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:58:55 | INFO | train_inner | epoch 017:     61 / 97 loss=7.417, ppl=170.9, wps=21695.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=0.989, loss_scale=32, train_wall=264, gb_free=8.2, wall=4969
2022-03-04 10:59:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:00:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.929 | ppl 243.67 | wps 39454.9 | wpb 510.9 | bsz 1 | num_updates 1635 | best_loss 7.929
2022-03-04 11:00:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1635 updates
2022-03-04 11:00:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:00:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:00:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 17 @ 1635 updates, score 7.929) (writing took 6.5164894592016935 seconds)
2022-03-04 11:00:51 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 11:00:51 | INFO | train | epoch 017 | loss 7.356 | ppl 163.8 | wps 21634.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 1635 | lr 0.000204434 | gnorm 0.981 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 5084
2022-03-04 11:00:51 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 11:00:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:03:56 | INFO | train_inner | epoch 018:     65 / 97 loss=7.254, ppl=152.66, wps=21770.7, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=1.001, loss_scale=32, train_wall=263, gb_free=8.2, wall=5270
2022-03-04 11:05:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:05:33 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.865 | ppl 233.06 | wps 40087.1 | wpb 510.9 | bsz 1 | num_updates 1732 | best_loss 7.865
2022-03-04 11:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1732 updates
2022-03-04 11:05:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:05:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:05:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 18 @ 1732 updates, score 7.865) (writing took 7.075496485456824 seconds)
2022-03-04 11:05:40 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 11:05:40 | INFO | train | epoch 018 | loss 7.197 | ppl 146.77 | wps 21947.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 1732 | lr 0.000216557 | gnorm 0.979 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 5373
2022-03-04 11:05:40 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 11:05:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:06:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:08:56 | INFO | train_inner | epoch 019:     69 / 97 loss=7.088, ppl=136.02, wps=21810.4, ups=0.33, wpb=65495, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.994, loss_scale=16, train_wall=262, gb_free=8.2, wall=5570
2022-03-04 11:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:10:23 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.774 | ppl 218.82 | wps 38854.1 | wpb 510.9 | bsz 1 | num_updates 1828 | best_loss 7.774
2022-03-04 11:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1828 updates
2022-03-04 11:10:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:10:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 19 @ 1828 updates, score 7.774) (writing took 7.178621597588062 seconds)
2022-03-04 11:10:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 11:10:30 | INFO | train | epoch 019 | loss 7.041 | ppl 131.69 | wps 21676 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 1828 | lr 0.000228554 | gnorm 1.003 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 5663
2022-03-04 11:10:30 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 11:10:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:13:54 | INFO | train_inner | epoch 020:     72 / 97 loss=6.932, ppl=122.08, wps=21982, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=0.988, loss_scale=32, train_wall=259, gb_free=8.2, wall=5868
2022-03-04 11:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:15:12 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.721 | ppl 210.97 | wps 38287.6 | wpb 510.9 | bsz 1 | num_updates 1925 | best_loss 7.721
2022-03-04 11:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1925 updates
2022-03-04 11:15:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:15:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 20 @ 1925 updates, score 7.721) (writing took 6.7123581282794476 seconds)
2022-03-04 11:15:19 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 11:15:19 | INFO | train | epoch 020 | loss 6.889 | ppl 118.53 | wps 22004.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 1925 | lr 0.000240677 | gnorm 0.983 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 5952
2022-03-04 11:15:19 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 11:15:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:18:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:18:54 | INFO | train_inner | epoch 021:     76 / 97 loss=6.774, ppl=109.46, wps=21826.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.981, loss_scale=32, train_wall=262, gb_free=8.2, wall=6168
2022-03-04 11:19:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:20:01 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.658 | ppl 201.95 | wps 37658.7 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 7.658
2022-03-04 11:20:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 11:20:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:20:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:20:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 21 @ 2021 updates, score 7.658) (writing took 6.229593180119991 seconds)
2022-03-04 11:20:07 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 11:20:07 | INFO | train | epoch 021 | loss 6.74 | ppl 106.93 | wps 21798.4 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 0.987 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 6241
2022-03-04 11:20:07 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 11:20:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:23:52 | INFO | train_inner | epoch 022:     79 / 97 loss=6.626, ppl=98.79, wps=22006.8, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=0.982, loss_scale=32, train_wall=260, gb_free=8.2, wall=6465
2022-03-04 11:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:24:50 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.634 | ppl 198.61 | wps 38692 | wpb 510.9 | bsz 1 | num_updates 2118 | best_loss 7.634
2022-03-04 11:24:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2118 updates
2022-03-04 11:24:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:24:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 22 @ 2118 updates, score 7.634) (writing took 5.9109071008861065 seconds)
2022-03-04 11:24:56 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 11:24:56 | INFO | train | epoch 022 | loss 6.595 | ppl 96.69 | wps 22025.8 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 2118 | lr 0.000264797 | gnorm 0.975 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 6529
2022-03-04 11:24:56 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 11:24:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:25:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:28:51 | INFO | train_inner | epoch 023:     83 / 97 loss=6.48, ppl=89.26, wps=21875.3, ups=0.33, wpb=65495, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=0.971, loss_scale=32, train_wall=262, gb_free=8.2, wall=6765
2022-03-04 11:29:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:29:37 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.623 | ppl 197.09 | wps 38008.1 | wpb 510.9 | bsz 1 | num_updates 2214 | best_loss 7.623
2022-03-04 11:29:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2214 updates
2022-03-04 11:29:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:29:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt
2022-03-04 11:29:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_best.pt (epoch 23 @ 2214 updates, score 7.623) (writing took 6.460241531953216 seconds)
2022-03-04 11:29:44 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 11:29:44 | INFO | train | epoch 023 | loss 6.455 | ppl 87.73 | wps 21809.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2214 | lr 0.000276795 | gnorm 0.978 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 6817
2022-03-04 11:29:44 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 11:29:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:32:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:33:52 | INFO | train_inner | epoch 024:     87 / 97 loss=6.335, ppl=80.76, wps=21790, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.989, loss_scale=32, train_wall=262, gb_free=8.2, wall=7065
2022-03-04 11:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:34:27 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.648 | ppl 200.62 | wps 37215.4 | wpb 510.9 | bsz 1 | num_updates 2310 | best_loss 7.623
2022-03-04 11:34:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2310 updates
2022-03-04 11:34:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:34:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 24 @ 2310 updates, score 7.648) (writing took 3.7128143664449453 seconds)
2022-03-04 11:34:31 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 11:34:31 | INFO | train | epoch 024 | loss 6.322 | ppl 80.01 | wps 21899.6 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2310 | lr 0.000288792 | gnorm 1.007 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 7104
2022-03-04 11:34:31 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 11:34:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:38:50 | INFO | train_inner | epoch 025:     90 / 97 loss=6.205, ppl=73.79, wps=21947.7, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=1.004, loss_scale=32, train_wall=263, gb_free=8.2, wall=7364
2022-03-04 11:38:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:39:17 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.643 | ppl 199.93 | wps 37639.2 | wpb 510.9 | bsz 1 | num_updates 2406 | best_loss 7.623
2022-03-04 11:39:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2406 updates
2022-03-04 11:39:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:39:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 25 @ 2406 updates, score 7.643) (writing took 3.3794218450784683 seconds)
2022-03-04 11:39:20 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 11:39:20 | INFO | train | epoch 025 | loss 6.189 | ppl 72.98 | wps 21752.9 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2406 | lr 0.00030079 | gnorm 0.987 | loss_scale 32 | train_wall 255 | gb_free 8.2 | wall 7393
2022-03-04 11:39:20 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 11:39:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:43:53 | INFO | train_inner | epoch 026:     94 / 97 loss=6.071, ppl=67.21, wps=21643.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=1.004, loss_scale=32, train_wall=267, gb_free=8.2, wall=7666
2022-03-04 11:44:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:44:08 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.715 | ppl 210.13 | wps 37358.8 | wpb 510.9 | bsz 1 | num_updates 2503 | best_loss 7.623
2022-03-04 11:44:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2503 updates
2022-03-04 11:44:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:44:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:44:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 26 @ 2503 updates, score 7.715) (writing took 3.52028613910079 seconds)
2022-03-04 11:44:11 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 11:44:11 | INFO | train | epoch 026 | loss 6.063 | ppl 66.85 | wps 21821.8 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 2503 | lr 0.000312912 | gnorm 1.004 | loss_scale 32 | train_wall 256 | gb_free 8.2 | wall 7684
2022-03-04 11:44:11 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 11:44:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:45:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:48:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:48:58 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.794 | ppl 221.97 | wps 37476.1 | wpb 510.9 | bsz 1 | num_updates 2599 | best_loss 7.623
2022-03-04 11:48:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2599 updates
2022-03-04 11:48:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:49:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 27 @ 2599 updates, score 7.794) (writing took 3.6239502523094416 seconds)
2022-03-04 11:49:02 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 11:49:02 | INFO | train | epoch 027 | loss 5.937 | ppl 61.27 | wps 21604.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2599 | lr 0.00032491 | gnorm 1.013 | loss_scale 32 | train_wall 256 | gb_free 8.2 | wall 7975
2022-03-04 11:49:02 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 11:49:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:49:05 | INFO | train_inner | epoch 028:      1 / 97 loss=5.94, ppl=61.4, wps=20971, ups=0.32, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=1.013, loss_scale=32, train_wall=267, gb_free=8.2, wall=7979
2022-03-04 11:52:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:53:50 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.788 | ppl 220.95 | wps 37309.1 | wpb 510.9 | bsz 1 | num_updates 2695 | best_loss 7.623
2022-03-04 11:53:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2695 updates
2022-03-04 11:53:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:53:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:53:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 28 @ 2695 updates, score 7.788) (writing took 3.5649877302348614 seconds)
2022-03-04 11:53:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 11:53:53 | INFO | train | epoch 028 | loss 5.813 | ppl 56.2 | wps 21581.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2695 | lr 0.000336908 | gnorm 0.997 | loss_scale 32 | train_wall 257 | gb_free 8.2 | wall 8267
2022-03-04 11:53:53 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 11:53:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:54:08 | INFO | train_inner | epoch 029:      5 / 97 loss=5.807, ppl=55.99, wps=21607.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=1.007, loss_scale=32, train_wall=267, gb_free=8.2, wall=8282
2022-03-04 11:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:58:40 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.926 | ppl 243.27 | wps 37924.2 | wpb 510.9 | bsz 1 | num_updates 2792 | best_loss 7.623
2022-03-04 11:58:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2792 updates
2022-03-04 11:58:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:58:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 11:58:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 29 @ 2792 updates, score 7.926) (writing took 3.7993371579796076 seconds)
2022-03-04 11:58:43 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 11:58:43 | INFO | train | epoch 029 | loss 5.693 | ppl 51.74 | wps 21901.5 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 2792 | lr 0.00034903 | gnorm 1.017 | loss_scale 32 | train_wall 255 | gb_free 8.2 | wall 8557
2022-03-04 11:58:44 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 11:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:59:07 | INFO | train_inner | epoch 030:      8 / 97 loss=5.679, ppl=51.23, wps=21926.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=1.007, loss_scale=64, train_wall=263, gb_free=8.2, wall=8580
2022-03-04 11:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:59:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:03:31 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.991 | ppl 254.34 | wps 38262.5 | wpb 510.9 | bsz 1 | num_updates 2887 | best_loss 7.623
2022-03-04 12:03:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2887 updates
2022-03-04 12:03:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:03:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:03:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 30 @ 2887 updates, score 7.991) (writing took 4.244636250659823 seconds)
2022-03-04 12:03:35 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 12:03:35 | INFO | train | epoch 030 | loss 5.574 | ppl 47.63 | wps 21325.9 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 2887 | lr 0.000360903 | gnorm 1.024 | loss_scale 16 | train_wall 257 | gb_free 8.2 | wall 8849
2022-03-04 12:03:35 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 12:03:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:04:13 | INFO | train_inner | epoch 031:     13 / 97 loss=5.559, ppl=47.13, wps=21380.7, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=1.016, loss_scale=16, train_wall=270, gb_free=8.2, wall=8887
2022-03-04 12:08:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:08:22 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.078 | ppl 270.18 | wps 38188.6 | wpb 510.9 | bsz 1 | num_updates 2984 | best_loss 7.623
2022-03-04 12:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2984 updates
2022-03-04 12:08:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:08:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 31 @ 2984 updates, score 8.078) (writing took 3.8168560042977333 seconds)
2022-03-04 12:08:26 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 12:08:26 | INFO | train | epoch 031 | loss 5.461 | ppl 44.05 | wps 21828.7 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 2984 | lr 0.000373025 | gnorm 1.038 | loss_scale 32 | train_wall 256 | gb_free 8.2 | wall 9140
2022-03-04 12:08:26 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 12:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:09:13 | INFO | train_inner | epoch 032:     16 / 97 loss=5.439, ppl=43.39, wps=21839.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=1.057, loss_scale=32, train_wall=264, gb_free=8.2, wall=9187
2022-03-04 12:12:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:13:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:13:14 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.128 | ppl 279.69 | wps 38730.5 | wpb 510.9 | bsz 1 | num_updates 3080 | best_loss 7.623
2022-03-04 12:13:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3080 updates
2022-03-04 12:13:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:13:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:13:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 32 @ 3080 updates, score 8.128) (writing took 4.06239540874958 seconds)
2022-03-04 12:13:18 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 12:13:18 | INFO | train | epoch 032 | loss 5.343 | ppl 40.6 | wps 21581.7 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3080 | lr 0.000385023 | gnorm 1.056 | loss_scale 32 | train_wall 256 | gb_free 8.2 | wall 9431
2022-03-04 12:13:18 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 12:13:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:14:16 | INFO | train_inner | epoch 033:     20 / 97 loss=5.322, ppl=40, wps=21612.7, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=1.057, loss_scale=32, train_wall=267, gb_free=8.2, wall=9490
2022-03-04 12:17:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:18:02 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.19 | ppl 292.12 | wps 40403.9 | wpb 510.9 | bsz 1 | num_updates 3177 | best_loss 7.623
2022-03-04 12:18:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3177 updates
2022-03-04 12:18:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:18:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:18:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 33 @ 3177 updates, score 8.19) (writing took 3.989822644740343 seconds)
2022-03-04 12:18:06 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 12:18:06 | INFO | train | epoch 033 | loss 5.233 | ppl 37.62 | wps 22029.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 3177 | lr 0.000397146 | gnorm 1.051 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 9719
2022-03-04 12:18:06 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 12:18:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:19:12 | INFO | train_inner | epoch 034:     23 / 97 loss=5.205, ppl=36.9, wps=22156.7, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=1.041, loss_scale=64, train_wall=261, gb_free=8.2, wall=9785
2022-03-04 12:19:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:22:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:22:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:22:51 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.247 | ppl 303.75 | wps 39465 | wpb 510.9 | bsz 1 | num_updates 3272 | best_loss 7.623
2022-03-04 12:22:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3272 updates
2022-03-04 12:22:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:22:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:22:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 34 @ 3272 updates, score 8.247) (writing took 3.515412025153637 seconds)
2022-03-04 12:22:54 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 12:22:54 | INFO | train | epoch 034 | loss 5.117 | ppl 34.69 | wps 21591.3 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 3272 | lr 0.000409018 | gnorm 1.074 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 10007
2022-03-04 12:22:54 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 12:22:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:24:14 | INFO | train_inner | epoch 035:     28 / 97 loss=5.082, ppl=33.86, wps=21635.9, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=1.093, loss_scale=16, train_wall=267, gb_free=8.2, wall=10088
2022-03-04 12:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:27:41 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.459 | ppl 351.79 | wps 38450.6 | wpb 510.9 | bsz 1 | num_updates 3369 | best_loss 7.623
2022-03-04 12:27:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3369 updates
2022-03-04 12:27:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:27:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:27:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 35 @ 3369 updates, score 8.459) (writing took 3.315582660958171 seconds)
2022-03-04 12:27:44 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 12:27:44 | INFO | train | epoch 035 | loss 5.01 | ppl 32.22 | wps 21916.2 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 3369 | lr 0.000421141 | gnorm 1.089 | loss_scale 16 | train_wall 256 | gb_free 8.2 | wall 10297
2022-03-04 12:27:44 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 12:27:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:29:14 | INFO | train_inner | epoch 036:     31 / 97 loss=4.973, ppl=31.41, wps=21839, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=1.096, loss_scale=32, train_wall=265, gb_free=8.2, wall=10388
2022-03-04 12:32:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:32:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:32:32 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.409 | ppl 339.96 | wps 38025.3 | wpb 510.9 | bsz 1 | num_updates 3465 | best_loss 7.623
2022-03-04 12:32:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3465 updates
2022-03-04 12:32:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:32:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:32:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 36 @ 3465 updates, score 8.409) (writing took 3.2833178602159023 seconds)
2022-03-04 12:32:35 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 12:32:35 | INFO | train | epoch 036 | loss 4.903 | ppl 29.91 | wps 21606.7 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3465 | lr 0.000433138 | gnorm 1.135 | loss_scale 16 | train_wall 257 | gb_free 8.2 | wall 10588
2022-03-04 12:32:35 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 12:32:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:34:16 | INFO | train_inner | epoch 037:     35 / 97 loss=4.86, ppl=29.05, wps=21745.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=1.109, loss_scale=16, train_wall=266, gb_free=8.2, wall=10689
2022-03-04 12:37:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:37:18 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.53 | ppl 369.65 | wps 38661.6 | wpb 510.9 | bsz 1 | num_updates 3562 | best_loss 7.623
2022-03-04 12:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3562 updates
2022-03-04 12:37:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:37:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:37:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 37 @ 3562 updates, score 8.53) (writing took 3.139144817367196 seconds)
2022-03-04 12:37:21 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 12:37:21 | INFO | train | epoch 037 | loss 4.795 | ppl 27.77 | wps 22212 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 3562 | lr 0.000445261 | gnorm 1.095 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 10874
2022-03-04 12:37:21 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 12:37:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:39:10 | INFO | train_inner | epoch 038:     38 / 97 loss=4.759, ppl=27.08, wps=22214.7, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=1.108, loss_scale=32, train_wall=260, gb_free=8.2, wall=10984
2022-03-04 12:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:41:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:42:03 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.593 | ppl 386.27 | wps 40316.5 | wpb 510.9 | bsz 1 | num_updates 3658 | best_loss 7.623
2022-03-04 12:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3658 updates
2022-03-04 12:42:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:42:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:42:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 38 @ 3658 updates, score 8.593) (writing took 4.411914935335517 seconds)
2022-03-04 12:42:07 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 12:42:07 | INFO | train | epoch 038 | loss 4.688 | ppl 25.78 | wps 21951.8 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 3658 | lr 0.000457259 | gnorm 1.126 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 11161
2022-03-04 12:42:07 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 12:42:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:44:07 | INFO | train_inner | epoch 039:     42 / 97 loss=4.648, ppl=25.07, wps=22047.2, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.16, loss_scale=16, train_wall=261, gb_free=8.2, wall=11281
2022-03-04 12:46:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:46:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.8 | ppl 445.81 | wps 38866.8 | wpb 510.9 | bsz 1 | num_updates 3755 | best_loss 7.623
2022-03-04 12:46:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3755 updates
2022-03-04 12:46:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:46:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:46:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 39 @ 3755 updates, score 8.8) (writing took 4.0291213393211365 seconds)
2022-03-04 12:46:54 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 12:46:54 | INFO | train | epoch 039 | loss 4.587 | ppl 24.04 | wps 22177.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 3755 | lr 0.000469381 | gnorm 1.139 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 11447
2022-03-04 12:46:54 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 12:46:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:47:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:49:05 | INFO | train_inner | epoch 040:     46 / 97 loss=4.535, ppl=23.19, wps=22042.2, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.124, loss_scale=16, train_wall=261, gb_free=8.2, wall=11578
2022-03-04 12:51:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:51:37 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.748 | ppl 429.8 | wps 38524.2 | wpb 510.9 | bsz 1 | num_updates 3851 | best_loss 7.623
2022-03-04 12:51:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3851 updates
2022-03-04 12:51:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:51:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:51:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 40 @ 3851 updates, score 8.748) (writing took 4.364110469818115 seconds)
2022-03-04 12:51:41 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 12:51:41 | INFO | train | epoch 040 | loss 4.478 | ppl 22.29 | wps 21896.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3851 | lr 0.000481379 | gnorm 1.124 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 11734
2022-03-04 12:51:41 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 12:51:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:54:02 | INFO | train_inner | epoch 041:     49 / 97 loss=4.431, ppl=21.58, wps=21998.6, ups=0.34, wpb=65495, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.15, loss_scale=32, train_wall=262, gb_free=8.2, wall=11876
2022-03-04 12:55:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:56:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:56:25 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.95 | ppl 494.39 | wps 38994.6 | wpb 510.9 | bsz 1 | num_updates 3947 | best_loss 7.623
2022-03-04 12:56:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3947 updates
2022-03-04 12:56:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:56:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 12:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 41 @ 3947 updates, score 8.95) (writing took 4.2448029685765505 seconds)
2022-03-04 12:56:29 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 12:56:29 | INFO | train | epoch 041 | loss 4.38 | ppl 20.83 | wps 21791.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3947 | lr 0.000493376 | gnorm 1.179 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 12023
2022-03-04 12:56:29 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 12:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:59:01 | INFO | train_inner | epoch 042:     53 / 97 loss=4.322, ppl=20, wps=21928.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.151, loss_scale=16, train_wall=263, gb_free=8.2, wall=12174
2022-03-04 13:01:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:01:12 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.007 | ppl 514.56 | wps 39458.5 | wpb 510.9 | bsz 1 | num_updates 4044 | best_loss 7.623
2022-03-04 13:01:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4044 updates
2022-03-04 13:01:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:01:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:01:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 42 @ 4044 updates, score 9.007) (writing took 4.5281186662614346 seconds)
2022-03-04 13:01:17 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 13:01:17 | INFO | train | epoch 042 | loss 4.283 | ppl 19.46 | wps 22125.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4044 | lr 0.000497272 | gnorm 1.182 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 12310
2022-03-04 13:01:17 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 13:01:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:02:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:04:00 | INFO | train_inner | epoch 043:     57 / 97 loss=4.226, ppl=18.71, wps=21934.2, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.203, loss_scale=16, train_wall=262, gb_free=8.2, wall=12473
2022-03-04 13:05:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:05:59 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.148 | ppl 567.36 | wps 40816.4 | wpb 510.9 | bsz 1 | num_updates 4140 | best_loss 7.623
2022-03-04 13:05:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4140 updates
2022-03-04 13:05:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:06:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 43 @ 4140 updates, score 9.148) (writing took 4.243679702281952 seconds)
2022-03-04 13:06:03 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 13:06:03 | INFO | train | epoch 043 | loss 4.17 | ppl 18 | wps 21921.3 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 4140 | lr 0.000491473 | gnorm 1.123 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 12597
2022-03-04 13:06:03 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 13:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:08:54 | INFO | train_inner | epoch 044:     60 / 97 loss=4.108, ppl=17.24, wps=22254.6, ups=0.34, wpb=65495, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.101, loss_scale=32, train_wall=259, gb_free=8.2, wall=12767
2022-03-04 13:10:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:10:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:10:45 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.095 | ppl 546.98 | wps 40086.1 | wpb 510.9 | bsz 1 | num_updates 4236 | best_loss 7.623
2022-03-04 13:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4236 updates
2022-03-04 13:10:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:10:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:10:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 44 @ 4236 updates, score 9.095) (writing took 4.2318423967808485 seconds)
2022-03-04 13:10:50 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 13:10:50 | INFO | train | epoch 044 | loss 4.07 | ppl 16.8 | wps 21961.5 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 4236 | lr 0.000485872 | gnorm 1.143 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 12883
2022-03-04 13:10:50 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 13:10:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:13:51 | INFO | train_inner | epoch 045:     64 / 97 loss=4.002, ppl=16.03, wps=22050.4, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=1.147, loss_scale=16, train_wall=261, gb_free=8.2, wall=13064
2022-03-04 13:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:15:32 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.351 | ppl 652.86 | wps 39468.4 | wpb 510.9 | bsz 1 | num_updates 4333 | best_loss 7.623
2022-03-04 13:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4333 updates
2022-03-04 13:15:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 45 @ 4333 updates, score 9.351) (writing took 4.29298597574234 seconds)
2022-03-04 13:15:36 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 13:15:36 | INFO | train | epoch 045 | loss 3.967 | ppl 15.64 | wps 22201.1 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4333 | lr 0.000480403 | gnorm 1.12 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 13169
2022-03-04 13:15:36 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 13:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:18:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:18:49 | INFO | train_inner | epoch 046:     68 / 97 loss=3.908, ppl=15.01, wps=21988, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.133, loss_scale=16, train_wall=262, gb_free=8.2, wall=13362
2022-03-04 13:20:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:20:18 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.265 | ppl 615.25 | wps 39716.6 | wpb 510.9 | bsz 1 | num_updates 4429 | best_loss 7.623
2022-03-04 13:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4429 updates
2022-03-04 13:20:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:20:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:20:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 46 @ 4429 updates, score 9.265) (writing took 3.525334833189845 seconds)
2022-03-04 13:20:21 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 13:20:21 | INFO | train | epoch 046 | loss 3.871 | ppl 14.63 | wps 22009.4 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 4429 | lr 0.000475168 | gnorm 1.164 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 13455
2022-03-04 13:20:21 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 13:20:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:23:43 | INFO | train_inner | epoch 047:     71 / 97 loss=3.802, ppl=13.95, wps=22261.8, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=1.119, loss_scale=16, train_wall=259, gb_free=8.2, wall=13656
2022-03-04 13:24:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:25:03 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.542 | ppl 745.34 | wps 38629.2 | wpb 510.9 | bsz 1 | num_updates 4526 | best_loss 7.623
2022-03-04 13:25:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4526 updates
2022-03-04 13:25:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:25:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:25:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 47 @ 4526 updates, score 9.542) (writing took 3.138317111879587 seconds)
2022-03-04 13:25:07 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 13:25:07 | INFO | train | epoch 047 | loss 3.775 | ppl 13.69 | wps 22284.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4526 | lr 0.000470049 | gnorm 1.111 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 13740
2022-03-04 13:25:07 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 13:25:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:28:37 | INFO | train_inner | epoch 048:     74 / 97 loss=3.706, ppl=13.05, wps=22259.7, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.127, loss_scale=32, train_wall=260, gb_free=8.2, wall=13950
2022-03-04 13:29:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:29:50 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.667 | ppl 813.1 | wps 37009.3 | wpb 510.9 | bsz 1 | num_updates 4623 | best_loss 7.623
2022-03-04 13:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4623 updates
2022-03-04 13:29:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:29:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:29:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 48 @ 4623 updates, score 9.667) (writing took 4.550445768982172 seconds)
2022-03-04 13:29:54 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 13:29:54 | INFO | train | epoch 048 | loss 3.684 | ppl 12.85 | wps 22076.8 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4623 | lr 0.000465091 | gnorm 1.132 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 14028
2022-03-04 13:29:54 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 13:29:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:31:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:32:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:33:40 | INFO | train_inner | epoch 049:     79 / 97 loss=3.615, ppl=12.25, wps=21591.9, ups=0.33, wpb=65495, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=1.126, loss_scale=16, train_wall=266, gb_free=8.2, wall=14254
2022-03-04 13:34:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:34:38 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.832 | ppl 911.58 | wps 38882.8 | wpb 510.9 | bsz 1 | num_updates 4718 | best_loss 7.623
2022-03-04 13:34:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4718 updates
2022-03-04 13:34:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 49 @ 4718 updates, score 9.832) (writing took 3.284442724660039 seconds)
2022-03-04 13:34:41 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 13:34:41 | INFO | train | epoch 049 | loss 3.592 | ppl 12.06 | wps 21684.7 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 4718 | lr 0.000460385 | gnorm 1.116 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 14315
2022-03-04 13:34:41 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 13:34:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:38:34 | INFO | train_inner | epoch 050:     82 / 97 loss=3.534, ppl=11.58, wps=22270.5, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=1.133, loss_scale=32, train_wall=259, gb_free=8.2, wall=14548
2022-03-04 13:39:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:39:24 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.79 | ppl 885.06 | wps 38295.7 | wpb 510.9 | bsz 1 | num_updates 4815 | best_loss 7.623
2022-03-04 13:39:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4815 updates
2022-03-04 13:39:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:39:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:39:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 50 @ 4815 updates, score 9.79) (writing took 3.2543820589780807 seconds)
2022-03-04 13:39:27 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 13:39:27 | INFO | train | epoch 050 | loss 3.514 | ppl 11.42 | wps 22238.5 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4815 | lr 0.000455724 | gnorm 1.139 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 14600
2022-03-04 13:39:27 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 13:39:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:41:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:43:33 | INFO | train_inner | epoch 051:     86 / 97 loss=3.446, ppl=10.9, wps=21975.7, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.128, loss_scale=16, train_wall=263, gb_free=8.2, wall=14846
2022-03-04 13:44:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:44:10 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.922 | ppl 969.99 | wps 38525.3 | wpb 510.9 | bsz 1 | num_updates 4911 | best_loss 7.623
2022-03-04 13:44:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4911 updates
2022-03-04 13:44:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:44:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:44:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 51 @ 4911 updates, score 9.922) (writing took 4.463075650855899 seconds)
2022-03-04 13:44:14 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 13:44:14 | INFO | train | epoch 051 | loss 3.43 | ppl 10.78 | wps 21873.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 4911 | lr 0.000451248 | gnorm 1.125 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 14888
2022-03-04 13:44:14 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 13:44:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:48:28 | INFO | train_inner | epoch 052:     89 / 97 loss=3.359, ppl=10.26, wps=22145.1, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1.12, loss_scale=32, train_wall=260, gb_free=8.2, wall=15142
2022-03-04 13:48:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:48:57 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.977 | ppl 1007.48 | wps 38171.6 | wpb 510.9 | bsz 1 | num_updates 5008 | best_loss 7.623
2022-03-04 13:48:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5008 updates
2022-03-04 13:48:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:49:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:49:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 52 @ 5008 updates, score 9.977) (writing took 3.5605720225721598 seconds)
2022-03-04 13:49:01 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 13:49:01 | INFO | train | epoch 052 | loss 3.35 | ppl 10.2 | wps 22192.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5008 | lr 0.000446856 | gnorm 1.116 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 15174
2022-03-04 13:49:01 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 13:49:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:53:22 | INFO | train_inner | epoch 053:     92 / 97 loss=3.286, ppl=9.75, wps=22294.2, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=1.111, loss_scale=32, train_wall=259, gb_free=8.2, wall=15435
2022-03-04 13:53:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:53:42 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 10.061 | ppl 1068.26 | wps 38990 | wpb 510.9 | bsz 1 | num_updates 5105 | best_loss 7.623
2022-03-04 13:53:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5105 updates
2022-03-04 13:53:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:53:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:53:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 53 @ 5105 updates, score 10.061) (writing took 3.8020155020058155 seconds)
2022-03-04 13:53:46 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 13:53:46 | INFO | train | epoch 053 | loss 3.275 | ppl 9.68 | wps 22285.6 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5105 | lr 0.000442591 | gnorm 1.12 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 15459
2022-03-04 13:53:46 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 13:53:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:54:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:55:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:58:20 | INFO | train_inner | epoch 054:     97 / 97 loss=3.205, ppl=9.22, wps=21973.6, ups=0.34, wpb=65451.9, bsz=127.8, num_updates=5200, lr=0.000438529, gnorm=1.126, loss_scale=16, train_wall=262, gb_free=8.2, wall=15733
2022-03-04 13:58:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:58:26 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 10.207 | ppl 1181.62 | wps 38390.8 | wpb 510.9 | bsz 1 | num_updates 5200 | best_loss 7.623
2022-03-04 13:58:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5200 updates
2022-03-04 13:58:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:58:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 13:58:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 54 @ 5200 updates, score 10.207) (writing took 3.2987912744283676 seconds)
2022-03-04 13:58:29 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 13:58:29 | INFO | train | epoch 054 | loss 3.198 | ppl 9.17 | wps 21960.1 | ups 0.34 | wpb 65490.6 | bsz 127.9 | num_updates 5200 | lr 0.000438529 | gnorm 1.118 | loss_scale 16 | train_wall 250 | gb_free 8.2 | wall 15742
2022-03-04 13:58:29 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 13:58:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:03:09 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 10.217 | ppl 1190.41 | wps 38389.3 | wpb 510.9 | bsz 1 | num_updates 5297 | best_loss 7.623
2022-03-04 14:03:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5297 updates
2022-03-04 14:03:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:03:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:03:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 55 @ 5297 updates, score 10.217) (writing took 3.3805313017219305 seconds)
2022-03-04 14:03:13 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 14:03:13 | INFO | train | epoch 055 | loss 3.131 | ppl 8.76 | wps 22382.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5297 | lr 0.000434495 | gnorm 1.118 | loss_scale 32 | train_wall 250 | gb_free 8.2 | wall 16026
2022-03-04 14:03:13 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 14:03:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:03:21 | INFO | train_inner | epoch 056:      3 / 97 loss=3.128, ppl=8.74, wps=21720.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=5300, lr=0.000434372, gnorm=1.121, loss_scale=32, train_wall=258, gb_free=8.2, wall=16035
2022-03-04 14:07:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:07:56 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 10.308 | ppl 1267.52 | wps 38817 | wpb 510.9 | bsz 1 | num_updates 5394 | best_loss 7.623
2022-03-04 14:07:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5394 updates
2022-03-04 14:07:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 56 @ 5394 updates, score 10.308) (writing took 3.2122400272637606 seconds)
2022-03-04 14:07:59 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 14:07:59 | INFO | train | epoch 056 | loss 3.062 | ppl 8.35 | wps 22158.8 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5394 | lr 0.000430571 | gnorm 1.135 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 16313
2022-03-04 14:07:59 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 14:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:08:17 | INFO | train_inner | epoch 057:      6 / 97 loss=3.055, ppl=8.31, wps=22185.2, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=1.135, loss_scale=64, train_wall=260, gb_free=8.2, wall=16330
2022-03-04 14:08:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:12:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:12:43 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 10.477 | ppl 1425.04 | wps 38970.8 | wpb 510.9 | bsz 1 | num_updates 5490 | best_loss 7.623
2022-03-04 14:12:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5490 updates
2022-03-04 14:12:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:12:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 57 @ 5490 updates, score 10.477) (writing took 3.2314648404717445 seconds)
2022-03-04 14:12:46 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 14:12:46 | INFO | train | epoch 057 | loss 2.994 | ppl 7.97 | wps 21955.1 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 1.122 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 16599
2022-03-04 14:12:46 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 14:12:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:13:14 | INFO | train_inner | epoch 058:     10 / 97 loss=2.984, ppl=7.91, wps=22014.5, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.12, loss_scale=32, train_wall=262, gb_free=8.2, wall=16628
2022-03-04 14:14:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:15:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:17:28 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 10.642 | ppl 1598.02 | wps 39743.3 | wpb 510.9 | bsz 1 | num_updates 5585 | best_loss 7.623
2022-03-04 14:17:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5585 updates
2022-03-04 14:17:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:17:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:17:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 58 @ 5585 updates, score 10.642) (writing took 4.48306099139154 seconds)
2022-03-04 14:17:33 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 14:17:33 | INFO | train | epoch 058 | loss 2.932 | ppl 7.63 | wps 21698.2 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 5585 | lr 0.000423144 | gnorm 1.124 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 16886
2022-03-04 14:17:33 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 14:17:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:18:15 | INFO | train_inner | epoch 059:     15 / 97 loss=2.917, ppl=7.55, wps=21759.1, ups=0.33, wpb=65495, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.115, loss_scale=16, train_wall=265, gb_free=8.2, wall=16929
2022-03-04 14:22:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:22:12 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 10.604 | ppl 1556.64 | wps 38491.4 | wpb 510.9 | bsz 1 | num_updates 5682 | best_loss 7.623
2022-03-04 14:22:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5682 updates
2022-03-04 14:22:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:22:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:22:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 59 @ 5682 updates, score 10.604) (writing took 4.162056367844343 seconds)
2022-03-04 14:22:16 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 14:22:16 | INFO | train | epoch 059 | loss 2.872 | ppl 7.32 | wps 22373.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5682 | lr 0.000419517 | gnorm 1.115 | loss_scale 32 | train_wall 249 | gb_free 8.2 | wall 17170
2022-03-04 14:22:16 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 14:22:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:23:09 | INFO | train_inner | epoch 060:     18 / 97 loss=2.859, ppl=7.26, wps=22317.2, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.125, loss_scale=32, train_wall=258, gb_free=8.2, wall=17222
2022-03-04 14:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:26:58 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 10.789 | ppl 1769.41 | wps 38775.5 | wpb 510.9 | bsz 1 | num_updates 5779 | best_loss 7.623
2022-03-04 14:26:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5779 updates
2022-03-04 14:26:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:27:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:27:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 60 @ 5779 updates, score 10.789) (writing took 4.124685168266296 seconds)
2022-03-04 14:27:02 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 14:27:02 | INFO | train | epoch 060 | loss 2.811 | ppl 7.02 | wps 22220.6 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5779 | lr 0.000415981 | gnorm 1.12 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 17456
2022-03-04 14:27:02 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 14:27:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:28:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:28:06 | INFO | train_inner | epoch 061:     22 / 97 loss=2.799, ppl=6.96, wps=22030.7, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.114, loss_scale=16, train_wall=261, gb_free=8.2, wall=17519
2022-03-04 14:31:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:31:45 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 10.891 | ppl 1898.71 | wps 36824 | wpb 510.9 | bsz 1 | num_updates 5875 | best_loss 7.623
2022-03-04 14:31:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5875 updates
2022-03-04 14:31:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 61 @ 5875 updates, score 10.891) (writing took 4.093136744573712 seconds)
2022-03-04 14:31:49 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 14:31:49 | INFO | train | epoch 061 | loss 2.753 | ppl 6.74 | wps 21912.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 5875 | lr 0.000412568 | gnorm 1.13 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 17743
2022-03-04 14:31:49 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 14:31:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:33:02 | INFO | train_inner | epoch 062:     25 / 97 loss=2.738, ppl=6.67, wps=22142.4, ups=0.34, wpb=65495, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.129, loss_scale=16, train_wall=260, gb_free=8.2, wall=17815
2022-03-04 14:34:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:36:32 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 10.87 | ppl 1871.59 | wps 38218 | wpb 510.9 | bsz 1 | num_updates 5971 | best_loss 7.623
2022-03-04 14:36:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5971 updates
2022-03-04 14:36:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:36:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:36:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 62 @ 5971 updates, score 10.87) (writing took 3.1930512990802526 seconds)
2022-03-04 14:36:35 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 14:36:35 | INFO | train | epoch 062 | loss 2.697 | ppl 6.48 | wps 21993.8 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 5971 | lr 0.000409238 | gnorm 1.126 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 18029
2022-03-04 14:36:35 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 14:36:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:37:59 | INFO | train_inner | epoch 063:     29 / 97 loss=2.679, ppl=6.4, wps=22042.3, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.133, loss_scale=16, train_wall=262, gb_free=8.2, wall=18112
2022-03-04 14:41:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:41:16 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 11.054 | ppl 2125.49 | wps 40080.9 | wpb 510.9 | bsz 1 | num_updates 6068 | best_loss 7.623
2022-03-04 14:41:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6068 updates
2022-03-04 14:41:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:41:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 63 @ 6068 updates, score 11.054) (writing took 3.4147351253777742 seconds)
2022-03-04 14:41:20 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 14:41:20 | INFO | train | epoch 063 | loss 2.646 | ppl 6.26 | wps 22311.8 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6068 | lr 0.000405954 | gnorm 1.133 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 18313
2022-03-04 14:41:20 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 14:41:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:42:53 | INFO | train_inner | epoch 064:     32 / 97 loss=2.628, ppl=6.18, wps=22285.6, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.121, loss_scale=32, train_wall=259, gb_free=8.2, wall=18406
2022-03-04 14:45:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:46:04 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 11.236 | ppl 2412 | wps 39248.4 | wpb 510.9 | bsz 1 | num_updates 6164 | best_loss 7.623
2022-03-04 14:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6164 updates
2022-03-04 14:46:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:46:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 64 @ 6164 updates, score 11.236) (writing took 4.138813903555274 seconds)
2022-03-04 14:46:08 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 14:46:08 | INFO | train | epoch 064 | loss 2.592 | ppl 6.03 | wps 21842.6 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 6164 | lr 0.000402781 | gnorm 1.129 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 18601
2022-03-04 14:46:08 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 14:46:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:47:51 | INFO | train_inner | epoch 065:     36 / 97 loss=2.572, ppl=5.95, wps=21921.3, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.133, loss_scale=16, train_wall=263, gb_free=8.2, wall=18705
2022-03-04 14:50:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:50:50 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 11.19 | ppl 2336.54 | wps 39782.5 | wpb 510.9 | bsz 1 | num_updates 6261 | best_loss 7.623
2022-03-04 14:50:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6261 updates
2022-03-04 14:50:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:50:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 65 @ 6261 updates, score 11.19) (writing took 4.255730984732509 seconds)
2022-03-04 14:50:54 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 14:50:54 | INFO | train | epoch 065 | loss 2.543 | ppl 5.83 | wps 22173.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6261 | lr 0.000399648 | gnorm 1.127 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 18888
2022-03-04 14:50:54 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 14:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:52:47 | INFO | train_inner | epoch 066:     39 / 97 loss=2.525, ppl=5.76, wps=22182.7, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.131, loss_scale=32, train_wall=260, gb_free=8.2, wall=19000
2022-03-04 14:55:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:55:36 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 11.301 | ppl 2523.5 | wps 39674.6 | wpb 510.9 | bsz 1 | num_updates 6358 | best_loss 7.623
2022-03-04 14:55:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6358 updates
2022-03-04 14:55:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:55:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 14:55:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 66 @ 6358 updates, score 11.301) (writing took 4.182633204385638 seconds)
2022-03-04 14:55:41 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 14:55:41 | INFO | train | epoch 066 | loss 2.494 | ppl 5.63 | wps 22180.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6358 | lr 0.000396588 | gnorm 1.125 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 19174
2022-03-04 14:55:41 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 14:55:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:57:41 | INFO | train_inner | epoch 067:     42 / 97 loss=2.471, ppl=5.55, wps=22216.1, ups=0.34, wpb=65495, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.126, loss_scale=32, train_wall=259, gb_free=8.2, wall=19295
2022-03-04 14:58:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:58:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:00:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:00:23 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 11.443 | ppl 2784.53 | wps 40999.4 | wpb 510.9 | bsz 1 | num_updates 6453 | best_loss 7.623
2022-03-04 15:00:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6453 updates
2022-03-04 15:00:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:00:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 67 @ 6453 updates, score 11.443) (writing took 3.5764644611626863 seconds)
2022-03-04 15:00:26 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 15:00:26 | INFO | train | epoch 067 | loss 2.445 | ppl 5.45 | wps 21778 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 6453 | lr 0.000393658 | gnorm 1.129 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 19460
2022-03-04 15:00:26 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 15:00:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:02:42 | INFO | train_inner | epoch 068:     47 / 97 loss=2.426, ppl=5.37, wps=21826.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1.115, loss_scale=16, train_wall=265, gb_free=8.2, wall=19595
2022-03-04 15:05:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:05:09 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 11.476 | ppl 2847.93 | wps 39996.4 | wpb 510.9 | bsz 1 | num_updates 6550 | best_loss 7.623
2022-03-04 15:05:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6550 updates
2022-03-04 15:05:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:05:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:05:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 68 @ 6550 updates, score 11.476) (writing took 3.9309330601245165 seconds)
2022-03-04 15:05:13 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 15:05:13 | INFO | train | epoch 068 | loss 2.403 | ppl 5.29 | wps 22166.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6550 | lr 0.000390732 | gnorm 1.131 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 19746
2022-03-04 15:05:13 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 15:05:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:07:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:07:39 | INFO | train_inner | epoch 069:     51 / 97 loss=2.384, ppl=5.22, wps=21988.6, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.144, loss_scale=16, train_wall=262, gb_free=8.2, wall=19893
2022-03-04 15:09:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:09:55 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 11.592 | ppl 3088.06 | wps 39850.2 | wpb 510.9 | bsz 1 | num_updates 6646 | best_loss 7.623
2022-03-04 15:09:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6646 updates
2022-03-04 15:09:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:09:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:09:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 69 @ 6646 updates, score 11.592) (writing took 3.9853353053331375 seconds)
2022-03-04 15:09:59 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 15:09:59 | INFO | train | epoch 069 | loss 2.357 | ppl 5.12 | wps 21980.4 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 6646 | lr 0.0003879 | gnorm 1.133 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 20032
2022-03-04 15:09:59 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 15:09:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:12:34 | INFO | train_inner | epoch 070:     54 / 97 loss=2.327, ppl=5.02, wps=22245.3, ups=0.34, wpb=65495, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.122, loss_scale=16, train_wall=259, gb_free=8.2, wall=20187
2022-03-04 15:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:14:41 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 11.809 | ppl 3587.81 | wps 40166 | wpb 510.9 | bsz 1 | num_updates 6743 | best_loss 7.623
2022-03-04 15:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6743 updates
2022-03-04 15:14:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:14:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:14:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 70 @ 6743 updates, score 11.809) (writing took 4.01099594309926 seconds)
2022-03-04 15:14:45 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 15:14:45 | INFO | train | epoch 070 | loss 2.317 | ppl 4.98 | wps 22214.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6743 | lr 0.0003851 | gnorm 1.132 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 20318
2022-03-04 15:14:45 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 15:14:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:17:28 | INFO | train_inner | epoch 071:     57 / 97 loss=2.296, ppl=4.91, wps=22267.8, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.133, loss_scale=32, train_wall=259, gb_free=8.2, wall=20481
2022-03-04 15:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:19:27 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 11.855 | ppl 3703.77 | wps 39939.5 | wpb 510.9 | bsz 1 | num_updates 6840 | best_loss 7.623
2022-03-04 15:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6840 updates
2022-03-04 15:19:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 71 @ 6840 updates, score 11.855) (writing took 3.243533531203866 seconds)
2022-03-04 15:19:30 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 15:19:30 | INFO | train | epoch 071 | loss 2.274 | ppl 4.84 | wps 22297.4 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6840 | lr 0.00038236 | gnorm 1.13 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 20603
2022-03-04 15:19:30 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 15:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:20:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:22:25 | INFO | train_inner | epoch 072:     61 / 97 loss=2.25, ppl=4.76, wps=22029, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.121, loss_scale=32, train_wall=262, gb_free=8.2, wall=20779
2022-03-04 15:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:24:13 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 11.894 | ppl 3805.65 | wps 39581.7 | wpb 510.9 | bsz 1 | num_updates 6936 | best_loss 7.623
2022-03-04 15:24:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6936 updates
2022-03-04 15:24:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:24:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 72 @ 6936 updates, score 11.894) (writing took 4.453857531771064 seconds)
2022-03-04 15:24:18 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 15:24:18 | INFO | train | epoch 072 | loss 2.233 | ppl 4.7 | wps 21825.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 6936 | lr 0.000379704 | gnorm 1.114 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 20891
2022-03-04 15:24:18 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 15:24:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:25:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:27:24 | INFO | train_inner | epoch 073:     65 / 97 loss=2.216, ppl=4.65, wps=21901.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.144, loss_scale=16, train_wall=263, gb_free=8.2, wall=21078
2022-03-04 15:28:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:29:00 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 11.928 | ppl 3897.13 | wps 41700.1 | wpb 510.9 | bsz 1 | num_updates 7032 | best_loss 7.623
2022-03-04 15:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7032 updates
2022-03-04 15:29:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:29:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 73 @ 7032 updates, score 11.928) (writing took 3.3446197006851435 seconds)
2022-03-04 15:29:03 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 15:29:03 | INFO | train | epoch 073 | loss 2.197 | ppl 4.59 | wps 22055.5 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 7032 | lr 0.000377104 | gnorm 1.145 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 21176
2022-03-04 15:29:03 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 15:29:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:31:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:32:19 | INFO | train_inner | epoch 074:     69 / 97 loss=2.172, ppl=4.51, wps=22194.3, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.138, loss_scale=16, train_wall=261, gb_free=8.2, wall=21373
2022-03-04 15:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:33:43 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 12.016 | ppl 4142.43 | wps 41890.6 | wpb 510.9 | bsz 1 | num_updates 7128 | best_loss 7.623
2022-03-04 15:33:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7128 updates
2022-03-04 15:33:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:33:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:33:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 74 @ 7128 updates, score 12.016) (writing took 3.2764280531555414 seconds)
2022-03-04 15:33:46 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 15:33:46 | INFO | train | epoch 074 | loss 2.156 | ppl 4.46 | wps 22193.2 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 7128 | lr 0.000374555 | gnorm 1.135 | loss_scale 16 | train_wall 250 | gb_free 8.2 | wall 21460
2022-03-04 15:33:46 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 15:33:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:37:11 | INFO | train_inner | epoch 075:     72 / 97 loss=2.132, ppl=4.38, wps=22462.4, ups=0.34, wpb=65495, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.135, loss_scale=16, train_wall=257, gb_free=8.2, wall=21664
2022-03-04 15:38:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:38:26 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 11.99 | ppl 4067.87 | wps 41984.9 | wpb 510.9 | bsz 1 | num_updates 7225 | best_loss 7.623
2022-03-04 15:38:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7225 updates
2022-03-04 15:38:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:38:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 75 @ 7225 updates, score 11.99) (writing took 3.5452768988907337 seconds)
2022-03-04 15:38:29 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 15:38:29 | INFO | train | epoch 075 | loss 2.124 | ppl 4.36 | wps 22442.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7225 | lr 0.000372033 | gnorm 1.138 | loss_scale 32 | train_wall 250 | gb_free 8.2 | wall 21743
2022-03-04 15:38:29 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 15:38:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:41:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:42:06 | INFO | train_inner | epoch 076:     76 / 97 loss=2.093, ppl=4.27, wps=22187, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.128, loss_scale=16, train_wall=260, gb_free=8.2, wall=21959
2022-03-04 15:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:43:11 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 12.157 | ppl 4567.76 | wps 39247.3 | wpb 510.9 | bsz 1 | num_updates 7321 | best_loss 7.623
2022-03-04 15:43:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7321 updates
2022-03-04 15:43:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:43:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:43:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 76 @ 7321 updates, score 12.157) (writing took 3.290609734132886 seconds)
2022-03-04 15:43:14 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 15:43:14 | INFO | train | epoch 076 | loss 2.085 | ppl 4.24 | wps 22061.1 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 7321 | lr 0.000369585 | gnorm 1.122 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 22028
2022-03-04 15:43:14 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 15:43:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:47:01 | INFO | train_inner | epoch 077:     79 / 97 loss=2.065, ppl=4.18, wps=22221.8, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.144, loss_scale=16, train_wall=260, gb_free=8.2, wall=22254
2022-03-04 15:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:47:57 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 12.289 | ppl 5003.68 | wps 39985.9 | wpb 510.9 | bsz 1 | num_updates 7418 | best_loss 7.623
2022-03-04 15:47:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7418 updates
2022-03-04 15:47:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:48:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 77 @ 7418 updates, score 12.289) (writing took 3.2732741348445415 seconds)
2022-03-04 15:48:00 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 15:48:00 | INFO | train | epoch 077 | loss 2.054 | ppl 4.15 | wps 22221 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7418 | lr 0.000367161 | gnorm 1.146 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 22314
2022-03-04 15:48:00 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 15:48:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:51:55 | INFO | train_inner | epoch 078:     82 / 97 loss=2.027, ppl=4.08, wps=22282.9, ups=0.34, wpb=65495, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.114, loss_scale=32, train_wall=259, gb_free=8.2, wall=22548
2022-03-04 15:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:52:43 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 12.232 | ppl 4811 | wps 39605 | wpb 510.9 | bsz 1 | num_updates 7515 | best_loss 7.623
2022-03-04 15:52:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7515 updates
2022-03-04 15:52:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:52:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:52:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 78 @ 7515 updates, score 12.232) (writing took 3.182639569044113 seconds)
2022-03-04 15:52:46 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 15:52:46 | INFO | train | epoch 078 | loss 2.019 | ppl 4.05 | wps 22236.8 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7515 | lr 0.000364784 | gnorm 1.116 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 22599
2022-03-04 15:52:46 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 15:52:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:53:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:56:52 | INFO | train_inner | epoch 079:     86 / 97 loss=1.995, ppl=3.98, wps=22045.6, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.146, loss_scale=16, train_wall=262, gb_free=8.2, wall=22845
2022-03-04 15:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:57:28 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 12.355 | ppl 5238.9 | wps 40192.7 | wpb 510.9 | bsz 1 | num_updates 7611 | best_loss 7.623
2022-03-04 15:57:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7611 updates
2022-03-04 15:57:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 15:57:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 79 @ 7611 updates, score 12.355) (writing took 4.368331082165241 seconds)
2022-03-04 15:57:32 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 15:57:32 | INFO | train | epoch 079 | loss 1.986 | ppl 3.96 | wps 21941.9 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 7611 | lr 0.000362476 | gnorm 1.145 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 22886
2022-03-04 15:57:32 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 15:57:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:01:48 | INFO | train_inner | epoch 080:     89 / 97 loss=1.963, ppl=3.9, wps=22123.4, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.126, loss_scale=32, train_wall=260, gb_free=8.2, wall=23141
2022-03-04 16:02:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:02:16 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 12.358 | ppl 5248.92 | wps 39866.4 | wpb 510.9 | bsz 1 | num_updates 7708 | best_loss 7.623
2022-03-04 16:02:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7708 updates
2022-03-04 16:02:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:02:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:02:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 80 @ 7708 updates, score 12.358) (writing took 4.382899433374405 seconds)
2022-03-04 16:02:20 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 16:02:20 | INFO | train | epoch 080 | loss 1.957 | ppl 3.88 | wps 22067.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7708 | lr 0.000360188 | gnorm 1.127 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 23174
2022-03-04 16:02:20 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 16:02:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:06:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:06:47 | INFO | train_inner | epoch 081:     93 / 97 loss=1.931, ppl=3.81, wps=21905, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=7800, lr=0.000358057, gnorm=1.125, loss_scale=32, train_wall=263, gb_free=8.2, wall=23440
2022-03-04 16:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:07:03 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 12.526 | ppl 5896.33 | wps 40258.8 | wpb 510.9 | bsz 1 | num_updates 7804 | best_loss 7.623
2022-03-04 16:07:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7804 updates
2022-03-04 16:07:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 81 @ 7804 updates, score 12.526) (writing took 4.4444149769842625 seconds)
2022-03-04 16:07:08 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 16:07:08 | INFO | train | epoch 081 | loss 1.926 | ppl 3.8 | wps 21863.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 7804 | lr 0.000357966 | gnorm 1.122 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 23461
2022-03-04 16:07:08 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 16:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:11:42 | INFO | train_inner | epoch 082:     96 / 97 loss=1.9, ppl=3.73, wps=22205.8, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=7900, lr=0.000355784, gnorm=1.124, loss_scale=32, train_wall=259, gb_free=8.2, wall=23735
2022-03-04 16:11:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:11:50 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 12.609 | ppl 6247.08 | wps 38816.9 | wpb 510.9 | bsz 1 | num_updates 7901 | best_loss 7.623
2022-03-04 16:11:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7901 updates
2022-03-04 16:11:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:11:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:11:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 82 @ 7901 updates, score 12.609) (writing took 3.8449065387248993 seconds)
2022-03-04 16:11:54 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 16:11:54 | INFO | train | epoch 082 | loss 1.897 | ppl 3.73 | wps 22205.6 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7901 | lr 0.000355762 | gnorm 1.123 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 23747
2022-03-04 16:11:54 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 16:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:12:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:16:36 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 12.604 | ppl 6227.28 | wps 39357 | wpb 510.9 | bsz 1 | num_updates 7997 | best_loss 7.623
2022-03-04 16:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7997 updates
2022-03-04 16:16:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:16:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 83 @ 7997 updates, score 12.604) (writing took 4.166748197749257 seconds)
2022-03-04 16:16:40 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 16:16:40 | INFO | train | epoch 083 | loss 1.865 | ppl 3.64 | wps 21949.9 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 7997 | lr 0.00035362 | gnorm 1.122 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 24034
2022-03-04 16:16:40 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 16:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:16:49 | INFO | train_inner | epoch 084:      3 / 97 loss=1.865, ppl=3.64, wps=21308.1, ups=0.33, wpb=65451.9, bsz=127.8, num_updates=8000, lr=0.000353553, gnorm=1.123, loss_scale=32, train_wall=262, gb_free=8.2, wall=24042
2022-03-04 16:18:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:21:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:21:23 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 12.709 | ppl 6693.93 | wps 39172.7 | wpb 510.9 | bsz 1 | num_updates 8093 | best_loss 7.623
2022-03-04 16:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8093 updates
2022-03-04 16:21:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:21:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:21:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 84 @ 8093 updates, score 12.709) (writing took 4.38857538998127 seconds)
2022-03-04 16:21:27 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 16:21:27 | INFO | train | epoch 084 | loss 1.841 | ppl 3.58 | wps 21940.1 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 8093 | lr 0.000351516 | gnorm 1.129 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 24320
2022-03-04 16:21:27 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 16:21:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:21:47 | INFO | train_inner | epoch 085:      7 / 97 loss=1.836, ppl=3.57, wps=21987.3, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.128, loss_scale=32, train_wall=262, gb_free=8.2, wall=24340
2022-03-04 16:25:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:26:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:26:09 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 12.8 | ppl 7129.26 | wps 39899 | wpb 510.9 | bsz 1 | num_updates 8189 | best_loss 7.623
2022-03-04 16:26:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8189 updates
2022-03-04 16:26:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:26:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:26:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 85 @ 8189 updates, score 12.8) (writing took 4.394637515768409 seconds)
2022-03-04 16:26:13 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 16:26:13 | INFO | train | epoch 085 | loss 1.813 | ppl 3.51 | wps 21942 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 8189 | lr 0.00034945 | gnorm 1.123 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 24607
2022-03-04 16:26:13 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 16:26:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:26:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:26:47 | INFO | train_inner | epoch 086:     12 / 97 loss=1.806, ppl=3.5, wps=21795.7, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.123, loss_scale=16, train_wall=264, gb_free=8.2, wall=24641
2022-03-04 16:30:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:30:55 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 12.827 | ppl 7266.54 | wps 40370.9 | wpb 510.9 | bsz 1 | num_updates 8285 | best_loss 7.623
2022-03-04 16:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8285 updates
2022-03-04 16:30:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:31:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:31:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 86 @ 8285 updates, score 12.827) (writing took 4.354018686339259 seconds)
2022-03-04 16:31:00 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 16:31:00 | INFO | train | epoch 086 | loss 1.787 | ppl 3.45 | wps 21965.3 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 8285 | lr 0.000347419 | gnorm 1.127 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 24893
2022-03-04 16:31:00 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 16:31:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:31:42 | INFO | train_inner | epoch 087:     15 / 97 loss=1.779, ppl=3.43, wps=22200.1, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.124, loss_scale=16, train_wall=259, gb_free=8.2, wall=24936
2022-03-04 16:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:35:42 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 12.928 | ppl 7793.92 | wps 40143.4 | wpb 510.9 | bsz 1 | num_updates 8382 | best_loss 7.623
2022-03-04 16:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8382 updates
2022-03-04 16:35:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:35:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:35:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 87 @ 8382 updates, score 12.928) (writing took 4.255745554342866 seconds)
2022-03-04 16:35:46 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 16:35:46 | INFO | train | epoch 087 | loss 1.761 | ppl 3.39 | wps 22165.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8382 | lr 0.000345403 | gnorm 1.12 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 25180
2022-03-04 16:35:46 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 16:35:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:36:37 | INFO | train_inner | epoch 088:     18 / 97 loss=1.759, ppl=3.38, wps=22195.3, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.122, loss_scale=32, train_wall=259, gb_free=8.2, wall=25231
2022-03-04 16:39:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:39:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:40:29 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 12.98 | ppl 8077.27 | wps 38761.7 | wpb 510.9 | bsz 1 | num_updates 8477 | best_loss 7.623
2022-03-04 16:40:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8477 updates
2022-03-04 16:40:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:40:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:40:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 88 @ 8477 updates, score 12.98) (writing took 3.27292338386178 seconds)
2022-03-04 16:40:32 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 16:40:32 | INFO | train | epoch 088 | loss 1.736 | ppl 3.33 | wps 21781.3 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 8477 | lr 0.000343462 | gnorm 1.126 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 25465
2022-03-04 16:40:32 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 16:40:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:41:37 | INFO | train_inner | epoch 089:     23 / 97 loss=1.725, ppl=3.3, wps=21870.9, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.126, loss_scale=16, train_wall=264, gb_free=8.2, wall=25530
2022-03-04 16:45:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:45:11 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 13.045 | ppl 8454.09 | wps 38915.3 | wpb 510.9 | bsz 1 | num_updates 8574 | best_loss 7.623
2022-03-04 16:45:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8574 updates
2022-03-04 16:45:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:45:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:45:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 89 @ 8574 updates, score 13.045) (writing took 3.109674559906125 seconds)
2022-03-04 16:45:14 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 16:45:14 | INFO | train | epoch 089 | loss 1.711 | ppl 3.27 | wps 22501.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8574 | lr 0.000341514 | gnorm 1.116 | loss_scale 16 | train_wall 249 | gb_free 8.2 | wall 25748
2022-03-04 16:45:14 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 16:45:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:46:28 | INFO | train_inner | epoch 090:     26 / 97 loss=1.704, ppl=3.26, wps=22486.8, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.106, loss_scale=32, train_wall=257, gb_free=8.2, wall=25821
2022-03-04 16:49:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:49:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:49:54 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 13.178 | ppl 9269.02 | wps 39418.5 | wpb 510.9 | bsz 1 | num_updates 8670 | best_loss 7.623
2022-03-04 16:49:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8670 updates
2022-03-04 16:49:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:49:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:49:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 90 @ 8670 updates, score 13.178) (writing took 4.329144813120365 seconds)
2022-03-04 16:49:58 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 16:49:58 | INFO | train | epoch 090 | loss 1.689 | ppl 3.23 | wps 22143.7 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 8670 | lr 0.000339618 | gnorm 1.128 | loss_scale 16 | train_wall 249 | gb_free 8.2 | wall 26032
2022-03-04 16:49:58 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 16:49:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:51:23 | INFO | train_inner | epoch 091:     30 / 97 loss=1.681, ppl=3.21, wps=22179.6, ups=0.34, wpb=65495, bsz=127.9, num_updates=8700, lr=0.000339032, gnorm=1.131, loss_scale=16, train_wall=259, gb_free=8.2, wall=26117
2022-03-04 16:54:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:54:38 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 13.232 | ppl 9623.4 | wps 39062.9 | wpb 510.9 | bsz 1 | num_updates 8767 | best_loss 7.623
2022-03-04 16:54:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8767 updates
2022-03-04 16:54:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:54:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 91 @ 8767 updates, score 13.232) (writing took 4.305923419073224 seconds)
2022-03-04 16:54:42 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 16:54:42 | INFO | train | epoch 091 | loss 1.666 | ppl 3.17 | wps 22364.5 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8767 | lr 0.000337734 | gnorm 1.114 | loss_scale 16 | train_wall 249 | gb_free 8.2 | wall 26316
2022-03-04 16:54:42 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 16:54:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:56:16 | INFO | train_inner | epoch 092:     33 / 97 loss=1.661, ppl=3.16, wps=22369.3, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.123, loss_scale=32, train_wall=257, gb_free=8.2, wall=26410
2022-03-04 16:57:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:59:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:59:22 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 13.264 | ppl 9839.72 | wps 38850.1 | wpb 510.9 | bsz 1 | num_updates 8863 | best_loss 7.623
2022-03-04 16:59:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8863 updates
2022-03-04 16:59:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:59:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 16:59:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 92 @ 8863 updates, score 13.264) (writing took 4.557378675788641 seconds)
2022-03-04 16:59:26 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 16:59:26 | INFO | train | epoch 092 | loss 1.643 | ppl 3.12 | wps 22145.9 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 8863 | lr 0.0003359 | gnorm 1.123 | loss_scale 16 | train_wall 249 | gb_free 8.2 | wall 26600
2022-03-04 16:59:26 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 16:59:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:01:12 | INFO | train_inner | epoch 093:     37 / 97 loss=1.63, ppl=3.09, wps=22158.8, ups=0.34, wpb=65495, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.123, loss_scale=16, train_wall=259, gb_free=8.2, wall=26705
2022-03-04 17:04:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:04:06 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 13.378 | ppl 10642.6 | wps 39121.9 | wpb 510.9 | bsz 1 | num_updates 8960 | best_loss 7.623
2022-03-04 17:04:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8960 updates
2022-03-04 17:04:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:04:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 93 @ 8960 updates, score 13.378) (writing took 4.251784833148122 seconds)
2022-03-04 17:04:10 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 17:04:10 | INFO | train | epoch 093 | loss 1.624 | ppl 3.08 | wps 22394.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8960 | lr 0.000334077 | gnorm 1.127 | loss_scale 32 | train_wall 249 | gb_free 8.2 | wall 26883
2022-03-04 17:04:10 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 17:04:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:06:04 | INFO | train_inner | epoch 094:     40 / 97 loss=1.613, ppl=3.06, wps=22406.9, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.109, loss_scale=32, train_wall=257, gb_free=8.2, wall=26997
2022-03-04 17:06:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:08:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:08:48 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 13.436 | ppl 11084.3 | wps 41630.4 | wpb 510.9 | bsz 1 | num_updates 9056 | best_loss 7.623
2022-03-04 17:08:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9056 updates
2022-03-04 17:08:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:08:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:08:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 94 @ 9056 updates, score 13.436) (writing took 4.356012422591448 seconds)
2022-03-04 17:08:53 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 17:08:53 | INFO | train | epoch 094 | loss 1.6 | ppl 3.03 | wps 22229.5 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 9056 | lr 0.000332301 | gnorm 1.116 | loss_scale 16 | train_wall 248 | gb_free 8.2 | wall 27166
2022-03-04 17:08:53 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 17:08:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:10:58 | INFO | train_inner | epoch 095:     44 / 97 loss=1.594, ppl=3.02, wps=22260.6, ups=0.34, wpb=65495, bsz=127.9, num_updates=9100, lr=0.000331497, gnorm=1.127, loss_scale=16, train_wall=259, gb_free=8.2, wall=27292
2022-03-04 17:13:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:13:31 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 13.411 | ppl 10895.3 | wps 40315.3 | wpb 510.9 | bsz 1 | num_updates 9153 | best_loss 7.623
2022-03-04 17:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9153 updates
2022-03-04 17:13:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:13:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 95 @ 9153 updates, score 13.411) (writing took 3.506833141669631 seconds)
2022-03-04 17:13:34 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 17:13:34 | INFO | train | epoch 095 | loss 1.582 | ppl 2.99 | wps 22544.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 9153 | lr 0.000330536 | gnorm 1.12 | loss_scale 32 | train_wall 248 | gb_free 8.2 | wall 27448
2022-03-04 17:13:34 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 17:13:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:14:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:15:51 | INFO | train_inner | epoch 096:     48 / 97 loss=1.576, ppl=2.98, wps=22384.5, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.125, loss_scale=16, train_wall=258, gb_free=8.2, wall=27584
2022-03-04 17:18:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:18:12 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 13.395 | ppl 10774 | wps 41725.8 | wpb 510.9 | bsz 1 | num_updates 9249 | best_loss 7.623
2022-03-04 17:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9249 updates
2022-03-04 17:18:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:18:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:18:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 96 @ 9249 updates, score 13.395) (writing took 3.47562138736248 seconds)
2022-03-04 17:18:15 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 17:18:15 | INFO | train | epoch 096 | loss 1.56 | ppl 2.95 | wps 22378.7 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 9249 | lr 0.000328816 | gnorm 1.122 | loss_scale 16 | train_wall 248 | gb_free 8.2 | wall 27729
2022-03-04 17:18:15 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 17:18:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:20:40 | INFO | train_inner | epoch 097:     51 / 97 loss=1.548, ppl=2.92, wps=22660, ups=0.35, wpb=65492.9, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.112, loss_scale=16, train_wall=255, gb_free=8.2, wall=27873
2022-03-04 17:21:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:22:56 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 13.586 | ppl 12294.3 | wps 39521.6 | wpb 510.9 | bsz 1 | num_updates 9345 | best_loss 7.623
2022-03-04 17:22:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9345 updates
2022-03-04 17:22:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:23:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:23:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 97 @ 9345 updates, score 13.586) (writing took 3.412838824093342 seconds)
2022-03-04 17:23:00 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 17:23:00 | INFO | train | epoch 097 | loss 1.542 | ppl 2.91 | wps 22101.3 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 9345 | lr 0.000327122 | gnorm 1.128 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 28013
2022-03-04 17:23:00 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 17:23:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:25:39 | INFO | train_inner | epoch 098:     55 / 97 loss=1.529, ppl=2.89, wps=21881.2, ups=0.33, wpb=65495, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.127, loss_scale=16, train_wall=264, gb_free=8.2, wall=28173
2022-03-04 17:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:27:43 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 13.592 | ppl 12350.3 | wps 41231.7 | wpb 510.9 | bsz 1 | num_updates 9442 | best_loss 7.623
2022-03-04 17:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9442 updates
2022-03-04 17:27:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:27:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 98 @ 9442 updates, score 13.592) (writing took 3.330961909145117 seconds)
2022-03-04 17:27:46 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 17:27:46 | INFO | train | epoch 098 | loss 1.521 | ppl 2.87 | wps 22169.4 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 9442 | lr 0.000325438 | gnorm 1.11 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 28300
2022-03-04 17:27:46 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 17:27:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:30:33 | INFO | train_inner | epoch 099:     58 / 97 loss=1.512, ppl=2.85, wps=22258.4, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.112, loss_scale=32, train_wall=260, gb_free=8.2, wall=28467
2022-03-04 17:30:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:32:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:32:30 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 13.716 | ppl 13459.2 | wps 38564.9 | wpb 510.9 | bsz 1 | num_updates 9538 | best_loss 7.623
2022-03-04 17:32:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9538 updates
2022-03-04 17:32:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:32:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:32:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 99 @ 9538 updates, score 13.716) (writing took 3.2107285633683205 seconds)
2022-03-04 17:32:34 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 17:32:34 | INFO | train | epoch 099 | loss 1.502 | ppl 2.83 | wps 21895.9 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 9538 | lr 0.000323796 | gnorm 1.107 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 28587
2022-03-04 17:32:34 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 17:32:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:35:36 | INFO | train_inner | epoch 100:     62 / 97 loss=1.494, ppl=2.82, wps=21654.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.115, loss_scale=16, train_wall=267, gb_free=8.2, wall=28769
2022-03-04 17:37:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:37:24 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 13.63 | ppl 12675.8 | wps 37406 | wpb 510.9 | bsz 1 | num_updates 9635 | best_loss 7.623
2022-03-04 17:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9635 updates
2022-03-04 17:37:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:37:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 100 @ 9635 updates, score 13.63) (writing took 4.699282985180616 seconds)
2022-03-04 17:37:29 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 17:37:29 | INFO | train | epoch 100 | loss 1.486 | ppl 2.8 | wps 21532.5 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 9635 | lr 0.000322162 | gnorm 1.122 | loss_scale 32 | train_wall 259 | gb_free 8.2 | wall 28882
2022-03-04 17:37:29 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 17:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:40:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:40:44 | INFO | train_inner | epoch 101:     66 / 97 loss=1.476, ppl=2.78, wps=21239.1, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.117, loss_scale=16, train_wall=271, gb_free=8.2, wall=29078
2022-03-04 17:42:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:42:21 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 13.817 | ppl 14436.2 | wps 36284.5 | wpb 510.9 | bsz 1 | num_updates 9731 | best_loss 7.623
2022-03-04 17:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9731 updates
2022-03-04 17:42:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:42:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 101 @ 9731 updates, score 13.817) (writing took 3.602170890197158 seconds)
2022-03-04 17:42:25 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 17:42:25 | INFO | train | epoch 101 | loss 1.466 | ppl 2.76 | wps 21216.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 9731 | lr 0.000320569 | gnorm 1.116 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 29178
2022-03-04 17:42:25 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 17:42:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:51 | INFO | train_inner | epoch 102:     69 / 97 loss=1.457, ppl=2.75, wps=21328.5, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.115, loss_scale=16, train_wall=270, gb_free=8.2, wall=29385
2022-03-04 17:47:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:47:20 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 13.752 | ppl 13800 | wps 36159.5 | wpb 510.9 | bsz 1 | num_updates 9828 | best_loss 7.623
2022-03-04 17:47:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9828 updates
2022-03-04 17:47:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:47:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:47:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 102 @ 9828 updates, score 13.752) (writing took 3.7945754881948233 seconds)
2022-03-04 17:47:24 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 17:47:24 | INFO | train | epoch 102 | loss 1.45 | ppl 2.73 | wps 21268.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 9828 | lr 0.000318983 | gnorm 1.117 | loss_scale 32 | train_wall 262 | gb_free 8.2 | wall 29477
2022-03-04 17:47:24 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 17:47:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:50:57 | INFO | train_inner | epoch 103:     72 / 97 loss=1.437, ppl=2.71, wps=21405.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.121, loss_scale=32, train_wall=269, gb_free=8.2, wall=29691
2022-03-04 17:52:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:52:18 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 13.814 | ppl 14398.5 | wps 37064.3 | wpb 510.9 | bsz 1 | num_updates 9924 | best_loss 7.623
2022-03-04 17:52:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9924 updates
2022-03-04 17:52:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:52:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:52:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 103 @ 9924 updates, score 13.814) (writing took 3.749641163274646 seconds)
2022-03-04 17:52:22 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 17:52:22 | INFO | train | epoch 103 | loss 1.432 | ppl 2.7 | wps 21080.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 9924 | lr 0.000317436 | gnorm 1.114 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 29775
2022-03-04 17:52:22 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 17:52:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:56:07 | INFO | train_inner | epoch 104:     76 / 97 loss=1.422, ppl=2.68, wps=21144.4, ups=0.32, wpb=65495, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.104, loss_scale=16, train_wall=272, gb_free=8.2, wall=30000
2022-03-04 17:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:57:16 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 13.854 | ppl 14802.1 | wps 34986 | wpb 510.9 | bsz 1 | num_updates 10021 | best_loss 7.623
2022-03-04 17:57:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10021 updates
2022-03-04 17:57:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:57:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 17:57:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 104 @ 10021 updates, score 13.854) (writing took 3.8810854386538267 seconds)
2022-03-04 17:57:19 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 17:57:19 | INFO | train | epoch 104 | loss 1.417 | ppl 2.67 | wps 21343.8 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 10021 | lr 0.000315896 | gnorm 1.103 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 30073
2022-03-04 17:57:19 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 17:57:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:01:16 | INFO | train_inner | epoch 105:     79 / 97 loss=1.407, ppl=2.65, wps=21211.2, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.103, loss_scale=32, train_wall=271, gb_free=8.2, wall=30309
2022-03-04 18:01:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:02:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:02:15 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 13.928 | ppl 15581.5 | wps 36211.9 | wpb 510.9 | bsz 1 | num_updates 10117 | best_loss 7.623
2022-03-04 18:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10117 updates
2022-03-04 18:02:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:02:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:02:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 105 @ 10117 updates, score 13.928) (writing took 4.037291344255209 seconds)
2022-03-04 18:02:19 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 18:02:19 | INFO | train | epoch 105 | loss 1.402 | ppl 2.64 | wps 20973.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10117 | lr 0.000314394 | gnorm 1.106 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 30373
2022-03-04 18:02:19 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 18:02:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:06:26 | INFO | train_inner | epoch 106:     83 / 97 loss=1.388, ppl=2.62, wps=21078.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10200, lr=0.000313112, gnorm=1.11, loss_scale=16, train_wall=273, gb_free=8.2, wall=30620
2022-03-04 18:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:07:14 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 14.008 | ppl 16477.7 | wps 35590.3 | wpb 510.9 | bsz 1 | num_updates 10214 | best_loss 7.623
2022-03-04 18:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10214 updates
2022-03-04 18:07:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:07:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:07:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 106 @ 10214 updates, score 14.008) (writing took 3.8464903496205807 seconds)
2022-03-04 18:07:18 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 18:07:18 | INFO | train | epoch 106 | loss 1.384 | ppl 2.61 | wps 21294.7 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 10214 | lr 0.000312897 | gnorm 1.106 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 30671
2022-03-04 18:07:18 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 18:07:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:11:34 | INFO | train_inner | epoch 107:     86 / 97 loss=1.375, ppl=2.59, wps=21288.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.11, loss_scale=32, train_wall=270, gb_free=8.2, wall=30927
2022-03-04 18:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:12:13 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 14.03 | ppl 16732 | wps 36530.1 | wpb 510.9 | bsz 1 | num_updates 10311 | best_loss 7.623
2022-03-04 18:12:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10311 updates
2022-03-04 18:12:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:12:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:12:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 107 @ 10311 updates, score 14.03) (writing took 3.733575964346528 seconds)
2022-03-04 18:12:16 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 18:12:16 | INFO | train | epoch 107 | loss 1.372 | ppl 2.59 | wps 21265.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 10311 | lr 0.000311422 | gnorm 1.115 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 30970
2022-03-04 18:12:16 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 18:12:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:16:45 | INFO | train_inner | epoch 108:     90 / 97 loss=1.358, ppl=2.56, wps=21032.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.106, loss_scale=16, train_wall=274, gb_free=8.2, wall=31239
2022-03-04 18:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:17:12 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 14.12 | ppl 17808.8 | wps 35536.5 | wpb 510.9 | bsz 1 | num_updates 10407 | best_loss 7.623
2022-03-04 18:17:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10407 updates
2022-03-04 18:17:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:17:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:17:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 108 @ 10407 updates, score 14.12) (writing took 3.84181840531528 seconds)
2022-03-04 18:17:16 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 18:17:16 | INFO | train | epoch 108 | loss 1.353 | ppl 2.55 | wps 20964.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10407 | lr 0.000309983 | gnorm 1.102 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 31270
2022-03-04 18:17:16 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 18:17:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:20:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:21:56 | INFO | train_inner | epoch 109:     94 / 97 loss=1.342, ppl=2.54, wps=21103.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10500, lr=0.000308607, gnorm=1.104, loss_scale=16, train_wall=273, gb_free=8.2, wall=31549
2022-03-04 18:22:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:22:11 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 14.103 | ppl 17591.7 | wps 35361 | wpb 510.9 | bsz 1 | num_updates 10503 | best_loss 7.623
2022-03-04 18:22:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10503 updates
2022-03-04 18:22:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:22:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:22:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 109 @ 10503 updates, score 14.103) (writing took 3.9481534864753485 seconds)
2022-03-04 18:22:15 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 18:22:15 | INFO | train | epoch 109 | loss 1.339 | ppl 2.53 | wps 21049.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10503 | lr 0.000308563 | gnorm 1.103 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 31568
2022-03-04 18:22:15 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 18:22:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:27:03 | INFO | train_inner | epoch 110:     97 / 97 loss=1.329, ppl=2.51, wps=21308.8, ups=0.33, wpb=65451.9, bsz=127.8, num_updates=10600, lr=0.000307148, gnorm=1.118, loss_scale=16, train_wall=270, gb_free=8.2, wall=31856
2022-03-04 18:27:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:27:09 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 14.177 | ppl 18525.5 | wps 34366 | wpb 510.9 | bsz 1 | num_updates 10600 | best_loss 7.623
2022-03-04 18:27:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10600 updates
2022-03-04 18:27:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:27:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:27:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 110 @ 10600 updates, score 14.177) (writing took 3.9688161853700876 seconds)
2022-03-04 18:27:13 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 18:27:13 | INFO | train | epoch 110 | loss 1.327 | ppl 2.51 | wps 21281 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 10600 | lr 0.000307148 | gnorm 1.118 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 31867
2022-03-04 18:27:13 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 18:27:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:30:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:32:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:32:08 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 14.248 | ppl 19455 | wps 36356.4 | wpb 510.9 | bsz 1 | num_updates 10696 | best_loss 7.623
2022-03-04 18:32:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10696 updates
2022-03-04 18:32:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:32:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:32:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 111 @ 10696 updates, score 14.248) (writing took 3.63384922593832 seconds)
2022-03-04 18:32:11 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 18:32:11 | INFO | train | epoch 111 | loss 1.31 | ppl 2.48 | wps 21097.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10696 | lr 0.000305766 | gnorm 1.094 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 32165
2022-03-04 18:32:11 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 18:32:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:32:23 | INFO | train_inner | epoch 112:      4 / 97 loss=1.308, ppl=2.48, wps=20440, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.093, loss_scale=16, train_wall=273, gb_free=8.2, wall=32177
2022-03-04 18:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:37:02 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 14.292 | ppl 20061.2 | wps 38109 | wpb 510.9 | bsz 1 | num_updates 10793 | best_loss 7.623
2022-03-04 18:37:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10793 updates
2022-03-04 18:37:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:37:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 112 @ 10793 updates, score 14.292) (writing took 3.805437482893467 seconds)
2022-03-04 18:37:06 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 18:37:06 | INFO | train | epoch 112 | loss 1.299 | ppl 2.46 | wps 21571.5 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 10793 | lr 0.000304389 | gnorm 1.09 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 32459
2022-03-04 18:37:06 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 18:37:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:37:26 | INFO | train_inner | epoch 113:      7 / 97 loss=1.295, ppl=2.45, wps=21638.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.089, loss_scale=32, train_wall=267, gb_free=8.2, wall=32479
2022-03-04 18:41:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:42:00 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 14.323 | ppl 20499.2 | wps 36561.1 | wpb 510.9 | bsz 1 | num_updates 10890 | best_loss 7.623
2022-03-04 18:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10890 updates
2022-03-04 18:42:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:42:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:42:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 113 @ 10890 updates, score 14.323) (writing took 3.7896160818636417 seconds)
2022-03-04 18:42:04 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 18:42:04 | INFO | train | epoch 113 | loss 1.285 | ppl 2.44 | wps 21341.3 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 10890 | lr 0.00030303 | gnorm 1.096 | loss_scale 32 | train_wall 262 | gb_free 8.2 | wall 32757
2022-03-04 18:42:04 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 18:42:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:42:33 | INFO | train_inner | epoch 114:     10 / 97 loss=1.282, ppl=2.43, wps=21321.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.095, loss_scale=32, train_wall=270, gb_free=8.2, wall=32787
2022-03-04 18:43:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:46:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:46:59 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 14.365 | ppl 21097.8 | wps 36440.1 | wpb 510.9 | bsz 1 | num_updates 10986 | best_loss 7.623
2022-03-04 18:46:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10986 updates
2022-03-04 18:46:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:47:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:47:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 114 @ 10986 updates, score 14.365) (writing took 4.036194819957018 seconds)
2022-03-04 18:47:03 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 18:47:03 | INFO | train | epoch 114 | loss 1.272 | ppl 2.42 | wps 20965.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10986 | lr 0.000301703 | gnorm 1.09 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 33057
2022-03-04 18:47:03 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 18:47:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:47:45 | INFO | train_inner | epoch 115:     14 / 97 loss=1.268, ppl=2.41, wps=21015.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.096, loss_scale=32, train_wall=274, gb_free=8.2, wall=33098
2022-03-04 18:47:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:52:00 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 14.406 | ppl 21713.6 | wps 36495 | wpb 510.9 | bsz 1 | num_updates 11082 | best_loss 7.623
2022-03-04 18:52:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11082 updates
2022-03-04 18:52:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:52:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:52:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 115 @ 11082 updates, score 14.406) (writing took 3.950642677024007 seconds)
2022-03-04 18:52:04 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 18:52:04 | INFO | train | epoch 115 | loss 1.258 | ppl 2.39 | wps 20945.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11082 | lr 0.000300394 | gnorm 1.117 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 33357
2022-03-04 18:52:04 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 18:52:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:52:57 | INFO | train_inner | epoch 116:     18 / 97 loss=1.254, ppl=2.39, wps=20967.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.106, loss_scale=16, train_wall=274, gb_free=8.2, wall=33411
2022-03-04 18:56:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:56:59 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 14.476 | ppl 22791 | wps 35399.5 | wpb 510.9 | bsz 1 | num_updates 11178 | best_loss 7.623
2022-03-04 18:56:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11178 updates
2022-03-04 18:56:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:57:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 18:57:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 116 @ 11178 updates, score 14.476) (writing took 4.199959266930819 seconds)
2022-03-04 18:57:03 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 18:57:03 | INFO | train | epoch 116 | loss 1.245 | ppl 2.37 | wps 20979 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11178 | lr 0.000299101 | gnorm 1.087 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 33657
2022-03-04 18:57:03 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 18:57:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:58:08 | INFO | train_inner | epoch 117:     22 / 97 loss=1.241, ppl=2.36, wps=21049.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.093, loss_scale=16, train_wall=273, gb_free=8.2, wall=33722
2022-03-04 19:01:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:01:57 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 14.5 | ppl 23174 | wps 35812.4 | wpb 510.9 | bsz 1 | num_updates 11275 | best_loss 7.623
2022-03-04 19:01:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11275 updates
2022-03-04 19:01:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:02:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 117 @ 11275 updates, score 14.5) (writing took 3.9914103858172894 seconds)
2022-03-04 19:02:01 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 19:02:01 | INFO | train | epoch 117 | loss 1.235 | ppl 2.35 | wps 21311.1 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 11275 | lr 0.000297812 | gnorm 1.1 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 33955
2022-03-04 19:02:01 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 19:02:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:03:16 | INFO | train_inner | epoch 118:     25 / 97 loss=1.231, ppl=2.35, wps=21297.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.089, loss_scale=16, train_wall=270, gb_free=8.2, wall=34029
2022-03-04 19:06:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:06:55 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 14.632 | ppl 25395.6 | wps 34594.9 | wpb 510.9 | bsz 1 | num_updates 11372 | best_loss 7.623
2022-03-04 19:06:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11372 updates
2022-03-04 19:06:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:06:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 118 @ 11372 updates, score 14.632) (writing took 3.733640141785145 seconds)
2022-03-04 19:06:59 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 19:06:59 | INFO | train | epoch 118 | loss 1.222 | ppl 2.33 | wps 21344.1 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 11372 | lr 0.000296539 | gnorm 1.08 | loss_scale 32 | train_wall 261 | gb_free 8.2 | wall 34252
2022-03-04 19:06:59 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 19:06:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:08:24 | INFO | train_inner | epoch 119:     28 / 97 loss=1.221, ppl=2.33, wps=21278.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.086, loss_scale=32, train_wall=270, gb_free=8.2, wall=34337
2022-03-04 19:09:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:11:55 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 14.646 | ppl 25633 | wps 35805.9 | wpb 510.9 | bsz 1 | num_updates 11468 | best_loss 7.623
2022-03-04 19:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11468 updates
2022-03-04 19:11:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:11:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:11:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 119 @ 11468 updates, score 14.646) (writing took 3.7346663642674685 seconds)
2022-03-04 19:11:59 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 19:11:59 | INFO | train | epoch 119 | loss 1.211 | ppl 2.31 | wps 20953.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11468 | lr 0.000295295 | gnorm 1.088 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 34552
2022-03-04 19:11:59 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 19:11:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:36 | INFO | train_inner | epoch 120:     32 / 97 loss=1.202, ppl=2.3, wps=20969.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.092, loss_scale=16, train_wall=274, gb_free=8.2, wall=34649
2022-03-04 19:16:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:16:56 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 14.611 | ppl 25028 | wps 35704.3 | wpb 510.9 | bsz 1 | num_updates 11565 | best_loss 7.623
2022-03-04 19:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11565 updates
2022-03-04 19:16:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:17:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:17:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 120 @ 11565 updates, score 14.611) (writing took 3.9751713555306196 seconds)
2022-03-04 19:17:00 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 19:17:00 | INFO | train | epoch 120 | loss 1.199 | ppl 2.3 | wps 21131.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11565 | lr 0.000294054 | gnorm 1.092 | loss_scale 32 | train_wall 264 | gb_free 8.2 | wall 34853
2022-03-04 19:17:00 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 19:17:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:18:44 | INFO | train_inner | epoch 121:     35 / 97 loss=1.197, ppl=2.29, wps=21250.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.089, loss_scale=32, train_wall=271, gb_free=8.2, wall=34958
2022-03-04 19:21:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:21:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:21:56 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 14.698 | ppl 26579.1 | wps 35528.7 | wpb 510.9 | bsz 1 | num_updates 11661 | best_loss 7.623
2022-03-04 19:21:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11661 updates
2022-03-04 19:21:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:21:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:22:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 121 @ 11661 updates, score 14.698) (writing took 3.8771646320819855 seconds)
2022-03-04 19:22:00 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 19:22:00 | INFO | train | epoch 121 | loss 1.188 | ppl 2.28 | wps 20964.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11661 | lr 0.000292841 | gnorm 1.077 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 35153
2022-03-04 19:22:00 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 19:22:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:23:56 | INFO | train_inner | epoch 122:     39 / 97 loss=1.184, ppl=2.27, wps=21012, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=1.075, loss_scale=16, train_wall=274, gb_free=8.2, wall=35269
2022-03-04 19:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:26:56 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 14.718 | ppl 26943.8 | wps 36055.4 | wpb 510.9 | bsz 1 | num_updates 11758 | best_loss 7.623
2022-03-04 19:26:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11758 updates
2022-03-04 19:26:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:26:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:26:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 122 @ 11758 updates, score 14.718) (writing took 3.703331472352147 seconds)
2022-03-04 19:26:59 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 19:26:59 | INFO | train | epoch 122 | loss 1.176 | ppl 2.26 | wps 21186 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11758 | lr 0.000291631 | gnorm 1.08 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 35453
2022-03-04 19:26:59 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 19:26:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:29:04 | INFO | train_inner | epoch 123:     42 / 97 loss=1.169, ppl=2.25, wps=21242.2, ups=0.32, wpb=65495, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.079, loss_scale=32, train_wall=271, gb_free=8.2, wall=35578
2022-03-04 19:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:31:55 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 14.815 | ppl 28828.3 | wps 34770.7 | wpb 510.9 | bsz 1 | num_updates 11855 | best_loss 7.623
2022-03-04 19:31:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11855 updates
2022-03-04 19:31:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:31:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:31:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 123 @ 11855 updates, score 14.815) (writing took 3.770084986463189 seconds)
2022-03-04 19:31:59 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 19:31:59 | INFO | train | epoch 123 | loss 1.165 | ppl 2.24 | wps 21205.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11855 | lr 0.000290435 | gnorm 1.079 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 35752
2022-03-04 19:31:59 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 19:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:34:13 | INFO | train_inner | epoch 124:     45 / 97 loss=1.163, ppl=2.24, wps=21233.6, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.075, loss_scale=32, train_wall=271, gb_free=8.2, wall=35886
2022-03-04 19:35:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:36:54 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 14.936 | ppl 31344.2 | wps 34848.3 | wpb 510.9 | bsz 1 | num_updates 11951 | best_loss 7.623
2022-03-04 19:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11951 updates
2022-03-04 19:36:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:36:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 124 @ 11951 updates, score 14.936) (writing took 3.7694255337119102 seconds)
2022-03-04 19:36:58 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 19:36:58 | INFO | train | epoch 124 | loss 1.155 | ppl 2.23 | wps 21048.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11951 | lr 0.000289266 | gnorm 1.083 | loss_scale 32 | train_wall 262 | gb_free 8.2 | wall 36051
2022-03-04 19:36:58 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 19:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:39:27 | INFO | train_inner | epoch 125:     50 / 97 loss=1.149, ppl=2.22, wps=20803.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.089, loss_scale=16, train_wall=277, gb_free=8.2, wall=36201
2022-03-04 19:41:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:41:53 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 14.779 | ppl 28120.5 | wps 36354 | wpb 510.9 | bsz 1 | num_updates 12047 | best_loss 7.623
2022-03-04 19:41:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12047 updates
2022-03-04 19:41:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:41:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 125 @ 12047 updates, score 14.779) (writing took 3.8281132336705923 seconds)
2022-03-04 19:41:57 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 19:41:57 | INFO | train | epoch 125 | loss 1.145 | ppl 2.21 | wps 21021.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12047 | lr 0.000288111 | gnorm 1.083 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 36350
2022-03-04 19:41:57 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 19:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:44:35 | INFO | train_inner | epoch 126:     53 / 97 loss=1.14, ppl=2.2, wps=21261.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=1.077, loss_scale=16, train_wall=271, gb_free=8.2, wall=36509
2022-03-04 19:46:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:46:52 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 14.98 | ppl 32312.5 | wps 35861.1 | wpb 510.9 | bsz 1 | num_updates 12144 | best_loss 7.623
2022-03-04 19:46:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12144 updates
2022-03-04 19:46:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:46:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:46:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 126 @ 12144 updates, score 14.98) (writing took 3.7879130244255066 seconds)
2022-03-04 19:46:56 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 19:46:56 | INFO | train | epoch 126 | loss 1.133 | ppl 2.19 | wps 21227.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 12144 | lr 0.000286959 | gnorm 1.076 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 36649
2022-03-04 19:46:56 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 19:46:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:48:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:49:45 | INFO | train_inner | epoch 127:     57 / 97 loss=1.125, ppl=2.18, wps=21131.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12200, lr=0.000286299, gnorm=1.077, loss_scale=16, train_wall=272, gb_free=8.2, wall=36819
2022-03-04 19:51:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:51:51 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 14.95 | ppl 31658.9 | wps 36485.2 | wpb 510.9 | bsz 1 | num_updates 12240 | best_loss 7.623
2022-03-04 19:51:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12240 updates
2022-03-04 19:51:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:51:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:51:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 127 @ 12240 updates, score 14.95) (writing took 3.7936444357037544 seconds)
2022-03-04 19:51:55 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 19:51:55 | INFO | train | epoch 127 | loss 1.123 | ppl 2.18 | wps 21059.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12240 | lr 0.000285831 | gnorm 1.079 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 36948
2022-03-04 19:51:55 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 19:51:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:54:51 | INFO | train_inner | epoch 128:     60 / 97 loss=1.121, ppl=2.18, wps=21423.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=1.083, loss_scale=32, train_wall=269, gb_free=8.2, wall=37124
2022-03-04 19:56:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:56:48 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 14.994 | ppl 32623.9 | wps 35955.2 | wpb 510.9 | bsz 1 | num_updates 12337 | best_loss 7.623
2022-03-04 19:56:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12337 updates
2022-03-04 19:56:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:56:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 19:56:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 128 @ 12337 updates, score 14.994) (writing took 3.7646820042282343 seconds)
2022-03-04 19:56:51 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 19:56:51 | INFO | train | epoch 128 | loss 1.114 | ppl 2.16 | wps 21397.8 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 12337 | lr 0.000284705 | gnorm 1.078 | loss_scale 32 | train_wall 261 | gb_free 8.2 | wall 37245
2022-03-04 19:56:51 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 19:56:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:58:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:00:01 | INFO | train_inner | epoch 129:     64 / 97 loss=1.108, ppl=2.16, wps=21126.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=1.081, loss_scale=16, train_wall=273, gb_free=8.2, wall=37435
2022-03-04 20:01:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:01:46 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 15.04 | ppl 33679.7 | wps 35205.1 | wpb 510.9 | bsz 1 | num_updates 12433 | best_loss 7.623
2022-03-04 20:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12433 updates
2022-03-04 20:01:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:01:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 129 @ 12433 updates, score 15.04) (writing took 3.7859614100307226 seconds)
2022-03-04 20:01:50 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 20:01:50 | INFO | train | epoch 129 | loss 1.105 | ppl 2.15 | wps 21071.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12433 | lr 0.000283604 | gnorm 1.081 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 37543
2022-03-04 20:01:50 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 20:01:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:05:10 | INFO | train_inner | epoch 130:     67 / 97 loss=1.099, ppl=2.14, wps=21180.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=1.08, loss_scale=32, train_wall=272, gb_free=8.2, wall=37744
2022-03-04 20:06:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:06:47 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 14.979 | ppl 32294.3 | wps 34785.1 | wpb 510.9 | bsz 1 | num_updates 12530 | best_loss 7.623
2022-03-04 20:06:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12530 updates
2022-03-04 20:06:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:06:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 130 @ 12530 updates, score 14.979) (writing took 3.8061182592064142 seconds)
2022-03-04 20:06:51 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 20:06:51 | INFO | train | epoch 130 | loss 1.097 | ppl 2.14 | wps 21087.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 12530 | lr 0.000282504 | gnorm 1.074 | loss_scale 32 | train_wall 264 | gb_free 8.2 | wall 37844
2022-03-04 20:06:51 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 20:06:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:08:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:10:22 | INFO | train_inner | epoch 131:     71 / 97 loss=1.091, ppl=2.13, wps=21047.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=1.069, loss_scale=16, train_wall=273, gb_free=8.2, wall=38055
2022-03-04 20:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:11:46 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 15.073 | ppl 34460.6 | wps 35395.1 | wpb 510.9 | bsz 1 | num_updates 12626 | best_loss 7.623
2022-03-04 20:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12626 updates
2022-03-04 20:11:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 131 @ 12626 updates, score 15.073) (writing took 3.7721430007368326 seconds)
2022-03-04 20:11:49 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 20:11:49 | INFO | train | epoch 131 | loss 1.084 | ppl 2.12 | wps 21073.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12626 | lr 0.000281428 | gnorm 1.073 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 38143
2022-03-04 20:11:49 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 20:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:15:29 | INFO | train_inner | epoch 132:     74 / 97 loss=1.08, ppl=2.11, wps=21299.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=1.071, loss_scale=32, train_wall=270, gb_free=8.2, wall=38362
2022-03-04 20:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:16:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:16:44 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 15.059 | ppl 34133.5 | wps 35015.8 | wpb 510.9 | bsz 1 | num_updates 12722 | best_loss 7.623
2022-03-04 20:16:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12722 updates
2022-03-04 20:16:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:16:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 132 @ 12722 updates, score 15.059) (writing took 3.776692133396864 seconds)
2022-03-04 20:16:48 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 20:16:48 | INFO | train | epoch 132 | loss 1.076 | ppl 2.11 | wps 21065.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12722 | lr 0.000280364 | gnorm 1.071 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 38441
2022-03-04 20:16:48 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 20:16:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:20:39 | INFO | train_inner | epoch 133:     78 / 97 loss=1.068, ppl=2.1, wps=21115.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=1.065, loss_scale=16, train_wall=273, gb_free=8.2, wall=38673
2022-03-04 20:21:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:21:42 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 15.168 | ppl 36822.8 | wps 34881.7 | wpb 510.9 | bsz 1 | num_updates 12819 | best_loss 7.623
2022-03-04 20:21:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12819 updates
2022-03-04 20:21:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:21:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 133 @ 12819 updates, score 15.168) (writing took 3.7814223282039165 seconds)
2022-03-04 20:21:46 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 20:21:46 | INFO | train | epoch 133 | loss 1.068 | ppl 2.1 | wps 21319.5 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 12819 | lr 0.000279301 | gnorm 1.063 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 38739
2022-03-04 20:21:46 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 20:21:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:25:51 | INFO | train_inner | epoch 134:     82 / 97 loss=1.065, ppl=2.09, wps=20972.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=1.077, loss_scale=16, train_wall=274, gb_free=8.2, wall=38985
2022-03-04 20:26:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:26:42 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 15.139 | ppl 36071.3 | wps 36626.1 | wpb 510.9 | bsz 1 | num_updates 12915 | best_loss 7.623
2022-03-04 20:26:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12915 updates
2022-03-04 20:26:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:26:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 134 @ 12915 updates, score 15.139) (writing took 3.8468081168830395 seconds)
2022-03-04 20:26:46 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 20:26:46 | INFO | train | epoch 134 | loss 1.061 | ppl 2.09 | wps 20961.6 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12915 | lr 0.000278261 | gnorm 1.083 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 39039
2022-03-04 20:26:46 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 20:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:30:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:31:04 | INFO | train_inner | epoch 135:     86 / 97 loss=1.054, ppl=2.08, wps=20966.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=1.073, loss_scale=16, train_wall=275, gb_free=8.2, wall=39297
2022-03-04 20:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:31:43 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 15.142 | ppl 36151.8 | wps 36033 | wpb 510.9 | bsz 1 | num_updates 13011 | best_loss 7.623
2022-03-04 20:31:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13011 updates
2022-03-04 20:31:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:31:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:31:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 135 @ 13011 updates, score 15.142) (writing took 3.7368296552449465 seconds)
2022-03-04 20:31:46 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 20:31:46 | INFO | train | epoch 135 | loss 1.052 | ppl 2.07 | wps 20923 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13011 | lr 0.000277233 | gnorm 1.067 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 39340
2022-03-04 20:31:46 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 20:31:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:36:12 | INFO | train_inner | epoch 136:     89 / 97 loss=1.043, ppl=2.06, wps=21233.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13100, lr=0.000276289, gnorm=1.057, loss_scale=16, train_wall=271, gb_free=8.2, wall=39606
2022-03-04 20:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:36:42 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 15.251 | ppl 39001.3 | wps 36120.6 | wpb 510.9 | bsz 1 | num_updates 13108 | best_loss 7.623
2022-03-04 20:36:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13108 updates
2022-03-04 20:36:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:36:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:36:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 136 @ 13108 updates, score 15.251) (writing took 3.819204069674015 seconds)
2022-03-04 20:36:46 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 20:36:46 | INFO | train | epoch 136 | loss 1.041 | ppl 2.06 | wps 21215.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 13108 | lr 0.000276205 | gnorm 1.055 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 39639
2022-03-04 20:36:46 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 20:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:40:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:41:23 | INFO | train_inner | epoch 137:     93 / 97 loss=1.037, ppl=2.05, wps=21059.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13200, lr=0.000275241, gnorm=1.073, loss_scale=16, train_wall=273, gb_free=8.2, wall=39917
2022-03-04 20:41:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:41:41 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 15.329 | ppl 41174.1 | wps 35067.9 | wpb 510.9 | bsz 1 | num_updates 13204 | best_loss 7.623
2022-03-04 20:41:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13204 updates
2022-03-04 20:41:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:41:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:41:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 137 @ 13204 updates, score 15.329) (writing took 3.6913182232528925 seconds)
2022-03-04 20:41:45 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 20:41:45 | INFO | train | epoch 137 | loss 1.034 | ppl 2.05 | wps 20998.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13204 | lr 0.000275199 | gnorm 1.073 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 39939
2022-03-04 20:41:45 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 20:41:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:46:30 | INFO | train_inner | epoch 138:     96 / 97 loss=1.029, ppl=2.04, wps=21359.4, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13300, lr=0.000274204, gnorm=1.079, loss_scale=16, train_wall=270, gb_free=8.2, wall=40223
2022-03-04 20:46:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:46:39 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 15.408 | ppl 43488 | wps 35485.5 | wpb 510.9 | bsz 1 | num_updates 13301 | best_loss 7.623
2022-03-04 20:46:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13301 updates
2022-03-04 20:46:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:46:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:46:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 138 @ 13301 updates, score 15.408) (writing took 3.7308443896472454 seconds)
2022-03-04 20:46:43 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 20:46:43 | INFO | train | epoch 138 | loss 1.027 | ppl 2.04 | wps 21358.2 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 13301 | lr 0.000274194 | gnorm 1.079 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 40236
2022-03-04 20:46:43 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 20:46:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:51:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:51:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:51:37 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 15.395 | ppl 43086 | wps 35609.2 | wpb 510.9 | bsz 1 | num_updates 13397 | best_loss 7.623
2022-03-04 20:51:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13397 updates
2022-03-04 20:51:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:51:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:51:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 139 @ 13397 updates, score 15.395) (writing took 3.926065318286419 seconds)
2022-03-04 20:51:41 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 20:51:41 | INFO | train | epoch 139 | loss 1.017 | ppl 2.02 | wps 21050.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13397 | lr 0.00027321 | gnorm 1.061 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 40535
2022-03-04 20:51:41 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 20:51:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:51:50 | INFO | train_inner | epoch 140:      3 / 97 loss=1.016, ppl=2.02, wps=20429.5, ups=0.31, wpb=65451.9, bsz=127.8, num_updates=13400, lr=0.000273179, gnorm=1.06, loss_scale=16, train_wall=273, gb_free=8.2, wall=40544
2022-03-04 20:56:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:56:35 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 15.358 | ppl 42009.3 | wps 36720.3 | wpb 510.9 | bsz 1 | num_updates 13494 | best_loss 7.623
2022-03-04 20:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13494 updates
2022-03-04 20:56:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:56:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 20:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 140 @ 13494 updates, score 15.358) (writing took 3.9070971105247736 seconds)
2022-03-04 20:56:39 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 20:56:39 | INFO | train | epoch 140 | loss 1.01 | ppl 2.01 | wps 21309 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 13494 | lr 0.000272226 | gnorm 1.055 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 40833
2022-03-04 20:56:39 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 20:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:56:57 | INFO | train_inner | epoch 141:      6 / 97 loss=1.008, ppl=2.01, wps=21354.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1.053, loss_scale=16, train_wall=270, gb_free=8.2, wall=40850
2022-03-04 20:57:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:01:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:01:31 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 15.406 | ppl 43424.9 | wps 37218.7 | wpb 510.9 | bsz 1 | num_updates 13590 | best_loss 7.623
2022-03-04 21:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13590 updates
2022-03-04 21:01:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:01:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:01:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 141 @ 13590 updates, score 15.406) (writing took 4.185072658583522 seconds)
2022-03-04 21:01:35 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 21:01:35 | INFO | train | epoch 141 | loss 1.002 | ppl 2 | wps 21239.6 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13590 | lr 0.000271263 | gnorm 1.077 | loss_scale 16 | train_wall 260 | gb_free 8.2 | wall 41129
2022-03-04 21:01:35 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 21:01:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:02:05 | INFO | train_inner | epoch 142:     10 / 97 loss=1.001, ppl=2, wps=21269.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=1.08, loss_scale=16, train_wall=271, gb_free=8.2, wall=41158
2022-03-04 21:04:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:06:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:06:26 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 15.477 | ppl 45603.8 | wps 36663.2 | wpb 510.9 | bsz 1 | num_updates 13686 | best_loss 7.623
2022-03-04 21:06:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13686 updates
2022-03-04 21:06:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:06:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 142 @ 13686 updates, score 15.477) (writing took 3.8244839068502188 seconds)
2022-03-04 21:06:30 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 21:06:30 | INFO | train | epoch 142 | loss 0.994 | ppl 1.99 | wps 21316.7 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 13686 | lr 0.00027031 | gnorm 1.059 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 41424
2022-03-04 21:06:30 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 21:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:07:11 | INFO | train_inner | epoch 143:     14 / 97 loss=0.989, ppl=1.98, wps=21363.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1.056, loss_scale=16, train_wall=270, gb_free=8.2, wall=41465
2022-03-04 21:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:11:22 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 15.541 | ppl 47674.6 | wps 37236.5 | wpb 510.9 | bsz 1 | num_updates 13783 | best_loss 7.623
2022-03-04 21:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13783 updates
2022-03-04 21:11:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:11:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:11:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 143 @ 13783 updates, score 15.541) (writing took 3.7329938020557165 seconds)
2022-03-04 21:11:26 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 21:11:26 | INFO | train | epoch 143 | loss 0.986 | ppl 1.98 | wps 21477.7 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 13783 | lr 0.000269357 | gnorm 1.045 | loss_scale 32 | train_wall 260 | gb_free 8.2 | wall 41719
2022-03-04 21:11:26 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 21:11:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:12:19 | INFO | train_inner | epoch 144:     18 / 97 loss=0.987, ppl=1.98, wps=21312.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=1.046, loss_scale=16, train_wall=271, gb_free=8.2, wall=41772
2022-03-04 21:16:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:16:16 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 15.516 | ppl 46861.6 | wps 37575.5 | wpb 510.9 | bsz 1 | num_updates 13879 | best_loss 7.623
2022-03-04 21:16:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13879 updates
2022-03-04 21:16:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:16:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:16:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 144 @ 13879 updates, score 15.516) (writing took 4.0148983132094145 seconds)
2022-03-04 21:16:20 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 21:16:20 | INFO | train | epoch 144 | loss 0.98 | ppl 1.97 | wps 21417.3 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 13879 | lr 0.000268424 | gnorm 1.056 | loss_scale 16 | train_wall 258 | gb_free 8.2 | wall 42013
2022-03-04 21:16:20 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 21:16:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:17:21 | INFO | train_inner | epoch 145:     21 / 97 loss=0.976, ppl=1.97, wps=21650.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=1.048, loss_scale=16, train_wall=266, gb_free=8.2, wall=42075
2022-03-04 21:18:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:21:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:21:11 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 15.505 | ppl 46495.9 | wps 36927 | wpb 510.9 | bsz 1 | num_updates 13975 | best_loss 7.623
2022-03-04 21:21:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13975 updates
2022-03-04 21:21:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:21:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:21:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 145 @ 13975 updates, score 15.505) (writing took 3.3761300332844257 seconds)
2022-03-04 21:21:14 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 21:21:14 | INFO | train | epoch 145 | loss 0.971 | ppl 1.96 | wps 21356.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 13975 | lr 0.0002675 | gnorm 1.051 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 42307
2022-03-04 21:21:14 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 21:21:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:22:27 | INFO | train_inner | epoch 146:     25 / 97 loss=0.971, ppl=1.96, wps=21408.6, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=1.055, loss_scale=16, train_wall=270, gb_free=8.2, wall=42381
2022-03-04 21:25:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:26:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:26:07 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 15.589 | ppl 49306.5 | wps 34665.4 | wpb 510.9 | bsz 1 | num_updates 14071 | best_loss 7.623
2022-03-04 21:26:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14071 updates
2022-03-04 21:26:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:26:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:26:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 146 @ 14071 updates, score 15.589) (writing took 3.889196926727891 seconds)
2022-03-04 21:26:11 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 21:26:11 | INFO | train | epoch 146 | loss 0.963 | ppl 1.95 | wps 21169.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14071 | lr 0.000266586 | gnorm 1.04 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 42604
2022-03-04 21:26:11 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 21:26:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:27:37 | INFO | train_inner | epoch 147:     29 / 97 loss=0.961, ppl=1.95, wps=21153.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=1.043, loss_scale=16, train_wall=272, gb_free=8.2, wall=42690
2022-03-04 21:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:31:05 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 15.631 | ppl 50743.1 | wps 35486.8 | wpb 510.9 | bsz 1 | num_updates 14168 | best_loss 7.623
2022-03-04 21:31:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14168 updates
2022-03-04 21:31:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:31:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:31:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 147 @ 14168 updates, score 15.631) (writing took 3.7690257746726274 seconds)
2022-03-04 21:31:08 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 21:31:08 | INFO | train | epoch 147 | loss 0.958 | ppl 1.94 | wps 21357.1 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 14168 | lr 0.000265672 | gnorm 1.049 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 42902
2022-03-04 21:31:08 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 21:31:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:32:42 | INFO | train_inner | epoch 148:     32 / 97 loss=0.954, ppl=1.94, wps=21421.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=1.045, loss_scale=32, train_wall=269, gb_free=8.2, wall=42996
2022-03-04 21:32:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:35:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:35:59 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 15.658 | ppl 51698.3 | wps 36154 | wpb 510.9 | bsz 1 | num_updates 14264 | best_loss 7.623
2022-03-04 21:35:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14264 updates
2022-03-04 21:35:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:36:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:36:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 148 @ 14264 updates, score 15.658) (writing took 4.3844647239893675 seconds)
2022-03-04 21:36:04 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 21:36:04 | INFO | train | epoch 148 | loss 0.951 | ppl 1.93 | wps 21302.7 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 14264 | lr 0.000264776 | gnorm 1.041 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 43197
2022-03-04 21:36:04 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 21:36:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:37:50 | INFO | train_inner | epoch 149:     36 / 97 loss=0.948, ppl=1.93, wps=21314.3, ups=0.33, wpb=65495, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=1.041, loss_scale=16, train_wall=270, gb_free=8.2, wall=43303
2022-03-04 21:39:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:40:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:40:56 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 15.803 | ppl 57188.2 | wps 35726.8 | wpb 510.9 | bsz 1 | num_updates 14360 | best_loss 7.623
2022-03-04 21:40:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14360 updates
2022-03-04 21:40:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:40:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 149 @ 14360 updates, score 15.803) (writing took 3.623899009078741 seconds)
2022-03-04 21:40:59 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-04 21:40:59 | INFO | train | epoch 149 | loss 0.945 | ppl 1.92 | wps 21266.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14360 | lr 0.00026389 | gnorm 1.053 | loss_scale 16 | train_wall 260 | gb_free 8.2 | wall 43493
2022-03-04 21:40:59 | INFO | fairseq.trainer | begin training epoch 150
2022-03-04 21:40:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:42:58 | INFO | train_inner | epoch 150:     40 / 97 loss=0.943, ppl=1.92, wps=21277.9, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=1.053, loss_scale=16, train_wall=271, gb_free=8.2, wall=43611
2022-03-04 21:45:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:45:50 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 15.768 | ppl 55816.5 | wps 37792.2 | wpb 510.9 | bsz 1 | num_updates 14457 | best_loss 7.623
2022-03-04 21:45:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14457 updates
2022-03-04 21:45:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:45:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:45:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 150 @ 14457 updates, score 15.768) (writing took 3.6454899348318577 seconds)
2022-03-04 21:45:53 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-04 21:45:53 | INFO | train | epoch 150 | loss 0.938 | ppl 1.92 | wps 21613 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 14457 | lr 0.000263003 | gnorm 1.042 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 43787
2022-03-04 21:45:53 | INFO | fairseq.trainer | begin training epoch 151
2022-03-04 21:45:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:48:00 | INFO | train_inner | epoch 151:     43 / 97 loss=0.934, ppl=1.91, wps=21625.6, ups=0.33, wpb=65495, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=1.04, loss_scale=32, train_wall=267, gb_free=8.2, wall=43914
2022-03-04 21:49:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:50:44 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 15.757 | ppl 55385.2 | wps 37353.3 | wpb 510.9 | bsz 1 | num_updates 14553 | best_loss 7.623
2022-03-04 21:50:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14553 updates
2022-03-04 21:50:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:50:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:50:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 151 @ 14553 updates, score 15.757) (writing took 3.6523704044520855 seconds)
2022-03-04 21:50:48 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-04 21:50:48 | INFO | train | epoch 151 | loss 0.93 | ppl 1.91 | wps 21358.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 14553 | lr 0.000262134 | gnorm 1.048 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 44081
2022-03-04 21:50:48 | INFO | fairseq.trainer | begin training epoch 152
2022-03-04 21:50:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:53:06 | INFO | train_inner | epoch 152:     47 / 97 loss=0.925, ppl=1.9, wps=21404.7, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=14600, lr=0.000261712, gnorm=1.041, loss_scale=16, train_wall=270, gb_free=8.2, wall=44220
2022-03-04 21:55:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:55:39 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 15.813 | ppl 57562.2 | wps 36047.7 | wpb 510.9 | bsz 1 | num_updates 14650 | best_loss 7.623
2022-03-04 21:55:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14650 updates
2022-03-04 21:55:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:55:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 21:55:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 152 @ 14650 updates, score 15.813) (writing took 4.304747566580772 seconds)
2022-03-04 21:55:43 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-04 21:55:43 | INFO | train | epoch 152 | loss 0.925 | ppl 1.9 | wps 21492.8 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 14650 | lr 0.000261265 | gnorm 1.055 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 44376
2022-03-04 21:55:43 | INFO | fairseq.trainer | begin training epoch 153
2022-03-04 21:55:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:56:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:58:15 | INFO | train_inner | epoch 153:     51 / 97 loss=0.925, ppl=1.9, wps=21223.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=1.066, loss_scale=16, train_wall=271, gb_free=8.2, wall=44528
2022-03-04 22:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:00:36 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 15.86 | ppl 59478.9 | wps 35478.8 | wpb 510.9 | bsz 1 | num_updates 14746 | best_loss 7.623
2022-03-04 22:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14746 updates
2022-03-04 22:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:00:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:00:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 153 @ 14746 updates, score 15.86) (writing took 4.3290680311620235 seconds)
2022-03-04 22:00:41 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-04 22:00:41 | INFO | train | epoch 153 | loss 0.919 | ppl 1.89 | wps 21136 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14746 | lr 0.000260413 | gnorm 1.051 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 44674
2022-03-04 22:00:41 | INFO | fairseq.trainer | begin training epoch 154
2022-03-04 22:00:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:03:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:03:25 | INFO | train_inner | epoch 154:     55 / 97 loss=0.915, ppl=1.89, wps=21137.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=1.042, loss_scale=16, train_wall=272, gb_free=8.2, wall=44838
2022-03-04 22:05:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:05:35 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 15.94 | ppl 62866.8 | wps 35899.4 | wpb 510.9 | bsz 1 | num_updates 14842 | best_loss 7.623
2022-03-04 22:05:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14842 updates
2022-03-04 22:05:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:05:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:05:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 154 @ 14842 updates, score 15.94) (writing took 3.9659502618014812 seconds)
2022-03-04 22:05:38 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-04 22:05:38 | INFO | train | epoch 154 | loss 0.911 | ppl 1.88 | wps 21104.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14842 | lr 0.00025957 | gnorm 1.036 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 44972
2022-03-04 22:05:38 | INFO | fairseq.trainer | begin training epoch 155
2022-03-04 22:05:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:08:31 | INFO | train_inner | epoch 155:     58 / 97 loss=0.909, ppl=1.88, wps=21357.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=1.042, loss_scale=16, train_wall=269, gb_free=8.2, wall=45145
2022-03-04 22:10:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:10:32 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 15.995 | ppl 65300.5 | wps 36378.2 | wpb 510.9 | bsz 1 | num_updates 14938 | best_loss 7.623
2022-03-04 22:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14938 updates
2022-03-04 22:10:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 155 @ 14938 updates, score 15.995) (writing took 3.9387693367898464 seconds)
2022-03-04 22:10:36 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-04 22:10:36 | INFO | train | epoch 155 | loss 0.907 | ppl 1.88 | wps 21102.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14938 | lr 0.000258734 | gnorm 1.046 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 45270
2022-03-04 22:10:36 | INFO | fairseq.trainer | begin training epoch 156
2022-03-04 22:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:13:41 | INFO | train_inner | epoch 156:     62 / 97 loss=0.9, ppl=1.87, wps=21156.4, ups=0.32, wpb=65495, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=1.037, loss_scale=16, train_wall=272, gb_free=8.2, wall=45454
2022-03-04 22:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:15:30 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 15.917 | ppl 61859 | wps 36248.9 | wpb 510.9 | bsz 1 | num_updates 15035 | best_loss 7.623
2022-03-04 22:15:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15035 updates
2022-03-04 22:15:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:15:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 156 @ 15035 updates, score 15.917) (writing took 3.925810454413295 seconds)
2022-03-04 22:15:34 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-04 22:15:34 | INFO | train | epoch 156 | loss 0.899 | ppl 1.86 | wps 21337.9 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 15035 | lr 0.000257898 | gnorm 1.029 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 45568
2022-03-04 22:15:34 | INFO | fairseq.trainer | begin training epoch 157
2022-03-04 22:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:18:48 | INFO | train_inner | epoch 157:     65 / 97 loss=0.898, ppl=1.86, wps=21355.2, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=1.026, loss_scale=32, train_wall=270, gb_free=8.2, wall=45761
2022-03-04 22:19:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:20:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:20:28 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 15.87 | ppl 59876.3 | wps 36956 | wpb 510.9 | bsz 1 | num_updates 15131 | best_loss 7.623
2022-03-04 22:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15131 updates
2022-03-04 22:20:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:20:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:20:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 157 @ 15131 updates, score 15.87) (writing took 3.789992919191718 seconds)
2022-03-04 22:20:32 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-04 22:20:32 | INFO | train | epoch 157 | loss 0.895 | ppl 1.86 | wps 21136.6 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15131 | lr 0.000257079 | gnorm 1.026 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 45865
2022-03-04 22:20:32 | INFO | fairseq.trainer | begin training epoch 158
2022-03-04 22:20:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:23:56 | INFO | train_inner | epoch 158:     69 / 97 loss=0.895, ppl=1.86, wps=21226, ups=0.32, wpb=65495, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=1.033, loss_scale=16, train_wall=272, gb_free=8.2, wall=46070
2022-03-04 22:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:25:25 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 16.05 | ppl 67843.5 | wps 36689.8 | wpb 510.9 | bsz 1 | num_updates 15228 | best_loss 7.623
2022-03-04 22:25:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15228 updates
2022-03-04 22:25:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:25:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:25:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 158 @ 15228 updates, score 16.05) (writing took 3.846004042774439 seconds)
2022-03-04 22:25:29 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-04 22:25:29 | INFO | train | epoch 158 | loss 0.889 | ppl 1.85 | wps 21344.1 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 15228 | lr 0.000256259 | gnorm 1.029 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 46163
2022-03-04 22:25:29 | INFO | fairseq.trainer | begin training epoch 159
2022-03-04 22:25:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:26:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:29:06 | INFO | train_inner | epoch 159:     73 / 97 loss=0.883, ppl=1.84, wps=21150.1, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=1.033, loss_scale=16, train_wall=272, gb_free=8.2, wall=46379
2022-03-04 22:30:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:30:23 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 16.095 | ppl 69974.3 | wps 37363.5 | wpb 510.9 | bsz 1 | num_updates 15324 | best_loss 7.623
2022-03-04 22:30:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15324 updates
2022-03-04 22:30:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:30:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 159 @ 15324 updates, score 16.095) (writing took 3.845688285306096 seconds)
2022-03-04 22:30:27 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-04 22:30:27 | INFO | train | epoch 159 | loss 0.882 | ppl 1.84 | wps 21129.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15324 | lr 0.000255455 | gnorm 1.035 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 46460
2022-03-04 22:30:27 | INFO | fairseq.trainer | begin training epoch 160
2022-03-04 22:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:34:09 | INFO | train_inner | epoch 160:     76 / 97 loss=0.879, ppl=1.84, wps=21612.9, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=1.036, loss_scale=32, train_wall=267, gb_free=8.2, wall=46682
2022-03-04 22:34:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:35:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:35:17 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 16.006 | ppl 65792.9 | wps 36519 | wpb 510.9 | bsz 1 | num_updates 15420 | best_loss 7.623
2022-03-04 22:35:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15420 updates
2022-03-04 22:35:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:35:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:35:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 160 @ 15420 updates, score 16.006) (writing took 3.790848523378372 seconds)
2022-03-04 22:35:21 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-04 22:35:21 | INFO | train | epoch 160 | loss 0.878 | ppl 1.84 | wps 21385.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 15420 | lr 0.000254658 | gnorm 1.047 | loss_scale 16 | train_wall 258 | gb_free 8.2 | wall 46754
2022-03-04 22:35:21 | INFO | fairseq.trainer | begin training epoch 161
2022-03-04 22:35:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:39:16 | INFO | train_inner | epoch 161:     80 / 97 loss=0.874, ppl=1.83, wps=21348.7, ups=0.33, wpb=65495, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=1.035, loss_scale=16, train_wall=270, gb_free=8.2, wall=46989
2022-03-04 22:40:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:40:12 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 16.08 | ppl 69267.8 | wps 36349.2 | wpb 510.9 | bsz 1 | num_updates 15517 | best_loss 7.623
2022-03-04 22:40:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15517 updates
2022-03-04 22:40:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:40:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 161 @ 15517 updates, score 16.08) (writing took 3.473225263878703 seconds)
2022-03-04 22:40:16 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-04 22:40:16 | INFO | train | epoch 161 | loss 0.87 | ppl 1.83 | wps 21549 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 15517 | lr 0.000253861 | gnorm 1.023 | loss_scale 16 | train_wall 260 | gb_free 8.2 | wall 47049
2022-03-04 22:40:16 | INFO | fairseq.trainer | begin training epoch 162
2022-03-04 22:40:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:43:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:44:24 | INFO | train_inner | epoch 162:     84 / 97 loss=0.866, ppl=1.82, wps=21265.4, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=1.031, loss_scale=16, train_wall=271, gb_free=8.2, wall=47297
2022-03-04 22:45:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:45:09 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 15.976 | ppl 64465.8 | wps 35565.3 | wpb 510.9 | bsz 1 | num_updates 15613 | best_loss 7.623
2022-03-04 22:45:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15613 updates
2022-03-04 22:45:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:45:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:45:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 162 @ 15613 updates, score 15.976) (writing took 4.0222944263368845 seconds)
2022-03-04 22:45:13 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-04 22:45:13 | INFO | train | epoch 162 | loss 0.866 | ppl 1.82 | wps 21166.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15613 | lr 0.000253079 | gnorm 1.032 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 47346
2022-03-04 22:45:13 | INFO | fairseq.trainer | begin training epoch 163
2022-03-04 22:45:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:49:30 | INFO | train_inner | epoch 163:     87 / 97 loss=0.862, ppl=1.82, wps=21391.7, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=15700, lr=0.000252377, gnorm=1.02, loss_scale=16, train_wall=269, gb_free=8.2, wall=47603
2022-03-04 22:50:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:50:06 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 16.011 | ppl 66058.8 | wps 35919.1 | wpb 510.9 | bsz 1 | num_updates 15710 | best_loss 7.623
2022-03-04 22:50:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15710 updates
2022-03-04 22:50:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:50:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 163 @ 15710 updates, score 16.011) (writing took 4.126040771603584 seconds)
2022-03-04 22:50:10 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-04 22:50:10 | INFO | train | epoch 163 | loss 0.86 | ppl 1.82 | wps 21365.4 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 15710 | lr 0.000252297 | gnorm 1.02 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 47643
2022-03-04 22:50:10 | INFO | fairseq.trainer | begin training epoch 164
2022-03-04 22:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:52:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:54:40 | INFO | train_inner | epoch 164:     91 / 97 loss=0.858, ppl=1.81, wps=21139, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=15800, lr=0.000251577, gnorm=1.03, loss_scale=16, train_wall=272, gb_free=8.2, wall=47913
2022-03-04 22:54:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:55:04 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 16.085 | ppl 69522.2 | wps 35507.5 | wpb 510.9 | bsz 1 | num_updates 15806 | best_loss 7.623
2022-03-04 22:55:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15806 updates
2022-03-04 22:55:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:55:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 22:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 164 @ 15806 updates, score 16.085) (writing took 3.7790969740599394 seconds)
2022-03-04 22:55:08 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-04 22:55:08 | INFO | train | epoch 164 | loss 0.855 | ppl 1.81 | wps 21127.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15806 | lr 0.00025153 | gnorm 1.027 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 47941
2022-03-04 22:55:08 | INFO | fairseq.trainer | begin training epoch 165
2022-03-04 22:55:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:59:49 | INFO | train_inner | epoch 165:     95 / 97 loss=0.851, ppl=1.8, wps=21173.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=15900, lr=0.000250785, gnorm=1.028, loss_scale=16, train_wall=272, gb_free=8.2, wall=48222
2022-03-04 22:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:00:01 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 16.202 | ppl 75410.9 | wps 35502.8 | wpb 510.9 | bsz 1 | num_updates 15902 | best_loss 7.623
2022-03-04 23:00:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15902 updates
2022-03-04 23:00:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:00:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 165 @ 15902 updates, score 16.202) (writing took 3.8746995758265257 seconds)
2022-03-04 23:00:05 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-04 23:00:05 | INFO | train | epoch 165 | loss 0.849 | ppl 1.8 | wps 21142 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15902 | lr 0.000250769 | gnorm 1.028 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 48238
2022-03-04 23:00:05 | INFO | fairseq.trainer | begin training epoch 166
2022-03-04 23:00:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:04:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:04:59 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 16.175 | ppl 74002.1 | wps 35533.6 | wpb 510.9 | bsz 1 | num_updates 15999 | best_loss 7.623
2022-03-04 23:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15999 updates
2022-03-04 23:04:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:05:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 166 @ 15999 updates, score 16.175) (writing took 3.738407811149955 seconds)
2022-03-04 23:05:03 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-04 23:05:03 | INFO | train | epoch 166 | loss 0.846 | ppl 1.8 | wps 21343.1 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 15999 | lr 0.000250008 | gnorm 1.022 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 48536
2022-03-04 23:05:03 | INFO | fairseq.trainer | begin training epoch 167
2022-03-04 23:05:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:05:06 | INFO | train_inner | epoch 167:      1 / 97 loss=0.846, ppl=1.8, wps=20673.5, ups=0.32, wpb=65451.9, bsz=127.8, num_updates=16000, lr=0.00025, gnorm=1.023, loss_scale=16, train_wall=269, gb_free=8.2, wall=48539
2022-03-04 23:07:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:09:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:09:56 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 16.279 | ppl 79512.7 | wps 35992.5 | wpb 510.9 | bsz 1 | num_updates 16095 | best_loss 7.623
2022-03-04 23:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16095 updates
2022-03-04 23:09:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:10:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:10:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 167 @ 16095 updates, score 16.279) (writing took 3.875575542449951 seconds)
2022-03-04 23:10:00 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-04 23:10:00 | INFO | train | epoch 167 | loss 0.84 | ppl 1.79 | wps 21144.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 16095 | lr 0.000249261 | gnorm 1.022 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 48833
2022-03-04 23:10:00 | INFO | fairseq.trainer | begin training epoch 168
2022-03-04 23:10:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:10:15 | INFO | train_inner | epoch 168:      5 / 97 loss=0.839, ppl=1.79, wps=21168.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=1.024, loss_scale=16, train_wall=272, gb_free=8.2, wall=48848
2022-03-04 23:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:14:56 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 16.207 | ppl 75621.8 | wps 34463.7 | wpb 510.9 | bsz 1 | num_updates 16192 | best_loss 7.623
2022-03-04 23:14:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16192 updates
2022-03-04 23:14:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:15:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 168 @ 16192 updates, score 16.207) (writing took 4.135231081396341 seconds)
2022-03-04 23:15:00 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-04 23:15:00 | INFO | train | epoch 168 | loss 0.836 | ppl 1.78 | wps 21134.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 16192 | lr 0.000248513 | gnorm 1.034 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 49134
2022-03-04 23:15:00 | INFO | fairseq.trainer | begin training epoch 169
2022-03-04 23:15:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:15:25 | INFO | train_inner | epoch 169:      8 / 97 loss=0.834, ppl=1.78, wps=21118.8, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=1.029, loss_scale=32, train_wall=272, gb_free=8.2, wall=49159
2022-03-04 23:16:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:19:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:20:04 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 16.261 | ppl 78537.7 | wps 34494.4 | wpb 510.9 | bsz 1 | num_updates 16288 | best_loss 7.623
2022-03-04 23:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16288 updates
2022-03-04 23:20:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:20:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:20:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 169 @ 16288 updates, score 16.261) (writing took 3.8016725052148104 seconds)
2022-03-04 23:20:07 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-04 23:20:07 | INFO | train | epoch 169 | loss 0.829 | ppl 1.78 | wps 20490 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16288 | lr 0.00024778 | gnorm 1.004 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 49441
2022-03-04 23:20:07 | INFO | fairseq.trainer | begin training epoch 170
2022-03-04 23:20:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:20:44 | INFO | train_inner | epoch 170:     12 / 97 loss=0.827, ppl=1.77, wps=20536.4, ups=0.31, wpb=65495, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=1.008, loss_scale=16, train_wall=280, gb_free=8.2, wall=49477
2022-03-04 23:24:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:25:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:25:10 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 16.233 | ppl 77020.6 | wps 34232.5 | wpb 510.9 | bsz 1 | num_updates 16384 | best_loss 7.623
2022-03-04 23:25:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16384 updates
2022-03-04 23:25:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:25:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:25:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 170 @ 16384 updates, score 16.233) (writing took 3.812707943841815 seconds)
2022-03-04 23:25:14 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-04 23:25:14 | INFO | train | epoch 170 | loss 0.824 | ppl 1.77 | wps 20479.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16384 | lr 0.000247053 | gnorm 1.015 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 49748
2022-03-04 23:25:14 | INFO | fairseq.trainer | begin training epoch 171
2022-03-04 23:25:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:26:03 | INFO | train_inner | epoch 171:     16 / 97 loss=0.822, ppl=1.77, wps=20518.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=1.011, loss_scale=16, train_wall=281, gb_free=8.2, wall=49797
2022-03-04 23:30:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:30:19 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 16.202 | ppl 75407.2 | wps 34223 | wpb 510.9 | bsz 1 | num_updates 16481 | best_loss 7.623
2022-03-04 23:30:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16481 updates
2022-03-04 23:30:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:30:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:30:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 171 @ 16481 updates, score 16.202) (writing took 3.8570491038262844 seconds)
2022-03-04 23:30:22 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-04 23:30:22 | INFO | train | epoch 171 | loss 0.82 | ppl 1.77 | wps 20614.6 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 16481 | lr 0.000246325 | gnorm 1.025 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 50056
2022-03-04 23:30:22 | INFO | fairseq.trainer | begin training epoch 172
2022-03-04 23:30:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:31:21 | INFO | train_inner | epoch 172:     19 / 97 loss=0.817, ppl=1.76, wps=20635.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=1.027, loss_scale=32, train_wall=279, gb_free=8.2, wall=50114
2022-03-04 23:33:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:35:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:35:27 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 16.363 | ppl 84295.4 | wps 33923.2 | wpb 510.9 | bsz 1 | num_updates 16577 | best_loss 7.623
2022-03-04 23:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16577 updates
2022-03-04 23:35:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:35:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:35:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 172 @ 16577 updates, score 16.363) (writing took 3.787395378574729 seconds)
2022-03-04 23:35:30 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-04 23:35:30 | INFO | train | epoch 172 | loss 0.813 | ppl 1.76 | wps 20414.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16577 | lr 0.000245611 | gnorm 1.009 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 50364
2022-03-04 23:35:30 | INFO | fairseq.trainer | begin training epoch 173
2022-03-04 23:35:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:36:41 | INFO | train_inner | epoch 173:     23 / 97 loss=0.813, ppl=1.76, wps=20436.6, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=1.006, loss_scale=16, train_wall=281, gb_free=8.2, wall=50434
2022-03-04 23:40:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:40:36 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 16.432 | ppl 88409.1 | wps 33953.5 | wpb 510.9 | bsz 1 | num_updates 16674 | best_loss 7.623
2022-03-04 23:40:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16674 updates
2022-03-04 23:40:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:40:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:40:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 173 @ 16674 updates, score 16.432) (writing took 3.8232636339962482 seconds)
2022-03-04 23:40:40 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-04 23:40:40 | INFO | train | epoch 173 | loss 0.811 | ppl 1.75 | wps 20545.5 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 16674 | lr 0.000244895 | gnorm 1.017 | loss_scale 32 | train_wall 271 | gb_free 8.2 | wall 50673
2022-03-04 23:40:40 | INFO | fairseq.trainer | begin training epoch 174
2022-03-04 23:40:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:40:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:42:03 | INFO | train_inner | epoch 174:     27 / 97 loss=0.809, ppl=1.75, wps=20373.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=1.014, loss_scale=16, train_wall=282, gb_free=8.2, wall=50756
2022-03-04 23:45:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:45:45 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 16.419 | ppl 87625.2 | wps 33822.5 | wpb 510.9 | bsz 1 | num_updates 16770 | best_loss 7.623
2022-03-04 23:45:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16770 updates
2022-03-04 23:45:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:45:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 174 @ 16770 updates, score 16.419) (writing took 3.7992645986378193 seconds)
2022-03-04 23:45:48 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-04 23:45:48 | INFO | train | epoch 174 | loss 0.804 | ppl 1.75 | wps 20362.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16770 | lr 0.000244193 | gnorm 1.013 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 50982
2022-03-04 23:45:48 | INFO | fairseq.trainer | begin training epoch 175
2022-03-04 23:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:47:20 | INFO | train_inner | epoch 175:     30 / 97 loss=0.8, ppl=1.74, wps=20601.4, ups=0.31, wpb=65495, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=1.009, loss_scale=16, train_wall=279, gb_free=8.2, wall=51074
2022-03-04 23:49:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:50:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:50:53 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 16.37 | ppl 84689.2 | wps 33671.1 | wpb 510.9 | bsz 1 | num_updates 16866 | best_loss 7.623
2022-03-04 23:50:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16866 updates
2022-03-04 23:50:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 175 @ 16866 updates, score 16.37) (writing took 3.854881301522255 seconds)
2022-03-04 23:50:57 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-04 23:50:57 | INFO | train | epoch 175 | loss 0.801 | ppl 1.74 | wps 20374 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 1.012 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 51290
2022-03-04 23:50:57 | INFO | fairseq.trainer | begin training epoch 176
2022-03-04 23:50:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:52:41 | INFO | train_inner | epoch 176:     34 / 97 loss=0.8, ppl=1.74, wps=20413.8, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=16900, lr=0.000243252, gnorm=1.012, loss_scale=16, train_wall=282, gb_free=8.2, wall=51395
2022-03-04 23:55:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:56:00 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 16.398 | ppl 86361.8 | wps 33946.5 | wpb 510.9 | bsz 1 | num_updates 16963 | best_loss 7.623
2022-03-04 23:56:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16963 updates
2022-03-04 23:56:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:56:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-04 23:56:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 176 @ 16963 updates, score 16.398) (writing took 3.850070348009467 seconds)
2022-03-04 23:56:04 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-04 23:56:04 | INFO | train | epoch 176 | loss 0.796 | ppl 1.74 | wps 20718.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 16963 | lr 0.0002428 | gnorm 1.011 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 51597
2022-03-04 23:56:04 | INFO | fairseq.trainer | begin training epoch 177
2022-03-04 23:56:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:57:57 | INFO | train_inner | epoch 177:     37 / 97 loss=0.796, ppl=1.74, wps=20751.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=1.018, loss_scale=32, train_wall=277, gb_free=8.2, wall=51710
2022-03-05 00:00:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:01:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:01:06 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 16.406 | ppl 86835 | wps 33617.1 | wpb 510.9 | bsz 1 | num_updates 17059 | best_loss 7.623
2022-03-05 00:01:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17059 updates
2022-03-05 00:01:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:01:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:01:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 177 @ 17059 updates, score 16.406) (writing took 3.7887999583035707 seconds)
2022-03-05 00:01:10 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-05 00:01:10 | INFO | train | epoch 177 | loss 0.793 | ppl 1.73 | wps 20511.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17059 | lr 0.000242116 | gnorm 1.011 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 51904
2022-03-05 00:01:10 | INFO | fairseq.trainer | begin training epoch 178
2022-03-05 00:01:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:03:16 | INFO | train_inner | epoch 178:     41 / 97 loss=0.79, ppl=1.73, wps=20504.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=1.007, loss_scale=16, train_wall=281, gb_free=8.2, wall=52030
2022-03-05 00:06:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:06:15 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 16.497 | ppl 92505.5 | wps 33566.2 | wpb 510.9 | bsz 1 | num_updates 17156 | best_loss 7.623
2022-03-05 00:06:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17156 updates
2022-03-05 00:06:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:06:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:06:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 178 @ 17156 updates, score 16.497) (writing took 3.8843452632427216 seconds)
2022-03-05 00:06:19 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-05 00:06:19 | INFO | train | epoch 178 | loss 0.787 | ppl 1.73 | wps 20548.2 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 17156 | lr 0.00024143 | gnorm 1.004 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 52213
2022-03-05 00:06:19 | INFO | fairseq.trainer | begin training epoch 179
2022-03-05 00:06:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:08:36 | INFO | train_inner | epoch 179:     44 / 97 loss=0.786, ppl=1.72, wps=20495.7, ups=0.31, wpb=65495, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=1.001, loss_scale=32, train_wall=280, gb_free=8.2, wall=52349
2022-03-05 00:08:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:11:26 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 16.483 | ppl 91621.6 | wps 33838.6 | wpb 510.9 | bsz 1 | num_updates 17252 | best_loss 7.623
2022-03-05 00:11:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17252 updates
2022-03-05 00:11:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 179 @ 17252 updates, score 16.483) (writing took 3.7787132635712624 seconds)
2022-03-05 00:11:30 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-05 00:11:30 | INFO | train | epoch 179 | loss 0.783 | ppl 1.72 | wps 20252.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17252 | lr 0.000240758 | gnorm 1.008 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 52523
2022-03-05 00:11:30 | INFO | fairseq.trainer | begin training epoch 180
2022-03-05 00:11:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:13:59 | INFO | train_inner | epoch 180:     48 / 97 loss=0.781, ppl=1.72, wps=20273.6, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=1.001, loss_scale=16, train_wall=283, gb_free=8.2, wall=52672
2022-03-05 00:15:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:16:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:16:36 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 16.493 | ppl 92231.8 | wps 33628.5 | wpb 510.9 | bsz 1 | num_updates 17348 | best_loss 7.623
2022-03-05 00:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17348 updates
2022-03-05 00:16:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:16:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 180 @ 17348 updates, score 16.493) (writing took 3.8548905942589045 seconds)
2022-03-05 00:16:40 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-05 00:16:40 | INFO | train | epoch 180 | loss 0.778 | ppl 1.72 | wps 20244.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17348 | lr 0.000240091 | gnorm 1.002 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 52834
2022-03-05 00:16:40 | INFO | fairseq.trainer | begin training epoch 181
2022-03-05 00:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:19:21 | INFO | train_inner | epoch 181:     52 / 97 loss=0.776, ppl=1.71, wps=20301.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=1.012, loss_scale=16, train_wall=283, gb_free=8.2, wall=52995
2022-03-05 00:21:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:21:48 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 16.513 | ppl 93512.1 | wps 32728.7 | wpb 510.9 | bsz 1 | num_updates 17445 | best_loss 7.623
2022-03-05 00:21:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17445 updates
2022-03-05 00:21:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:21:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:21:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 181 @ 17445 updates, score 16.513) (writing took 3.8820351641625166 seconds)
2022-03-05 00:21:51 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-05 00:21:51 | INFO | train | epoch 181 | loss 0.776 | ppl 1.71 | wps 20420.5 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 17445 | lr 0.000239422 | gnorm 1.009 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 53145
2022-03-05 00:21:51 | INFO | fairseq.trainer | begin training epoch 182
2022-03-05 00:21:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:23:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:24:45 | INFO | train_inner | epoch 182:     56 / 97 loss=0.773, ppl=1.71, wps=20218.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=1.006, loss_scale=16, train_wall=284, gb_free=8.2, wall=53319
2022-03-05 00:26:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:26:59 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 16.601 | ppl 99395 | wps 33638 | wpb 510.9 | bsz 1 | num_updates 17541 | best_loss 7.623
2022-03-05 00:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17541 updates
2022-03-05 00:26:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:27:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:27:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 182 @ 17541 updates, score 16.601) (writing took 3.8259943034499884 seconds)
2022-03-05 00:27:02 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-05 00:27:02 | INFO | train | epoch 182 | loss 0.77 | ppl 1.71 | wps 20219.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17541 | lr 0.000238766 | gnorm 0.998 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 53456
2022-03-05 00:27:02 | INFO | fairseq.trainer | begin training epoch 183
2022-03-05 00:27:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:30:05 | INFO | train_inner | epoch 183:     59 / 97 loss=0.769, ppl=1.7, wps=20468, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=1, loss_scale=32, train_wall=281, gb_free=8.2, wall=53639
2022-03-05 00:32:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:32:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:32:09 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 16.515 | ppl 93646.6 | wps 36537.7 | wpb 510.9 | bsz 1 | num_updates 17637 | best_loss 7.623
2022-03-05 00:32:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17637 updates
2022-03-05 00:32:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:32:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:32:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 183 @ 17637 updates, score 16.515) (writing took 3.768077962100506 seconds)
2022-03-05 00:32:13 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-05 00:32:13 | INFO | train | epoch 183 | loss 0.767 | ppl 1.7 | wps 20247.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17637 | lr 0.000238115 | gnorm 1 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 53766
2022-03-05 00:32:13 | INFO | fairseq.trainer | begin training epoch 184
2022-03-05 00:32:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:35:28 | INFO | train_inner | epoch 184:     63 / 97 loss=0.764, ppl=1.7, wps=20328.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.99, loss_scale=16, train_wall=283, gb_free=8.2, wall=53961
2022-03-05 00:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:37:19 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 16.612 | ppl 100171 | wps 33922.8 | wpb 510.9 | bsz 1 | num_updates 17734 | best_loss 7.623
2022-03-05 00:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17734 updates
2022-03-05 00:37:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:37:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:37:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 184 @ 17734 updates, score 16.612) (writing took 3.7607704047113657 seconds)
2022-03-05 00:37:23 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-05 00:37:23 | INFO | train | epoch 184 | loss 0.763 | ppl 1.7 | wps 20498.4 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 17734 | lr 0.000237463 | gnorm 0.997 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 54076
2022-03-05 00:37:23 | INFO | fairseq.trainer | begin training epoch 185
2022-03-05 00:37:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:39:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:40:49 | INFO | train_inner | epoch 185:     67 / 97 loss=0.761, ppl=1.69, wps=20394.5, ups=0.31, wpb=65495, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=1.006, loss_scale=16, train_wall=282, gb_free=8.2, wall=54282
2022-03-05 00:42:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:42:27 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 16.608 | ppl 99892.9 | wps 34386 | wpb 510.9 | bsz 1 | num_updates 17830 | best_loss 7.623
2022-03-05 00:42:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17830 updates
2022-03-05 00:42:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:42:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:42:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 185 @ 17830 updates, score 16.608) (writing took 3.756212778389454 seconds)
2022-03-05 00:42:31 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-05 00:42:31 | INFO | train | epoch 185 | loss 0.758 | ppl 1.69 | wps 20390.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17830 | lr 0.000236823 | gnorm 1.01 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 54384
2022-03-05 00:42:31 | INFO | fairseq.trainer | begin training epoch 186
2022-03-05 00:42:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:46:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:46:08 | INFO | train_inner | epoch 186:     71 / 97 loss=0.756, ppl=1.69, wps=20481.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17900, lr=0.00023636, gnorm=1, loss_scale=16, train_wall=281, gb_free=8.2, wall=54602
2022-03-05 00:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:47:35 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 16.728 | ppl 108537 | wps 33761.9 | wpb 510.9 | bsz 1 | num_updates 17926 | best_loss 7.623
2022-03-05 00:47:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17926 updates
2022-03-05 00:47:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:47:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:47:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 186 @ 17926 updates, score 16.728) (writing took 3.9781849440187216 seconds)
2022-03-05 00:47:39 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-05 00:47:39 | INFO | train | epoch 186 | loss 0.753 | ppl 1.69 | wps 20423.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17926 | lr 0.000236188 | gnorm 0.99 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 54692
2022-03-05 00:47:39 | INFO | fairseq.trainer | begin training epoch 187
2022-03-05 00:47:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:51:26 | INFO | train_inner | epoch 187:     74 / 97 loss=0.753, ppl=1.69, wps=20641.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=1.002, loss_scale=16, train_wall=278, gb_free=8.2, wall=54919
2022-03-05 00:52:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:52:43 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 16.698 | ppl 106315 | wps 33666.6 | wpb 510.9 | bsz 1 | num_updates 18023 | best_loss 7.623
2022-03-05 00:52:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18023 updates
2022-03-05 00:52:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:52:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 187 @ 18023 updates, score 16.698) (writing took 3.8970078211277723 seconds)
2022-03-05 00:52:47 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-05 00:52:47 | INFO | train | epoch 187 | loss 0.751 | ppl 1.68 | wps 20632.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 18023 | lr 0.000235552 | gnorm 1.005 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 55000
2022-03-05 00:52:47 | INFO | fairseq.trainer | begin training epoch 188
2022-03-05 00:52:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:56:45 | INFO | train_inner | epoch 188:     78 / 97 loss=0.747, ppl=1.68, wps=20505.6, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.992, loss_scale=16, train_wall=281, gb_free=8.2, wall=55239
2022-03-05 00:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:57:49 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 16.711 | ppl 107311 | wps 34164.9 | wpb 510.9 | bsz 1 | num_updates 18119 | best_loss 7.623
2022-03-05 00:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18119 updates
2022-03-05 00:57:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:57:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 00:57:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 188 @ 18119 updates, score 16.711) (writing took 3.8366012554615736 seconds)
2022-03-05 00:57:53 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-05 00:57:53 | INFO | train | epoch 188 | loss 0.745 | ppl 1.68 | wps 20520.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18119 | lr 0.000234927 | gnorm 0.985 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 55307
2022-03-05 00:57:53 | INFO | fairseq.trainer | begin training epoch 189
2022-03-05 00:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:02:04 | INFO | train_inner | epoch 189:     82 / 97 loss=0.743, ppl=1.67, wps=20542.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.986, loss_scale=16, train_wall=280, gb_free=8.2, wall=55557
2022-03-05 01:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:02:56 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 16.653 | ppl 103038 | wps 34011.6 | wpb 510.9 | bsz 1 | num_updates 18215 | best_loss 7.623
2022-03-05 01:02:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18215 updates
2022-03-05 01:02:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:03:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 189 @ 18215 updates, score 16.653) (writing took 3.7653761245310307 seconds)
2022-03-05 01:03:00 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 01:03:00 | INFO | train | epoch 189 | loss 0.742 | ppl 1.67 | wps 20504.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18215 | lr 0.000234307 | gnorm 0.987 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 55613
2022-03-05 01:03:00 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 01:03:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:07:20 | INFO | train_inner | epoch 190:     85 / 97 loss=0.74, ppl=1.67, wps=20754.8, ups=0.32, wpb=65495, bsz=127.9, num_updates=18300, lr=0.000233762, gnorm=0.99, loss_scale=16, train_wall=277, gb_free=8.2, wall=55873
2022-03-05 01:07:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:08:02 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 16.73 | ppl 108736 | wps 34099.3 | wpb 510.9 | bsz 1 | num_updates 18312 | best_loss 7.623
2022-03-05 01:08:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18312 updates
2022-03-05 01:08:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:08:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:08:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 190 @ 18312 updates, score 16.73) (writing took 3.756776310503483 seconds)
2022-03-05 01:08:06 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 01:08:06 | INFO | train | epoch 190 | loss 0.739 | ppl 1.67 | wps 20739.7 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 18312 | lr 0.000233686 | gnorm 0.988 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 55920
2022-03-05 01:08:06 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 01:08:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:08:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:12:40 | INFO | train_inner | epoch 191:     89 / 97 loss=0.736, ppl=1.67, wps=20408.1, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=18400, lr=0.000233126, gnorm=0.99, loss_scale=16, train_wall=282, gb_free=8.2, wall=56194
2022-03-05 01:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:13:11 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 16.934 | ppl 125231 | wps 34194.6 | wpb 510.9 | bsz 1 | num_updates 18408 | best_loss 7.623
2022-03-05 01:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18408 updates
2022-03-05 01:13:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 191 @ 18408 updates, score 16.934) (writing took 3.9473664555698633 seconds)
2022-03-05 01:13:15 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 01:13:15 | INFO | train | epoch 191 | loss 0.735 | ppl 1.66 | wps 20350 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18408 | lr 0.000233076 | gnorm 0.992 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 56228
2022-03-05 01:13:15 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 01:13:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:15:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:18:00 | INFO | train_inner | epoch 192:     93 / 97 loss=0.734, ppl=1.66, wps=20524.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=18500, lr=0.000232495, gnorm=1, loss_scale=16, train_wall=280, gb_free=8.2, wall=56513
2022-03-05 01:18:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:18:18 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 16.874 | ppl 120130 | wps 34655.4 | wpb 510.9 | bsz 1 | num_updates 18504 | best_loss 7.623
2022-03-05 01:18:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18504 updates
2022-03-05 01:18:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:18:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 192 @ 18504 updates, score 16.874) (writing took 3.761969232931733 seconds)
2022-03-05 01:18:22 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 01:18:22 | INFO | train | epoch 192 | loss 0.732 | ppl 1.66 | wps 20514.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18504 | lr 0.00023247 | gnorm 0.999 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 56535
2022-03-05 01:18:22 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 01:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:22:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:23:18 | INFO | train_inner | epoch 193:     97 / 97 loss=0.728, ppl=1.66, wps=20537.6, ups=0.31, wpb=65451.9, bsz=127.8, num_updates=18600, lr=0.000231869, gnorm=0.989, loss_scale=16, train_wall=280, gb_free=8.2, wall=56832
2022-03-05 01:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:23:25 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 16.852 | ppl 118330 | wps 34212.2 | wpb 510.9 | bsz 1 | num_updates 18600 | best_loss 7.623
2022-03-05 01:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18600 updates
2022-03-05 01:23:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:23:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 193 @ 18600 updates, score 16.852) (writing took 3.692132333293557 seconds)
2022-03-05 01:23:28 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 01:23:28 | INFO | train | epoch 193 | loss 0.726 | ppl 1.65 | wps 20495.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18600 | lr 0.000231869 | gnorm 0.987 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 56842
2022-03-05 01:23:28 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 01:23:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:28:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:28:35 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 16.781 | ppl 112623 | wps 33382.2 | wpb 510.9 | bsz 1 | num_updates 18697 | best_loss 7.623
2022-03-05 01:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18697 updates
2022-03-05 01:28:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:28:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:28:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 194 @ 18697 updates, score 16.781) (writing took 3.8950807489454746 seconds)
2022-03-05 01:28:38 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 01:28:38 | INFO | train | epoch 194 | loss 0.724 | ppl 1.65 | wps 20484.2 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 18697 | lr 0.000231267 | gnorm 0.983 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 57152
2022-03-05 01:28:38 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 01:28:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:28:48 | INFO | train_inner | epoch 195:      3 / 97 loss=0.723, ppl=1.65, wps=19869.8, ups=0.3, wpb=65492.9, bsz=127.9, num_updates=18700, lr=0.000231249, gnorm=0.983, loss_scale=16, train_wall=280, gb_free=8.2, wall=57161
2022-03-05 01:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:33:46 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 16.815 | ppl 115307 | wps 33799 | wpb 510.9 | bsz 1 | num_updates 18793 | best_loss 7.623
2022-03-05 01:33:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18793 updates
2022-03-05 01:33:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:33:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:33:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 195 @ 18793 updates, score 16.815) (writing took 3.8523524925112724 seconds)
2022-03-05 01:33:50 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 01:33:50 | INFO | train | epoch 195 | loss 0.719 | ppl 1.65 | wps 20193.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18793 | lr 0.000230676 | gnorm 0.981 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 57463
2022-03-05 01:33:50 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 01:33:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:12 | INFO | train_inner | epoch 196:      7 / 97 loss=0.718, ppl=1.64, wps=20232.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.979, loss_scale=16, train_wall=284, gb_free=8.2, wall=57485
2022-03-05 01:38:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:38:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:38:57 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 16.81 | ppl 114909 | wps 33049.3 | wpb 510.9 | bsz 1 | num_updates 18889 | best_loss 7.623
2022-03-05 01:38:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18889 updates
2022-03-05 01:38:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:39:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:39:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 196 @ 18889 updates, score 16.81) (writing took 3.8039833959192038 seconds)
2022-03-05 01:39:01 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 01:39:01 | INFO | train | epoch 196 | loss 0.717 | ppl 1.64 | wps 20220.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18889 | lr 0.000230089 | gnorm 0.993 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 57774
2022-03-05 01:39:01 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 01:39:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:39:35 | INFO | train_inner | epoch 197:     11 / 97 loss=0.716, ppl=1.64, wps=20252.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.992, loss_scale=16, train_wall=284, gb_free=8.2, wall=57808
2022-03-05 01:44:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:44:08 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 16.897 | ppl 122045 | wps 33753.2 | wpb 510.9 | bsz 1 | num_updates 18986 | best_loss 7.623
2022-03-05 01:44:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18986 updates
2022-03-05 01:44:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:44:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:44:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 197 @ 18986 updates, score 16.897) (writing took 3.849747123196721 seconds)
2022-03-05 01:44:12 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 01:44:12 | INFO | train | epoch 197 | loss 0.713 | ppl 1.64 | wps 20413.2 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 18986 | lr 0.0002295 | gnorm 0.984 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 58085
2022-03-05 01:44:12 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 01:44:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:44:55 | INFO | train_inner | epoch 198:     14 / 97 loss=0.71, ppl=1.64, wps=20448.7, ups=0.31, wpb=65495, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.98, loss_scale=16, train_wall=281, gb_free=8.2, wall=58129
2022-03-05 01:45:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:49:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:49:18 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 16.906 | ppl 122799 | wps 33520.9 | wpb 510.9 | bsz 1 | num_updates 19082 | best_loss 7.623
2022-03-05 01:49:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19082 updates
2022-03-05 01:49:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 198 @ 19082 updates, score 16.906) (writing took 3.850425912067294 seconds)
2022-03-05 01:49:22 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 01:49:22 | INFO | train | epoch 198 | loss 0.709 | ppl 1.64 | wps 20256.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19082 | lr 0.000228922 | gnorm 0.985 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 58396
2022-03-05 01:49:22 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 01:49:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:50:18 | INFO | train_inner | epoch 199:     18 / 97 loss=0.709, ppl=1.63, wps=20289.3, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.988, loss_scale=16, train_wall=283, gb_free=8.2, wall=58451
2022-03-05 01:52:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:54:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:54:29 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 16.911 | ppl 123241 | wps 33534.7 | wpb 510.9 | bsz 1 | num_updates 19178 | best_loss 7.623
2022-03-05 01:54:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19178 updates
2022-03-05 01:54:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:54:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 199 @ 19178 updates, score 16.911) (writing took 3.745008245110512 seconds)
2022-03-05 01:54:33 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 01:54:33 | INFO | train | epoch 199 | loss 0.707 | ppl 1.63 | wps 20255.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19178 | lr 0.000228349 | gnorm 0.976 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 58706
2022-03-05 01:54:33 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 01:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:55:41 | INFO | train_inner | epoch 200:     22 / 97 loss=0.705, ppl=1.63, wps=20292.2, ups=0.31, wpb=65495, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.976, loss_scale=16, train_wall=283, gb_free=8.2, wall=58774
2022-03-05 01:59:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:59:38 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 16.845 | ppl 117731 | wps 33606.4 | wpb 510.9 | bsz 1 | num_updates 19275 | best_loss 7.623
2022-03-05 01:59:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19275 updates
2022-03-05 01:59:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:59:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 01:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 200 @ 19275 updates, score 16.845) (writing took 3.7925896905362606 seconds)
2022-03-05 01:59:41 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 01:59:41 | INFO | train | epoch 200 | loss 0.703 | ppl 1.63 | wps 20577.3 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 19275 | lr 0.000227773 | gnorm 0.98 | loss_scale 32 | train_wall 271 | gb_free 8.2 | wall 59015
2022-03-05 01:59:41 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 01:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:59:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:01:02 | INFO | train_inner | epoch 201:     26 / 97 loss=0.701, ppl=1.63, wps=20409.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.982, loss_scale=16, train_wall=282, gb_free=8.2, wall=59095
2022-03-05 02:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:04:45 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 17.083 | ppl 138802 | wps 34689.6 | wpb 510.9 | bsz 1 | num_updates 19371 | best_loss 7.623
2022-03-05 02:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19371 updates
2022-03-05 02:04:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:04:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 201 @ 19371 updates, score 17.083) (writing took 3.7996627539396286 seconds)
2022-03-05 02:04:49 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 02:04:49 | INFO | train | epoch 201 | loss 0.699 | ppl 1.62 | wps 20461.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19371 | lr 0.000227208 | gnorm 0.98 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 59322
2022-03-05 02:04:49 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 02:04:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:06:18 | INFO | train_inner | epoch 202:     29 / 97 loss=0.696, ppl=1.62, wps=20694.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.971, loss_scale=16, train_wall=278, gb_free=8.2, wall=59411
2022-03-05 02:06:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:09:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:09:53 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 16.993 | ppl 130478 | wps 34266 | wpb 510.9 | bsz 1 | num_updates 19467 | best_loss 7.623
2022-03-05 02:09:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19467 updates
2022-03-05 02:09:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:09:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 202 @ 19467 updates, score 16.993) (writing took 3.8525576032698154 seconds)
2022-03-05 02:09:57 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 02:09:57 | INFO | train | epoch 202 | loss 0.696 | ppl 1.62 | wps 20386.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19467 | lr 0.000226647 | gnorm 0.973 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 59630
2022-03-05 02:09:57 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 02:09:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:11:39 | INFO | train_inner | epoch 203:     33 / 97 loss=0.696, ppl=1.62, wps=20417.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.982, loss_scale=16, train_wall=282, gb_free=8.2, wall=59732
2022-03-05 02:14:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:15:02 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 16.963 | ppl 127715 | wps 34196.3 | wpb 510.9 | bsz 1 | num_updates 19564 | best_loss 7.623
2022-03-05 02:15:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19564 updates
2022-03-05 02:15:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:15:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:15:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 203 @ 19564 updates, score 16.963) (writing took 3.881831791251898 seconds)
2022-03-05 02:15:06 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 02:15:06 | INFO | train | epoch 203 | loss 0.695 | ppl 1.62 | wps 20570.7 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 19564 | lr 0.000226085 | gnorm 0.982 | loss_scale 32 | train_wall 271 | gb_free 8.2 | wall 59939
2022-03-05 02:15:06 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 02:15:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:16:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:17:00 | INFO | train_inner | epoch 204:     37 / 97 loss=0.692, ppl=1.62, wps=20411.8, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.974, loss_scale=16, train_wall=282, gb_free=8.2, wall=60053
2022-03-05 02:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:20:11 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 17.011 | ppl 132037 | wps 34128.1 | wpb 510.9 | bsz 1 | num_updates 19660 | best_loss 7.623
2022-03-05 02:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19660 updates
2022-03-05 02:20:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:20:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 204 @ 19660 updates, score 17.011) (writing took 3.848895236849785 seconds)
2022-03-05 02:20:14 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 02:20:14 | INFO | train | epoch 204 | loss 0.69 | ppl 1.61 | wps 20378.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19660 | lr 0.000225532 | gnorm 0.971 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 60248
2022-03-05 02:20:14 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 02:20:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:22:17 | INFO | train_inner | epoch 205:     40 / 97 loss=0.689, ppl=1.61, wps=20622.8, ups=0.31, wpb=65495, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.971, loss_scale=16, train_wall=279, gb_free=8.2, wall=60371
2022-03-05 02:22:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:25:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:25:19 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 17.006 | ppl 131616 | wps 34609.3 | wpb 510.9 | bsz 1 | num_updates 19756 | best_loss 7.623
2022-03-05 02:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19756 updates
2022-03-05 02:25:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:25:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:25:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 205 @ 19756 updates, score 17.006) (writing took 4.0131530072540045 seconds)
2022-03-05 02:25:23 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 02:25:23 | INFO | train | epoch 205 | loss 0.686 | ppl 1.61 | wps 20360.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19756 | lr 0.000224983 | gnorm 0.976 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 60557
2022-03-05 02:25:23 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 02:25:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:27:38 | INFO | train_inner | epoch 206:     44 / 97 loss=0.685, ppl=1.61, wps=20431.2, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.974, loss_scale=16, train_wall=282, gb_free=8.2, wall=60691
2022-03-05 02:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:30:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:30:27 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 17.014 | ppl 132388 | wps 33892.1 | wpb 510.9 | bsz 1 | num_updates 19852 | best_loss 7.623
2022-03-05 02:30:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19852 updates
2022-03-05 02:30:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:30:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:30:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 206 @ 19852 updates, score 17.014) (writing took 3.7826467398554087 seconds)
2022-03-05 02:30:31 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 02:30:31 | INFO | train | epoch 206 | loss 0.682 | ppl 1.6 | wps 20426.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19852 | lr 0.000224439 | gnorm 0.958 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 60864
2022-03-05 02:30:31 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 02:30:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:32:57 | INFO | train_inner | epoch 207:     48 / 97 loss=0.682, ppl=1.6, wps=20489.6, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.956, loss_scale=16, train_wall=281, gb_free=8.2, wall=61011
2022-03-05 02:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:35:33 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 17.071 | ppl 137709 | wps 34163.5 | wpb 510.9 | bsz 1 | num_updates 19949 | best_loss 7.623
2022-03-05 02:35:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19949 updates
2022-03-05 02:35:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:35:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 207 @ 19949 updates, score 17.071) (writing took 3.8746024761348963 seconds)
2022-03-05 02:35:37 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 02:35:37 | INFO | train | epoch 207 | loss 0.681 | ppl 1.6 | wps 20747.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 19949 | lr 0.000223892 | gnorm 0.964 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 61171
2022-03-05 02:35:37 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 02:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:38:12 | INFO | train_inner | epoch 208:     51 / 97 loss=0.68, ppl=1.6, wps=20799.7, ups=0.32, wpb=65495, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.967, loss_scale=32, train_wall=277, gb_free=8.2, wall=61326
2022-03-05 02:39:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:40:38 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 17.139 | ppl 144374 | wps 34575.1 | wpb 510.9 | bsz 1 | num_updates 20045 | best_loss 7.623
2022-03-05 02:40:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20045 updates
2022-03-05 02:40:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:40:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:40:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 208 @ 20045 updates, score 17.139) (writing took 3.680446118116379 seconds)
2022-03-05 02:40:41 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 02:40:42 | INFO | train | epoch 208 | loss 0.679 | ppl 1.6 | wps 20661.6 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 20045 | lr 0.000223356 | gnorm 0.967 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 61475
2022-03-05 02:40:42 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 02:40:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:43:28 | INFO | train_inner | epoch 209:     55 / 97 loss=0.676, ppl=1.6, wps=20725.2, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.958, loss_scale=16, train_wall=278, gb_free=8.2, wall=61642
2022-03-05 02:45:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:45:43 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 17.131 | ppl 143569 | wps 32884.6 | wpb 510.9 | bsz 1 | num_updates 20142 | best_loss 7.623
2022-03-05 02:45:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20142 updates
2022-03-05 02:45:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 209 @ 20142 updates, score 17.131) (writing took 3.856447523459792 seconds)
2022-03-05 02:45:47 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 02:45:47 | INFO | train | epoch 209 | loss 0.675 | ppl 1.6 | wps 20819.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 20142 | lr 0.000222817 | gnorm 0.957 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 61780
2022-03-05 02:45:47 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 02:45:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:46:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:48:49 | INFO | train_inner | epoch 210:     59 / 97 loss=0.674, ppl=1.6, wps=20446.9, ups=0.31, wpb=65495, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.967, loss_scale=16, train_wall=281, gb_free=8.2, wall=61962
2022-03-05 02:50:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:50:53 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 17.148 | ppl 145185 | wps 32510.2 | wpb 510.9 | bsz 1 | num_updates 20238 | best_loss 7.623
2022-03-05 02:50:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20238 updates
2022-03-05 02:50:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 210 @ 20238 updates, score 17.148) (writing took 4.4606954138726 seconds)
2022-03-05 02:50:57 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 02:50:57 | INFO | train | epoch 210 | loss 0.672 | ppl 1.59 | wps 20226.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20238 | lr 0.000222288 | gnorm 0.977 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 62091
2022-03-05 02:50:57 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 02:50:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:53:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:54:12 | INFO | train_inner | epoch 211:     63 / 97 loss=0.671, ppl=1.59, wps=20230, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.979, loss_scale=16, train_wall=284, gb_free=8.2, wall=62286
2022-03-05 02:55:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:56:04 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 17.093 | ppl 139799 | wps 33406.5 | wpb 510.9 | bsz 1 | num_updates 20334 | best_loss 7.623
2022-03-05 02:56:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20334 updates
2022-03-05 02:56:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:56:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 02:56:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 211 @ 20334 updates, score 17.093) (writing took 4.064302407205105 seconds)
2022-03-05 02:56:08 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 02:56:08 | INFO | train | epoch 211 | loss 0.668 | ppl 1.59 | wps 20215.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20334 | lr 0.000221763 | gnorm 0.967 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 62402
2022-03-05 02:56:08 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 02:56:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:59:33 | INFO | train_inner | epoch 212:     66 / 97 loss=0.666, ppl=1.59, wps=20427.6, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.964, loss_scale=16, train_wall=281, gb_free=8.2, wall=62606
2022-03-05 03:00:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:01:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:01:16 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 16.993 | ppl 130474 | wps 33276.5 | wpb 510.9 | bsz 1 | num_updates 20430 | best_loss 7.623
2022-03-05 03:01:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20430 updates
2022-03-05 03:01:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:01:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:01:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 212 @ 20430 updates, score 16.993) (writing took 3.8705698121339083 seconds)
2022-03-05 03:01:20 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 03:01:20 | INFO | train | epoch 212 | loss 0.665 | ppl 1.59 | wps 20203 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20430 | lr 0.000221241 | gnorm 0.966 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 62713
2022-03-05 03:01:20 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 03:01:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:04:56 | INFO | train_inner | epoch 213:     70 / 97 loss=0.664, ppl=1.58, wps=20269.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.968, loss_scale=16, train_wall=284, gb_free=8.2, wall=62930
2022-03-05 03:06:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:06:26 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 17.165 | ppl 147002 | wps 33165.7 | wpb 510.9 | bsz 1 | num_updates 20527 | best_loss 7.623
2022-03-05 03:06:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20527 updates
2022-03-05 03:06:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:06:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 213 @ 20527 updates, score 17.165) (writing took 4.07013656757772 seconds)
2022-03-05 03:06:30 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 03:06:30 | INFO | train | epoch 213 | loss 0.663 | ppl 1.58 | wps 20440.4 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 20527 | lr 0.000220718 | gnorm 0.965 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 63024
2022-03-05 03:06:30 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 03:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:07:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:10:20 | INFO | train_inner | epoch 214:     74 / 97 loss=0.662, ppl=1.58, wps=20232.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.966, loss_scale=16, train_wall=284, gb_free=8.2, wall=63253
2022-03-05 03:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:11:38 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 17.183 | ppl 148788 | wps 33405.7 | wpb 510.9 | bsz 1 | num_updates 20623 | best_loss 7.623
2022-03-05 03:11:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20623 updates
2022-03-05 03:11:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:11:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:11:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 214 @ 20623 updates, score 17.183) (writing took 3.8155477680265903 seconds)
2022-03-05 03:11:41 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 03:11:41 | INFO | train | epoch 214 | loss 0.66 | ppl 1.58 | wps 20221.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20623 | lr 0.000220203 | gnorm 0.971 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 63335
2022-03-05 03:11:41 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 03:11:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:14:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:15:43 | INFO | train_inner | epoch 215:     78 / 97 loss=0.658, ppl=1.58, wps=20287.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.963, loss_scale=16, train_wall=284, gb_free=8.2, wall=63576
2022-03-05 03:16:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:16:48 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 17.131 | ppl 143566 | wps 33417.9 | wpb 510.9 | bsz 1 | num_updates 20719 | best_loss 7.623
2022-03-05 03:16:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20719 updates
2022-03-05 03:16:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:16:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:16:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 215 @ 20719 updates, score 17.131) (writing took 4.154295660555363 seconds)
2022-03-05 03:16:52 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 03:16:52 | INFO | train | epoch 215 | loss 0.657 | ppl 1.58 | wps 20237 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20719 | lr 0.000219693 | gnorm 0.96 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 63645
2022-03-05 03:16:52 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 03:16:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:21:02 | INFO | train_inner | epoch 216:     81 / 97 loss=0.656, ppl=1.58, wps=20518.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.963, loss_scale=16, train_wall=280, gb_free=8.2, wall=63895
2022-03-05 03:21:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:21:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:21:57 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 17.198 | ppl 150348 | wps 34023.4 | wpb 510.9 | bsz 1 | num_updates 20815 | best_loss 7.623
2022-03-05 03:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20815 updates
2022-03-05 03:21:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:22:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:22:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 216 @ 20815 updates, score 17.198) (writing took 3.8285892456769943 seconds)
2022-03-05 03:22:01 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 03:22:01 | INFO | train | epoch 216 | loss 0.654 | ppl 1.57 | wps 20357.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20815 | lr 0.000219185 | gnorm 0.96 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 63954
2022-03-05 03:22:01 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 03:22:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:26:22 | INFO | train_inner | epoch 217:     85 / 97 loss=0.652, ppl=1.57, wps=20446.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20900, lr=0.000218739, gnorm=0.955, loss_scale=16, train_wall=281, gb_free=8.2, wall=64216
2022-03-05 03:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:27:05 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 17.153 | ppl 145738 | wps 34094.4 | wpb 510.9 | bsz 1 | num_updates 20912 | best_loss 7.623
2022-03-05 03:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20912 updates
2022-03-05 03:27:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:27:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 217 @ 20912 updates, score 17.153) (writing took 3.8366952668875456 seconds)
2022-03-05 03:27:09 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 03:27:09 | INFO | train | epoch 217 | loss 0.651 | ppl 1.57 | wps 20623.1 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 20912 | lr 0.000218677 | gnorm 0.955 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 64262
2022-03-05 03:27:09 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 03:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:28:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:31:42 | INFO | train_inner | epoch 218:     89 / 97 loss=0.65, ppl=1.57, wps=20452.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21000, lr=0.000218218, gnorm=0.955, loss_scale=16, train_wall=281, gb_free=8.2, wall=64536
2022-03-05 03:32:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:32:13 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 17.154 | ppl 145853 | wps 34097.4 | wpb 510.9 | bsz 1 | num_updates 21008 | best_loss 7.623
2022-03-05 03:32:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 21008 updates
2022-03-05 03:32:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:32:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 218 @ 21008 updates, score 17.154) (writing took 3.8168467432260513 seconds)
2022-03-05 03:32:17 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 03:32:17 | INFO | train | epoch 218 | loss 0.649 | ppl 1.57 | wps 20412 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21008 | lr 0.000218176 | gnorm 0.954 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 64570
2022-03-05 03:32:17 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 03:32:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:35:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:37:03 | INFO | train_inner | epoch 219:     93 / 97 loss=0.646, ppl=1.56, wps=20446.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21100, lr=0.0002177, gnorm=0.947, loss_scale=16, train_wall=281, gb_free=8.2, wall=64856
2022-03-05 03:37:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:37:21 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 17.19 | ppl 149545 | wps 33952.8 | wpb 510.9 | bsz 1 | num_updates 21104 | best_loss 7.623
2022-03-05 03:37:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21104 updates
2022-03-05 03:37:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:37:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:37:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 219 @ 21104 updates, score 17.19) (writing took 3.807382320985198 seconds)
2022-03-05 03:37:25 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 03:37:25 | INFO | train | epoch 219 | loss 0.644 | ppl 1.56 | wps 20407.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21104 | lr 0.00021768 | gnorm 0.946 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 64878
2022-03-05 03:37:25 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 03:37:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:42:20 | INFO | train_inner | epoch 220:     96 / 97 loss=0.646, ppl=1.56, wps=20627.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21200, lr=0.000217186, gnorm=0.962, loss_scale=32, train_wall=279, gb_free=8.2, wall=65174
2022-03-05 03:42:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:42:29 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 17.285 | ppl 159669 | wps 34416.6 | wpb 510.9 | bsz 1 | num_updates 21201 | best_loss 7.623
2022-03-05 03:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21201 updates
2022-03-05 03:42:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:42:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:42:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 220 @ 21201 updates, score 17.285) (writing took 3.79976106621325 seconds)
2022-03-05 03:42:33 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 03:42:33 | INFO | train | epoch 220 | loss 0.645 | ppl 1.56 | wps 20612.9 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 21201 | lr 0.000217181 | gnorm 0.961 | loss_scale 32 | train_wall 270 | gb_free 8.2 | wall 65187
2022-03-05 03:42:33 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 03:42:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:42:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:47:37 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 17.264 | ppl 157381 | wps 34321.6 | wpb 510.9 | bsz 1 | num_updates 21297 | best_loss 7.623
2022-03-05 03:47:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21297 updates
2022-03-05 03:47:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:47:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:47:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 221 @ 21297 updates, score 17.264) (writing took 3.763221373781562 seconds)
2022-03-05 03:47:41 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 03:47:41 | INFO | train | epoch 221 | loss 0.64 | ppl 1.56 | wps 20415.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21297 | lr 0.000216691 | gnorm 0.962 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 65495
2022-03-05 03:47:41 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 03:47:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:47:50 | INFO | train_inner | epoch 222:      3 / 97 loss=0.639, ppl=1.56, wps=19819.9, ups=0.3, wpb=65451.9, bsz=127.8, num_updates=21300, lr=0.000216676, gnorm=0.961, loss_scale=16, train_wall=281, gb_free=8.2, wall=65504
2022-03-05 03:50:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:52:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:52:45 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 17.199 | ppl 150470 | wps 34104.6 | wpb 510.9 | bsz 1 | num_updates 21393 | best_loss 7.623
2022-03-05 03:52:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21393 updates
2022-03-05 03:52:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:52:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 222 @ 21393 updates, score 17.199) (writing took 3.9173384606838226 seconds)
2022-03-05 03:52:49 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 03:52:49 | INFO | train | epoch 222 | loss 0.636 | ppl 1.55 | wps 20406.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21393 | lr 0.000216204 | gnorm 0.941 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 65803
2022-03-05 03:52:49 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 03:52:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:53:11 | INFO | train_inner | epoch 223:      7 / 97 loss=0.636, ppl=1.55, wps=20440.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.942, loss_scale=16, train_wall=281, gb_free=8.2, wall=65824
2022-03-05 03:57:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:57:53 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 17.25 | ppl 155903 | wps 34719.3 | wpb 510.9 | bsz 1 | num_updates 21489 | best_loss 7.623
2022-03-05 03:57:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21489 updates
2022-03-05 03:57:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 03:57:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 223 @ 21489 updates, score 17.25) (writing took 3.762746822088957 seconds)
2022-03-05 03:57:57 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 03:57:57 | INFO | train | epoch 223 | loss 0.635 | ppl 1.55 | wps 20421.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21489 | lr 0.000215721 | gnorm 0.952 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 66111
2022-03-05 03:57:57 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 03:57:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:58:31 | INFO | train_inner | epoch 224:     11 / 97 loss=0.634, ppl=1.55, wps=20469.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.951, loss_scale=16, train_wall=281, gb_free=8.2, wall=66144
2022-03-05 04:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:03:00 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 17.383 | ppl 170977 | wps 34169.9 | wpb 510.9 | bsz 1 | num_updates 21586 | best_loss 7.623
2022-03-05 04:03:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21586 updates
2022-03-05 04:03:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:03:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:03:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 224 @ 21586 updates, score 17.383) (writing took 3.949727702885866 seconds)
2022-03-05 04:03:04 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 04:03:04 | INFO | train | epoch 224 | loss 0.634 | ppl 1.55 | wps 20715.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 21586 | lr 0.000215236 | gnorm 0.95 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 66417
2022-03-05 04:03:04 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 04:03:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:03:47 | INFO | train_inner | epoch 225:     14 / 97 loss=0.633, ppl=1.55, wps=20730.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.949, loss_scale=16, train_wall=277, gb_free=8.2, wall=66460
2022-03-05 04:05:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:08:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:08:06 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 17.377 | ppl 170204 | wps 33656.2 | wpb 510.9 | bsz 1 | num_updates 21682 | best_loss 7.623
2022-03-05 04:08:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21682 updates
2022-03-05 04:08:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:08:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:08:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 225 @ 21682 updates, score 17.377) (writing took 3.8133689928799868 seconds)
2022-03-05 04:08:10 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 04:08:10 | INFO | train | epoch 225 | loss 0.63 | ppl 1.55 | wps 20523.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21682 | lr 0.000214758 | gnorm 0.947 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 66724
2022-03-05 04:08:10 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 04:08:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:09:06 | INFO | train_inner | epoch 226:     18 / 97 loss=0.629, ppl=1.55, wps=20542, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.95, loss_scale=16, train_wall=280, gb_free=8.2, wall=66779
2022-03-05 04:12:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:13:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:13:15 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 17.341 | ppl 166063 | wps 33659.3 | wpb 510.9 | bsz 1 | num_updates 21778 | best_loss 7.623
2022-03-05 04:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21778 updates
2022-03-05 04:13:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:13:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 226 @ 21778 updates, score 17.341) (writing took 3.7409531325101852 seconds)
2022-03-05 04:13:19 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 04:13:19 | INFO | train | epoch 226 | loss 0.628 | ppl 1.55 | wps 20351.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21778 | lr 0.000214285 | gnorm 0.952 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 67032
2022-03-05 04:13:19 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 04:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:14:27 | INFO | train_inner | epoch 227:     22 / 97 loss=0.626, ppl=1.54, wps=20380.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.95, loss_scale=16, train_wall=282, gb_free=8.2, wall=67100
2022-03-05 04:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:18:24 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 17.421 | ppl 175543 | wps 33759.6 | wpb 510.9 | bsz 1 | num_updates 21875 | best_loss 7.623
2022-03-05 04:18:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21875 updates
2022-03-05 04:18:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:18:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:18:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 227 @ 21875 updates, score 17.421) (writing took 3.7160669397562742 seconds)
2022-03-05 04:18:28 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 04:18:28 | INFO | train | epoch 227 | loss 0.625 | ppl 1.54 | wps 20596.8 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 21875 | lr 0.000213809 | gnorm 0.951 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 67341
2022-03-05 04:18:28 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 04:18:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:19:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:19:48 | INFO | train_inner | epoch 228:     26 / 97 loss=0.624, ppl=1.54, wps=20417.8, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.944, loss_scale=16, train_wall=282, gb_free=8.2, wall=67421
2022-03-05 04:23:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:23:32 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 17.399 | ppl 172834 | wps 33216.9 | wpb 510.9 | bsz 1 | num_updates 21971 | best_loss 7.623
2022-03-05 04:23:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21971 updates
2022-03-05 04:23:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:23:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 228 @ 21971 updates, score 17.399) (writing took 3.8323245886713266 seconds)
2022-03-05 04:23:36 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 04:23:36 | INFO | train | epoch 228 | loss 0.622 | ppl 1.54 | wps 20365.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21971 | lr 0.000213341 | gnorm 0.943 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 67650
2022-03-05 04:23:36 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 04:23:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:25:06 | INFO | train_inner | epoch 229:     29 / 97 loss=0.621, ppl=1.54, wps=20560.6, ups=0.31, wpb=65495, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.947, loss_scale=16, train_wall=280, gb_free=8.2, wall=67740
2022-03-05 04:26:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:28:43 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 17.338 | ppl 165710 | wps 33789.1 | wpb 510.9 | bsz 1 | num_updates 22067 | best_loss 7.623
2022-03-05 04:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22067 updates
2022-03-05 04:28:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:28:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 229 @ 22067 updates, score 17.338) (writing took 3.793144503608346 seconds)
2022-03-05 04:28:46 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 04:28:46 | INFO | train | epoch 229 | loss 0.62 | ppl 1.54 | wps 20264.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22067 | lr 0.000212877 | gnorm 0.953 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 67960
2022-03-05 04:28:46 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 04:28:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:30:29 | INFO | train_inner | epoch 230:     33 / 97 loss=0.618, ppl=1.53, wps=20277.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.952, loss_scale=16, train_wall=284, gb_free=8.2, wall=68063
2022-03-05 04:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:33:54 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 17.392 | ppl 172037 | wps 33929.8 | wpb 510.9 | bsz 1 | num_updates 22164 | best_loss 7.623
2022-03-05 04:33:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22164 updates
2022-03-05 04:33:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:33:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:33:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 230 @ 22164 updates, score 17.392) (writing took 3.8046552278101444 seconds)
2022-03-05 04:33:57 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 04:33:57 | INFO | train | epoch 230 | loss 0.618 | ppl 1.53 | wps 20436.6 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 22164 | lr 0.00021241 | gnorm 0.943 | loss_scale 32 | train_wall 272 | gb_free 8.2 | wall 68271
2022-03-05 04:33:57 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 04:33:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:34:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:35:52 | INFO | train_inner | epoch 231:     37 / 97 loss=0.617, ppl=1.53, wps=20280.3, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=22200, lr=0.000212238, gnorm=0.935, loss_scale=16, train_wall=284, gb_free=8.2, wall=68385
2022-03-05 04:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:39:04 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 17.422 | ppl 175644 | wps 33607.9 | wpb 510.9 | bsz 1 | num_updates 22260 | best_loss 7.623
2022-03-05 04:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22260 updates
2022-03-05 04:39:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:39:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:39:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 231 @ 22260 updates, score 17.422) (writing took 3.7894584629684687 seconds)
2022-03-05 04:39:08 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 04:39:08 | INFO | train | epoch 231 | loss 0.614 | ppl 1.53 | wps 20250.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22260 | lr 0.000211952 | gnorm 0.935 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 68581
2022-03-05 04:39:08 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 04:39:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:41:11 | INFO | train_inner | epoch 232:     40 / 97 loss=0.613, ppl=1.53, wps=20517.5, ups=0.31, wpb=65495, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.935, loss_scale=16, train_wall=280, gb_free=8.2, wall=68705
2022-03-05 04:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:44:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:44:13 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 17.41 | ppl 174181 | wps 33901.6 | wpb 510.9 | bsz 1 | num_updates 22356 | best_loss 7.623
2022-03-05 04:44:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22356 updates
2022-03-05 04:44:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:44:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:44:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 232 @ 22356 updates, score 17.41) (writing took 3.659597411751747 seconds)
2022-03-05 04:44:17 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 04:44:17 | INFO | train | epoch 232 | loss 0.612 | ppl 1.53 | wps 20335.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22356 | lr 0.000211496 | gnorm 0.938 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 68890
2022-03-05 04:44:17 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 04:44:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:46:32 | INFO | train_inner | epoch 233:     44 / 97 loss=0.611, ppl=1.53, wps=20408.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.945, loss_scale=16, train_wall=282, gb_free=8.2, wall=69026
2022-03-05 04:48:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:49:21 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 17.507 | ppl 186232 | wps 34082.3 | wpb 510.9 | bsz 1 | num_updates 22452 | best_loss 7.623
2022-03-05 04:49:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22452 updates
2022-03-05 04:49:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:49:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 233 @ 22452 updates, score 17.507) (writing took 3.783815933391452 seconds)
2022-03-05 04:49:25 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 04:49:25 | INFO | train | epoch 233 | loss 0.611 | ppl 1.53 | wps 20399.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22452 | lr 0.000211044 | gnorm 0.948 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 69199
2022-03-05 04:49:25 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 04:49:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:51:53 | INFO | train_inner | epoch 234:     48 / 97 loss=0.611, ppl=1.53, wps=20413.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.944, loss_scale=16, train_wall=282, gb_free=8.2, wall=69346
2022-03-05 04:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:54:30 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 17.486 | ppl 183577 | wps 34277.2 | wpb 510.9 | bsz 1 | num_updates 22549 | best_loss 7.623
2022-03-05 04:54:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22549 updates
2022-03-05 04:54:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:54:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:54:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 234 @ 22549 updates, score 17.486) (writing took 3.7322046793997288 seconds)
2022-03-05 04:54:34 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 04:54:34 | INFO | train | epoch 234 | loss 0.609 | ppl 1.52 | wps 20576.9 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 22549 | lr 0.000210589 | gnorm 0.943 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 69507
2022-03-05 04:54:34 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 04:54:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:55:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:57:14 | INFO | train_inner | epoch 235:     52 / 97 loss=0.605, ppl=1.52, wps=20435.6, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.939, loss_scale=16, train_wall=282, gb_free=8.2, wall=69667
2022-03-05 04:59:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:59:38 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 17.511 | ppl 186752 | wps 33982.5 | wpb 510.9 | bsz 1 | num_updates 22645 | best_loss 7.623
2022-03-05 04:59:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22645 updates
2022-03-05 04:59:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:59:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 04:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 235 @ 22645 updates, score 17.511) (writing took 3.915425643324852 seconds)
2022-03-05 04:59:42 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 04:59:42 | INFO | train | epoch 235 | loss 0.605 | ppl 1.52 | wps 20389.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22645 | lr 0.000210142 | gnorm 0.938 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 69816
2022-03-05 04:59:42 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 04:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:02:31 | INFO | train_inner | epoch 236:     55 / 97 loss=0.606, ppl=1.52, wps=20616.8, ups=0.31, wpb=65495, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.938, loss_scale=16, train_wall=279, gb_free=8.2, wall=69985
2022-03-05 05:02:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:04:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:04:47 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 17.519 | ppl 187791 | wps 34379.7 | wpb 510.9 | bsz 1 | num_updates 22741 | best_loss 7.623
2022-03-05 05:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22741 updates
2022-03-05 05:04:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 236 @ 22741 updates, score 17.519) (writing took 3.857945803552866 seconds)
2022-03-05 05:04:51 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 05:04:51 | INFO | train | epoch 236 | loss 0.603 | ppl 1.52 | wps 20381.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22741 | lr 0.000209698 | gnorm 0.929 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 70124
2022-03-05 05:04:51 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 05:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:07:52 | INFO | train_inner | epoch 237:     59 / 97 loss=0.602, ppl=1.52, wps=20434.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.931, loss_scale=16, train_wall=282, gb_free=8.2, wall=70305
2022-03-05 05:09:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:09:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:09:55 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 17.595 | ppl 197971 | wps 33780 | wpb 510.9 | bsz 1 | num_updates 22837 | best_loss 7.623
2022-03-05 05:09:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22837 updates
2022-03-05 05:09:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:09:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:09:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 237 @ 22837 updates, score 17.595) (writing took 4.057816809043288 seconds)
2022-03-05 05:09:59 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 05:09:59 | INFO | train | epoch 237 | loss 0.602 | ppl 1.52 | wps 20369.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22837 | lr 0.000209257 | gnorm 0.938 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 70433
2022-03-05 05:09:59 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 05:09:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:13:13 | INFO | train_inner | epoch 238:     63 / 97 loss=0.602, ppl=1.52, wps=20386.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.945, loss_scale=16, train_wall=282, gb_free=8.2, wall=70626
2022-03-05 05:14:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:15:04 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 17.612 | ppl 200343 | wps 33700.4 | wpb 510.9 | bsz 1 | num_updates 22934 | best_loss 7.623
2022-03-05 05:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22934 updates
2022-03-05 05:15:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:15:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:15:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 238 @ 22934 updates, score 17.612) (writing took 3.8382298704236746 seconds)
2022-03-05 05:15:08 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 05:15:08 | INFO | train | epoch 238 | loss 0.6 | ppl 1.52 | wps 20592.7 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 22934 | lr 0.000208814 | gnorm 0.944 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 70741
2022-03-05 05:15:08 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 05:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:16:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:18:33 | INFO | train_inner | epoch 239:     67 / 97 loss=0.598, ppl=1.51, wps=20450.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.941, loss_scale=16, train_wall=281, gb_free=8.2, wall=70947
2022-03-05 05:20:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:20:11 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 17.589 | ppl 197123 | wps 34034.6 | wpb 510.9 | bsz 1 | num_updates 23030 | best_loss 7.623
2022-03-05 05:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23030 updates
2022-03-05 05:20:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:20:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:20:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 239 @ 23030 updates, score 17.589) (writing took 3.783353352919221 seconds)
2022-03-05 05:20:15 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 05:20:15 | INFO | train | epoch 239 | loss 0.597 | ppl 1.51 | wps 20460.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23030 | lr 0.000208379 | gnorm 0.942 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 71049
2022-03-05 05:20:15 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 05:20:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:23:49 | INFO | train_inner | epoch 240:     70 / 97 loss=0.595, ppl=1.51, wps=20735.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.939, loss_scale=32, train_wall=278, gb_free=8.2, wall=71262
2022-03-05 05:24:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:25:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:25:18 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 17.642 | ppl 204520 | wps 34209.8 | wpb 510.9 | bsz 1 | num_updates 23126 | best_loss 7.623
2022-03-05 05:25:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23126 updates
2022-03-05 05:25:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:25:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:25:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 240 @ 23126 updates, score 17.642) (writing took 3.762960212305188 seconds)
2022-03-05 05:25:22 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 05:25:22 | INFO | train | epoch 240 | loss 0.594 | ppl 1.51 | wps 20519.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23126 | lr 0.000207946 | gnorm 0.929 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 71355
2022-03-05 05:25:22 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 05:25:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:29:07 | INFO | train_inner | epoch 241:     74 / 97 loss=0.595, ppl=1.51, wps=20568.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.927, loss_scale=16, train_wall=280, gb_free=8.2, wall=71581
2022-03-05 05:30:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:30:24 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 17.542 | ppl 190875 | wps 33307.6 | wpb 510.9 | bsz 1 | num_updates 23223 | best_loss 7.623
2022-03-05 05:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23223 updates
2022-03-05 05:30:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:30:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 241 @ 23223 updates, score 17.542) (writing took 3.8541181050240993 seconds)
2022-03-05 05:30:28 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 05:30:28 | INFO | train | epoch 241 | loss 0.593 | ppl 1.51 | wps 20720.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 23223 | lr 0.000207511 | gnorm 0.931 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 71662
2022-03-05 05:30:28 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 05:30:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:31:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:34:30 | INFO | train_inner | epoch 242:     78 / 97 loss=0.591, ppl=1.51, wps=20325.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.935, loss_scale=16, train_wall=283, gb_free=8.2, wall=71903
2022-03-05 05:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:35:35 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 17.572 | ppl 194860 | wps 33361.5 | wpb 510.9 | bsz 1 | num_updates 23319 | best_loss 7.623
2022-03-05 05:35:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23319 updates
2022-03-05 05:35:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:35:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 242 @ 23319 updates, score 17.572) (writing took 3.840094819664955 seconds)
2022-03-05 05:35:39 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 05:35:39 | INFO | train | epoch 242 | loss 0.59 | ppl 1.51 | wps 20236.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23319 | lr 0.000207083 | gnorm 0.933 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 71972
2022-03-05 05:35:39 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 05:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:38:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:39:53 | INFO | train_inner | epoch 243:     82 / 97 loss=0.589, ppl=1.5, wps=20259.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.933, loss_scale=16, train_wall=284, gb_free=8.2, wall=72226
2022-03-05 05:40:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:40:46 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 17.714 | ppl 215051 | wps 33126.5 | wpb 510.9 | bsz 1 | num_updates 23415 | best_loss 7.623
2022-03-05 05:40:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23415 updates
2022-03-05 05:40:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:40:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:40:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 243 @ 23415 updates, score 17.714) (writing took 3.8693208042532206 seconds)
2022-03-05 05:40:50 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 05:40:50 | INFO | train | epoch 243 | loss 0.588 | ppl 1.5 | wps 20219.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23415 | lr 0.000206658 | gnorm 0.934 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 72283
2022-03-05 05:40:50 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 05:40:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:45:13 | INFO | train_inner | epoch 244:     85 / 97 loss=0.586, ppl=1.5, wps=20463.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23500, lr=0.000206284, gnorm=0.928, loss_scale=16, train_wall=281, gb_free=8.2, wall=72546
2022-03-05 05:45:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:45:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:45:56 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 17.634 | ppl 203410 | wps 33770.3 | wpb 510.9 | bsz 1 | num_updates 23511 | best_loss 7.623
2022-03-05 05:45:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23511 updates
2022-03-05 05:45:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:46:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:46:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 244 @ 23511 updates, score 17.634) (writing took 3.873840993270278 seconds)
2022-03-05 05:46:00 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 05:46:00 | INFO | train | epoch 244 | loss 0.585 | ppl 1.5 | wps 20251.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23511 | lr 0.000206236 | gnorm 0.928 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 72594
2022-03-05 05:46:00 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 05:46:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:50:35 | INFO | train_inner | epoch 245:     89 / 97 loss=0.584, ppl=1.5, wps=20335, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23600, lr=0.000205847, gnorm=0.944, loss_scale=16, train_wall=283, gb_free=8.2, wall=72868
2022-03-05 05:51:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:51:06 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 17.696 | ppl 212376 | wps 34029.1 | wpb 510.9 | bsz 1 | num_updates 23608 | best_loss 7.623
2022-03-05 05:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23608 updates
2022-03-05 05:51:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:51:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 245 @ 23608 updates, score 17.696) (writing took 4.08056321926415 seconds)
2022-03-05 05:51:10 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 05:51:10 | INFO | train | epoch 245 | loss 0.584 | ppl 1.5 | wps 20500.9 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 23608 | lr 0.000205812 | gnorm 0.942 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 72903
2022-03-05 05:51:10 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 05:51:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:52:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:55:57 | INFO | train_inner | epoch 246:     93 / 97 loss=0.582, ppl=1.5, wps=20370, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=0.919, loss_scale=16, train_wall=282, gb_free=8.2, wall=73190
2022-03-05 05:56:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:56:15 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 17.659 | ppl 207027 | wps 33945 | wpb 510.9 | bsz 1 | num_updates 23704 | best_loss 7.623
2022-03-05 05:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23704 updates
2022-03-05 05:56:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 05:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 246 @ 23704 updates, score 17.659) (writing took 3.8311421032994986 seconds)
2022-03-05 05:56:19 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 05:56:19 | INFO | train | epoch 246 | loss 0.581 | ppl 1.5 | wps 20348.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23704 | lr 0.000205395 | gnorm 0.918 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 73212
2022-03-05 05:56:19 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 05:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:59:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:01:18 | INFO | train_inner | epoch 247:     97 / 97 loss=0.58, ppl=1.5, wps=20385, ups=0.31, wpb=65451.9, bsz=127.8, num_updates=23800, lr=0.00020498, gnorm=0.92, loss_scale=16, train_wall=282, gb_free=8.2, wall=73511
2022-03-05 06:01:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:01:24 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 17.701 | ppl 213044 | wps 34060.4 | wpb 510.9 | bsz 1 | num_updates 23800 | best_loss 7.623
2022-03-05 06:01:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23800 updates
2022-03-05 06:01:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:01:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:01:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 247 @ 23800 updates, score 17.701) (writing took 3.812380999326706 seconds)
2022-03-05 06:01:28 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 06:01:28 | INFO | train | epoch 247 | loss 0.579 | ppl 1.49 | wps 20353.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23800 | lr 0.00020498 | gnorm 0.918 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 73521
2022-03-05 06:01:28 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 06:01:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:06:34 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 17.678 | ppl 209707 | wps 33790.9 | wpb 510.9 | bsz 1 | num_updates 23897 | best_loss 7.623
2022-03-05 06:06:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23897 updates
2022-03-05 06:06:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:06:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:06:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 248 @ 23897 updates, score 17.678) (writing took 3.935000192373991 seconds)
2022-03-05 06:06:38 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 06:06:38 | INFO | train | epoch 248 | loss 0.578 | ppl 1.49 | wps 20502.9 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 23897 | lr 0.000204564 | gnorm 0.925 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 73831
2022-03-05 06:06:38 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 06:06:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:06:47 | INFO | train_inner | epoch 249:      3 / 97 loss=0.577, ppl=1.49, wps=19877.7, ups=0.3, wpb=65492.9, bsz=127.9, num_updates=23900, lr=0.000204551, gnorm=0.924, loss_scale=32, train_wall=280, gb_free=8.2, wall=73841
2022-03-05 06:07:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:11:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:11:42 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 17.787 | ppl 226150 | wps 34052.6 | wpb 510.9 | bsz 1 | num_updates 23993 | best_loss 7.623
2022-03-05 06:11:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23993 updates
2022-03-05 06:11:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:11:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:11:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 249 @ 23993 updates, score 17.787) (writing took 3.84078161790967 seconds)
2022-03-05 06:11:46 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 06:11:46 | INFO | train | epoch 249 | loss 0.575 | ppl 1.49 | wps 20383.1 | ups 0.31 | wpb 65493.3 | bsz 127.9 | num_updates 23993 | lr 0.000204154 | gnorm 0.925 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 74140
2022-03-05 06:11:46 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 06:11:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:12:08 | INFO | train_inner | epoch 250:      7 / 97 loss=0.574, ppl=1.49, wps=20423.8, ups=0.31, wpb=65495, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.922, loss_scale=16, train_wall=282, gb_free=8.2, wall=74161
2022-03-05 06:15:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:16:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:16:51 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 17.785 | ppl 225896 | wps 34389.3 | wpb 510.9 | bsz 1 | num_updates 24089 | best_loss 7.623
2022-03-05 06:16:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24089 updates
2022-03-05 06:16:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:16:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:16:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 250 @ 24089 updates, score 17.785) (writing took 3.9548035468906164 seconds)
2022-03-05 06:16:55 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 06:16:55 | INFO | train | epoch 250 | loss 0.573 | ppl 1.49 | wps 20395.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24089 | lr 0.000203747 | gnorm 0.917 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 74448
2022-03-05 06:16:55 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 06:16:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:17:28 | INFO | train_inner | epoch 251:     11 / 97 loss=0.572, ppl=1.49, wps=20438.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.921, loss_scale=16, train_wall=281, gb_free=8.2, wall=74482
2022-03-05 06:21:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:21:59 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 17.687 | ppl 210947 | wps 34050 | wpb 510.9 | bsz 1 | num_updates 24186 | best_loss 7.623
2022-03-05 06:21:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24186 updates
2022-03-05 06:21:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 251 @ 24186 updates, score 17.687) (writing took 3.814224824309349 seconds)
2022-03-05 06:22:03 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 06:22:03 | INFO | train | epoch 251 | loss 0.572 | ppl 1.49 | wps 20602.7 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 24186 | lr 0.000203338 | gnorm 0.919 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 74756
2022-03-05 06:22:03 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 06:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:22:46 | INFO | train_inner | epoch 252:     14 / 97 loss=0.57, ppl=1.48, wps=20621.2, ups=0.31, wpb=65495, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.915, loss_scale=32, train_wall=279, gb_free=8.2, wall=74799
2022-03-05 06:23:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:27:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:27:07 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 17.667 | ppl 208076 | wps 33887.8 | wpb 510.9 | bsz 1 | num_updates 24282 | best_loss 7.623
2022-03-05 06:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24282 updates
2022-03-05 06:27:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:27:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:27:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 252 @ 24282 updates, score 17.667) (writing took 3.836709113791585 seconds)
2022-03-05 06:27:11 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 06:27:11 | INFO | train | epoch 252 | loss 0.57 | ppl 1.48 | wps 20384.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24282 | lr 0.000202935 | gnorm 0.917 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 75065
2022-03-05 06:27:11 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 06:27:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:28:07 | INFO | train_inner | epoch 253:     18 / 97 loss=0.569, ppl=1.48, wps=20414.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.921, loss_scale=16, train_wall=282, gb_free=8.2, wall=75120
2022-03-05 06:30:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:32:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:32:16 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 17.895 | ppl 243811 | wps 33781.5 | wpb 510.9 | bsz 1 | num_updates 24378 | best_loss 7.623
2022-03-05 06:32:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24378 updates
2022-03-05 06:32:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:32:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 253 @ 24378 updates, score 17.895) (writing took 3.8197776954621077 seconds)
2022-03-05 06:32:20 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 06:32:20 | INFO | train | epoch 253 | loss 0.568 | ppl 1.48 | wps 20353.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24378 | lr 0.000202535 | gnorm 0.926 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 75374
2022-03-05 06:32:20 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 06:32:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:33:28 | INFO | train_inner | epoch 254:     22 / 97 loss=0.567, ppl=1.48, wps=20393.3, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.923, loss_scale=16, train_wall=282, gb_free=8.2, wall=75441
2022-03-05 06:37:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:37:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:37:25 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 17.812 | ppl 230144 | wps 34207.2 | wpb 510.9 | bsz 1 | num_updates 24474 | best_loss 7.623
2022-03-05 06:37:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24474 updates
2022-03-05 06:37:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:37:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 254 @ 24474 updates, score 17.812) (writing took 3.804135451093316 seconds)
2022-03-05 06:37:29 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 06:37:29 | INFO | train | epoch 254 | loss 0.565 | ppl 1.48 | wps 20385.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24474 | lr 0.000202138 | gnorm 0.904 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 75682
2022-03-05 06:37:29 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 06:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:38:48 | INFO | train_inner | epoch 255:     26 / 97 loss=0.564, ppl=1.48, wps=20442.9, ups=0.31, wpb=65495, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.905, loss_scale=16, train_wall=281, gb_free=8.2, wall=75762
2022-03-05 06:42:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:42:31 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 17.754 | ppl 221033 | wps 34197 | wpb 510.9 | bsz 1 | num_updates 24571 | best_loss 7.623
2022-03-05 06:42:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24571 updates
2022-03-05 06:42:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:42:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:42:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 255 @ 24571 updates, score 17.754) (writing took 3.8781484235078096 seconds)
2022-03-05 06:42:35 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 06:42:35 | INFO | train | epoch 255 | loss 0.564 | ppl 1.48 | wps 20708.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 24571 | lr 0.000201738 | gnorm 0.922 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 75989
2022-03-05 06:42:35 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 06:42:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:44:04 | INFO | train_inner | epoch 256:     29 / 97 loss=0.563, ppl=1.48, wps=20746.5, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.919, loss_scale=32, train_wall=277, gb_free=8.2, wall=76077
2022-03-05 06:44:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:47:38 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 17.692 | ppl 211746 | wps 34680.5 | wpb 510.9 | bsz 1 | num_updates 24667 | best_loss 7.623
2022-03-05 06:47:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24667 updates
2022-03-05 06:47:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 256 @ 24667 updates, score 17.692) (writing took 3.7161099556833506 seconds)
2022-03-05 06:47:42 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 06:47:42 | INFO | train | epoch 256 | loss 0.561 | ppl 1.48 | wps 20528.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24667 | lr 0.000201345 | gnorm 0.912 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 76295
2022-03-05 06:47:42 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 06:47:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:49:22 | INFO | train_inner | epoch 257:     33 / 97 loss=0.561, ppl=1.47, wps=20586.9, ups=0.31, wpb=65495, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.914, loss_scale=16, train_wall=280, gb_free=8.2, wall=76395
2022-03-05 06:52:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:52:44 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 17.876 | ppl 240527 | wps 33717.9 | wpb 510.9 | bsz 1 | num_updates 24763 | best_loss 7.623
2022-03-05 06:52:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24763 updates
2022-03-05 06:52:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:52:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:52:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 257 @ 24763 updates, score 17.876) (writing took 3.8312044367194176 seconds)
2022-03-05 06:52:48 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 06:52:48 | INFO | train | epoch 257 | loss 0.559 | ppl 1.47 | wps 20524.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24763 | lr 0.000200955 | gnorm 0.916 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 76601
2022-03-05 06:52:48 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 06:52:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:54:41 | INFO | train_inner | epoch 258:     37 / 97 loss=0.558, ppl=1.47, wps=20502, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.912, loss_scale=16, train_wall=281, gb_free=8.2, wall=76715
2022-03-05 06:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:54 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 17.831 | ppl 233137 | wps 33360.2 | wpb 510.9 | bsz 1 | num_updates 24860 | best_loss 7.623
2022-03-05 06:57:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24860 updates
2022-03-05 06:57:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:57:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 06:57:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 258 @ 24860 updates, score 17.831) (writing took 3.877228692173958 seconds)
2022-03-05 06:57:58 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 06:57:58 | INFO | train | epoch 258 | loss 0.559 | ppl 1.47 | wps 20514.1 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 24860 | lr 0.000200562 | gnorm 0.915 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 76911
2022-03-05 06:57:58 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 06:57:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:59:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:00:04 | INFO | train_inner | epoch 259:     41 / 97 loss=0.559, ppl=1.47, wps=20279.1, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.918, loss_scale=16, train_wall=283, gb_free=8.2, wall=77038
2022-03-05 07:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:03:04 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 17.766 | ppl 222849 | wps 33460.7 | wpb 510.9 | bsz 1 | num_updates 24956 | best_loss 7.623
2022-03-05 07:03:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24956 updates
2022-03-05 07:03:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:03:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:03:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 259 @ 24956 updates, score 17.766) (writing took 3.9624392259866 seconds)
2022-03-05 07:03:08 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 07:03:08 | INFO | train | epoch 259 | loss 0.556 | ppl 1.47 | wps 20271.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24956 | lr 0.000200176 | gnorm 0.911 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 77221
2022-03-05 07:03:08 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 07:03:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:05:24 | INFO | train_inner | epoch 260:     44 / 97 loss=0.555, ppl=1.47, wps=20491.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25000, lr=0.0002, gnorm=0.915, loss_scale=16, train_wall=280, gb_free=8.2, wall=77357
2022-03-05 07:08:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:08:14 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 17.868 | ppl 239159 | wps 33448.5 | wpb 510.9 | bsz 1 | num_updates 25053 | best_loss 7.623
2022-03-05 07:08:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25053 updates
2022-03-05 07:08:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:08:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:08:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 260 @ 25053 updates, score 17.868) (writing took 3.832833431661129 seconds)
2022-03-05 07:08:18 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 07:08:18 | INFO | train | epoch 260 | loss 0.554 | ppl 1.47 | wps 20471.2 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 25053 | lr 0.000199788 | gnorm 0.914 | loss_scale 32 | train_wall 272 | gb_free 8.2 | wall 77531
2022-03-05 07:08:18 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 07:08:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:10:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:10:47 | INFO | train_inner | epoch 261:     48 / 97 loss=0.553, ppl=1.47, wps=20282.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.907, loss_scale=16, train_wall=283, gb_free=8.2, wall=77680
2022-03-05 07:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:13:25 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 17.824 | ppl 231974 | wps 33637.8 | wpb 510.9 | bsz 1 | num_updates 25149 | best_loss 7.623
2022-03-05 07:13:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25149 updates
2022-03-05 07:13:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:13:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 261 @ 25149 updates, score 17.824) (writing took 3.8010431714355946 seconds)
2022-03-05 07:13:28 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 07:13:28 | INFO | train | epoch 261 | loss 0.553 | ppl 1.47 | wps 20263.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25149 | lr 0.000199407 | gnorm 0.91 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 77842
2022-03-05 07:13:28 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 07:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:16:06 | INFO | train_inner | epoch 262:     51 / 97 loss=0.551, ppl=1.47, wps=20504.9, ups=0.31, wpb=65495, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.909, loss_scale=16, train_wall=280, gb_free=8.2, wall=78000
2022-03-05 07:17:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:18:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:18:35 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 17.836 | ppl 233918 | wps 33674.1 | wpb 510.9 | bsz 1 | num_updates 25245 | best_loss 7.623
2022-03-05 07:18:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25245 updates
2022-03-05 07:18:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:18:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:18:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 262 @ 25245 updates, score 17.836) (writing took 3.85175047442317 seconds)
2022-03-05 07:18:38 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 07:18:38 | INFO | train | epoch 262 | loss 0.551 | ppl 1.46 | wps 20274.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25245 | lr 0.000199027 | gnorm 0.911 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 78152
2022-03-05 07:18:38 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 07:18:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:21:29 | INFO | train_inner | epoch 263:     55 / 97 loss=0.549, ppl=1.46, wps=20278.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.919, loss_scale=16, train_wall=284, gb_free=8.2, wall=78323
2022-03-05 07:23:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:23:45 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 18.008 | ppl 263578 | wps 33699.3 | wpb 510.9 | bsz 1 | num_updates 25342 | best_loss 7.623
2022-03-05 07:23:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25342 updates
2022-03-05 07:23:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:23:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:23:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 263 @ 25342 updates, score 18.008) (writing took 3.7579099237918854 seconds)
2022-03-05 07:23:49 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 07:23:49 | INFO | train | epoch 263 | loss 0.549 | ppl 1.46 | wps 20441.8 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 25342 | lr 0.000198646 | gnorm 0.916 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 78463
2022-03-05 07:23:49 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 07:23:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:24:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:26:52 | INFO | train_inner | epoch 264:     59 / 97 loss=0.549, ppl=1.46, wps=20284.5, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.912, loss_scale=16, train_wall=284, gb_free=8.2, wall=78646
2022-03-05 07:28:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:28:55 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 17.872 | ppl 239854 | wps 33933.5 | wpb 510.9 | bsz 1 | num_updates 25438 | best_loss 7.623
2022-03-05 07:28:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25438 updates
2022-03-05 07:28:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:28:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:28:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 264 @ 25438 updates, score 17.872) (writing took 3.7818072233349085 seconds)
2022-03-05 07:28:59 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 07:28:59 | INFO | train | epoch 264 | loss 0.548 | ppl 1.46 | wps 20284.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25438 | lr 0.000198271 | gnorm 0.917 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 78773
2022-03-05 07:28:59 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 07:28:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:31:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:32:12 | INFO | train_inner | epoch 265:     63 / 97 loss=0.547, ppl=1.46, wps=20469.4, ups=0.31, wpb=65495, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.912, loss_scale=16, train_wall=281, gb_free=8.2, wall=78966
2022-03-05 07:33:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:34:02 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 17.852 | ppl 236554 | wps 34463.9 | wpb 510.9 | bsz 1 | num_updates 25534 | best_loss 7.623
2022-03-05 07:34:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25534 updates
2022-03-05 07:34:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:34:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 265 @ 25534 updates, score 17.852) (writing took 3.9914816319942474 seconds)
2022-03-05 07:34:06 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 07:34:06 | INFO | train | epoch 265 | loss 0.544 | ppl 1.46 | wps 20519.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25534 | lr 0.000197898 | gnorm 0.901 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 79079
2022-03-05 07:34:06 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 07:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:37:28 | INFO | train_inner | epoch 266:     66 / 97 loss=0.542, ppl=1.46, wps=20754.2, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.9, loss_scale=16, train_wall=277, gb_free=8.2, wall=79281
2022-03-05 07:38:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:39:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:39:08 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 17.844 | ppl 235345 | wps 34829.7 | wpb 510.9 | bsz 1 | num_updates 25630 | best_loss 7.623
2022-03-05 07:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25630 updates
2022-03-05 07:39:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:39:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:39:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 266 @ 25630 updates, score 17.844) (writing took 3.7293269615620375 seconds)
2022-03-05 07:39:12 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 07:39:12 | INFO | train | epoch 266 | loss 0.544 | ppl 1.46 | wps 20511.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25630 | lr 0.000197527 | gnorm 0.904 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 79386
2022-03-05 07:39:12 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 07:39:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:42:47 | INFO | train_inner | epoch 267:     70 / 97 loss=0.544, ppl=1.46, wps=20483.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.906, loss_scale=16, train_wall=281, gb_free=8.2, wall=79601
2022-03-05 07:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:44:16 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 17.849 | ppl 236164 | wps 34163.7 | wpb 510.9 | bsz 1 | num_updates 25727 | best_loss 7.623
2022-03-05 07:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25727 updates
2022-03-05 07:44:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 267 @ 25727 updates, score 17.849) (writing took 3.8077151253819466 seconds)
2022-03-05 07:44:20 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 07:44:20 | INFO | train | epoch 267 | loss 0.541 | ppl 1.46 | wps 20615.3 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 25727 | lr 0.000197154 | gnorm 0.903 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 79694
2022-03-05 07:44:20 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 07:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:46:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:48:08 | INFO | train_inner | epoch 268:     74 / 97 loss=0.539, ppl=1.45, wps=20436.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.904, loss_scale=16, train_wall=281, gb_free=8.2, wall=79921
2022-03-05 07:49:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:49:24 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 17.973 | ppl 257234 | wps 34332.5 | wpb 510.9 | bsz 1 | num_updates 25823 | best_loss 7.623
2022-03-05 07:49:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25823 updates
2022-03-05 07:49:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:49:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 268 @ 25823 updates, score 17.973) (writing took 3.9619056079536676 seconds)
2022-03-05 07:49:28 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 07:49:28 | INFO | train | epoch 268 | loss 0.538 | ppl 1.45 | wps 20406.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25823 | lr 0.000196787 | gnorm 0.901 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 80002
2022-03-05 07:49:28 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 07:49:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:53:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:53:28 | INFO | train_inner | epoch 269:     78 / 97 loss=0.538, ppl=1.45, wps=20459.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.9, loss_scale=16, train_wall=281, gb_free=8.2, wall=80241
2022-03-05 07:54:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:54:32 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 17.904 | ppl 245226 | wps 34358.5 | wpb 510.9 | bsz 1 | num_updates 25919 | best_loss 7.623
2022-03-05 07:54:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25919 updates
2022-03-05 07:54:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:54:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 269 @ 25919 updates, score 17.904) (writing took 3.8078607581555843 seconds)
2022-03-05 07:54:36 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 07:54:36 | INFO | train | epoch 269 | loss 0.537 | ppl 1.45 | wps 20421.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25919 | lr 0.000196422 | gnorm 0.898 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 80310
2022-03-05 07:54:36 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 07:54:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:58:45 | INFO | train_inner | epoch 270:     81 / 97 loss=0.536, ppl=1.45, wps=20645.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.9, loss_scale=16, train_wall=278, gb_free=8.2, wall=80559
2022-03-05 07:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:59:41 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 17.943 | ppl 252006 | wps 34205.4 | wpb 510.9 | bsz 1 | num_updates 26016 | best_loss 7.623
2022-03-05 07:59:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26016 updates
2022-03-05 07:59:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:59:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 07:59:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 270 @ 26016 updates, score 17.943) (writing took 3.8056034352630377 seconds)
2022-03-05 07:59:45 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 07:59:45 | INFO | train | epoch 270 | loss 0.537 | ppl 1.45 | wps 20604.2 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 26016 | lr 0.000196056 | gnorm 0.9 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 80618
2022-03-05 07:59:45 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 07:59:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:01:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:04:04 | INFO | train_inner | epoch 271:     85 / 97 loss=0.536, ppl=1.45, wps=20539.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=26100, lr=0.00019574, gnorm=0.891, loss_scale=16, train_wall=280, gb_free=8.2, wall=80877
2022-03-05 08:04:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:04:47 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 17.971 | ppl 257007 | wps 33922.5 | wpb 510.9 | bsz 1 | num_updates 26112 | best_loss 7.623
2022-03-05 08:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26112 updates
2022-03-05 08:04:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 271 @ 26112 updates, score 17.971) (writing took 3.7984207831323147 seconds)
2022-03-05 08:04:51 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 08:04:51 | INFO | train | epoch 271 | loss 0.535 | ppl 1.45 | wps 20525.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26112 | lr 0.000195695 | gnorm 0.895 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 80924
2022-03-05 08:04:51 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 08:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:08:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:09:22 | INFO | train_inner | epoch 272:     89 / 97 loss=0.534, ppl=1.45, wps=20599.9, ups=0.31, wpb=65495, bsz=127.9, num_updates=26200, lr=0.000195366, gnorm=0.903, loss_scale=16, train_wall=280, gb_free=8.2, wall=81195
2022-03-05 08:09:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:09:53 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 17.962 | ppl 255317 | wps 34218.9 | wpb 510.9 | bsz 1 | num_updates 26208 | best_loss 7.623
2022-03-05 08:09:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26208 updates
2022-03-05 08:09:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:09:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 272 @ 26208 updates, score 17.962) (writing took 3.87445299513638 seconds)
2022-03-05 08:09:57 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 08:09:57 | INFO | train | epoch 272 | loss 0.532 | ppl 1.45 | wps 20560.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26208 | lr 0.000195336 | gnorm 0.9 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 81230
2022-03-05 08:09:57 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 08:09:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:14:39 | INFO | train_inner | epoch 273:     92 / 97 loss=0.533, ppl=1.45, wps=20636.1, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=26300, lr=0.000194994, gnorm=0.902, loss_scale=16, train_wall=279, gb_free=8.2, wall=81513
2022-03-05 08:14:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:14:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:15:01 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 18.052 | ppl 271762 | wps 33607.7 | wpb 510.9 | bsz 1 | num_updates 26304 | best_loss 7.623
2022-03-05 08:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26304 updates
2022-03-05 08:15:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:15:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:15:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 273 @ 26304 updates, score 18.052) (writing took 3.8419244028627872 seconds)
2022-03-05 08:15:05 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 08:15:05 | INFO | train | epoch 273 | loss 0.532 | ppl 1.45 | wps 20390.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26304 | lr 0.00019498 | gnorm 0.902 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 81538
2022-03-05 08:15:05 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 08:15:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:20:01 | INFO | train_inner | epoch 274:     96 / 97 loss=0.529, ppl=1.44, wps=20346.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.898, loss_scale=16, train_wall=282, gb_free=8.2, wall=81835
2022-03-05 08:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:20:11 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 18.026 | ppl 266826 | wps 33484 | wpb 510.9 | bsz 1 | num_updates 26401 | best_loss 7.623
2022-03-05 08:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26401 updates
2022-03-05 08:20:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:20:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 274 @ 26401 updates, score 18.026) (writing took 3.8450180497020483 seconds)
2022-03-05 08:20:15 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 08:20:15 | INFO | train | epoch 274 | loss 0.528 | ppl 1.44 | wps 20519 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 26401 | lr 0.000194621 | gnorm 0.898 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 81848
2022-03-05 08:20:15 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 08:20:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:22:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:25:21 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 18.135 | ppl 287828 | wps 33419.3 | wpb 510.9 | bsz 1 | num_updates 26497 | best_loss 7.623
2022-03-05 08:25:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26497 updates
2022-03-05 08:25:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:25:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 275 @ 26497 updates, score 18.135) (writing took 3.88012214563787 seconds)
2022-03-05 08:25:25 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 08:25:25 | INFO | train | epoch 275 | loss 0.527 | ppl 1.44 | wps 20287.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26497 | lr 0.000194268 | gnorm 0.901 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 82158
2022-03-05 08:25:25 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 08:25:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:25:34 | INFO | train_inner | epoch 276:      3 / 97 loss=0.527, ppl=1.44, wps=19678.4, ups=0.3, wpb=65451.9, bsz=127.8, num_updates=26500, lr=0.000194257, gnorm=0.901, loss_scale=16, train_wall=283, gb_free=8.2, wall=82167
2022-03-05 08:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:30:31 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 18.041 | ppl 269624 | wps 33285.3 | wpb 510.9 | bsz 1 | num_updates 26594 | best_loss 7.623
2022-03-05 08:30:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26594 updates
2022-03-05 08:30:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:30:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 276 @ 26594 updates, score 18.041) (writing took 3.861925944685936 seconds)
2022-03-05 08:30:35 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 08:30:35 | INFO | train | epoch 276 | loss 0.526 | ppl 1.44 | wps 20482 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 26594 | lr 0.000193914 | gnorm 0.894 | loss_scale 32 | train_wall 272 | gb_free 8.2 | wall 82468
2022-03-05 08:30:35 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 08:30:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:30:53 | INFO | train_inner | epoch 277:      6 / 97 loss=0.525, ppl=1.44, wps=20499.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=26600, lr=0.000193892, gnorm=0.892, loss_scale=32, train_wall=280, gb_free=8.2, wall=82487
2022-03-05 08:31:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:35:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:35:41 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 18.04 | ppl 269565 | wps 33760.4 | wpb 510.9 | bsz 1 | num_updates 26690 | best_loss 7.623
2022-03-05 08:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26690 updates
2022-03-05 08:35:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:35:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:35:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 277 @ 26690 updates, score 18.04) (writing took 3.8177684005349874 seconds)
2022-03-05 08:35:45 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 08:35:45 | INFO | train | epoch 277 | loss 0.525 | ppl 1.44 | wps 20277.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26690 | lr 0.000193565 | gnorm 0.897 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 82778
2022-03-05 08:35:45 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 08:35:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:36:16 | INFO | train_inner | epoch 278:     10 / 97 loss=0.524, ppl=1.44, wps=20315.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.898, loss_scale=16, train_wall=283, gb_free=8.2, wall=82809
2022-03-05 08:39:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:40:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:40:52 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 18.036 | ppl 268746 | wps 33555.5 | wpb 510.9 | bsz 1 | num_updates 26786 | best_loss 7.623
2022-03-05 08:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26786 updates
2022-03-05 08:40:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:40:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 278 @ 26786 updates, score 18.036) (writing took 3.9147314075380564 seconds)
2022-03-05 08:40:56 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 08:40:56 | INFO | train | epoch 278 | loss 0.523 | ppl 1.44 | wps 20200.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26786 | lr 0.000193217 | gnorm 0.899 | loss_scale 16 | train_wall 273 | gb_free 8.2 | wall 83089
2022-03-05 08:40:56 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 08:40:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:41:39 | INFO | train_inner | epoch 279:     14 / 97 loss=0.522, ppl=1.44, wps=20237.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.9, loss_scale=16, train_wall=284, gb_free=8.2, wall=83133
2022-03-05 08:45:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:46:03 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 18.042 | ppl 269841 | wps 33369 | wpb 510.9 | bsz 1 | num_updates 26883 | best_loss 7.623
2022-03-05 08:46:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26883 updates
2022-03-05 08:46:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:46:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 279 @ 26883 updates, score 18.042) (writing took 3.820010308176279 seconds)
2022-03-05 08:46:07 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 08:46:07 | INFO | train | epoch 279 | loss 0.52 | ppl 1.43 | wps 20442.7 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 26883 | lr 0.000192868 | gnorm 0.89 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 83400
2022-03-05 08:46:07 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 08:46:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:46:59 | INFO | train_inner | epoch 280:     17 / 97 loss=0.52, ppl=1.43, wps=20465, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.886, loss_scale=32, train_wall=281, gb_free=8.2, wall=83453
2022-03-05 08:47:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:51:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:51:13 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 18.158 | ppl 292454 | wps 36010 | wpb 510.9 | bsz 1 | num_updates 26979 | best_loss 7.623
2022-03-05 08:51:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26979 updates
2022-03-05 08:51:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 280 @ 26979 updates, score 18.158) (writing took 3.850886734202504 seconds)
2022-03-05 08:51:17 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 08:51:17 | INFO | train | epoch 280 | loss 0.521 | ppl 1.43 | wps 20290.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26979 | lr 0.000192525 | gnorm 0.894 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 83710
2022-03-05 08:51:17 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 08:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:52:22 | INFO | train_inner | epoch 281:     21 / 97 loss=0.519, ppl=1.43, wps=20319.2, ups=0.31, wpb=65495, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.893, loss_scale=16, train_wall=283, gb_free=8.2, wall=83775
2022-03-05 08:54:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:56:22 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 18.15 | ppl 290768 | wps 34128.1 | wpb 510.9 | bsz 1 | num_updates 27075 | best_loss 7.623
2022-03-05 08:56:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27075 updates
2022-03-05 08:56:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:56:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 08:56:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 281 @ 27075 updates, score 18.15) (writing took 3.8298310209065676 seconds)
2022-03-05 08:56:25 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 08:56:25 | INFO | train | epoch 281 | loss 0.518 | ppl 1.43 | wps 20353.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27075 | lr 0.000192183 | gnorm 0.89 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 84019
2022-03-05 08:56:25 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 08:56:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:57:42 | INFO | train_inner | epoch 282:     25 / 97 loss=0.518, ppl=1.43, wps=20429.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.892, loss_scale=16, train_wall=282, gb_free=8.2, wall=84096
2022-03-05 09:01:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:01:30 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 18.023 | ppl 266403 | wps 33045.8 | wpb 510.9 | bsz 1 | num_updates 27172 | best_loss 7.623
2022-03-05 09:01:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27172 updates
2022-03-05 09:01:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 282 @ 27172 updates, score 18.023) (writing took 3.8038027472794056 seconds)
2022-03-05 09:01:34 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 09:01:34 | INFO | train | epoch 282 | loss 0.516 | ppl 1.43 | wps 20583.4 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 27172 | lr 0.00019184 | gnorm 0.888 | loss_scale 32 | train_wall 270 | gb_free 8.2 | wall 84327
2022-03-05 09:01:34 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 09:01:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:02:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:03:04 | INFO | train_inner | epoch 283:     29 / 97 loss=0.516, ppl=1.43, wps=20376.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.885, loss_scale=16, train_wall=282, gb_free=8.2, wall=84417
2022-03-05 09:06:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:06:39 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 18.164 | ppl 293799 | wps 34471.7 | wpb 510.9 | bsz 1 | num_updates 27268 | best_loss 7.623
2022-03-05 09:06:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27268 updates
2022-03-05 09:06:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:06:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:06:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 283 @ 27268 updates, score 18.164) (writing took 3.8228563833981752 seconds)
2022-03-05 09:06:43 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 09:06:43 | INFO | train | epoch 283 | loss 0.516 | ppl 1.43 | wps 20373.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27268 | lr 0.000191502 | gnorm 0.889 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 84636
2022-03-05 09:06:43 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 09:06:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:08:21 | INFO | train_inner | epoch 284:     32 / 97 loss=0.515, ppl=1.43, wps=20674.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.89, loss_scale=16, train_wall=278, gb_free=8.2, wall=84734
2022-03-05 09:08:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:11:45 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 18.111 | ppl 283166 | wps 34576.4 | wpb 510.9 | bsz 1 | num_updates 27364 | best_loss 7.623
2022-03-05 09:11:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27364 updates
2022-03-05 09:11:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 284 @ 27364 updates, score 18.111) (writing took 3.8130393531173468 seconds)
2022-03-05 09:11:49 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 09:11:49 | INFO | train | epoch 284 | loss 0.514 | ppl 1.43 | wps 20508.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27364 | lr 0.000191166 | gnorm 0.888 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 84943
2022-03-05 09:11:49 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 09:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:13:39 | INFO | train_inner | epoch 285:     36 / 97 loss=0.512, ppl=1.43, wps=20551.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.884, loss_scale=16, train_wall=280, gb_free=8.2, wall=85053
2022-03-05 09:16:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:16:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:16:52 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 18.118 | ppl 284563 | wps 34396.4 | wpb 510.9 | bsz 1 | num_updates 27460 | best_loss 7.623
2022-03-05 09:16:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27460 updates
2022-03-05 09:16:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:16:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:16:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 285 @ 27460 updates, score 18.118) (writing took 3.818031644448638 seconds)
2022-03-05 09:16:56 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 09:16:56 | INFO | train | epoch 285 | loss 0.511 | ppl 1.43 | wps 20532.1 | ups 0.31 | wpb 65533.8 | bsz 128 | num_updates 27460 | lr 0.000190831 | gnorm 0.885 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 85249
2022-03-05 09:16:56 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 09:16:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:18:58 | INFO | train_inner | epoch 286:     40 / 97 loss=0.511, ppl=1.43, wps=20527.4, ups=0.31, wpb=65531.7, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.892, loss_scale=16, train_wall=281, gb_free=8.2, wall=85372
2022-03-05 09:21:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:21:59 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 18.113 | ppl 283535 | wps 34543.5 | wpb 510.9 | bsz 1 | num_updates 27557 | best_loss 7.623
2022-03-05 09:21:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27557 updates
2022-03-05 09:21:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 286 @ 27557 updates, score 18.113) (writing took 3.78349632024765 seconds)
2022-03-05 09:22:03 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 09:22:03 | INFO | train | epoch 286 | loss 0.512 | ppl 1.43 | wps 20691.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 27557 | lr 0.000190495 | gnorm 0.895 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 85556
2022-03-05 09:22:03 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 09:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:24:13 | INFO | train_inner | epoch 287:     43 / 97 loss=0.509, ppl=1.42, wps=20789.4, ups=0.32, wpb=65495, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.889, loss_scale=32, train_wall=277, gb_free=8.2, wall=85687
2022-03-05 09:24:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:26:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:05 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 18.03 | ppl 267577 | wps 34380.1 | wpb 510.9 | bsz 1 | num_updates 27653 | best_loss 7.623
2022-03-05 09:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27653 updates
2022-03-05 09:27:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:27:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 287 @ 27653 updates, score 18.03) (writing took 3.7717924024909735 seconds)
2022-03-05 09:27:08 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 09:27:08 | INFO | train | epoch 287 | loss 0.508 | ppl 1.42 | wps 20561.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27653 | lr 0.000190164 | gnorm 0.88 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 85862
2022-03-05 09:27:08 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 09:27:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:29:32 | INFO | train_inner | epoch 288:     47 / 97 loss=0.51, ppl=1.42, wps=20556.1, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.883, loss_scale=16, train_wall=280, gb_free=8.2, wall=86005
2022-03-05 09:31:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:32:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:32:12 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 18.08 | ppl 277136 | wps 33761.9 | wpb 510.9 | bsz 1 | num_updates 27749 | best_loss 7.623
2022-03-05 09:32:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27749 updates
2022-03-05 09:32:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:32:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:32:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 288 @ 27749 updates, score 18.08) (writing took 4.035903923213482 seconds)
2022-03-05 09:32:16 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 09:32:16 | INFO | train | epoch 288 | loss 0.508 | ppl 1.42 | wps 20428.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27749 | lr 0.000189835 | gnorm 0.885 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 86170
2022-03-05 09:32:16 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 09:32:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:34:53 | INFO | train_inner | epoch 289:     51 / 97 loss=0.507, ppl=1.42, wps=20424.3, ups=0.31, wpb=65495, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.89, loss_scale=16, train_wall=281, gb_free=8.2, wall=86326
2022-03-05 09:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:37:21 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 18.367 | ppl 338145 | wps 33989.9 | wpb 510.9 | bsz 1 | num_updates 27846 | best_loss 7.623
2022-03-05 09:37:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27846 updates
2022-03-05 09:37:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:37:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:37:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 289 @ 27846 updates, score 18.367) (writing took 3.810061039403081 seconds)
2022-03-05 09:37:25 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 09:37:25 | INFO | train | epoch 289 | loss 0.507 | ppl 1.42 | wps 20590.3 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 27846 | lr 0.000189504 | gnorm 0.888 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 86478
2022-03-05 09:37:25 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 09:37:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:39:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:40:13 | INFO | train_inner | epoch 290:     55 / 97 loss=0.506, ppl=1.42, wps=20472.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.89, loss_scale=16, train_wall=281, gb_free=8.2, wall=86646
2022-03-05 09:42:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:42:29 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 18.187 | ppl 298514 | wps 33458.6 | wpb 510.9 | bsz 1 | num_updates 27942 | best_loss 7.623
2022-03-05 09:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27942 updates
2022-03-05 09:42:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:42:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:42:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 290 @ 27942 updates, score 18.187) (writing took 3.843525556847453 seconds)
2022-03-05 09:42:33 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 09:42:33 | INFO | train | epoch 290 | loss 0.504 | ppl 1.42 | wps 20399 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27942 | lr 0.000189178 | gnorm 0.892 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 86786
2022-03-05 09:42:33 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 09:42:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:45:32 | INFO | train_inner | epoch 291:     58 / 97 loss=0.503, ppl=1.42, wps=20524.6, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.884, loss_scale=16, train_wall=280, gb_free=8.2, wall=86965
2022-03-05 09:46:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:47:39 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 18.252 | ppl 312093 | wps 33119.4 | wpb 510.9 | bsz 1 | num_updates 28038 | best_loss 7.623
2022-03-05 09:47:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28038 updates
2022-03-05 09:47:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:47:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 291 @ 28038 updates, score 18.252) (writing took 3.9134761914610863 seconds)
2022-03-05 09:47:43 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 09:47:43 | INFO | train | epoch 291 | loss 0.503 | ppl 1.42 | wps 20308.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28038 | lr 0.000188854 | gnorm 0.882 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 87096
2022-03-05 09:47:43 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 09:47:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:50:55 | INFO | train_inner | epoch 292:     62 / 97 loss=0.504, ppl=1.42, wps=20274.4, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.888, loss_scale=16, train_wall=283, gb_free=8.2, wall=87288
2022-03-05 09:52:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:52:49 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 18.215 | ppl 304201 | wps 33952.8 | wpb 510.9 | bsz 1 | num_updates 28135 | best_loss 7.623
2022-03-05 09:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28135 updates
2022-03-05 09:52:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:52:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:52:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 292 @ 28135 updates, score 18.215) (writing took 3.840091684833169 seconds)
2022-03-05 09:52:53 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 09:52:53 | INFO | train | epoch 292 | loss 0.502 | ppl 1.42 | wps 20460.9 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 28135 | lr 0.000188528 | gnorm 0.89 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 87406
2022-03-05 09:52:53 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 09:52:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:53:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:56:17 | INFO | train_inner | epoch 293:     66 / 97 loss=0.5, ppl=1.41, wps=20303.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.88, loss_scale=16, train_wall=283, gb_free=8.2, wall=87611
2022-03-05 09:57:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:57:59 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 18.242 | ppl 310105 | wps 33726.1 | wpb 510.9 | bsz 1 | num_updates 28231 | best_loss 7.623
2022-03-05 09:57:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28231 updates
2022-03-05 09:57:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:58:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 09:58:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 293 @ 28231 updates, score 18.242) (writing took 3.785640448331833 seconds)
2022-03-05 09:58:03 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 09:58:03 | INFO | train | epoch 293 | loss 0.501 | ppl 1.42 | wps 20264.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28231 | lr 0.000188207 | gnorm 0.885 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 87717
2022-03-05 09:58:03 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 09:58:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:01:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:01:40 | INFO | train_inner | epoch 294:     70 / 97 loss=0.5, ppl=1.41, wps=20290.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.884, loss_scale=16, train_wall=283, gb_free=8.2, wall=87934
2022-03-05 10:03:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:03:10 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 18.191 | ppl 299316 | wps 33535 | wpb 510.9 | bsz 1 | num_updates 28327 | best_loss 7.623
2022-03-05 10:03:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28327 updates
2022-03-05 10:03:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:03:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:03:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 294 @ 28327 updates, score 18.191) (writing took 3.848291276022792 seconds)
2022-03-05 10:03:14 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 10:03:14 | INFO | train | epoch 294 | loss 0.498 | ppl 1.41 | wps 20259.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28327 | lr 0.000187888 | gnorm 0.875 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 88027
2022-03-05 10:03:14 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 10:03:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:07:00 | INFO | train_inner | epoch 295:     73 / 97 loss=0.499, ppl=1.41, wps=20504.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.886, loss_scale=16, train_wall=280, gb_free=8.2, wall=88253
2022-03-05 10:08:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:08:20 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 18.192 | ppl 299536 | wps 33652.8 | wpb 510.9 | bsz 1 | num_updates 28424 | best_loss 7.623
2022-03-05 10:08:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28424 updates
2022-03-05 10:08:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:08:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:08:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 295 @ 28424 updates, score 18.192) (writing took 3.8522444739937782 seconds)
2022-03-05 10:08:24 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 10:08:24 | INFO | train | epoch 295 | loss 0.497 | ppl 1.41 | wps 20478.6 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 28424 | lr 0.000187567 | gnorm 0.887 | loss_scale 32 | train_wall 272 | gb_free 8.2 | wall 88337
2022-03-05 10:08:24 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 10:08:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:10:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:12:22 | INFO | train_inner | epoch 296:     77 / 97 loss=0.495, ppl=1.41, wps=20289.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.885, loss_scale=16, train_wall=283, gb_free=8.2, wall=88576
2022-03-05 10:13:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:13:30 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 18.268 | ppl 315740 | wps 33665.9 | wpb 510.9 | bsz 1 | num_updates 28520 | best_loss 7.623
2022-03-05 10:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28520 updates
2022-03-05 10:13:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:13:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 296 @ 28520 updates, score 18.268) (writing took 3.8073182702064514 seconds)
2022-03-05 10:13:34 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 10:13:34 | INFO | train | epoch 296 | loss 0.496 | ppl 1.41 | wps 20295.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28520 | lr 0.000187251 | gnorm 0.887 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 88647
2022-03-05 10:13:34 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 10:13:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:17:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:17:43 | INFO | train_inner | epoch 297:     81 / 97 loss=0.496, ppl=1.41, wps=20405, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28600, lr=0.000186989, gnorm=0.878, loss_scale=16, train_wall=282, gb_free=8.2, wall=88897
2022-03-05 10:18:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:18:39 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 18.304 | ppl 323597 | wps 32987.8 | wpb 510.9 | bsz 1 | num_updates 28616 | best_loss 7.623
2022-03-05 10:18:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28616 updates
2022-03-05 10:18:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:18:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:18:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 297 @ 28616 updates, score 18.304) (writing took 3.9697245359420776 seconds)
2022-03-05 10:18:43 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 10:18:43 | INFO | train | epoch 297 | loss 0.494 | ppl 1.41 | wps 20329.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28616 | lr 0.000186937 | gnorm 0.877 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 88956
2022-03-05 10:18:43 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 10:18:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:23:02 | INFO | train_inner | epoch 298:     84 / 97 loss=0.493, ppl=1.41, wps=20582.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.879, loss_scale=16, train_wall=279, gb_free=8.2, wall=89215
2022-03-05 10:23:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:23:48 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 18.324 | ppl 328194 | wps 34035.6 | wpb 510.9 | bsz 1 | num_updates 28713 | best_loss 7.623
2022-03-05 10:23:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28713 updates
2022-03-05 10:23:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:23:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 298 @ 28713 updates, score 18.324) (writing took 3.833265218883753 seconds)
2022-03-05 10:23:52 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 10:23:52 | INFO | train | epoch 298 | loss 0.493 | ppl 1.41 | wps 20571.8 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 28713 | lr 0.000186621 | gnorm 0.879 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 89265
2022-03-05 10:23:52 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 10:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:25:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:28:22 | INFO | train_inner | epoch 299:     88 / 97 loss=0.493, ppl=1.41, wps=20423.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28800, lr=0.000186339, gnorm=0.878, loss_scale=16, train_wall=282, gb_free=8.2, wall=89536
2022-03-05 10:28:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:28:56 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 18.342 | ppl 332276 | wps 33567.8 | wpb 510.9 | bsz 1 | num_updates 28809 | best_loss 7.623
2022-03-05 10:28:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28809 updates
2022-03-05 10:28:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:29:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 299 @ 28809 updates, score 18.342) (writing took 3.8487284872680902 seconds)
2022-03-05 10:29:00 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 10:29:00 | INFO | train | epoch 299 | loss 0.491 | ppl 1.41 | wps 20379.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28809 | lr 0.00018631 | gnorm 0.874 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 89574
2022-03-05 10:29:00 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 10:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:33:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:33:43 | INFO | train_inner | epoch 300:     92 / 97 loss=0.492, ppl=1.41, wps=20391.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.875, loss_scale=16, train_wall=282, gb_free=8.2, wall=89857
2022-03-05 10:33:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:34:05 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 18.207 | ppl 302688 | wps 33730.5 | wpb 510.9 | bsz 1 | num_updates 28905 | best_loss 7.623
2022-03-05 10:34:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28905 updates
2022-03-05 10:34:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:34:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:34:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 300 @ 28905 updates, score 18.207) (writing took 3.9722628202289343 seconds)
2022-03-05 10:34:09 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 10:34:09 | INFO | train | epoch 300 | loss 0.491 | ppl 1.41 | wps 20353 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28905 | lr 0.000186 | gnorm 0.877 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 89882
2022-03-05 10:34:09 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 10:34:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:39:01 | INFO | train_inner | epoch 301:     95 / 97 loss=0.49, ppl=1.4, wps=20641.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=29000, lr=0.000185695, gnorm=0.876, loss_scale=16, train_wall=278, gb_free=8.2, wall=90174
2022-03-05 10:39:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:39:13 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 18.284 | ppl 319092 | wps 34065 | wpb 510.9 | bsz 1 | num_updates 29002 | best_loss 7.623
2022-03-05 10:39:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 29002 updates
2022-03-05 10:39:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:39:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:39:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 301 @ 29002 updates, score 18.284) (writing took 3.829731145873666 seconds)
2022-03-05 10:39:17 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 10:39:17 | INFO | train | epoch 301 | loss 0.49 | ppl 1.4 | wps 20639.9 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 29002 | lr 0.000185689 | gnorm 0.875 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 90190
2022-03-05 10:39:17 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 10:39:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:44:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:44:18 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 18.365 | ppl 337613 | wps 34828.8 | wpb 510.9 | bsz 1 | num_updates 29098 | best_loss 7.623
2022-03-05 10:44:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29098 updates
2022-03-05 10:44:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:44:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:44:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 302 @ 29098 updates, score 18.365) (writing took 3.7805865556001663 seconds)
2022-03-05 10:44:22 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 10:44:22 | INFO | train | epoch 302 | loss 0.488 | ppl 1.4 | wps 20602.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29098 | lr 0.000185382 | gnorm 0.872 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 90495
2022-03-05 10:44:22 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 10:44:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:44:28 | INFO | train_inner | epoch 303:      2 / 97 loss=0.488, ppl=1.4, wps=19982.7, ups=0.31, wpb=65451.9, bsz=127.8, num_updates=29100, lr=0.000185376, gnorm=0.872, loss_scale=16, train_wall=279, gb_free=8.2, wall=90502
2022-03-05 10:47:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:49:22 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 18.371 | ppl 339122 | wps 34071.1 | wpb 510.9 | bsz 1 | num_updates 29194 | best_loss 7.623
2022-03-05 10:49:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29194 updates
2022-03-05 10:49:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:49:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:49:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 303 @ 29194 updates, score 18.371) (writing took 3.7455771155655384 seconds)
2022-03-05 10:49:26 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 10:49:26 | INFO | train | epoch 303 | loss 0.486 | ppl 1.4 | wps 20673.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 29194 | lr 0.000185077 | gnorm 0.876 | loss_scale 16 | train_wall 267 | gb_free 8.2 | wall 90800
2022-03-05 10:49:26 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 10:49:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:49:45 | INFO | train_inner | epoch 304:      6 / 97 loss=0.485, ppl=1.4, wps=20697.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=29200, lr=0.000185058, gnorm=0.874, loss_scale=16, train_wall=278, gb_free=8.2, wall=90818
2022-03-05 10:54:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:54:29 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 18.299 | ppl 322475 | wps 33852.6 | wpb 510.9 | bsz 1 | num_updates 29291 | best_loss 7.623
2022-03-05 10:54:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29291 updates
2022-03-05 10:54:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:54:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 304 @ 29291 updates, score 18.299) (writing took 3.846388563513756 seconds)
2022-03-05 10:54:33 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 10:54:33 | INFO | train | epoch 304 | loss 0.485 | ppl 1.4 | wps 20698.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 29291 | lr 0.000184771 | gnorm 0.878 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 91106
2022-03-05 10:54:33 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 10:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:55:01 | INFO | train_inner | epoch 305:      9 / 97 loss=0.485, ppl=1.4, wps=20712.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=29300, lr=0.000184742, gnorm=0.879, loss_scale=32, train_wall=278, gb_free=8.2, wall=91134
2022-03-05 10:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:59:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:59:38 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 18.266 | ppl 315270 | wps 33880.9 | wpb 510.9 | bsz 1 | num_updates 29387 | best_loss 7.623
2022-03-05 10:59:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29387 updates
2022-03-05 10:59:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:59:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 10:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 305 @ 29387 updates, score 18.266) (writing took 3.8581428341567516 seconds)
2022-03-05 10:59:41 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 10:59:41 | INFO | train | epoch 305 | loss 0.484 | ppl 1.4 | wps 20389 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29387 | lr 0.000184469 | gnorm 0.864 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 91415
2022-03-05 10:59:41 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 10:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:00:21 | INFO | train_inner | epoch 306:     13 / 97 loss=0.483, ppl=1.4, wps=20423.6, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.863, loss_scale=16, train_wall=282, gb_free=8.2, wall=91455
2022-03-05 11:03:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:04:46 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 18.363 | ppl 337039 | wps 33527.9 | wpb 510.9 | bsz 1 | num_updates 29483 | best_loss 7.623
2022-03-05 11:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29483 updates
2022-03-05 11:04:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:04:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 306 @ 29483 updates, score 18.363) (writing took 3.9022538419812918 seconds)
2022-03-05 11:04:50 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 11:04:50 | INFO | train | epoch 306 | loss 0.482 | ppl 1.4 | wps 20398.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29483 | lr 0.000184168 | gnorm 0.874 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 91723
2022-03-05 11:04:50 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 11:04:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:05:42 | INFO | train_inner | epoch 307:     17 / 97 loss=0.481, ppl=1.4, wps=20408, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.872, loss_scale=16, train_wall=282, gb_free=8.2, wall=91776
2022-03-05 11:09:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:09:57 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 18.463 | ppl 361447 | wps 33058.1 | wpb 510.9 | bsz 1 | num_updates 29580 | best_loss 7.623
2022-03-05 11:09:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29580 updates
2022-03-05 11:09:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:10:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:10:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 307 @ 29580 updates, score 18.463) (writing took 3.8453662879765034 seconds)
2022-03-05 11:10:01 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 11:10:01 | INFO | train | epoch 307 | loss 0.481 | ppl 1.4 | wps 20421.7 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 29580 | lr 0.000183866 | gnorm 0.875 | loss_scale 32 | train_wall 273 | gb_free 8.2 | wall 92034
2022-03-05 11:10:01 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 11:10:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:10:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:11:06 | INFO | train_inner | epoch 308:     21 / 97 loss=0.481, ppl=1.4, wps=20235.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.877, loss_scale=16, train_wall=284, gb_free=8.2, wall=92099
2022-03-05 11:15:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:15:07 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 18.33 | ppl 329510 | wps 33179.3 | wpb 510.9 | bsz 1 | num_updates 29676 | best_loss 7.623
2022-03-05 11:15:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29676 updates
2022-03-05 11:15:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:15:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 308 @ 29676 updates, score 18.33) (writing took 3.8241938166320324 seconds)
2022-03-05 11:15:11 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 11:15:11 | INFO | train | epoch 308 | loss 0.48 | ppl 1.39 | wps 20246.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29676 | lr 0.000183568 | gnorm 0.871 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 92345
2022-03-05 11:15:11 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 11:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:16:26 | INFO | train_inner | epoch 309:     24 / 97 loss=0.479, ppl=1.39, wps=20472, ups=0.31, wpb=65495, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.872, loss_scale=16, train_wall=281, gb_free=8.2, wall=92419
2022-03-05 11:17:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:20:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:20:18 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 18.35 | ppl 334191 | wps 33381.4 | wpb 510.9 | bsz 1 | num_updates 29772 | best_loss 7.623
2022-03-05 11:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 29772 updates
2022-03-05 11:20:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:20:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt
2022-03-05 11:20:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#3/checkpoint_last.pt (epoch 309 @ 29772 updates, score 18.35) (writing took 3.890677371993661 seconds)
2022-03-05 11:20:22 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-03-05 11:20:22 | INFO | train | epoch 309 | loss 0.479 | ppl 1.39 | wps 20253.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29772 | lr 0.000183272 | gnorm 0.88 | loss_scale 16 | train_wall 272 | gb_free 8.2 | wall 92655
2022-03-05 11:20:22 | INFO | fairseq.trainer | begin training epoch 310
2022-03-05 11:20:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:21:49 | INFO | train_inner | epoch 310:     28 / 97 loss=0.478, ppl=1.39, wps=20303.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=29800, lr=0.000183186, gnorm=0.875, loss_scale=16, train_wall=283, gb_free=8.2, wall=92742
2022-03-05 11:24:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/cross_entropy.py", line 35, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4318, in multi_head_attention_forward
    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
KeyboardInterrupt
