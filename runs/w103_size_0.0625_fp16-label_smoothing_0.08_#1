Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 207133293: <w103_size_0.0625_fp16_label_smoothing_0.08_#1> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_label_smoothing_0.08_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:38:41 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 13:22:57 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 13:22:57 2022
Terminated at Sat Mar  5 11:25:23 2022
Results reported at Sat Mar  5 11:25:23 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.08 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   79267.50 sec.
    Max Memory :                                 8372 MB
    Average Memory :                             4286.15 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11628.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   79344 sec.
    Turnaround time :                            92802 sec.

The output (if any) follows:

2022-03-04 13:23:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.08, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 13:23:05 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 13:23:07 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 13:23:07 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 13:23:07 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 13:23:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 13:23:07 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 13:23:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 13:23:07 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 13:23:13 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 13:23:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 13:23:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-04 13:23:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 13:23:13 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 13:23:13 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 13:23:13 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 13:23:13 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 13:23:13 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 13:23:13 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 13:23:13 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 13:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:23:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 13:23:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:23:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:23:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:27:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:27:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.731 | nll_loss 14.461 | ppl 22555.7 | wps 45179.1 | wpb 510.9 | bsz 1 | num_updates 93
2022-03-04 13:27:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 93 updates
2022-03-04 13:27:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:27:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:27:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 1 @ 93 updates, score 14.731) (writing took 5.845484507270157 seconds)
2022-03-04 13:27:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 13:27:48 | INFO | train | epoch 001 | loss 16.075 | nll_loss 15.921 | ppl 62051.8 | wps 24579.6 | ups 0.38 | wpb 65489.7 | bsz 127.9 | num_updates 93 | lr 1.17227e-05 | gnorm 3.234 | loss_scale 8 | train_wall 242 | gb_free 21 | wall 275
2022-03-04 13:27:48 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 13:27:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:28:06 | INFO | train_inner | epoch 002:      7 / 97 loss=15.983, nll_loss=15.821, ppl=57878.9, wps=24640.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.12, loss_scale=8, train_wall=259, gb_free=21, wall=293
2022-03-04 13:29:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:31:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:32:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.225 | nll_loss 12.823 | ppl 7243.84 | wps 45255.4 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 13.225
2022-03-04 13:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 13:32:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:32:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:32:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 2 @ 189 updates, score 13.225) (writing took 5.812409471720457 seconds)
2022-03-04 13:32:06 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 13:32:06 | INFO | train | epoch 002 | loss 14.106 | nll_loss 13.785 | ppl 14119.6 | wps 24355.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.466 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 533
2022-03-04 13:32:06 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 13:32:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:32:34 | INFO | train_inner | epoch 003:     11 / 97 loss=13.959, nll_loss=13.626, ppl=12642.9, wps=24413.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.43, loss_scale=8, train_wall=235, gb_free=21, wall=561
2022-03-04 13:36:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:36:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.695 | nll_loss 11.121 | ppl 2226.66 | wps 45307.1 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.695
2022-03-04 13:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 13:36:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:36:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:36:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.695) (writing took 5.837703466415405 seconds)
2022-03-04 13:36:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 13:36:24 | INFO | train | epoch 003 | loss 12.444 | nll_loss 11.963 | ppl 3991.96 | wps 24623.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 1.026 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 791
2022-03-04 13:36:24 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 13:36:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:37:00 | INFO | train_inner | epoch 004:     14 / 97 loss=12.247, nll_loss=11.742, ppl=3426.34, wps=24654, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=0.957, loss_scale=16, train_wall=233, gb_free=21, wall=827
2022-03-04 13:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:40:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.992 | nll_loss 10.285 | ppl 1248 | wps 44753.7 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 10.992
2022-03-04 13:40:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 13:40:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:40:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 4 @ 383 updates, score 10.992) (writing took 6.142732888460159 seconds)
2022-03-04 13:40:42 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 13:40:42 | INFO | train | epoch 004 | loss 11.272 | nll_loss 10.63 | ppl 1584.57 | wps 24583.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.539 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 1049
2022-03-04 13:40:42 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 13:40:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:41:26 | INFO | train_inner | epoch 005:     17 / 97 loss=11.168, nll_loss=10.505, ppl=1453.06, wps=24616.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.502, loss_scale=32, train_wall=233, gb_free=21, wall=1093
2022-03-04 13:44:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:44:55 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.661 | nll_loss 9.889 | ppl 948.07 | wps 45386.7 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 10.661
2022-03-04 13:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 13:44:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:44:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:45:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 5 @ 480 updates, score 10.661) (writing took 6.00225505977869 seconds)
2022-03-04 13:45:01 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 13:45:01 | INFO | train | epoch 005 | loss 10.806 | nll_loss 10.066 | ppl 1071.65 | wps 24597.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.457 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1308
2022-03-04 13:45:01 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 13:45:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:45:52 | INFO | train_inner | epoch 006:     20 / 97 loss=10.746, nll_loss=9.994, ppl=1019.73, wps=24625.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.467, loss_scale=32, train_wall=233, gb_free=21, wall=1359
2022-03-04 13:47:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:49:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:49:13 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.382 | nll_loss 9.573 | ppl 761.42 | wps 45091.1 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 10.382
2022-03-04 13:49:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 13:49:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:49:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:49:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 6 @ 576 updates, score 10.382) (writing took 5.822799948044121 seconds)
2022-03-04 13:49:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 13:49:19 | INFO | train | epoch 006 | loss 10.508 | nll_loss 9.719 | ppl 843.06 | wps 24347.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.519 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1566
2022-03-04 13:49:19 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 13:49:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:50:20 | INFO | train_inner | epoch 007:     24 / 97 loss=10.442, nll_loss=9.645, ppl=800.42, wps=24405, ups=0.37, wpb=65495, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.53, loss_scale=32, train_wall=235, gb_free=21, wall=1627
2022-03-04 13:52:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:53:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.151 | nll_loss 9.307 | ppl 633.32 | wps 45049.2 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 10.151
2022-03-04 13:53:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 13:53:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:53:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 7 @ 672 updates, score 10.151) (writing took 5.775686313398182 seconds)
2022-03-04 13:53:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 13:53:37 | INFO | train | epoch 007 | loss 10.246 | nll_loss 9.425 | ppl 687.18 | wps 24370.9 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.593 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1824
2022-03-04 13:53:37 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 13:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:54:48 | INFO | train_inner | epoch 008:     28 / 97 loss=10.182, nll_loss=9.353, ppl=653.81, wps=24419.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.626, loss_scale=32, train_wall=235, gb_free=21, wall=1895
2022-03-04 13:57:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:57:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:57:49 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.958 | nll_loss 9.091 | ppl 545.28 | wps 45331 | wpb 510.9 | bsz 1 | num_updates 768 | best_loss 9.958
2022-03-04 13:57:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 768 updates
2022-03-04 13:57:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 13:57:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 8 @ 768 updates, score 9.958) (writing took 5.673643073998392 seconds)
2022-03-04 13:57:55 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 13:57:55 | INFO | train | epoch 008 | loss 10.014 | nll_loss 9.165 | ppl 573.96 | wps 24372.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 768 | lr 9.60808e-05 | gnorm 0.732 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 2082
2022-03-04 13:57:55 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 13:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:59:16 | INFO | train_inner | epoch 009:     32 / 97 loss=9.946, nll_loss=9.089, ppl=544.5, wps=24427.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.743, loss_scale=16, train_wall=235, gb_free=21, wall=2163
2022-03-04 14:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:02:07 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.754 | nll_loss 8.865 | ppl 466.25 | wps 45242.4 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 9.754
2022-03-04 14:02:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 14:02:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:02:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:02:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 9 @ 865 updates, score 9.754) (writing took 5.453857673332095 seconds)
2022-03-04 14:02:12 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 14:02:12 | INFO | train | epoch 009 | loss 9.797 | nll_loss 8.921 | ppl 484.73 | wps 24671.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.743 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 2339
2022-03-04 14:02:12 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 14:02:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:03:42 | INFO | train_inner | epoch 010:     35 / 97 loss=9.722, nll_loss=8.837, ppl=457.18, wps=24703.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.765, loss_scale=32, train_wall=233, gb_free=21, wall=2429
2022-03-04 14:06:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:06:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.591 | nll_loss 8.681 | ppl 410.52 | wps 45415.7 | wpb 510.9 | bsz 1 | num_updates 962 | best_loss 9.591
2022-03-04 14:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 962 updates
2022-03-04 14:06:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:06:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 10 @ 962 updates, score 9.591) (writing took 5.388587665744126 seconds)
2022-03-04 14:06:30 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 14:06:30 | INFO | train | epoch 010 | loss 9.591 | nll_loss 8.69 | ppl 412.96 | wps 24669.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 962 | lr 0.000120326 | gnorm 0.799 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2597
2022-03-04 14:06:30 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 14:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:08:07 | INFO | train_inner | epoch 011:     38 / 97 loss=9.515, nll_loss=8.604, ppl=389.17, wps=24692.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.825, loss_scale=32, train_wall=233, gb_free=21, wall=2694
2022-03-04 14:09:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:10:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:10:42 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.433 | nll_loss 8.501 | ppl 362.2 | wps 45286.5 | wpb 510.9 | bsz 1 | num_updates 1058 | best_loss 9.433
2022-03-04 14:10:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1058 updates
2022-03-04 14:10:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:10:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:10:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 11 @ 1058 updates, score 9.433) (writing took 5.41687526088208 seconds)
2022-03-04 14:10:47 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 14:10:47 | INFO | train | epoch 011 | loss 9.406 | nll_loss 8.481 | ppl 357.27 | wps 24408.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1058 | lr 0.000132324 | gnorm 0.831 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2854
2022-03-04 14:10:47 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 14:10:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:12:35 | INFO | train_inner | epoch 012:     42 / 97 loss=9.334, nll_loss=8.4, ppl=337.84, wps=24456.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.82, loss_scale=32, train_wall=235, gb_free=21, wall=2962
2022-03-04 14:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:14:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:14:59 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.303 | nll_loss 8.351 | ppl 326.44 | wps 45230.6 | wpb 510.9 | bsz 1 | num_updates 1154 | best_loss 9.303
2022-03-04 14:14:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1154 updates
2022-03-04 14:14:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:15:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:15:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 12 @ 1154 updates, score 9.303) (writing took 5.461109656840563 seconds)
2022-03-04 14:15:05 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 14:15:05 | INFO | train | epoch 012 | loss 9.238 | nll_loss 8.292 | ppl 313.45 | wps 24408.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1154 | lr 0.000144321 | gnorm 0.851 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3112
2022-03-04 14:15:05 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 14:15:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:17:02 | INFO | train_inner | epoch 013:     46 / 97 loss=9.164, nll_loss=8.209, ppl=295.9, wps=24454.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.87, loss_scale=32, train_wall=235, gb_free=21, wall=3229
2022-03-04 14:19:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:19:17 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.183 | nll_loss 8.214 | ppl 296.95 | wps 45185.4 | wpb 510.9 | bsz 1 | num_updates 1251 | best_loss 9.183
2022-03-04 14:19:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1251 updates
2022-03-04 14:19:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:19:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 13 @ 1251 updates, score 9.183) (writing took 5.594396557658911 seconds)
2022-03-04 14:19:23 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 14:19:23 | INFO | train | epoch 013 | loss 9.083 | nll_loss 8.117 | ppl 277.71 | wps 24624.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1251 | lr 0.000156444 | gnorm 0.879 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3370
2022-03-04 14:19:23 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 14:19:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:21:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:21:31 | INFO | train_inner | epoch 014:     50 / 97 loss=9.007, nll_loss=8.031, ppl=261.57, wps=24418.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.886, loss_scale=32, train_wall=235, gb_free=21, wall=3498
2022-03-04 14:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:23:35 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.089 | nll_loss 8.102 | ppl 274.66 | wps 45313.3 | wpb 510.9 | bsz 1 | num_updates 1347 | best_loss 9.089
2022-03-04 14:23:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1347 updates
2022-03-04 14:23:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:23:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:23:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 14 @ 1347 updates, score 9.089) (writing took 5.589919118210673 seconds)
2022-03-04 14:23:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 14:23:41 | INFO | train | epoch 014 | loss 8.934 | nll_loss 7.949 | ppl 247.11 | wps 24385.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1347 | lr 0.000168441 | gnorm 0.876 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3628
2022-03-04 14:23:41 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 14:23:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:25:56 | INFO | train_inner | epoch 015:     53 / 97 loss=8.862, nll_loss=7.868, ppl=233.58, wps=24661.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=0.905, loss_scale=32, train_wall=233, gb_free=21, wall=3763
2022-03-04 14:26:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:27:53 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.981 | nll_loss 7.983 | ppl 252.96 | wps 45270.7 | wpb 510.9 | bsz 1 | num_updates 1443 | best_loss 8.981
2022-03-04 14:27:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1443 updates
2022-03-04 14:27:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:27:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:27:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 15 @ 1443 updates, score 8.981) (writing took 5.472917496226728 seconds)
2022-03-04 14:27:59 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 14:27:59 | INFO | train | epoch 015 | loss 8.79 | nll_loss 7.787 | ppl 220.91 | wps 24380.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1443 | lr 0.000180439 | gnorm 0.896 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3886
2022-03-04 14:27:59 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 14:27:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:30:24 | INFO | train_inner | epoch 016:     57 / 97 loss=8.702, nll_loss=7.688, ppl=206.22, wps=24450.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=0.888, loss_scale=32, train_wall=235, gb_free=21, wall=4031
2022-03-04 14:32:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:32:11 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.893 | nll_loss 7.87 | ppl 233.88 | wps 45270.1 | wpb 510.9 | bsz 1 | num_updates 1540 | best_loss 8.893
2022-03-04 14:32:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1540 updates
2022-03-04 14:32:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:32:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:32:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 16 @ 1540 updates, score 8.893) (writing took 5.457339616492391 seconds)
2022-03-04 14:32:16 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 14:32:16 | INFO | train | epoch 016 | loss 8.649 | nll_loss 7.628 | ppl 197.83 | wps 24656.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1540 | lr 0.000192562 | gnorm 0.909 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4143
2022-03-04 14:32:16 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 14:32:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:32:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:34:52 | INFO | train_inner | epoch 017:     61 / 97 loss=8.573, nll_loss=7.542, ppl=186.34, wps=24438.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=0.906, loss_scale=32, train_wall=235, gb_free=21, wall=4299
2022-03-04 14:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:36:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.809 | nll_loss 7.782 | ppl 220.12 | wps 45286.7 | wpb 510.9 | bsz 1 | num_updates 1636 | best_loss 8.809
2022-03-04 14:36:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1636 updates
2022-03-04 14:36:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:36:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 17 @ 1636 updates, score 8.809) (writing took 5.425588494166732 seconds)
2022-03-04 14:36:34 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 14:36:34 | INFO | train | epoch 017 | loss 8.509 | nll_loss 7.47 | ppl 177.32 | wps 24392.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1636 | lr 0.000204559 | gnorm 0.912 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4401
2022-03-04 14:36:34 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 14:36:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:39:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:39:20 | INFO | train_inner | epoch 018:     65 / 97 loss=8.415, nll_loss=7.365, ppl=164.81, wps=24443.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=0.909, loss_scale=32, train_wall=235, gb_free=21, wall=4567
2022-03-04 14:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:40:46 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.743 | nll_loss 7.694 | ppl 207.05 | wps 45293.4 | wpb 510.9 | bsz 1 | num_updates 1732 | best_loss 8.743
2022-03-04 14:40:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1732 updates
2022-03-04 14:40:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 18 @ 1732 updates, score 8.743) (writing took 5.4549864185974 seconds)
2022-03-04 14:40:52 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 14:40:52 | INFO | train | epoch 018 | loss 8.37 | nll_loss 7.314 | ppl 159.1 | wps 24385.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1732 | lr 0.000216557 | gnorm 0.905 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4659
2022-03-04 14:40:52 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 14:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:43:45 | INFO | train_inner | epoch 019:     68 / 97 loss=8.281, nll_loss=7.213, ppl=148.35, wps=24665.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.932, loss_scale=32, train_wall=233, gb_free=21, wall=4833
2022-03-04 14:44:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:45:04 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.668 | nll_loss 7.611 | ppl 195.56 | wps 45269.3 | wpb 510.9 | bsz 1 | num_updates 1829 | best_loss 8.668
2022-03-04 14:45:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1829 updates
2022-03-04 14:45:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:45:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:45:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 19 @ 1829 updates, score 8.668) (writing took 5.37615127209574 seconds)
2022-03-04 14:45:10 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 14:45:10 | INFO | train | epoch 019 | loss 8.238 | nll_loss 7.165 | ppl 143.47 | wps 24650.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1829 | lr 0.000228679 | gnorm 0.931 | loss_scale 64 | train_wall 226 | gb_free 21 | wall 4917
2022-03-04 14:45:10 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 14:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:45:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:48:13 | INFO | train_inner | epoch 020:     72 / 97 loss=8.141, nll_loss=7.054, ppl=132.91, wps=24455.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=0.91, loss_scale=32, train_wall=235, gb_free=21, wall=5100
2022-03-04 14:49:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:49:22 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.624 | nll_loss 7.551 | ppl 187.53 | wps 45143.4 | wpb 510.9 | bsz 1 | num_updates 1925 | best_loss 8.624
2022-03-04 14:49:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1925 updates
2022-03-04 14:49:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:49:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 20 @ 1925 updates, score 8.624) (writing took 5.354321694932878 seconds)
2022-03-04 14:49:27 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 14:49:27 | INFO | train | epoch 020 | loss 8.104 | nll_loss 7.013 | ppl 129.12 | wps 24403.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1925 | lr 0.000240677 | gnorm 0.893 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5174
2022-03-04 14:49:27 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 14:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:52:41 | INFO | train_inner | epoch 021:     76 / 97 loss=8.009, nll_loss=6.905, ppl=119.8, wps=24450.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.913, loss_scale=32, train_wall=235, gb_free=21, wall=5368
2022-03-04 14:53:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:53:39 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.569 | nll_loss 7.501 | ppl 181.18 | wps 45478.4 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 8.569
2022-03-04 14:53:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 14:53:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:53:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:53:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 21 @ 2021 updates, score 8.569) (writing took 5.484500183723867 seconds)
2022-03-04 14:53:45 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 14:53:45 | INFO | train | epoch 021 | loss 7.982 | nll_loss 6.874 | ppl 117.3 | wps 24394.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 0.949 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5432
2022-03-04 14:53:45 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 14:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:57:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:57:09 | INFO | train_inner | epoch 022:     80 / 97 loss=7.882, nll_loss=6.762, ppl=108.51, wps=24453.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=0.915, loss_scale=32, train_wall=235, gb_free=21, wall=5636
2022-03-04 14:57:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:57:57 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.534 | nll_loss 7.449 | ppl 174.68 | wps 45383.2 | wpb 510.9 | bsz 1 | num_updates 2117 | best_loss 8.534
2022-03-04 14:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2117 updates
2022-03-04 14:57:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:57:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 14:58:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 22 @ 2117 updates, score 8.534) (writing took 5.454159157350659 seconds)
2022-03-04 14:58:02 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 14:58:02 | INFO | train | epoch 022 | loss 7.858 | nll_loss 6.734 | ppl 106.44 | wps 24407 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2117 | lr 0.000264672 | gnorm 0.905 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5690
2022-03-04 14:58:03 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 14:58:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:01:34 | INFO | train_inner | epoch 023:     83 / 97 loss=7.762, nll_loss=6.626, ppl=98.75, wps=24702.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=0.905, loss_scale=32, train_wall=233, gb_free=21, wall=5901
2022-03-04 15:02:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:02:14 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.515 | nll_loss 7.429 | ppl 172.32 | wps 45387.4 | wpb 510.9 | bsz 1 | num_updates 2214 | best_loss 8.515
2022-03-04 15:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2214 updates
2022-03-04 15:02:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 15:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 15:02:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 23 @ 2214 updates, score 8.515) (writing took 5.382960141636431 seconds)
2022-03-04 15:02:20 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 15:02:20 | INFO | train | epoch 023 | loss 7.74 | nll_loss 6.6 | ppl 96.99 | wps 24680.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2214 | lr 0.000276795 | gnorm 0.897 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5947
2022-03-04 15:02:20 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 15:02:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:03:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:06:02 | INFO | train_inner | epoch 024:     87 / 97 loss=7.638, nll_loss=6.484, ppl=89.52, wps=24467.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.908, loss_scale=32, train_wall=235, gb_free=21, wall=6169
2022-03-04 15:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:06:32 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.491 | nll_loss 7.387 | ppl 167.34 | wps 45346.7 | wpb 510.9 | bsz 1 | num_updates 2310 | best_loss 8.491
2022-03-04 15:06:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2310 updates
2022-03-04 15:06:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 15:06:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 15:06:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 24 @ 2310 updates, score 8.491) (writing took 5.418124902993441 seconds)
2022-03-04 15:06:37 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 15:06:37 | INFO | train | epoch 024 | loss 7.625 | nll_loss 6.47 | ppl 88.64 | wps 24410 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2310 | lr 0.000288792 | gnorm 0.923 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6204
2022-03-04 15:06:37 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 15:06:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:08:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:10:30 | INFO | train_inner | epoch 025:     91 / 97 loss=7.524, nll_loss=6.355, ppl=81.87, wps=24437.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=0.94, loss_scale=32, train_wall=235, gb_free=21, wall=6437
2022-03-04 15:10:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:10:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.489 | nll_loss 7.396 | ppl 168.45 | wps 45241.4 | wpb 510.9 | bsz 1 | num_updates 2406 | best_loss 8.489
2022-03-04 15:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2406 updates
2022-03-04 15:10:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 15:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt
2022-03-04 15:10:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_best.pt (epoch 25 @ 2406 updates, score 8.489) (writing took 5.407110313884914 seconds)
2022-03-04 15:10:55 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 15:10:55 | INFO | train | epoch 025 | loss 7.513 | nll_loss 6.343 | ppl 81.16 | wps 24389.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2406 | lr 0.00030079 | gnorm 0.927 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6462
2022-03-04 15:10:55 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 15:10:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:14:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:14:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:15:00 | INFO | train_inner | epoch 026:     96 / 97 loss=7.414, nll_loss=6.23, ppl=75.06, wps=24214.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=0.946, loss_scale=16, train_wall=238, gb_free=21, wall=6707
2022-03-04 15:15:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:15:08 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.513 | nll_loss 7.424 | ppl 171.75 | wps 45077.8 | wpb 510.9 | bsz 1 | num_updates 2501 | best_loss 8.489
2022-03-04 15:15:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2501 updates
2022-03-04 15:15:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:15:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:15:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 26 @ 2501 updates, score 8.513) (writing took 2.3393503138795495 seconds)
2022-03-04 15:15:10 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 15:15:10 | INFO | train | epoch 026 | loss 7.406 | nll_loss 6.221 | ppl 74.6 | wps 24431.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 2501 | lr 0.000312662 | gnorm 0.95 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 6717
2022-03-04 15:15:10 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 15:15:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:19:23 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.51 | nll_loss 7.415 | ppl 170.68 | wps 44620.3 | wpb 510.9 | bsz 1 | num_updates 2598 | best_loss 8.489
2022-03-04 15:19:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2598 updates
2022-03-04 15:19:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:19:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 27 @ 2598 updates, score 8.51) (writing took 2.3354112040251493 seconds)
2022-03-04 15:19:25 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 15:19:25 | INFO | train | epoch 027 | loss 7.299 | nll_loss 6.1 | ppl 68.58 | wps 24913.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2598 | lr 0.000324785 | gnorm 0.899 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 6972
2022-03-04 15:19:25 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 15:19:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:19:30 | INFO | train_inner | epoch 028:      2 / 97 loss=7.298, nll_loss=6.098, ppl=68.49, wps=24255.4, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=0.897, loss_scale=16, train_wall=233, gb_free=21, wall=6977
2022-03-04 15:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:23:37 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.53 | nll_loss 7.424 | ppl 171.67 | wps 45015.2 | wpb 510.9 | bsz 1 | num_updates 2695 | best_loss 8.489
2022-03-04 15:23:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2695 updates
2022-03-04 15:23:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:23:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:23:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 28 @ 2695 updates, score 8.53) (writing took 2.3770641731098294 seconds)
2022-03-04 15:23:40 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 15:23:40 | INFO | train | epoch 028 | loss 7.194 | nll_loss 5.98 | ppl 63.11 | wps 24916.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2695 | lr 0.000336908 | gnorm 0.922 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7227
2022-03-04 15:23:40 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 15:23:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:23:53 | INFO | train_inner | epoch 029:      5 / 97 loss=7.189, nll_loss=5.974, ppl=62.84, wps=24939.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=0.935, loss_scale=32, train_wall=233, gb_free=21, wall=7240
2022-03-04 15:26:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:27:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:27:52 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.541 | nll_loss 7.431 | ppl 172.55 | wps 45094.5 | wpb 510.9 | bsz 1 | num_updates 2791 | best_loss 8.489
2022-03-04 15:27:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2791 updates
2022-03-04 15:27:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:27:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:27:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 29 @ 2791 updates, score 8.541) (writing took 2.3342442689463496 seconds)
2022-03-04 15:27:54 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 15:27:54 | INFO | train | epoch 029 | loss 7.093 | nll_loss 5.865 | ppl 58.27 | wps 24689.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2791 | lr 0.000348905 | gnorm 0.934 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7482
2022-03-04 15:27:55 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 15:27:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:28:18 | INFO | train_inner | epoch 030:      9 / 97 loss=7.083, nll_loss=5.853, ppl=57.81, wps=24728.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=0.929, loss_scale=32, train_wall=235, gb_free=21, wall=7505
2022-03-04 15:31:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:32:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:32:07 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.601 | nll_loss 7.503 | ppl 181.37 | wps 45392 | wpb 510.9 | bsz 1 | num_updates 2887 | best_loss 8.489
2022-03-04 15:32:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2887 updates
2022-03-04 15:32:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:32:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:32:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 30 @ 2887 updates, score 8.601) (writing took 2.3363364348188043 seconds)
2022-03-04 15:32:09 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 15:32:09 | INFO | train | epoch 030 | loss 6.993 | nll_loss 5.75 | ppl 53.83 | wps 24679.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2887 | lr 0.000360903 | gnorm 0.943 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7736
2022-03-04 15:32:09 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 15:32:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:32:43 | INFO | train_inner | epoch 031:     13 / 97 loss=6.976, nll_loss=5.731, ppl=53.13, wps=24712.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=0.929, loss_scale=32, train_wall=235, gb_free=21, wall=7770
2022-03-04 15:34:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:36:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:36:22 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.617 | nll_loss 7.508 | ppl 181.99 | wps 44915.1 | wpb 510.9 | bsz 1 | num_updates 2983 | best_loss 8.489
2022-03-04 15:36:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2983 updates
2022-03-04 15:36:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:36:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 31 @ 2983 updates, score 8.617) (writing took 2.35370144341141 seconds)
2022-03-04 15:36:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 15:36:24 | INFO | train | epoch 031 | loss 6.896 | nll_loss 5.64 | ppl 49.88 | wps 24665.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2983 | lr 0.0003729 | gnorm 0.946 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 7991
2022-03-04 15:36:24 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 15:36:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:37:08 | INFO | train_inner | epoch 032:     17 / 97 loss=6.88, nll_loss=5.621, ppl=49.22, wps=24711.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=0.959, loss_scale=16, train_wall=235, gb_free=21, wall=8035
2022-03-04 15:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:40:37 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.639 | nll_loss 7.518 | ppl 183.23 | wps 44974.1 | wpb 510.9 | bsz 1 | num_updates 3080 | best_loss 8.489
2022-03-04 15:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3080 updates
2022-03-04 15:40:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:40:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 32 @ 3080 updates, score 8.639) (writing took 2.330240212380886 seconds)
2022-03-04 15:40:39 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 15:40:39 | INFO | train | epoch 032 | loss 6.799 | nll_loss 5.53 | ppl 46.19 | wps 24923.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3080 | lr 0.000385023 | gnorm 0.967 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 8246
2022-03-04 15:40:39 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 15:40:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:41:30 | INFO | train_inner | epoch 033:     20 / 97 loss=6.78, nll_loss=5.508, ppl=45.5, wps=24938.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=0.973, loss_scale=32, train_wall=233, gb_free=21, wall=8297
2022-03-04 15:44:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:44:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:44:52 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.687 | nll_loss 7.584 | ppl 191.84 | wps 45341.2 | wpb 510.9 | bsz 1 | num_updates 3176 | best_loss 8.489
2022-03-04 15:44:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3176 updates
2022-03-04 15:44:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:44:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:44:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 33 @ 3176 updates, score 8.687) (writing took 2.379107548855245 seconds)
2022-03-04 15:44:54 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 15:44:54 | INFO | train | epoch 033 | loss 6.704 | nll_loss 5.421 | ppl 42.84 | wps 24663.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3176 | lr 0.000397021 | gnorm 0.972 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 8501
2022-03-04 15:44:54 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 15:44:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:45:55 | INFO | train_inner | epoch 034:     24 / 97 loss=6.681, nll_loss=5.395, ppl=42.06, wps=24707.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=1.003, loss_scale=16, train_wall=235, gb_free=21, wall=8562
2022-03-04 15:49:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:49:07 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.769 | nll_loss 7.664 | ppl 202.87 | wps 44881 | wpb 510.9 | bsz 1 | num_updates 3273 | best_loss 8.489
2022-03-04 15:49:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3273 updates
2022-03-04 15:49:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:49:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:49:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 34 @ 3273 updates, score 8.769) (writing took 2.318081095814705 seconds)
2022-03-04 15:49:09 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 15:49:09 | INFO | train | epoch 034 | loss 6.613 | nll_loss 5.317 | ppl 39.85 | wps 24918.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3273 | lr 0.000409143 | gnorm 0.988 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 8756
2022-03-04 15:49:09 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 15:49:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:50:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:50:20 | INFO | train_inner | epoch 035:     28 / 97 loss=6.591, nll_loss=5.291, ppl=39.15, wps=24697.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=0.976, loss_scale=16, train_wall=236, gb_free=21, wall=8828
2022-03-04 15:53:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:53:22 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.778 | nll_loss 7.66 | ppl 202.31 | wps 45030.3 | wpb 510.9 | bsz 1 | num_updates 3369 | best_loss 8.489
2022-03-04 15:53:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3369 updates
2022-03-04 15:53:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:53:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:53:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 35 @ 3369 updates, score 8.778) (writing took 2.3972499622032046 seconds)
2022-03-04 15:53:24 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 15:53:24 | INFO | train | epoch 035 | loss 6.52 | nll_loss 5.21 | ppl 37.01 | wps 24650.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3369 | lr 0.000421141 | gnorm 0.987 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9011
2022-03-04 15:53:24 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 15:53:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:54:43 | INFO | train_inner | epoch 036:     31 / 97 loss=6.485, nll_loss=5.17, ppl=36, wps=24929.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=0.987, loss_scale=16, train_wall=233, gb_free=21, wall=9090
2022-03-04 15:56:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:57:36 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.851 | nll_loss 7.74 | ppl 213.71 | wps 45121.2 | wpb 510.9 | bsz 1 | num_updates 3465 | best_loss 8.489
2022-03-04 15:57:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3465 updates
2022-03-04 15:57:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:57:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 15:57:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 36 @ 3465 updates, score 8.851) (writing took 2.360098616220057 seconds)
2022-03-04 15:57:39 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 15:57:39 | INFO | train | epoch 036 | loss 6.441 | nll_loss 5.118 | ppl 34.74 | wps 24668.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3465 | lr 0.000433138 | gnorm 1.081 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9266
2022-03-04 15:57:39 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 15:57:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:59:08 | INFO | train_inner | epoch 037:     35 / 97 loss=6.408, nll_loss=5.081, ppl=33.84, wps=24704.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=1.02, loss_scale=16, train_wall=235, gb_free=21, wall=9355
2022-03-04 16:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:01:51 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.899 | nll_loss 7.796 | ppl 222.22 | wps 45141.5 | wpb 510.9 | bsz 1 | num_updates 3562 | best_loss 8.489
2022-03-04 16:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3562 updates
2022-03-04 16:01:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:01:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:01:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 37 @ 3562 updates, score 8.899) (writing took 2.375328970141709 seconds)
2022-03-04 16:01:54 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 16:01:54 | INFO | train | epoch 037 | loss 6.341 | nll_loss 5.004 | ppl 32.1 | wps 24912.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3562 | lr 0.000445261 | gnorm 0.973 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9521
2022-03-04 16:01:54 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 16:01:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:03:31 | INFO | train_inner | epoch 038:     38 / 97 loss=6.309, nll_loss=4.967, ppl=31.28, wps=24939.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=1.001, loss_scale=32, train_wall=233, gb_free=21, wall=9618
2022-03-04 16:06:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:06:06 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.883 | nll_loss 7.757 | ppl 216.34 | wps 45240.7 | wpb 510.9 | bsz 1 | num_updates 3659 | best_loss 8.489
2022-03-04 16:06:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3659 updates
2022-03-04 16:06:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:06:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 38 @ 3659 updates, score 8.883) (writing took 2.39131155051291 seconds)
2022-03-04 16:06:09 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 16:06:09 | INFO | train | epoch 038 | loss 6.256 | nll_loss 4.907 | ppl 30.01 | wps 24928.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3659 | lr 0.000457384 | gnorm 1.012 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 9776
2022-03-04 16:06:09 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 16:06:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:07:53 | INFO | train_inner | epoch 039:     41 / 97 loss=6.219, nll_loss=4.864, ppl=29.13, wps=24948.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.035, loss_scale=32, train_wall=233, gb_free=21, wall=9880
2022-03-04 16:08:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:08:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:10:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:10:21 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.948 | nll_loss 7.818 | ppl 225.7 | wps 45139.3 | wpb 510.9 | bsz 1 | num_updates 3754 | best_loss 8.489
2022-03-04 16:10:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3754 updates
2022-03-04 16:10:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 39 @ 3754 updates, score 8.948) (writing took 2.3805345660075545 seconds)
2022-03-04 16:10:23 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 16:10:23 | INFO | train | epoch 039 | loss 6.164 | nll_loss 4.801 | ppl 27.88 | wps 24428.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 3754 | lr 0.000469256 | gnorm 1.024 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10030
2022-03-04 16:10:23 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 16:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:12:21 | INFO | train_inner | epoch 040:     46 / 97 loss=6.124, nll_loss=4.755, ppl=27, wps=24488.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.063, loss_scale=16, train_wall=238, gb_free=21, wall=10148
2022-03-04 16:14:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:14:36 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.048 | nll_loss 7.943 | ppl 246.14 | wps 45158.8 | wpb 510.9 | bsz 1 | num_updates 3850 | best_loss 8.489
2022-03-04 16:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3850 updates
2022-03-04 16:14:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:14:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:14:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 40 @ 3850 updates, score 9.048) (writing took 2.4124982729554176 seconds)
2022-03-04 16:14:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:14:38 | INFO | train | epoch 040 | loss 6.083 | nll_loss 4.707 | ppl 26.11 | wps 24671.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3850 | lr 0.000481254 | gnorm 1.061 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10285
2022-03-04 16:14:38 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:14:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:16:46 | INFO | train_inner | epoch 041:     50 / 97 loss=6.037, nll_loss=4.654, ppl=25.18, wps=24725.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.036, loss_scale=16, train_wall=235, gb_free=21, wall=10413
2022-03-04 16:18:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:18:50 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.085 | nll_loss 7.952 | ppl 247.62 | wps 45337.3 | wpb 510.9 | bsz 1 | num_updates 3947 | best_loss 8.489
2022-03-04 16:18:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3947 updates
2022-03-04 16:18:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:18:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 41 @ 3947 updates, score 9.085) (writing took 2.424139170907438 seconds)
2022-03-04 16:18:53 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:18:53 | INFO | train | epoch 041 | loss 6.001 | nll_loss 4.613 | ppl 24.48 | wps 24958.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3947 | lr 0.000493376 | gnorm 1.058 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10540
2022-03-04 16:18:53 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:18:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:21:08 | INFO | train_inner | epoch 042:     53 / 97 loss=5.957, nll_loss=4.562, ppl=23.62, wps=24972.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.061, loss_scale=32, train_wall=233, gb_free=21, wall=10675
2022-03-04 16:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:23:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:23:05 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.156 | nll_loss 8.039 | ppl 263.09 | wps 45311.9 | wpb 510.9 | bsz 1 | num_updates 4043 | best_loss 8.489
2022-03-04 16:23:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4043 updates
2022-03-04 16:23:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 42 @ 4043 updates, score 9.156) (writing took 2.4802777459844947 seconds)
2022-03-04 16:23:07 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 16:23:07 | INFO | train | epoch 042 | loss 5.917 | nll_loss 4.516 | ppl 22.87 | wps 24684.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4043 | lr 0.000497334 | gnorm 1.081 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10794
2022-03-04 16:23:07 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 16:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:25:33 | INFO | train_inner | epoch 043:     57 / 97 loss=5.867, nll_loss=4.458, ppl=21.98, wps=24721.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.068, loss_scale=16, train_wall=235, gb_free=21, wall=10940
2022-03-04 16:27:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:27:20 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.184 | nll_loss 8.072 | ppl 269.09 | wps 45205.2 | wpb 510.9 | bsz 1 | num_updates 4140 | best_loss 8.489
2022-03-04 16:27:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4140 updates
2022-03-04 16:27:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:27:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:27:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 43 @ 4140 updates, score 9.184) (writing took 2.493820029310882 seconds)
2022-03-04 16:27:22 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 16:27:22 | INFO | train | epoch 043 | loss 5.826 | nll_loss 4.411 | ppl 21.28 | wps 24929.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4140 | lr 0.000491473 | gnorm 1.027 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11049
2022-03-04 16:27:22 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 16:27:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:28:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:29:58 | INFO | train_inner | epoch 044:     61 / 97 loss=5.773, nll_loss=4.35, ppl=20.39, wps=24722.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.044, loss_scale=16, train_wall=235, gb_free=21, wall=11205
2022-03-04 16:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:31:34 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.287 | nll_loss 8.166 | ppl 287.32 | wps 45312.2 | wpb 510.9 | bsz 1 | num_updates 4236 | best_loss 8.489
2022-03-04 16:31:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4236 updates
2022-03-04 16:31:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:31:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:31:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 44 @ 4236 updates, score 9.287) (writing took 2.57475649099797 seconds)
2022-03-04 16:31:37 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 16:31:37 | INFO | train | epoch 044 | loss 5.74 | nll_loss 4.312 | ppl 19.87 | wps 24683.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4236 | lr 0.000485872 | gnorm 1.066 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11304
2022-03-04 16:31:37 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 16:31:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:34:20 | INFO | train_inner | epoch 045:     64 / 97 loss=5.691, nll_loss=4.255, ppl=19.09, wps=24955.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=1.041, loss_scale=32, train_wall=233, gb_free=21, wall=11467
2022-03-04 16:35:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:35:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:35:49 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.359 | nll_loss 8.24 | ppl 302.25 | wps 45441 | wpb 510.9 | bsz 1 | num_updates 4332 | best_loss 8.489
2022-03-04 16:35:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4332 updates
2022-03-04 16:35:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:35:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 45 @ 4332 updates, score 9.359) (writing took 2.601160304620862 seconds)
2022-03-04 16:35:52 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 16:35:52 | INFO | train | epoch 045 | loss 5.654 | nll_loss 4.213 | ppl 18.54 | wps 24675.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4332 | lr 0.000480458 | gnorm 1.03 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11559
2022-03-04 16:35:52 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 16:35:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:38:45 | INFO | train_inner | epoch 046:     68 / 97 loss=5.596, nll_loss=4.146, ppl=17.7, wps=24714, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.024, loss_scale=16, train_wall=235, gb_free=21, wall=11732
2022-03-04 16:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:40:04 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.438 | nll_loss 8.338 | ppl 323.53 | wps 45404.1 | wpb 510.9 | bsz 1 | num_updates 4429 | best_loss 8.489
2022-03-04 16:40:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4429 updates
2022-03-04 16:40:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:40:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:40:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 46 @ 4429 updates, score 9.438) (writing took 2.6178754568099976 seconds)
2022-03-04 16:40:07 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 16:40:07 | INFO | train | epoch 046 | loss 5.57 | nll_loss 4.116 | ppl 17.34 | wps 24924.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4429 | lr 0.000475168 | gnorm 1.025 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11814
2022-03-04 16:40:07 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 16:40:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:41:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:43:10 | INFO | train_inner | epoch 047:     72 / 97 loss=5.512, nll_loss=4.049, ppl=16.55, wps=24708, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=1.033, loss_scale=16, train_wall=235, gb_free=21, wall=11997
2022-03-04 16:44:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:44:19 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.505 | nll_loss 8.39 | ppl 335.35 | wps 45124.9 | wpb 510.9 | bsz 1 | num_updates 4525 | best_loss 8.489
2022-03-04 16:44:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4525 updates
2022-03-04 16:44:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:44:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:44:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 47 @ 4525 updates, score 9.505) (writing took 2.5926484996452928 seconds)
2022-03-04 16:44:21 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 16:44:21 | INFO | train | epoch 047 | loss 5.488 | nll_loss 4.022 | ppl 16.24 | wps 24667 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4525 | lr 0.0004701 | gnorm 1.026 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 12069
2022-03-04 16:44:21 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 16:44:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:46:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:47:35 | INFO | train_inner | epoch 048:     76 / 97 loss=5.432, nll_loss=3.957, ppl=15.52, wps=24704.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.037, loss_scale=16, train_wall=235, gb_free=21, wall=12262
2022-03-04 16:48:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:48:34 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.585 | nll_loss 8.486 | ppl 358.46 | wps 45332.5 | wpb 510.9 | bsz 1 | num_updates 4621 | best_loss 8.489
2022-03-04 16:48:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4621 updates
2022-03-04 16:48:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:48:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:48:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 48 @ 4621 updates, score 9.585) (writing took 2.636139389127493 seconds)
2022-03-04 16:48:36 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 16:48:36 | INFO | train | epoch 048 | loss 5.413 | nll_loss 3.934 | ppl 15.28 | wps 24664.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4621 | lr 0.000465192 | gnorm 1.038 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 12323
2022-03-04 16:48:36 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 16:48:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:51:58 | INFO | train_inner | epoch 049:     79 / 97 loss=5.358, nll_loss=3.871, ppl=14.63, wps=24954.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=1.019, loss_scale=16, train_wall=233, gb_free=21, wall=12525
2022-03-04 16:52:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:52:49 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.622 | nll_loss 8.52 | ppl 366.97 | wps 45451.2 | wpb 510.9 | bsz 1 | num_updates 4718 | best_loss 8.489
2022-03-04 16:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4718 updates
2022-03-04 16:52:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:52:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:52:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 49 @ 4718 updates, score 9.622) (writing took 2.623313424177468 seconds)
2022-03-04 16:52:51 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 16:52:51 | INFO | train | epoch 049 | loss 5.335 | nll_loss 3.845 | ppl 14.37 | wps 24934.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4718 | lr 0.000460385 | gnorm 1 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 12578
2022-03-04 16:52:51 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 16:52:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:52:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:56:23 | INFO | train_inner | epoch 050:     83 / 97 loss=5.279, nll_loss=3.78, ppl=13.73, wps=24715.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=1.015, loss_scale=16, train_wall=235, gb_free=21, wall=12790
2022-03-04 16:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:57:03 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.642 | nll_loss 8.515 | ppl 365.7 | wps 45368.5 | wpb 510.9 | bsz 1 | num_updates 4814 | best_loss 8.489
2022-03-04 16:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4814 updates
2022-03-04 16:57:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:57:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 16:57:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 50 @ 4814 updates, score 9.642) (writing took 2.592049981467426 seconds)
2022-03-04 16:57:06 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 16:57:06 | INFO | train | epoch 050 | loss 5.267 | nll_loss 3.766 | ppl 13.6 | wps 24679.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4814 | lr 0.000455771 | gnorm 1.041 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 12833
2022-03-04 16:57:06 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 16:57:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:58:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:00:48 | INFO | train_inner | epoch 051:     87 / 97 loss=5.207, nll_loss=3.695, ppl=12.96, wps=24717, ups=0.38, wpb=65495, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.02, loss_scale=16, train_wall=235, gb_free=21, wall=13055
2022-03-04 17:01:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:01:18 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.795 | nll_loss 8.697 | ppl 415.13 | wps 45305.9 | wpb 510.9 | bsz 1 | num_updates 4910 | best_loss 8.489
2022-03-04 17:01:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4910 updates
2022-03-04 17:01:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:01:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:01:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 51 @ 4910 updates, score 9.795) (writing took 2.5993588706478477 seconds)
2022-03-04 17:01:21 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 17:01:21 | INFO | train | epoch 051 | loss 5.194 | nll_loss 3.681 | ppl 12.83 | wps 24677.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4910 | lr 0.000451294 | gnorm 1.001 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 13088
2022-03-04 17:01:21 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 17:01:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:04:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:05:13 | INFO | train_inner | epoch 052:     91 / 97 loss=5.14, nll_loss=3.618, ppl=12.28, wps=24710.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1.027, loss_scale=16, train_wall=235, gb_free=21, wall=13320
2022-03-04 17:05:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:05:33 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.943 | nll_loss 8.859 | ppl 464.48 | wps 45203 | wpb 510.9 | bsz 1 | num_updates 5006 | best_loss 8.489
2022-03-04 17:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5006 updates
2022-03-04 17:05:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:05:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:05:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 52 @ 5006 updates, score 9.943) (writing took 2.5986830601468682 seconds)
2022-03-04 17:05:36 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 17:05:36 | INFO | train | epoch 052 | loss 5.13 | nll_loss 3.606 | ppl 12.18 | wps 24670.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5006 | lr 0.000446946 | gnorm 1.029 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 13343
2022-03-04 17:05:36 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 17:05:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:09:36 | INFO | train_inner | epoch 053:     94 / 97 loss=5.071, nll_loss=3.538, ppl=11.61, wps=24937.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=1.022, loss_scale=16, train_wall=233, gb_free=21, wall=13583
2022-03-04 17:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:09:48 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.927 | nll_loss 8.822 | ppl 452.64 | wps 45255.9 | wpb 510.9 | bsz 1 | num_updates 5103 | best_loss 8.489
2022-03-04 17:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5103 updates
2022-03-04 17:09:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:09:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 53 @ 5103 updates, score 9.927) (writing took 2.6340492041781545 seconds)
2022-03-04 17:09:51 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 17:09:51 | INFO | train | epoch 053 | loss 5.065 | nll_loss 3.532 | ppl 11.56 | wps 24907.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5103 | lr 0.000442677 | gnorm 1.03 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 13598
2022-03-04 17:09:51 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 17:09:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:10:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:13:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:14:03 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 10.058 | nll_loss 8.98 | ppl 505.01 | wps 45388.7 | wpb 510.9 | bsz 1 | num_updates 5199 | best_loss 8.489
2022-03-04 17:14:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5199 updates
2022-03-04 17:14:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 54 @ 5199 updates, score 10.058) (writing took 2.6177108883857727 seconds)
2022-03-04 17:14:05 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 17:14:05 | INFO | train | epoch 054 | loss 5.001 | nll_loss 3.457 | ppl 10.98 | wps 24684.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5199 | lr 0.000438571 | gnorm 1.027 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 13852
2022-03-04 17:14:05 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 17:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:14:08 | INFO | train_inner | epoch 055:      1 / 97 loss=5.004, nll_loss=3.461, ppl=11.01, wps=24027.3, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=5200, lr=0.000438529, gnorm=1.029, loss_scale=16, train_wall=235, gb_free=21, wall=13855
2022-03-04 17:16:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:18:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:18:17 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 10.103 | nll_loss 9.026 | ppl 521.47 | wps 45386.6 | wpb 510.9 | bsz 1 | num_updates 5295 | best_loss 8.489
2022-03-04 17:18:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5295 updates
2022-03-04 17:18:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:18:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:18:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 55 @ 5295 updates, score 10.103) (writing took 2.604027901776135 seconds)
2022-03-04 17:18:20 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 17:18:20 | INFO | train | epoch 055 | loss 4.943 | nll_loss 3.39 | ppl 10.48 | wps 24685.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5295 | lr 0.000434577 | gnorm 1.044 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14107
2022-03-04 17:18:20 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 17:18:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:18:33 | INFO | train_inner | epoch 056:      5 / 97 loss=4.935, nll_loss=3.381, ppl=10.42, wps=24725.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5300, lr=0.000434372, gnorm=1.041, loss_scale=16, train_wall=235, gb_free=21, wall=14120
2022-03-04 17:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:22:32 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 10.186 | nll_loss 9.128 | ppl 559.67 | wps 45275 | wpb 510.9 | bsz 1 | num_updates 5391 | best_loss 8.489
2022-03-04 17:22:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5391 updates
2022-03-04 17:22:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:22:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:22:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 56 @ 5391 updates, score 10.186) (writing took 2.5970285311341286 seconds)
2022-03-04 17:22:35 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 17:22:35 | INFO | train | epoch 056 | loss 4.882 | nll_loss 3.319 | ppl 9.98 | wps 24671.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5391 | lr 0.000430691 | gnorm 1.01 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14362
2022-03-04 17:22:35 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 17:22:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:22:58 | INFO | train_inner | epoch 057:      9 / 97 loss=4.874, nll_loss=3.309, ppl=9.91, wps=24711.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=1.025, loss_scale=16, train_wall=235, gb_free=21, wall=14385
2022-03-04 17:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:26:47 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 10.241 | nll_loss 9.159 | ppl 571.54 | wps 45283.8 | wpb 510.9 | bsz 1 | num_updates 5488 | best_loss 8.489
2022-03-04 17:26:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5488 updates
2022-03-04 17:26:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:26:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:26:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 57 @ 5488 updates, score 10.241) (writing took 2.6223836168646812 seconds)
2022-03-04 17:26:50 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 17:26:50 | INFO | train | epoch 057 | loss 4.828 | nll_loss 3.256 | ppl 9.55 | wps 24924.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5488 | lr 0.000426867 | gnorm 1.036 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14617
2022-03-04 17:26:50 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 17:26:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:27:20 | INFO | train_inner | epoch 058:     12 / 97 loss=4.82, nll_loss=3.247, ppl=9.5, wps=24943.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.033, loss_scale=16, train_wall=233, gb_free=21, wall=14647
2022-03-04 17:27:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:30:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:31:02 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 10.302 | nll_loss 9.233 | ppl 601.78 | wps 45412.2 | wpb 510.9 | bsz 1 | num_updates 5584 | best_loss 8.489
2022-03-04 17:31:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5584 updates
2022-03-04 17:31:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:31:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:31:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 58 @ 5584 updates, score 10.302) (writing took 2.5502831591293216 seconds)
2022-03-04 17:31:05 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 17:31:05 | INFO | train | epoch 058 | loss 4.772 | nll_loss 3.191 | ppl 9.14 | wps 24662.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5584 | lr 0.000423182 | gnorm 1.044 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14872
2022-03-04 17:31:05 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 17:31:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:31:46 | INFO | train_inner | epoch 059:     16 / 97 loss=4.762, nll_loss=3.179, ppl=9.06, wps=24703.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.034, loss_scale=16, train_wall=235, gb_free=21, wall=14913
2022-03-04 17:33:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:35:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:35:17 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 10.295 | nll_loss 9.217 | ppl 595.14 | wps 45401.8 | wpb 510.9 | bsz 1 | num_updates 5680 | best_loss 8.489
2022-03-04 17:35:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5680 updates
2022-03-04 17:35:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:35:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:35:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 59 @ 5680 updates, score 10.295) (writing took 2.5897864140570164 seconds)
2022-03-04 17:35:19 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 17:35:19 | INFO | train | epoch 059 | loss 4.72 | nll_loss 3.131 | ppl 8.76 | wps 24668.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5680 | lr 0.000419591 | gnorm 1.041 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 15126
2022-03-04 17:35:19 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 17:35:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:36:11 | INFO | train_inner | epoch 060:     20 / 97 loss=4.709, nll_loss=3.118, ppl=8.68, wps=24708.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.041, loss_scale=16, train_wall=235, gb_free=21, wall=15178
2022-03-04 17:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:39:32 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 10.446 | nll_loss 9.384 | ppl 668.32 | wps 45280.6 | wpb 510.9 | bsz 1 | num_updates 5777 | best_loss 8.489
2022-03-04 17:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5777 updates
2022-03-04 17:39:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:39:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:39:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 60 @ 5777 updates, score 10.446) (writing took 2.569789184257388 seconds)
2022-03-04 17:39:34 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 17:39:34 | INFO | train | epoch 060 | loss 4.67 | nll_loss 3.073 | ppl 8.42 | wps 24934.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5777 | lr 0.000416053 | gnorm 1.024 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 15381
2022-03-04 17:39:34 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 17:39:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:40:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:40:35 | INFO | train_inner | epoch 061:     24 / 97 loss=4.657, nll_loss=3.057, ppl=8.33, wps=24722.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.029, loss_scale=16, train_wall=235, gb_free=21, wall=15443
2022-03-04 17:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:43:46 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 10.481 | nll_loss 9.402 | ppl 676.39 | wps 45329.8 | wpb 510.9 | bsz 1 | num_updates 5873 | best_loss 8.489
2022-03-04 17:43:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5873 updates
2022-03-04 17:43:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:43:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:43:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 61 @ 5873 updates, score 10.481) (writing took 2.4484281353652477 seconds)
2022-03-04 17:43:49 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 17:43:49 | INFO | train | epoch 061 | loss 4.621 | nll_loss 3.016 | ppl 8.09 | wps 24695.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5873 | lr 0.000412639 | gnorm 1.06 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 15636
2022-03-04 17:43:49 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 17:43:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:44:58 | INFO | train_inner | epoch 062:     27 / 97 loss=4.606, nll_loss=2.998, ppl=7.99, wps=24967.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.045, loss_scale=16, train_wall=233, gb_free=21, wall=15705
2022-03-04 17:45:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:47:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:48:01 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 10.533 | nll_loss 9.467 | ppl 707.47 | wps 45390.3 | wpb 510.9 | bsz 1 | num_updates 5969 | best_loss 8.489
2022-03-04 17:48:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5969 updates
2022-03-04 17:48:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:48:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:48:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 62 @ 5969 updates, score 10.533) (writing took 2.436004969291389 seconds)
2022-03-04 17:48:03 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 17:48:03 | INFO | train | epoch 062 | loss 4.573 | nll_loss 2.959 | ppl 7.78 | wps 24703 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5969 | lr 0.000409307 | gnorm 1.032 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 15890
2022-03-04 17:48:03 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 17:48:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:49:22 | INFO | train_inner | epoch 063:     31 / 97 loss=4.557, nll_loss=2.942, ppl=7.68, wps=24742.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.048, loss_scale=16, train_wall=235, gb_free=21, wall=15970
2022-03-04 17:51:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:52:16 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 10.664 | nll_loss 9.611 | ppl 782.09 | wps 45277.8 | wpb 510.9 | bsz 1 | num_updates 6065 | best_loss 8.489
2022-03-04 17:52:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6065 updates
2022-03-04 17:52:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:52:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:52:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 63 @ 6065 updates, score 10.664) (writing took 2.4687461890280247 seconds)
2022-03-04 17:52:18 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 17:52:18 | INFO | train | epoch 063 | loss 4.529 | nll_loss 2.909 | ppl 7.51 | wps 24684.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6065 | lr 0.000406055 | gnorm 1.05 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16145
2022-03-04 17:52:18 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 17:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:53:47 | INFO | train_inner | epoch 064:     35 / 97 loss=4.513, nll_loss=2.89, ppl=7.41, wps=24728.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.044, loss_scale=16, train_wall=235, gb_free=21, wall=16234
2022-03-04 17:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:56:30 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10.702 | nll_loss 9.632 | ppl 793.29 | wps 45940.3 | wpb 510.9 | bsz 1 | num_updates 6162 | best_loss 8.489
2022-03-04 17:56:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6162 updates
2022-03-04 17:56:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:56:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 17:56:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 64 @ 6162 updates, score 10.702) (writing took 2.55428502894938 seconds)
2022-03-04 17:56:33 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 17:56:33 | INFO | train | epoch 064 | loss 4.486 | nll_loss 2.859 | ppl 7.25 | wps 24934.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6162 | lr 0.000402846 | gnorm 1.056 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16400
2022-03-04 17:56:33 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 17:56:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:57:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:58:12 | INFO | train_inner | epoch 065:     39 / 97 loss=4.465, nll_loss=2.834, ppl=7.13, wps=24720, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.035, loss_scale=16, train_wall=235, gb_free=21, wall=16499
2022-03-04 18:00:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:00:45 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 10.688 | nll_loss 9.624 | ppl 789.03 | wps 45449.7 | wpb 510.9 | bsz 1 | num_updates 6258 | best_loss 8.489
2022-03-04 18:00:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6258 updates
2022-03-04 18:00:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:00:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:00:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 65 @ 6258 updates, score 10.688) (writing took 2.5187821071594954 seconds)
2022-03-04 18:00:48 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 18:00:48 | INFO | train | epoch 065 | loss 4.439 | nll_loss 2.805 | ppl 6.99 | wps 24683.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6258 | lr 0.000399744 | gnorm 1.025 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16655
2022-03-04 18:00:48 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 18:00:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:01:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:02:37 | INFO | train_inner | epoch 066:     43 / 97 loss=4.425, nll_loss=2.787, ppl=6.9, wps=24718.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.041, loss_scale=8, train_wall=235, gb_free=21, wall=16764
2022-03-04 18:04:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:05:00 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 10.801 | nll_loss 9.747 | ppl 859.59 | wps 45313.4 | wpb 510.9 | bsz 1 | num_updates 6354 | best_loss 8.489
2022-03-04 18:05:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6354 updates
2022-03-04 18:05:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:05:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 66 @ 6354 updates, score 10.801) (writing took 2.45542629994452 seconds)
2022-03-04 18:05:02 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 18:05:02 | INFO | train | epoch 066 | loss 4.402 | nll_loss 2.761 | ppl 6.78 | wps 24703.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6354 | lr 0.000396713 | gnorm 1.06 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 16909
2022-03-04 18:05:02 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 18:05:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:06:59 | INFO | train_inner | epoch 067:     46 / 97 loss=4.383, nll_loss=2.738, ppl=6.67, wps=24985.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.054, loss_scale=16, train_wall=233, gb_free=21, wall=17026
2022-03-04 18:09:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:09:14 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.896 | nll_loss 9.845 | ppl 919.93 | wps 45552.4 | wpb 510.9 | bsz 1 | num_updates 6451 | best_loss 8.489
2022-03-04 18:09:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6451 updates
2022-03-04 18:09:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:09:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 67 @ 6451 updates, score 10.896) (writing took 2.5188446110114455 seconds)
2022-03-04 18:09:17 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 18:09:17 | INFO | train | epoch 067 | loss 4.361 | nll_loss 2.713 | ppl 6.56 | wps 24950.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6451 | lr 0.000393719 | gnorm 1.028 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 17164
2022-03-04 18:09:17 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 18:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:11:22 | INFO | train_inner | epoch 068:     49 / 97 loss=4.345, nll_loss=2.695, ppl=6.48, wps=24970.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1.037, loss_scale=16, train_wall=233, gb_free=21, wall=17289
2022-03-04 18:12:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:13:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:13:29 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.939 | nll_loss 9.896 | ppl 952.93 | wps 45372.4 | wpb 510.9 | bsz 1 | num_updates 6547 | best_loss 8.489
2022-03-04 18:13:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6547 updates
2022-03-04 18:13:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:13:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:13:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 68 @ 6547 updates, score 10.939) (writing took 2.470220592804253 seconds)
2022-03-04 18:13:31 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 18:13:31 | INFO | train | epoch 068 | loss 4.322 | nll_loss 2.669 | ppl 6.36 | wps 24700.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6547 | lr 0.000390822 | gnorm 1.053 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 17418
2022-03-04 18:13:31 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 18:13:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:15:46 | INFO | train_inner | epoch 069:     53 / 97 loss=4.305, nll_loss=2.648, ppl=6.27, wps=24736.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.039, loss_scale=16, train_wall=235, gb_free=21, wall=17553
2022-03-04 18:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:17:44 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.926 | nll_loss 9.873 | ppl 937.75 | wps 44444.7 | wpb 510.9 | bsz 1 | num_updates 6644 | best_loss 8.489
2022-03-04 18:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6644 updates
2022-03-04 18:17:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:17:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 69 @ 6644 updates, score 10.926) (writing took 2.6859364649280906 seconds)
2022-03-04 18:17:46 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 18:17:46 | INFO | train | epoch 069 | loss 4.286 | nll_loss 2.626 | ppl 6.17 | wps 24908.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6644 | lr 0.000387958 | gnorm 1.025 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 17673
2022-03-04 18:17:46 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 18:17:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:18:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:20:13 | INFO | train_inner | epoch 070:     57 / 97 loss=4.262, nll_loss=2.598, ppl=6.06, wps=24578.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.046, loss_scale=16, train_wall=236, gb_free=21, wall=17820
2022-03-04 18:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:22:01 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 11.013 | nll_loss 9.956 | ppl 992.97 | wps 42821.9 | wpb 510.9 | bsz 1 | num_updates 6740 | best_loss 8.489
2022-03-04 18:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6740 updates
2022-03-04 18:22:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 70 @ 6740 updates, score 11.013) (writing took 2.6082053054124117 seconds)
2022-03-04 18:22:03 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 18:22:03 | INFO | train | epoch 070 | loss 4.247 | nll_loss 2.581 | ppl 5.98 | wps 24460.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6740 | lr 0.000385186 | gnorm 1.042 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 17930
2022-03-04 18:22:03 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 18:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:23:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:24:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:24:43 | INFO | train_inner | epoch 071:     62 / 97 loss=4.232, nll_loss=2.564, ppl=5.91, wps=24271, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.039, loss_scale=8, train_wall=239, gb_free=21, wall=18090
2022-03-04 18:26:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:26:18 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 11.047 | nll_loss 9.995 | ppl 1020.5 | wps 42687.8 | wpb 510.9 | bsz 1 | num_updates 6835 | best_loss 8.489
2022-03-04 18:26:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6835 updates
2022-03-04 18:26:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 71 @ 6835 updates, score 11.047) (writing took 2.6251256354153156 seconds)
2022-03-04 18:26:20 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 18:26:20 | INFO | train | epoch 071 | loss 4.214 | nll_loss 2.542 | ppl 5.83 | wps 24192.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 6835 | lr 0.000382499 | gnorm 1.048 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 18187
2022-03-04 18:26:20 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 18:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:29:07 | INFO | train_inner | epoch 072:     65 / 97 loss=4.189, nll_loss=2.514, ppl=5.71, wps=24735.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.039, loss_scale=8, train_wall=234, gb_free=21, wall=18355
2022-03-04 18:30:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:30:35 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 11.061 | nll_loss 10.019 | ppl 1037.77 | wps 44718.4 | wpb 510.9 | bsz 1 | num_updates 6932 | best_loss 8.489
2022-03-04 18:30:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6932 updates
2022-03-04 18:30:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:30:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 72 @ 6932 updates, score 11.061) (writing took 2.670259390026331 seconds)
2022-03-04 18:30:37 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 18:30:37 | INFO | train | epoch 072 | loss 4.182 | nll_loss 2.505 | ppl 5.68 | wps 24737.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6932 | lr 0.000379814 | gnorm 1.04 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 18444
2022-03-04 18:30:37 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 18:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:33:32 | INFO | train_inner | epoch 073:     68 / 97 loss=4.16, nll_loss=2.48, ppl=5.58, wps=24740.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.047, loss_scale=16, train_wall=235, gb_free=21, wall=18619
2022-03-04 18:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:34:52 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 11.184 | nll_loss 10.149 | ppl 1135.6 | wps 43840.6 | wpb 510.9 | bsz 1 | num_updates 7029 | best_loss 8.489
2022-03-04 18:34:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7029 updates
2022-03-04 18:34:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:34:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 73 @ 7029 updates, score 11.184) (writing took 2.5836101341992617 seconds)
2022-03-04 18:34:54 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 18:34:54 | INFO | train | epoch 073 | loss 4.149 | nll_loss 2.468 | ppl 5.53 | wps 24704.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7029 | lr 0.000377184 | gnorm 1.051 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 18701
2022-03-04 18:34:54 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 18:34:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:36:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:37:59 | INFO | train_inner | epoch 074:     72 / 97 loss=4.134, nll_loss=2.45, ppl=5.46, wps=24504.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.049, loss_scale=16, train_wall=237, gb_free=21, wall=18887
2022-03-04 18:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:39:08 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 11.196 | nll_loss 10.168 | ppl 1150.15 | wps 45007.4 | wpb 510.9 | bsz 1 | num_updates 7125 | best_loss 8.489
2022-03-04 18:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7125 updates
2022-03-04 18:39:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:39:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:39:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 74 @ 7125 updates, score 11.196) (writing took 2.7120801135897636 seconds)
2022-03-04 18:39:11 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 18:39:11 | INFO | train | epoch 074 | loss 4.116 | nll_loss 2.429 | ppl 5.38 | wps 24492.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7125 | lr 0.000374634 | gnorm 1.043 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 18958
2022-03-04 18:39:11 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 18:39:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:42:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:42:26 | INFO | train_inner | epoch 075:     76 / 97 loss=4.09, nll_loss=2.398, ppl=5.27, wps=24547.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.049, loss_scale=16, train_wall=236, gb_free=21, wall=19153
2022-03-04 18:43:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:43:25 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 11.317 | nll_loss 10.295 | ppl 1256.07 | wps 43236.2 | wpb 510.9 | bsz 1 | num_updates 7221 | best_loss 8.489
2022-03-04 18:43:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7221 updates
2022-03-04 18:43:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:43:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:43:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 75 @ 7221 updates, score 11.317) (writing took 2.6030027493834496 seconds)
2022-03-04 18:43:28 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 18:43:28 | INFO | train | epoch 075 | loss 4.084 | nll_loss 2.392 | ppl 5.25 | wps 24480.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7221 | lr 0.000372136 | gnorm 1.043 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 19215
2022-03-04 18:43:28 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 18:43:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:46:51 | INFO | train_inner | epoch 076:     79 / 97 loss=4.063, nll_loss=2.368, ppl=5.16, wps=24737.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.035, loss_scale=16, train_wall=234, gb_free=21, wall=19418
2022-03-04 18:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:47:42 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 11.293 | nll_loss 10.275 | ppl 1238.84 | wps 44368.2 | wpb 510.9 | bsz 1 | num_updates 7318 | best_loss 8.489
2022-03-04 18:47:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7318 updates
2022-03-04 18:47:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:47:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 76 @ 7318 updates, score 11.293) (writing took 2.7503811735659838 seconds)
2022-03-04 18:47:45 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 18:47:45 | INFO | train | epoch 076 | loss 4.055 | nll_loss 2.359 | ppl 5.13 | wps 24730.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7318 | lr 0.000369661 | gnorm 1.036 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 19472
2022-03-04 18:47:45 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 18:47:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:47:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:51:18 | INFO | train_inner | epoch 077:     83 / 97 loss=4.031, nll_loss=2.331, ppl=5.03, wps=24514, ups=0.37, wpb=65495, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.034, loss_scale=16, train_wall=237, gb_free=21, wall=19685
2022-03-04 18:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:51:59 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 11.342 | nll_loss 10.312 | ppl 1271 | wps 45215.4 | wpb 510.9 | bsz 1 | num_updates 7414 | best_loss 8.489
2022-03-04 18:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7414 updates
2022-03-04 18:51:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 77 @ 7414 updates, score 11.342) (writing took 2.661504835821688 seconds)
2022-03-04 18:52:01 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 18:52:01 | INFO | train | epoch 077 | loss 4.026 | nll_loss 2.325 | ppl 5.01 | wps 24497.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7414 | lr 0.00036726 | gnorm 1.038 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 19728
2022-03-04 18:52:01 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 18:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:53:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:55:45 | INFO | train_inner | epoch 078:     87 / 97 loss=4.007, nll_loss=2.303, ppl=4.93, wps=24534.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.049, loss_scale=8, train_wall=237, gb_free=21, wall=19952
2022-03-04 18:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:56:16 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 11.378 | nll_loss 10.355 | ppl 1309.35 | wps 45341.7 | wpb 510.9 | bsz 1 | num_updates 7510 | best_loss 8.489
2022-03-04 18:56:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7510 updates
2022-03-04 18:56:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:56:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 18:56:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 78 @ 7510 updates, score 11.378) (writing took 2.5515851695090532 seconds)
2022-03-04 18:56:18 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 18:56:18 | INFO | train | epoch 078 | loss 3.999 | nll_loss 2.293 | ppl 4.9 | wps 24493.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7510 | lr 0.000364905 | gnorm 1.047 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 19985
2022-03-04 18:56:18 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 18:56:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:00:09 | INFO | train_inner | epoch 079:     90 / 97 loss=3.978, nll_loss=2.269, ppl=4.82, wps=24790.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.05, loss_scale=16, train_wall=234, gb_free=21, wall=20216
2022-03-04 19:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:00:32 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 11.451 | nll_loss 10.431 | ppl 1380.67 | wps 44856.6 | wpb 510.9 | bsz 1 | num_updates 7607 | best_loss 8.489
2022-03-04 19:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7607 updates
2022-03-04 19:00:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:00:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 79 @ 7607 updates, score 11.451) (writing took 2.657243832014501 seconds)
2022-03-04 19:00:35 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 19:00:35 | INFO | train | epoch 079 | loss 3.973 | nll_loss 2.263 | ppl 4.8 | wps 24742.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7607 | lr 0.000362571 | gnorm 1.05 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 20242
2022-03-04 19:00:35 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 19:00:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:04:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:04:37 | INFO | train_inner | epoch 080:     94 / 97 loss=3.949, nll_loss=2.236, ppl=4.71, wps=24469.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.041, loss_scale=16, train_wall=237, gb_free=21, wall=20484
2022-03-04 19:04:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:04:49 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 11.499 | nll_loss 10.488 | ppl 1436.44 | wps 45320.5 | wpb 510.9 | bsz 1 | num_updates 7703 | best_loss 8.489
2022-03-04 19:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7703 updates
2022-03-04 19:04:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 80 @ 7703 updates, score 11.499) (writing took 2.6920976396650076 seconds)
2022-03-04 19:04:52 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 19:04:52 | INFO | train | epoch 080 | loss 3.944 | nll_loss 2.23 | ppl 4.69 | wps 24436.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7703 | lr 0.000360305 | gnorm 1.04 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 20499
2022-03-04 19:04:52 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 19:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:07:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:07 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 11.518 | nll_loss 10.498 | ppl 1445.76 | wps 42260.8 | wpb 510.9 | bsz 1 | num_updates 7799 | best_loss 8.489
2022-03-04 19:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7799 updates
2022-03-04 19:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:09:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:09:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 81 @ 7799 updates, score 11.518) (writing took 2.6950625395402312 seconds)
2022-03-04 19:09:10 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 19:09:10 | INFO | train | epoch 081 | loss 3.919 | nll_loss 2.202 | ppl 4.6 | wps 24366.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7799 | lr 0.00035808 | gnorm 1.042 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 20757
2022-03-04 19:09:10 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 19:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:09:13 | INFO | train_inner | epoch 082:      1 / 97 loss=3.922, nll_loss=2.204, ppl=4.61, wps=23727.6, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=7800, lr=0.000358057, gnorm=1.041, loss_scale=8, train_wall=237, gb_free=21, wall=20760
2022-03-04 19:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:13:25 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 11.529 | nll_loss 10.511 | ppl 1458.74 | wps 44001.4 | wpb 510.9 | bsz 1 | num_updates 7896 | best_loss 8.489
2022-03-04 19:13:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7896 updates
2022-03-04 19:13:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:13:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 82 @ 7896 updates, score 11.529) (writing took 2.5973201002925634 seconds)
2022-03-04 19:13:27 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 19:13:27 | INFO | train | epoch 082 | loss 3.895 | nll_loss 2.173 | ppl 4.51 | wps 24694.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7896 | lr 0.000355874 | gnorm 1.028 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 21014
2022-03-04 19:13:27 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 19:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:38 | INFO | train_inner | epoch 083:      4 / 97 loss=3.891, nll_loss=2.169, ppl=4.5, wps=24716, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7900, lr=0.000355784, gnorm=1.028, loss_scale=16, train_wall=235, gb_free=21, wall=21025
2022-03-04 19:17:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:17:42 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 11.557 | nll_loss 10.548 | ppl 1496.94 | wps 44448.4 | wpb 510.9 | bsz 1 | num_updates 7993 | best_loss 8.489
2022-03-04 19:17:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7993 updates
2022-03-04 19:17:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 83 @ 7993 updates, score 11.557) (writing took 2.615237004123628 seconds)
2022-03-04 19:17:45 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 19:17:45 | INFO | train | epoch 083 | loss 3.872 | nll_loss 2.147 | ppl 4.43 | wps 24681 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7993 | lr 0.000353708 | gnorm 1.054 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 21272
2022-03-04 19:17:45 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 19:17:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:18:03 | INFO | train_inner | epoch 084:      7 / 97 loss=3.867, nll_loss=2.142, ppl=4.41, wps=24704.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8000, lr=0.000353553, gnorm=1.049, loss_scale=16, train_wall=235, gb_free=21, wall=21290
2022-03-04 19:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:22:00 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 11.638 | nll_loss 10.633 | ppl 1588.42 | wps 42036.3 | wpb 510.9 | bsz 1 | num_updates 8089 | best_loss 8.489
2022-03-04 19:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8089 updates
2022-03-04 19:22:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 84 @ 8089 updates, score 11.638) (writing took 2.604355708695948 seconds)
2022-03-04 19:22:03 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 19:22:03 | INFO | train | epoch 084 | loss 3.846 | nll_loss 2.117 | ppl 4.34 | wps 24363.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8089 | lr 0.000351603 | gnorm 1.028 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 21530
2022-03-04 19:22:03 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 19:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:22:31 | INFO | train_inner | epoch 085:     11 / 97 loss=3.843, nll_loss=2.113, ppl=4.33, wps=24413.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.03, loss_scale=16, train_wall=237, gb_free=21, wall=21558
2022-03-04 19:25:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:26:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:26:17 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 11.671 | nll_loss 10.671 | ppl 1630.19 | wps 45042.8 | wpb 510.9 | bsz 1 | num_updates 8185 | best_loss 8.489
2022-03-04 19:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8185 updates
2022-03-04 19:26:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 85 @ 8185 updates, score 11.671) (writing took 2.660549779422581 seconds)
2022-03-04 19:26:20 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 19:26:20 | INFO | train | epoch 085 | loss 3.823 | nll_loss 2.091 | ppl 4.26 | wps 24444.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8185 | lr 0.000349535 | gnorm 1.03 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 21787
2022-03-04 19:26:20 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 19:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:26:59 | INFO | train_inner | epoch 086:     15 / 97 loss=3.816, nll_loss=2.083, ppl=4.24, wps=24490.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.032, loss_scale=16, train_wall=237, gb_free=21, wall=21826
2022-03-04 19:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:30:34 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 11.717 | nll_loss 10.725 | ppl 1692.35 | wps 45283.2 | wpb 510.9 | bsz 1 | num_updates 8282 | best_loss 8.489
2022-03-04 19:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8282 updates
2022-03-04 19:30:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:30:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 86 @ 8282 updates, score 11.717) (writing took 2.6607074718922377 seconds)
2022-03-04 19:30:37 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 19:30:37 | INFO | train | epoch 086 | loss 3.804 | nll_loss 2.069 | ppl 4.2 | wps 24751.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8282 | lr 0.000347482 | gnorm 1.046 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 22044
2022-03-04 19:30:37 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 19:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:31:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:31:26 | INFO | train_inner | epoch 087:     19 / 97 loss=3.799, nll_loss=2.063, ppl=4.18, wps=24523.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.042, loss_scale=16, train_wall=237, gb_free=21, wall=22093
2022-03-04 19:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:34:52 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.735 | nll_loss 10.738 | ppl 1707.88 | wps 43353.5 | wpb 510.9 | bsz 1 | num_updates 8378 | best_loss 8.489
2022-03-04 19:34:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8378 updates
2022-03-04 19:34:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:34:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 87 @ 8378 updates, score 11.735) (writing took 2.621116563677788 seconds)
2022-03-04 19:34:55 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 19:34:55 | INFO | train | epoch 087 | loss 3.781 | nll_loss 2.042 | ppl 4.12 | wps 24379 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8378 | lr 0.000345485 | gnorm 1.027 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 22302
2022-03-04 19:34:55 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 19:34:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:35:51 | INFO | train_inner | epoch 088:     22 / 97 loss=3.773, nll_loss=2.033, ppl=4.09, wps=24670, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.035, loss_scale=16, train_wall=235, gb_free=21, wall=22358
2022-03-04 19:37:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:37:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:39:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:39:09 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.812 | nll_loss 10.831 | ppl 1822.1 | wps 42476.4 | wpb 510.9 | bsz 1 | num_updates 8473 | best_loss 8.489
2022-03-04 19:39:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8473 updates
2022-03-04 19:39:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:39:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:39:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 88 @ 8473 updates, score 11.812) (writing took 2.593107287772 seconds)
2022-03-04 19:39:12 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 19:39:12 | INFO | train | epoch 088 | loss 3.76 | nll_loss 2.018 | ppl 4.05 | wps 24175.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 8473 | lr 0.000343543 | gnorm 1.061 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 22559
2022-03-04 19:39:12 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 19:39:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:40:22 | INFO | train_inner | epoch 089:     27 / 97 loss=3.753, nll_loss=2.01, ppl=4.03, wps=24217.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.048, loss_scale=8, train_wall=239, gb_free=21, wall=22629
2022-03-04 19:43:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:43:26 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.853 | nll_loss 10.866 | ppl 1865.93 | wps 45189.9 | wpb 510.9 | bsz 1 | num_updates 8570 | best_loss 8.489
2022-03-04 19:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8570 updates
2022-03-04 19:43:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:43:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:43:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 89 @ 8570 updates, score 11.853) (writing took 2.5794015554711223 seconds)
2022-03-04 19:43:29 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 19:43:29 | INFO | train | epoch 089 | loss 3.741 | nll_loss 1.996 | ppl 3.99 | wps 24730.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8570 | lr 0.000341593 | gnorm 1.05 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 22816
2022-03-04 19:43:29 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 19:43:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:44:46 | INFO | train_inner | epoch 090:     30 / 97 loss=3.734, nll_loss=1.988, ppl=3.97, wps=24777.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.049, loss_scale=16, train_wall=234, gb_free=21, wall=22893
2022-03-04 19:47:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:47:43 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.86 | nll_loss 10.866 | ppl 1866.48 | wps 44454.2 | wpb 510.9 | bsz 1 | num_updates 8667 | best_loss 8.489
2022-03-04 19:47:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8667 updates
2022-03-04 19:47:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:47:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 90 @ 8667 updates, score 11.86) (writing took 2.6335619715973735 seconds)
2022-03-04 19:47:45 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 19:47:45 | INFO | train | epoch 090 | loss 3.721 | nll_loss 1.974 | ppl 3.93 | wps 24771.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8667 | lr 0.000339677 | gnorm 1.047 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 23072
2022-03-04 19:47:45 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 19:47:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:49:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:49:15 | INFO | train_inner | epoch 091:     35 / 97 loss=3.714, nll_loss=1.965, ppl=3.9, wps=24333.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8700, lr=0.000339032, gnorm=1.065, loss_scale=8, train_wall=239, gb_free=21, wall=23162
2022-03-04 19:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:51:59 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.832 | nll_loss 10.834 | ppl 1826.03 | wps 45299.1 | wpb 510.9 | bsz 1 | num_updates 8762 | best_loss 8.489
2022-03-04 19:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8762 updates
2022-03-04 19:51:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:52:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 91 @ 8762 updates, score 11.832) (writing took 2.6507592471316457 seconds)
2022-03-04 19:52:02 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 19:52:02 | INFO | train | epoch 091 | loss 3.698 | nll_loss 1.947 | ppl 3.86 | wps 24277 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 8762 | lr 0.00033783 | gnorm 1.044 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 23329
2022-03-04 19:52:02 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 19:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:53:39 | INFO | train_inner | epoch 092:     38 / 97 loss=3.691, nll_loss=1.939, ppl=3.84, wps=24798.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.028, loss_scale=8, train_wall=234, gb_free=21, wall=23426
2022-03-04 19:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:56:16 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.974 | nll_loss 11 | ppl 2048.63 | wps 45338.2 | wpb 510.9 | bsz 1 | num_updates 8859 | best_loss 8.489
2022-03-04 19:56:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8859 updates
2022-03-04 19:56:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:56:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 19:56:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 92 @ 8859 updates, score 11.974) (writing took 2.7103525018319488 seconds)
2022-03-04 19:56:18 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 19:56:18 | INFO | train | epoch 092 | loss 3.683 | nll_loss 1.929 | ppl 3.81 | wps 24740.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8859 | lr 0.000335976 | gnorm 1.038 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 23585
2022-03-04 19:56:18 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 19:56:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:58:04 | INFO | train_inner | epoch 093:     41 / 97 loss=3.674, nll_loss=1.92, ppl=3.78, wps=24725.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.036, loss_scale=16, train_wall=235, gb_free=21, wall=23691
2022-03-04 20:00:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:00:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:00:33 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.877 | nll_loss 10.889 | ppl 1896.57 | wps 45437.1 | wpb 510.9 | bsz 1 | num_updates 8955 | best_loss 8.489
2022-03-04 20:00:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8955 updates
2022-03-04 20:00:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:00:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 93 @ 8955 updates, score 11.877) (writing took 2.6529803602024913 seconds)
2022-03-04 20:00:35 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 20:00:35 | INFO | train | epoch 093 | loss 3.662 | nll_loss 1.906 | ppl 3.75 | wps 24459.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8955 | lr 0.00033417 | gnorm 1.026 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 23842
2022-03-04 20:00:35 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 20:00:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:02:31 | INFO | train_inner | epoch 094:     45 / 97 loss=3.654, nll_loss=1.896, ppl=3.72, wps=24529.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.031, loss_scale=16, train_wall=237, gb_free=21, wall=23958
2022-03-04 20:04:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:04:49 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.897 | nll_loss 10.912 | ppl 1927.19 | wps 45285.7 | wpb 510.9 | bsz 1 | num_updates 9052 | best_loss 8.489
2022-03-04 20:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9052 updates
2022-03-04 20:04:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 94 @ 9052 updates, score 11.897) (writing took 2.705113305710256 seconds)
2022-03-04 20:04:52 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 20:04:52 | INFO | train | epoch 094 | loss 3.647 | nll_loss 1.889 | ppl 3.7 | wps 24737.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9052 | lr 0.000332375 | gnorm 1.037 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 24099
2022-03-04 20:04:52 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 20:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:06:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:06:58 | INFO | train_inner | epoch 095:     49 / 97 loss=3.64, nll_loss=1.881, ppl=3.68, wps=24509, ups=0.37, wpb=65495, bsz=127.9, num_updates=9100, lr=0.000331497, gnorm=1.037, loss_scale=16, train_wall=237, gb_free=21, wall=24225
2022-03-04 20:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:09:07 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.977 | nll_loss 10.999 | ppl 2046.44 | wps 44412.4 | wpb 510.9 | bsz 1 | num_updates 9148 | best_loss 8.489
2022-03-04 20:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9148 updates
2022-03-04 20:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 95 @ 9148 updates, score 11.977) (writing took 2.764741954393685 seconds)
2022-03-04 20:09:09 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 20:09:09 | INFO | train | epoch 095 | loss 3.628 | nll_loss 1.867 | ppl 3.65 | wps 24445 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9148 | lr 0.000330626 | gnorm 1.029 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24356
2022-03-04 20:09:09 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 20:09:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:11:23 | INFO | train_inner | epoch 096:     52 / 97 loss=3.618, nll_loss=1.856, ppl=3.62, wps=24725.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.027, loss_scale=16, train_wall=234, gb_free=21, wall=24490
2022-03-04 20:12:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:13:24 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.944 | nll_loss 10.965 | ppl 1999.29 | wps 45294.1 | wpb 510.9 | bsz 1 | num_updates 9244 | best_loss 8.489
2022-03-04 20:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9244 updates
2022-03-04 20:13:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:13:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:13:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 96 @ 9244 updates, score 11.944) (writing took 2.71657045930624 seconds)
2022-03-04 20:13:26 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 20:13:26 | INFO | train | epoch 096 | loss 3.611 | nll_loss 1.848 | ppl 3.6 | wps 24455.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9244 | lr 0.000328905 | gnorm 1.044 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24614
2022-03-04 20:13:26 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 20:13:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:15:51 | INFO | train_inner | epoch 097:     56 / 97 loss=3.605, nll_loss=1.84, ppl=3.58, wps=24471.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.041, loss_scale=16, train_wall=237, gb_free=21, wall=24758
2022-03-04 20:17:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:17:41 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 12.008 | nll_loss 11.034 | ppl 2097.44 | wps 43644.2 | wpb 510.9 | bsz 1 | num_updates 9341 | best_loss 8.489
2022-03-04 20:17:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9341 updates
2022-03-04 20:17:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:17:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:17:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 97 @ 9341 updates, score 12.008) (writing took 2.705071210861206 seconds)
2022-03-04 20:17:44 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 20:17:44 | INFO | train | epoch 097 | loss 3.597 | nll_loss 1.832 | ppl 3.56 | wps 24654.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9341 | lr 0.000327192 | gnorm 1.045 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24871
2022-03-04 20:17:44 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 20:17:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:17:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:20:19 | INFO | train_inner | epoch 098:     60 / 97 loss=3.586, nll_loss=1.819, ppl=3.53, wps=24381.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.045, loss_scale=16, train_wall=238, gb_free=21, wall=25026
2022-03-04 20:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:22:00 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 12.017 | nll_loss 11.037 | ppl 2101.28 | wps 44145 | wpb 510.9 | bsz 1 | num_updates 9437 | best_loss 8.489
2022-03-04 20:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9437 updates
2022-03-04 20:22:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:22:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:22:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 98 @ 9437 updates, score 12.017) (writing took 2.6991943875327706 seconds)
2022-03-04 20:22:02 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 20:22:02 | INFO | train | epoch 098 | loss 3.579 | nll_loss 1.812 | ppl 3.51 | wps 24338.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9437 | lr 0.000325524 | gnorm 1.033 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25129
2022-03-04 20:22:02 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 20:22:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:23:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:24:47 | INFO | train_inner | epoch 099:     64 / 97 loss=3.573, nll_loss=1.804, ppl=3.49, wps=24479.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.036, loss_scale=8, train_wall=237, gb_free=21, wall=25294
2022-03-04 20:26:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:26:16 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 12.018 | nll_loss 11.039 | ppl 2104.62 | wps 43542.1 | wpb 510.9 | bsz 1 | num_updates 9533 | best_loss 8.489
2022-03-04 20:26:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9533 updates
2022-03-04 20:26:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:26:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:26:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 99 @ 9533 updates, score 12.018) (writing took 2.8535976950079203 seconds)
2022-03-04 20:26:19 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 20:26:19 | INFO | train | epoch 099 | loss 3.564 | nll_loss 1.794 | ppl 3.47 | wps 24478 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9533 | lr 0.000323881 | gnorm 1.036 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 25386
2022-03-04 20:26:19 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 20:26:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:29:11 | INFO | train_inner | epoch 100:     67 / 97 loss=3.551, nll_loss=1.78, ppl=3.43, wps=24762, ups=0.38, wpb=65495, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.034, loss_scale=16, train_wall=234, gb_free=21, wall=25558
2022-03-04 20:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:30:34 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 12.085 | nll_loss 11.12 | ppl 2226.39 | wps 43621.6 | wpb 510.9 | bsz 1 | num_updates 9630 | best_loss 8.489
2022-03-04 20:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9630 updates
2022-03-04 20:30:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:30:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:30:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 100 @ 9630 updates, score 12.085) (writing took 2.771815133281052 seconds)
2022-03-04 20:30:36 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 20:30:36 | INFO | train | epoch 100 | loss 3.547 | nll_loss 1.775 | ppl 3.42 | wps 24698.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9630 | lr 0.000322245 | gnorm 1.034 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 25644
2022-03-04 20:30:37 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 20:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:31:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:33:39 | INFO | train_inner | epoch 101:     71 / 97 loss=3.538, nll_loss=1.765, ppl=3.4, wps=24486.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.037, loss_scale=8, train_wall=237, gb_free=21, wall=25826
2022-03-04 20:34:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:34:50 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 12.097 | nll_loss 11.122 | ppl 2228.44 | wps 45164.4 | wpb 510.9 | bsz 1 | num_updates 9726 | best_loss 8.489
2022-03-04 20:34:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9726 updates
2022-03-04 20:34:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:34:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 101 @ 9726 updates, score 12.097) (writing took 2.786847141571343 seconds)
2022-03-04 20:34:53 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 20:34:53 | INFO | train | epoch 101 | loss 3.533 | nll_loss 1.759 | ppl 3.38 | wps 24503.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9726 | lr 0.000320651 | gnorm 1.033 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 25900
2022-03-04 20:34:53 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 20:34:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:38:03 | INFO | train_inner | epoch 102:     74 / 97 loss=3.525, nll_loss=1.75, ppl=3.36, wps=24788.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.045, loss_scale=16, train_wall=234, gb_free=21, wall=26090
2022-03-04 20:39:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:39:07 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 12.087 | nll_loss 11.117 | ppl 2220.88 | wps 43796.5 | wpb 510.9 | bsz 1 | num_updates 9823 | best_loss 8.489
2022-03-04 20:39:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9823 updates
2022-03-04 20:39:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 102 @ 9823 updates, score 12.087) (writing took 2.7886437317356467 seconds)
2022-03-04 20:39:10 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 20:39:10 | INFO | train | epoch 102 | loss 3.519 | nll_loss 1.743 | ppl 3.35 | wps 24728.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9823 | lr 0.000319064 | gnorm 1.055 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 26157
2022-03-04 20:39:10 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 20:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:42:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:42:31 | INFO | train_inner | epoch 103:     78 / 97 loss=3.505, nll_loss=1.727, ppl=3.31, wps=24488.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.035, loss_scale=16, train_wall=237, gb_free=21, wall=26358
2022-03-04 20:43:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:43:24 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 12.209 | nll_loss 11.258 | ppl 2449 | wps 43252.8 | wpb 510.9 | bsz 1 | num_updates 9919 | best_loss 8.489
2022-03-04 20:43:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9919 updates
2022-03-04 20:43:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:43:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 103 @ 9919 updates, score 12.209) (writing took 2.7692003808915615 seconds)
2022-03-04 20:43:27 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 20:43:27 | INFO | train | epoch 103 | loss 3.502 | nll_loss 1.724 | ppl 3.3 | wps 24441 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9919 | lr 0.000317516 | gnorm 1.024 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 26414
2022-03-04 20:43:27 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 20:43:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:46:56 | INFO | train_inner | epoch 104:     81 / 97 loss=3.495, nll_loss=1.717, ppl=3.29, wps=24679.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.024, loss_scale=16, train_wall=235, gb_free=21, wall=26623
2022-03-04 20:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:47:42 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 12.128 | nll_loss 11.166 | ppl 2297.6 | wps 43041.1 | wpb 510.9 | bsz 1 | num_updates 10016 | best_loss 8.489
2022-03-04 20:47:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10016 updates
2022-03-04 20:47:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:47:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 104 @ 10016 updates, score 12.128) (writing took 2.5893924441188574 seconds)
2022-03-04 20:47:45 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 20:47:45 | INFO | train | epoch 104 | loss 3.49 | nll_loss 1.71 | ppl 3.27 | wps 24675.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10016 | lr 0.000315975 | gnorm 1.031 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26672
2022-03-04 20:47:45 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 20:47:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:48:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:51:24 | INFO | train_inner | epoch 105:     85 / 97 loss=3.48, nll_loss=1.698, ppl=3.25, wps=24466.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.034, loss_scale=16, train_wall=237, gb_free=21, wall=26891
2022-03-04 20:51:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:51:59 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 12.163 | nll_loss 11.205 | ppl 2359.94 | wps 43633.6 | wpb 510.9 | bsz 1 | num_updates 10111 | best_loss 8.489
2022-03-04 20:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10111 updates
2022-03-04 20:51:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:52:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:52:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 105 @ 10111 updates, score 12.163) (writing took 2.704368555918336 seconds)
2022-03-04 20:52:02 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 20:52:02 | INFO | train | epoch 105 | loss 3.474 | nll_loss 1.692 | ppl 3.23 | wps 24182 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 10111 | lr 0.000314487 | gnorm 1.03 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 26929
2022-03-04 20:52:02 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 20:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:55:51 | INFO | train_inner | epoch 106:     89 / 97 loss=3.465, nll_loss=1.682, ppl=3.21, wps=24483.9, ups=0.37, wpb=65533.9, bsz=128, num_updates=10200, lr=0.000313112, gnorm=1.029, loss_scale=8, train_wall=237, gb_free=21, wall=27158
2022-03-04 20:56:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:56:17 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 12.196 | nll_loss 11.243 | ppl 2423 | wps 43509.3 | wpb 510.9 | bsz 1 | num_updates 10208 | best_loss 8.489
2022-03-04 20:56:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10208 updates
2022-03-04 20:56:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 20:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 106 @ 10208 updates, score 12.196) (writing took 2.70854034088552 seconds)
2022-03-04 20:56:19 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 20:56:19 | INFO | train | epoch 106 | loss 3.464 | nll_loss 1.681 | ppl 3.21 | wps 24689.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10208 | lr 0.000312989 | gnorm 1.029 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 27186
2022-03-04 20:56:19 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 20:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:00:16 | INFO | train_inner | epoch 107:     92 / 97 loss=3.454, nll_loss=1.67, ppl=3.18, wps=24726.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.036, loss_scale=16, train_wall=235, gb_free=21, wall=27423
2022-03-04 21:00:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:00:34 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 12.211 | nll_loss 11.26 | ppl 2451.99 | wps 42830.6 | wpb 510.9 | bsz 1 | num_updates 10305 | best_loss 8.489
2022-03-04 21:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10305 updates
2022-03-04 21:00:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:00:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:00:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 107 @ 10305 updates, score 12.211) (writing took 2.80148958042264 seconds)
2022-03-04 21:00:37 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 21:00:37 | INFO | train | epoch 107 | loss 3.451 | nll_loss 1.666 | ppl 3.17 | wps 24680.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10305 | lr 0.000311513 | gnorm 1.032 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 27444
2022-03-04 21:00:37 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 21:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:03:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:04:44 | INFO | train_inner | epoch 108:     96 / 97 loss=3.439, nll_loss=1.652, ppl=3.14, wps=24476.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.017, loss_scale=16, train_wall=237, gb_free=21, wall=27691
2022-03-04 21:04:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:04:51 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 12.217 | nll_loss 11.264 | ppl 2459.22 | wps 42144.5 | wpb 510.9 | bsz 1 | num_updates 10401 | best_loss 8.489
2022-03-04 21:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10401 updates
2022-03-04 21:04:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 108 @ 10401 updates, score 12.217) (writing took 2.699215065687895 seconds)
2022-03-04 21:04:54 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 21:04:54 | INFO | train | epoch 108 | loss 3.435 | nll_loss 1.649 | ppl 3.14 | wps 24439.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10401 | lr 0.000310072 | gnorm 1.017 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 27701
2022-03-04 21:04:54 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 21:04:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:07:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:09:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:09:09 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 12.245 | nll_loss 11.294 | ppl 2510.3 | wps 44108.6 | wpb 510.9 | bsz 1 | num_updates 10497 | best_loss 8.489
2022-03-04 21:09:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10497 updates
2022-03-04 21:09:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:09:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:09:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 109 @ 10497 updates, score 12.245) (writing took 2.6813067942857742 seconds)
2022-03-04 21:09:12 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 21:09:12 | INFO | train | epoch 109 | loss 3.426 | nll_loss 1.637 | ppl 3.11 | wps 24398.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10497 | lr 0.000308651 | gnorm 1.021 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 27959
2022-03-04 21:09:12 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 21:09:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:09:20 | INFO | train_inner | epoch 110:      3 / 97 loss=3.424, nll_loss=1.636, ppl=3.11, wps=23730, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=10500, lr=0.000308607, gnorm=1.022, loss_scale=8, train_wall=237, gb_free=21, wall=27967
2022-03-04 21:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:13:26 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 12.285 | nll_loss 11.339 | ppl 2589.66 | wps 42696.6 | wpb 510.9 | bsz 1 | num_updates 10594 | best_loss 8.489
2022-03-04 21:13:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10594 updates
2022-03-04 21:13:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:13:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:13:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 110 @ 10594 updates, score 12.285) (writing took 2.6785592855885625 seconds)
2022-03-04 21:13:29 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 21:13:29 | INFO | train | epoch 110 | loss 3.413 | nll_loss 1.623 | ppl 3.08 | wps 24702.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10594 | lr 0.000307235 | gnorm 1.028 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 28216
2022-03-04 21:13:29 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 21:13:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:13:44 | INFO | train_inner | epoch 111:      6 / 97 loss=3.412, nll_loss=1.622, ppl=3.08, wps=24724.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10600, lr=0.000307148, gnorm=1.027, loss_scale=16, train_wall=235, gb_free=21, wall=28231
2022-03-04 21:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:17:43 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 12.312 | nll_loss 11.371 | ppl 2647.86 | wps 44123 | wpb 510.9 | bsz 1 | num_updates 10691 | best_loss 8.489
2022-03-04 21:17:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10691 updates
2022-03-04 21:17:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:17:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 111 @ 10691 updates, score 12.312) (writing took 2.7569072293117642 seconds)
2022-03-04 21:17:46 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 21:17:46 | INFO | train | epoch 111 | loss 3.401 | nll_loss 1.61 | ppl 3.05 | wps 24697 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10691 | lr 0.000305838 | gnorm 1.025 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28473
2022-03-04 21:17:46 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 21:17:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:18:09 | INFO | train_inner | epoch 112:      9 / 97 loss=3.396, nll_loss=1.605, ppl=3.04, wps=24718.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.025, loss_scale=16, train_wall=235, gb_free=21, wall=28496
2022-03-04 21:18:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:21:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:22:01 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 12.343 | nll_loss 11.409 | ppl 2718.38 | wps 43460.3 | wpb 510.9 | bsz 1 | num_updates 10787 | best_loss 8.489
2022-03-04 21:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10787 updates
2022-03-04 21:22:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 112 @ 10787 updates, score 12.343) (writing took 2.7076576901599765 seconds)
2022-03-04 21:22:03 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 21:22:03 | INFO | train | epoch 112 | loss 3.387 | nll_loss 1.594 | ppl 3.02 | wps 24433.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10787 | lr 0.000304474 | gnorm 1.017 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28731
2022-03-04 21:22:04 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 21:22:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:22:37 | INFO | train_inner | epoch 113:     13 / 97 loss=3.383, nll_loss=1.59, ppl=3.01, wps=24467.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.013, loss_scale=16, train_wall=237, gb_free=21, wall=28764
2022-03-04 21:23:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:26:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:26:19 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 12.384 | nll_loss 11.452 | ppl 2801.86 | wps 44203.9 | wpb 510.9 | bsz 1 | num_updates 10883 | best_loss 8.489
2022-03-04 21:26:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10883 updates
2022-03-04 21:26:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:26:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:26:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 113 @ 10883 updates, score 12.384) (writing took 2.651794766075909 seconds)
2022-03-04 21:26:21 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 21:26:21 | INFO | train | epoch 113 | loss 3.378 | nll_loss 1.584 | ppl 3 | wps 24392.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10883 | lr 0.000303128 | gnorm 1.041 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 28988
2022-03-04 21:26:21 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 21:26:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:27:05 | INFO | train_inner | epoch 114:     17 / 97 loss=3.375, nll_loss=1.581, ppl=2.99, wps=24439.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.043, loss_scale=8, train_wall=237, gb_free=21, wall=29032
2022-03-04 21:30:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:30:36 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 12.314 | nll_loss 11.36 | ppl 2628.25 | wps 43923.7 | wpb 510.9 | bsz 1 | num_updates 10980 | best_loss 8.489
2022-03-04 21:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10980 updates
2022-03-04 21:30:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:30:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:30:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 114 @ 10980 updates, score 12.314) (writing took 2.7348145889118314 seconds)
2022-03-04 21:30:39 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 21:30:39 | INFO | train | epoch 114 | loss 3.366 | nll_loss 1.571 | ppl 2.97 | wps 24650.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10980 | lr 0.000301786 | gnorm 1.028 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29246
2022-03-04 21:30:39 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 21:30:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:31:31 | INFO | train_inner | epoch 115:     20 / 97 loss=3.363, nll_loss=1.568, ppl=2.96, wps=24656.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.026, loss_scale=16, train_wall=235, gb_free=21, wall=29298
2022-03-04 21:33:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:34:54 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 12.351 | nll_loss 11.413 | ppl 2726.03 | wps 45162.1 | wpb 510.9 | bsz 1 | num_updates 11076 | best_loss 8.489
2022-03-04 21:34:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11076 updates
2022-03-04 21:34:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:34:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:34:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 115 @ 11076 updates, score 12.351) (writing took 2.634361713193357 seconds)
2022-03-04 21:34:56 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 21:34:56 | INFO | train | epoch 115 | loss 3.354 | nll_loss 1.557 | ppl 2.94 | wps 24420.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11076 | lr 0.000300475 | gnorm 1.021 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 29503
2022-03-04 21:34:56 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 21:34:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:35:58 | INFO | train_inner | epoch 116:     24 / 97 loss=3.349, nll_loss=1.552, ppl=2.93, wps=24458.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.027, loss_scale=8, train_wall=237, gb_free=21, wall=29565
2022-03-04 21:39:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:39:11 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 12.33 | nll_loss 11.391 | ppl 2685.95 | wps 44271.8 | wpb 510.9 | bsz 1 | num_updates 11173 | best_loss 8.489
2022-03-04 21:39:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11173 updates
2022-03-04 21:39:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:39:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 116 @ 11173 updates, score 12.33) (writing took 2.630639393813908 seconds)
2022-03-04 21:39:14 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 21:39:14 | INFO | train | epoch 116 | loss 3.344 | nll_loss 1.546 | ppl 2.92 | wps 24669.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11173 | lr 0.000299168 | gnorm 1.021 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 29761
2022-03-04 21:39:14 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 21:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:40:24 | INFO | train_inner | epoch 117:     27 / 97 loss=3.342, nll_loss=1.543, ppl=2.91, wps=24701.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.018, loss_scale=16, train_wall=235, gb_free=21, wall=29831
2022-03-04 21:43:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:43:28 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 12.377 | nll_loss 11.446 | ppl 2790.23 | wps 44582.1 | wpb 510.9 | bsz 1 | num_updates 11270 | best_loss 8.489
2022-03-04 21:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11270 updates
2022-03-04 21:43:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:43:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:43:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 117 @ 11270 updates, score 12.377) (writing took 2.671984483487904 seconds)
2022-03-04 21:43:31 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 21:43:31 | INFO | train | epoch 117 | loss 3.335 | nll_loss 1.536 | ppl 2.9 | wps 24707.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11270 | lr 0.000297878 | gnorm 1.031 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30018
2022-03-04 21:43:31 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 21:43:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:44:48 | INFO | train_inner | epoch 118:     30 / 97 loss=3.329, nll_loss=1.529, ppl=2.89, wps=24733.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.027, loss_scale=16, train_wall=234, gb_free=21, wall=30095
2022-03-04 21:45:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:47:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:47:45 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 12.371 | nll_loss 11.434 | ppl 2767.65 | wps 44881.5 | wpb 510.9 | bsz 1 | num_updates 11366 | best_loss 8.489
2022-03-04 21:47:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11366 updates
2022-03-04 21:47:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:47:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:47:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 118 @ 11366 updates, score 12.371) (writing took 2.7022362519055605 seconds)
2022-03-04 21:47:48 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 21:47:48 | INFO | train | epoch 118 | loss 3.323 | nll_loss 1.523 | ppl 2.87 | wps 24462.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11366 | lr 0.000296617 | gnorm 1.015 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30275
2022-03-04 21:47:48 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 21:47:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:49:16 | INFO | train_inner | epoch 119:     34 / 97 loss=3.32, nll_loss=1.519, ppl=2.87, wps=24494.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.013, loss_scale=16, train_wall=237, gb_free=21, wall=30363
2022-03-04 21:50:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:51:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:52:03 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 12.37 | nll_loss 11.436 | ppl 2770.45 | wps 43371.9 | wpb 510.9 | bsz 1 | num_updates 11462 | best_loss 8.489
2022-03-04 21:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11462 updates
2022-03-04 21:52:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:52:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:52:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 119 @ 11462 updates, score 12.37) (writing took 2.6755087515339255 seconds)
2022-03-04 21:52:06 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 21:52:06 | INFO | train | epoch 119 | loss 3.313 | nll_loss 1.511 | ppl 2.85 | wps 24418.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11462 | lr 0.000295372 | gnorm 1.016 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30533
2022-03-04 21:52:06 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 21:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:53:43 | INFO | train_inner | epoch 120:     38 / 97 loss=3.308, nll_loss=1.506, ppl=2.84, wps=24479.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.016, loss_scale=16, train_wall=237, gb_free=21, wall=30630
2022-03-04 21:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:56:19 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 12.425 | nll_loss 11.494 | ppl 2883.52 | wps 45398.2 | wpb 510.9 | bsz 1 | num_updates 11559 | best_loss 8.489
2022-03-04 21:56:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11559 updates
2022-03-04 21:56:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:56:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 21:56:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 120 @ 11559 updates, score 12.425) (writing took 2.7288903100416064 seconds)
2022-03-04 21:56:22 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 21:56:22 | INFO | train | epoch 120 | loss 3.304 | nll_loss 1.501 | ppl 2.83 | wps 24752.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11559 | lr 0.00029413 | gnorm 1.02 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 30789
2022-03-04 21:56:22 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 21:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:56:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:58:10 | INFO | train_inner | epoch 121:     42 / 97 loss=3.302, nll_loss=1.499, ppl=2.83, wps=24540.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.03, loss_scale=16, train_wall=237, gb_free=21, wall=30897
2022-03-04 22:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:00:36 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 12.412 | nll_loss 11.476 | ppl 2848.76 | wps 45092.7 | wpb 510.9 | bsz 1 | num_updates 11655 | best_loss 8.489
2022-03-04 22:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11655 updates
2022-03-04 22:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 121 @ 11655 updates, score 12.412) (writing took 2.7853007027879357 seconds)
2022-03-04 22:00:39 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 22:00:39 | INFO | train | epoch 121 | loss 3.292 | nll_loss 1.488 | ppl 2.8 | wps 24464.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11655 | lr 0.000292917 | gnorm 1.026 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31046
2022-03-04 22:00:39 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 22:00:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:02:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:02:38 | INFO | train_inner | epoch 122:     46 / 97 loss=3.287, nll_loss=1.482, ppl=2.79, wps=24484.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=1.009, loss_scale=16, train_wall=237, gb_free=21, wall=31165
2022-03-04 22:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:54 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 12.471 | nll_loss 11.548 | ppl 2994.97 | wps 43691.1 | wpb 510.9 | bsz 1 | num_updates 11751 | best_loss 8.489
2022-03-04 22:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11751 updates
2022-03-04 22:04:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:04:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:04:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 122 @ 11751 updates, score 12.471) (writing took 2.8056545723229647 seconds)
2022-03-04 22:04:57 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 22:04:57 | INFO | train | epoch 122 | loss 3.282 | nll_loss 1.477 | ppl 2.78 | wps 24428.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11751 | lr 0.000291718 | gnorm 0.999 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31304
2022-03-04 22:04:57 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 22:04:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:05:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:07:05 | INFO | train_inner | epoch 123:     50 / 97 loss=3.279, nll_loss=1.473, ppl=2.78, wps=24457.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.008, loss_scale=8, train_wall=237, gb_free=21, wall=31432
2022-03-04 22:09:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:09:11 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 12.473 | nll_loss 11.554 | ppl 3007.22 | wps 44226 | wpb 510.9 | bsz 1 | num_updates 11847 | best_loss 8.489
2022-03-04 22:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11847 updates
2022-03-04 22:09:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:09:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 123 @ 11847 updates, score 12.473) (writing took 2.7114816326647997 seconds)
2022-03-04 22:09:14 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 22:09:14 | INFO | train | epoch 123 | loss 3.275 | nll_loss 1.469 | ppl 2.77 | wps 24454 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11847 | lr 0.000290533 | gnorm 1.027 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 31561
2022-03-04 22:09:14 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 22:09:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:11:30 | INFO | train_inner | epoch 124:     53 / 97 loss=3.267, nll_loss=1.46, ppl=2.75, wps=24763, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.035, loss_scale=16, train_wall=234, gb_free=21, wall=31697
2022-03-04 22:13:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:13:28 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 12.461 | nll_loss 11.535 | ppl 2966.86 | wps 45224 | wpb 510.9 | bsz 1 | num_updates 11944 | best_loss 8.489
2022-03-04 22:13:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11944 updates
2022-03-04 22:13:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:13:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 124 @ 11944 updates, score 12.461) (writing took 2.7753506461158395 seconds)
2022-03-04 22:13:30 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 22:13:30 | INFO | train | epoch 124 | loss 3.266 | nll_loss 1.459 | ppl 2.75 | wps 24745.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11944 | lr 0.000289351 | gnorm 1.033 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 31817
2022-03-04 22:13:30 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 22:13:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:15:54 | INFO | train_inner | epoch 125:     56 / 97 loss=3.262, nll_loss=1.455, ppl=2.74, wps=24775.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.016, loss_scale=16, train_wall=234, gb_free=21, wall=31961
2022-03-04 22:16:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:17:44 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 12.454 | nll_loss 11.537 | ppl 2971.04 | wps 45087.4 | wpb 510.9 | bsz 1 | num_updates 12040 | best_loss 8.489
2022-03-04 22:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12040 updates
2022-03-04 22:17:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 125 @ 12040 updates, score 12.454) (writing took 2.757436839863658 seconds)
2022-03-04 22:17:47 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 22:17:47 | INFO | train | epoch 125 | loss 3.255 | nll_loss 1.447 | ppl 2.73 | wps 24492.8 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 12040 | lr 0.000288195 | gnorm 1.007 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32074
2022-03-04 22:17:47 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 22:17:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:20:21 | INFO | train_inner | epoch 126:     60 / 97 loss=3.25, nll_loss=1.441, ppl=2.71, wps=24545.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=1.003, loss_scale=16, train_wall=237, gb_free=21, wall=32228
2022-03-04 22:21:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:22:01 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 12.45 | nll_loss 11.517 | ppl 2931.05 | wps 45152.7 | wpb 510.9 | bsz 1 | num_updates 12137 | best_loss 8.489
2022-03-04 22:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12137 updates
2022-03-04 22:22:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:22:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:22:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 126 @ 12137 updates, score 12.45) (writing took 2.7520441748201847 seconds)
2022-03-04 22:22:04 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 22:22:04 | INFO | train | epoch 126 | loss 3.247 | nll_loss 1.438 | ppl 2.71 | wps 24739.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12137 | lr 0.000287041 | gnorm 1.008 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32331
2022-03-04 22:22:04 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 22:22:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:22:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:22:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:24:51 | INFO | train_inner | epoch 127:     65 / 97 loss=3.243, nll_loss=1.433, ppl=2.7, wps=24290.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12200, lr=0.000286299, gnorm=1.015, loss_scale=8, train_wall=239, gb_free=21, wall=32498
2022-03-04 22:26:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:26:18 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 12.472 | nll_loss 11.553 | ppl 3005.52 | wps 45082.4 | wpb 510.9 | bsz 1 | num_updates 12232 | best_loss 8.489
2022-03-04 22:26:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12232 updates
2022-03-04 22:26:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:26:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 127 @ 12232 updates, score 12.472) (writing took 2.71481750626117 seconds)
2022-03-04 22:26:21 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 22:26:21 | INFO | train | epoch 127 | loss 3.238 | nll_loss 1.428 | ppl 2.69 | wps 24238.6 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 12232 | lr 0.000285924 | gnorm 1.005 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 32588
2022-03-04 22:26:21 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 22:26:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:29:15 | INFO | train_inner | epoch 128:     68 / 97 loss=3.235, nll_loss=1.424, ppl=2.68, wps=24763, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=1.002, loss_scale=16, train_wall=234, gb_free=21, wall=32762
2022-03-04 22:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:30:34 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 12.553 | nll_loss 11.635 | ppl 3181.17 | wps 44739 | wpb 510.9 | bsz 1 | num_updates 12329 | best_loss 8.489
2022-03-04 22:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12329 updates
2022-03-04 22:30:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:30:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 128 @ 12329 updates, score 12.553) (writing took 2.7084890976548195 seconds)
2022-03-04 22:30:37 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 22:30:37 | INFO | train | epoch 128 | loss 3.232 | nll_loss 1.422 | ppl 2.68 | wps 24761.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12329 | lr 0.000284797 | gnorm 1.015 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 32844
2022-03-04 22:30:37 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 22:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:33:39 | INFO | train_inner | epoch 129:     71 / 97 loss=3.228, nll_loss=1.417, ppl=2.67, wps=24787.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=1.016, loss_scale=16, train_wall=234, gb_free=21, wall=33026
2022-03-04 22:33:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:34:51 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 12.544 | nll_loss 11.631 | ppl 3170.7 | wps 42869.6 | wpb 510.9 | bsz 1 | num_updates 12425 | best_loss 8.489
2022-03-04 22:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12425 updates
2022-03-04 22:34:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:34:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 129 @ 12425 updates, score 12.544) (writing took 2.688003127463162 seconds)
2022-03-04 22:34:54 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 22:34:54 | INFO | train | epoch 129 | loss 3.221 | nll_loss 1.409 | ppl 2.66 | wps 24479.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12425 | lr 0.000283695 | gnorm 0.994 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33101
2022-03-04 22:34:54 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 22:34:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:37:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:38:09 | INFO | train_inner | epoch 130:     76 / 97 loss=3.215, nll_loss=1.403, ppl=2.64, wps=24260.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=0.999, loss_scale=8, train_wall=239, gb_free=21, wall=33296
2022-03-04 22:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:39:08 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 12.543 | nll_loss 11.627 | ppl 3162.22 | wps 42357.9 | wpb 510.9 | bsz 1 | num_updates 12521 | best_loss 8.489
2022-03-04 22:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12521 updates
2022-03-04 22:39:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:39:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:39:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 130 @ 12521 updates, score 12.543) (writing took 2.9573786817491055 seconds)
2022-03-04 22:39:11 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 22:39:11 | INFO | train | epoch 130 | loss 3.213 | nll_loss 1.401 | ppl 2.64 | wps 24425 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12521 | lr 0.000282605 | gnorm 1.011 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 33358
2022-03-04 22:39:11 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 22:39:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:42:34 | INFO | train_inner | epoch 131:     79 / 97 loss=3.208, nll_loss=1.394, ppl=2.63, wps=24733.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=1.01, loss_scale=8, train_wall=234, gb_free=21, wall=33561
2022-03-04 22:43:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:43:25 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 12.554 | nll_loss 11.646 | ppl 3204.2 | wps 42233 | wpb 510.9 | bsz 1 | num_updates 12618 | best_loss 8.489
2022-03-04 22:43:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12618 updates
2022-03-04 22:43:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:43:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:43:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 131 @ 12618 updates, score 12.554) (writing took 2.803877408616245 seconds)
2022-03-04 22:43:28 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 22:43:28 | INFO | train | epoch 131 | loss 3.206 | nll_loss 1.393 | ppl 2.63 | wps 24726.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12618 | lr 0.000281517 | gnorm 1.016 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33615
2022-03-04 22:43:28 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 22:43:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:44:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:47:01 | INFO | train_inner | epoch 132:     83 / 97 loss=3.2, nll_loss=1.386, ppl=2.61, wps=24516.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=1.022, loss_scale=8, train_wall=236, gb_free=21, wall=33828
2022-03-04 22:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:47:42 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 12.595 | nll_loss 11.693 | ppl 3309.94 | wps 45333 | wpb 510.9 | bsz 1 | num_updates 12714 | best_loss 8.489
2022-03-04 22:47:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12714 updates
2022-03-04 22:47:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:47:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 132 @ 12714 updates, score 12.595) (writing took 2.8239508932456374 seconds)
2022-03-04 22:47:45 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 22:47:45 | INFO | train | epoch 132 | loss 3.197 | nll_loss 1.383 | ppl 2.61 | wps 24515.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12714 | lr 0.000280452 | gnorm 1.021 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 33872
2022-03-04 22:47:45 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 22:47:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:50:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:51:28 | INFO | train_inner | epoch 133:     87 / 97 loss=3.192, nll_loss=1.377, ppl=2.6, wps=24566.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=1.008, loss_scale=8, train_wall=236, gb_free=21, wall=34095
2022-03-04 22:51:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:51:58 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 12.52 | nll_loss 11.605 | ppl 3114.72 | wps 43328.3 | wpb 510.9 | bsz 1 | num_updates 12810 | best_loss 8.489
2022-03-04 22:51:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12810 updates
2022-03-04 22:51:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 133 @ 12810 updates, score 12.52) (writing took 2.7117678290233016 seconds)
2022-03-04 22:52:01 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 22:52:01 | INFO | train | epoch 133 | loss 3.189 | nll_loss 1.374 | ppl 2.59 | wps 24517.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12810 | lr 0.000279399 | gnorm 1.001 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 34128
2022-03-04 22:52:01 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 22:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:55:52 | INFO | train_inner | epoch 134:     90 / 97 loss=3.182, nll_loss=1.366, ppl=2.58, wps=24807.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=1.002, loss_scale=8, train_wall=234, gb_free=21, wall=34359
2022-03-04 22:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:56:15 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 12.538 | nll_loss 11.638 | ppl 3187.8 | wps 43477.1 | wpb 510.9 | bsz 1 | num_updates 12907 | best_loss 8.489
2022-03-04 22:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12907 updates
2022-03-04 22:56:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:56:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 22:56:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 134 @ 12907 updates, score 12.538) (writing took 2.7435258459299803 seconds)
2022-03-04 22:56:18 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 22:56:18 | INFO | train | epoch 134 | loss 3.181 | nll_loss 1.365 | ppl 2.58 | wps 24774.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12907 | lr 0.000278348 | gnorm 1.004 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34385
2022-03-04 22:56:18 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 22:56:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:59:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:00:19 | INFO | train_inner | epoch 135:     94 / 97 loss=3.178, nll_loss=1.362, ppl=2.57, wps=24510, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=0.987, loss_scale=8, train_wall=236, gb_free=21, wall=34626
2022-03-04 23:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:00:32 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 12.565 | nll_loss 11.655 | ppl 3224.58 | wps 45237.3 | wpb 510.9 | bsz 1 | num_updates 13003 | best_loss 8.489
2022-03-04 23:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13003 updates
2022-03-04 23:00:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:00:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:00:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 135 @ 13003 updates, score 12.565) (writing took 2.7748592011630535 seconds)
2022-03-04 23:00:34 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 23:00:34 | INFO | train | epoch 135 | loss 3.173 | nll_loss 1.357 | ppl 2.56 | wps 24486.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13003 | lr 0.000277318 | gnorm 0.984 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 34641
2022-03-04 23:00:34 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 23:00:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:04:44 | INFO | train_inner | epoch 136:     97 / 97 loss=3.169, nll_loss=1.353, ppl=2.55, wps=24716, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=13100, lr=0.000276289, gnorm=1.001, loss_scale=8, train_wall=235, gb_free=21, wall=34891
2022-03-04 23:04:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:04:49 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 12.553 | nll_loss 11.645 | ppl 3203.58 | wps 45188.3 | wpb 510.9 | bsz 1 | num_updates 13100 | best_loss 8.489
2022-03-04 23:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13100 updates
2022-03-04 23:04:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 136 @ 13100 updates, score 12.553) (writing took 2.76273394562304 seconds)
2022-03-04 23:04:52 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 23:04:52 | INFO | train | epoch 136 | loss 3.168 | nll_loss 1.351 | ppl 2.55 | wps 24691.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13100 | lr 0.000276289 | gnorm 1 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 34899
2022-03-04 23:04:52 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 23:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:09:05 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 12.573 | nll_loss 11.668 | ppl 3254.19 | wps 45074.6 | wpb 510.9 | bsz 1 | num_updates 13197 | best_loss 8.489
2022-03-04 23:09:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13197 updates
2022-03-04 23:09:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:09:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 137 @ 13197 updates, score 12.573) (writing took 2.839959157630801 seconds)
2022-03-04 23:09:08 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 23:09:08 | INFO | train | epoch 137 | loss 3.162 | nll_loss 1.344 | ppl 2.54 | wps 24743 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13197 | lr 0.000275272 | gnorm 1.011 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35155
2022-03-04 23:09:08 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 23:09:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:09:16 | INFO | train_inner | epoch 138:      3 / 97 loss=3.16, nll_loss=1.342, ppl=2.54, wps=24056.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13200, lr=0.000275241, gnorm=1.011, loss_scale=16, train_wall=234, gb_free=21, wall=35163
2022-03-04 23:10:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:13:23 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 12.56 | nll_loss 11.65 | ppl 3213.63 | wps 43229.8 | wpb 510.9 | bsz 1 | num_updates 13293 | best_loss 8.489
2022-03-04 23:13:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13293 updates
2022-03-04 23:13:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:13:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:13:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 138 @ 13293 updates, score 12.56) (writing took 2.7134340750053525 seconds)
2022-03-04 23:13:25 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 23:13:25 | INFO | train | epoch 138 | loss 3.151 | nll_loss 1.332 | ppl 2.52 | wps 24464 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13293 | lr 0.000274276 | gnorm 1.002 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35412
2022-03-04 23:13:25 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 23:13:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:13:43 | INFO | train_inner | epoch 139:      7 / 97 loss=3.149, nll_loss=1.33, ppl=2.51, wps=24505.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13300, lr=0.000274204, gnorm=1.003, loss_scale=16, train_wall=237, gb_free=21, wall=35430
2022-03-04 23:14:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:17:40 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 12.627 | nll_loss 11.734 | ppl 3406.73 | wps 43010.6 | wpb 510.9 | bsz 1 | num_updates 13389 | best_loss 8.489
2022-03-04 23:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13389 updates
2022-03-04 23:17:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 139 @ 13389 updates, score 12.627) (writing took 2.8487155102193356 seconds)
2022-03-04 23:17:43 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 23:17:43 | INFO | train | epoch 139 | loss 3.145 | nll_loss 1.326 | ppl 2.51 | wps 24390.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13389 | lr 0.000273291 | gnorm 1.009 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 35670
2022-03-04 23:17:43 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 23:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:18:11 | INFO | train_inner | epoch 140:     11 / 97 loss=3.143, nll_loss=1.323, ppl=2.5, wps=24441.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13400, lr=0.000273179, gnorm=1.002, loss_scale=8, train_wall=237, gb_free=21, wall=35698
2022-03-04 23:20:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:21:58 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 12.564 | nll_loss 11.658 | ppl 3230.78 | wps 43084 | wpb 510.9 | bsz 1 | num_updates 13485 | best_loss 8.489
2022-03-04 23:21:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13485 updates
2022-03-04 23:21:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:22:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:22:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 140 @ 13485 updates, score 12.564) (writing took 2.8257817616686225 seconds)
2022-03-04 23:22:00 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 23:22:00 | INFO | train | epoch 140 | loss 3.138 | nll_loss 1.318 | ppl 2.49 | wps 24431.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13485 | lr 0.000272317 | gnorm 0.998 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 35927
2022-03-04 23:22:00 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 23:22:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:22:39 | INFO | train_inner | epoch 141:     15 / 97 loss=3.133, nll_loss=1.312, ppl=2.48, wps=24461.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1, loss_scale=8, train_wall=237, gb_free=21, wall=35966
2022-03-04 23:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:26:16 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 12.631 | nll_loss 11.734 | ppl 3406.37 | wps 43847.5 | wpb 510.9 | bsz 1 | num_updates 13582 | best_loss 8.489
2022-03-04 23:26:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13582 updates
2022-03-04 23:26:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:26:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:26:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 141 @ 13582 updates, score 12.631) (writing took 2.748505460098386 seconds)
2022-03-04 23:26:18 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 23:26:18 | INFO | train | epoch 141 | loss 3.131 | nll_loss 1.31 | ppl 2.48 | wps 24636.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13582 | lr 0.000271343 | gnorm 1.001 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 36185
2022-03-04 23:26:18 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 23:26:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:27:05 | INFO | train_inner | epoch 142:     18 / 97 loss=3.129, nll_loss=1.309, ppl=2.48, wps=24643.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=1.001, loss_scale=16, train_wall=235, gb_free=21, wall=36232
2022-03-04 23:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:30:33 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 12.626 | nll_loss 11.724 | ppl 3382.79 | wps 43812.8 | wpb 510.9 | bsz 1 | num_updates 13679 | best_loss 8.489
2022-03-04 23:30:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13679 updates
2022-03-04 23:30:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:30:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:30:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 142 @ 13679 updates, score 12.626) (writing took 2.7276007188484073 seconds)
2022-03-04 23:30:36 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 23:30:36 | INFO | train | epoch 142 | loss 3.125 | nll_loss 1.304 | ppl 2.47 | wps 24661.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13679 | lr 0.000270379 | gnorm 1.005 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 36443
2022-03-04 23:30:36 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 23:30:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:31:30 | INFO | train_inner | epoch 143:     21 / 97 loss=3.122, nll_loss=1.301, ppl=2.46, wps=24717.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1, loss_scale=16, train_wall=235, gb_free=21, wall=36497
2022-03-04 23:31:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:34:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:34:50 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 12.669 | nll_loss 11.77 | ppl 3491.87 | wps 45326.8 | wpb 510.9 | bsz 1 | num_updates 13775 | best_loss 8.489
2022-03-04 23:34:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13775 updates
2022-03-04 23:34:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:34:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 143 @ 13775 updates, score 12.669) (writing took 2.7428718507289886 seconds)
2022-03-04 23:34:53 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 23:34:53 | INFO | train | epoch 143 | loss 3.117 | nll_loss 1.296 | ppl 2.45 | wps 24457.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13775 | lr 0.000269435 | gnorm 0.99 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 36700
2022-03-04 23:34:53 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 23:34:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:35:57 | INFO | train_inner | epoch 144:     25 / 97 loss=3.116, nll_loss=1.293, ppl=2.45, wps=24503.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=0.993, loss_scale=16, train_wall=237, gb_free=21, wall=36764
2022-03-04 23:37:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:39:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:39:07 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 12.67 | nll_loss 11.783 | ppl 3524.65 | wps 43970.8 | wpb 510.9 | bsz 1 | num_updates 13871 | best_loss 8.489
2022-03-04 23:39:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13871 updates
2022-03-04 23:39:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 144 @ 13871 updates, score 12.67) (writing took 2.857955375686288 seconds)
2022-03-04 23:39:10 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 23:39:10 | INFO | train | epoch 144 | loss 3.112 | nll_loss 1.289 | ppl 2.44 | wps 24468.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13871 | lr 0.000268501 | gnorm 0.991 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36957
2022-03-04 23:39:10 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 23:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:40:25 | INFO | train_inner | epoch 145:     29 / 97 loss=3.111, nll_loss=1.288, ppl=2.44, wps=24476.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=1.004, loss_scale=16, train_wall=237, gb_free=21, wall=37032
2022-03-04 23:43:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:43:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:43:24 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 12.629 | nll_loss 11.735 | ppl 3409.71 | wps 45311.6 | wpb 510.9 | bsz 1 | num_updates 13967 | best_loss 8.489
2022-03-04 23:43:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13967 updates
2022-03-04 23:43:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:43:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 145 @ 13967 updates, score 12.629) (writing took 2.7869507102295756 seconds)
2022-03-04 23:43:27 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 23:43:27 | INFO | train | epoch 145 | loss 3.104 | nll_loss 1.281 | ppl 2.43 | wps 24463.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13967 | lr 0.000267577 | gnorm 0.99 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37214
2022-03-04 23:43:27 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 23:43:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:44:52 | INFO | train_inner | epoch 146:     33 / 97 loss=3.102, nll_loss=1.278, ppl=2.43, wps=24518, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=0.977, loss_scale=16, train_wall=237, gb_free=21, wall=37299
2022-03-04 23:46:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:47:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:47:41 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 12.717 | nll_loss 11.833 | ppl 3648.09 | wps 45159 | wpb 510.9 | bsz 1 | num_updates 14063 | best_loss 8.489
2022-03-04 23:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14063 updates
2022-03-04 23:47:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:47:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:47:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 146 @ 14063 updates, score 12.717) (writing took 2.6459779776632786 seconds)
2022-03-04 23:47:44 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 23:47:44 | INFO | train | epoch 146 | loss 3.099 | nll_loss 1.276 | ppl 2.42 | wps 24462.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14063 | lr 0.000266662 | gnorm 0.991 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 37471
2022-03-04 23:47:44 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 23:47:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:49:19 | INFO | train_inner | epoch 147:     37 / 97 loss=3.095, nll_loss=1.271, ppl=2.41, wps=24538.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=0.991, loss_scale=8, train_wall=237, gb_free=21, wall=37566
2022-03-04 23:51:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:51:57 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 12.639 | nll_loss 11.746 | ppl 3434.23 | wps 45166.9 | wpb 510.9 | bsz 1 | num_updates 14160 | best_loss 8.489
2022-03-04 23:51:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14160 updates
2022-03-04 23:51:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:52:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 147 @ 14160 updates, score 12.639) (writing took 2.669179467484355 seconds)
2022-03-04 23:52:00 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 23:52:00 | INFO | train | epoch 147 | loss 3.093 | nll_loss 1.269 | ppl 2.41 | wps 24798.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14160 | lr 0.000265747 | gnorm 0.986 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 37727
2022-03-04 23:52:00 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 23:52:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:53:43 | INFO | train_inner | epoch 148:     40 / 97 loss=3.09, nll_loss=1.266, ppl=2.4, wps=24786.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=0.987, loss_scale=16, train_wall=234, gb_free=21, wall=37830
2022-03-04 23:56:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:56:15 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 12.652 | nll_loss 11.763 | ppl 3476.66 | wps 42561.7 | wpb 510.9 | bsz 1 | num_updates 14257 | best_loss 8.489
2022-03-04 23:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14257 updates
2022-03-04 23:56:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:56:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-04 23:56:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 148 @ 14257 updates, score 12.652) (writing took 2.7665723226964474 seconds)
2022-03-04 23:56:17 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 23:56:17 | INFO | train | epoch 148 | loss 3.088 | nll_loss 1.264 | ppl 2.4 | wps 24695.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14257 | lr 0.000264841 | gnorm 1.008 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37984
2022-03-04 23:56:17 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 23:56:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:57:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:58:11 | INFO | train_inner | epoch 149:     44 / 97 loss=3.085, nll_loss=1.261, ppl=2.4, wps=24462, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=1.001, loss_scale=16, train_wall=237, gb_free=21, wall=38098
2022-03-05 00:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:00:32 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 12.618 | nll_loss 11.718 | ppl 3368.62 | wps 44374.9 | wpb 510.9 | bsz 1 | num_updates 14353 | best_loss 8.489
2022-03-05 00:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14353 updates
2022-03-05 00:00:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:00:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 149 @ 14353 updates, score 12.618) (writing took 2.829887144267559 seconds)
2022-03-05 00:00:35 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-05 00:00:35 | INFO | train | epoch 149 | loss 3.078 | nll_loss 1.253 | ppl 2.38 | wps 24390.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14353 | lr 0.000263954 | gnorm 0.982 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 38242
2022-03-05 00:00:35 | INFO | fairseq.trainer | begin training epoch 150
2022-03-05 00:00:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:00:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:02:39 | INFO | train_inner | epoch 150:     48 / 97 loss=3.076, nll_loss=1.251, ppl=2.38, wps=24396.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=0.985, loss_scale=8, train_wall=238, gb_free=21, wall=38366
2022-03-05 00:04:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:04:50 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 12.674 | nll_loss 11.788 | ppl 3535.08 | wps 44534.8 | wpb 510.9 | bsz 1 | num_updates 14449 | best_loss 8.489
2022-03-05 00:04:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14449 updates
2022-03-05 00:04:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:04:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 150 @ 14449 updates, score 12.674) (writing took 2.7325215255841613 seconds)
2022-03-05 00:04:53 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-05 00:04:53 | INFO | train | epoch 150 | loss 3.074 | nll_loss 1.248 | ppl 2.38 | wps 24421.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14449 | lr 0.000263076 | gnorm 0.993 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 38500
2022-03-05 00:04:53 | INFO | fairseq.trainer | begin training epoch 151
2022-03-05 00:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:07:04 | INFO | train_inner | epoch 151:     51 / 97 loss=3.07, nll_loss=1.244, ppl=2.37, wps=24758, ups=0.38, wpb=65495, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=0.985, loss_scale=16, train_wall=234, gb_free=21, wall=38631
2022-03-05 00:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:09:07 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 12.679 | nll_loss 11.786 | ppl 3530.93 | wps 45143.8 | wpb 510.9 | bsz 1 | num_updates 14546 | best_loss 8.489
2022-03-05 00:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14546 updates
2022-03-05 00:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 151 @ 14546 updates, score 12.679) (writing took 2.7972964411601424 seconds)
2022-03-05 00:09:09 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-05 00:09:09 | INFO | train | epoch 151 | loss 3.068 | nll_loss 1.242 | ppl 2.37 | wps 24732.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14546 | lr 0.000262197 | gnorm 0.982 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38756
2022-03-05 00:09:09 | INFO | fairseq.trainer | begin training epoch 152
2022-03-05 00:09:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:09:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:11:31 | INFO | train_inner | epoch 152:     55 / 97 loss=3.065, nll_loss=1.239, ppl=2.36, wps=24508.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=14600, lr=0.000261712, gnorm=0.99, loss_scale=8, train_wall=237, gb_free=21, wall=38898
2022-03-05 00:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:13:24 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 12.683 | nll_loss 11.797 | ppl 3558.2 | wps 45183.3 | wpb 510.9 | bsz 1 | num_updates 14642 | best_loss 8.489
2022-03-05 00:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14642 updates
2022-03-05 00:13:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:13:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:13:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 152 @ 14642 updates, score 12.683) (writing took 2.6437264904379845 seconds)
2022-03-05 00:13:26 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-05 00:13:26 | INFO | train | epoch 152 | loss 3.062 | nll_loss 1.235 | ppl 2.35 | wps 24469.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14642 | lr 0.000261336 | gnorm 0.989 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 39013
2022-03-05 00:13:26 | INFO | fairseq.trainer | begin training epoch 153
2022-03-05 00:13:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:15:55 | INFO | train_inner | epoch 153:     58 / 97 loss=3.059, nll_loss=1.233, ppl=2.35, wps=24763, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=0.984, loss_scale=16, train_wall=234, gb_free=21, wall=39162
2022-03-05 00:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:17:40 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 12.676 | nll_loss 11.793 | ppl 3547.39 | wps 45247.6 | wpb 510.9 | bsz 1 | num_updates 14739 | best_loss 8.489
2022-03-05 00:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14739 updates
2022-03-05 00:17:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 153 @ 14739 updates, score 12.676) (writing took 2.6713434914126992 seconds)
2022-03-05 00:17:43 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-05 00:17:43 | INFO | train | epoch 153 | loss 3.057 | nll_loss 1.23 | ppl 2.35 | wps 24762.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14739 | lr 0.000260475 | gnorm 0.978 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39270
2022-03-05 00:17:43 | INFO | fairseq.trainer | begin training epoch 154
2022-03-05 00:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:19:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:20:22 | INFO | train_inner | epoch 154:     62 / 97 loss=3.055, nll_loss=1.228, ppl=2.34, wps=24531.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=0.982, loss_scale=8, train_wall=237, gb_free=21, wall=39429
2022-03-05 00:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:21:57 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 12.707 | nll_loss 11.818 | ppl 3610.79 | wps 43989 | wpb 510.9 | bsz 1 | num_updates 14835 | best_loss 8.489
2022-03-05 00:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14835 updates
2022-03-05 00:21:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:22:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:22:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 154 @ 14835 updates, score 12.707) (writing took 2.826623418368399 seconds)
2022-03-05 00:22:00 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-05 00:22:00 | INFO | train | epoch 154 | loss 3.051 | nll_loss 1.224 | ppl 2.34 | wps 24469 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14835 | lr 0.000259631 | gnorm 0.986 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 39527
2022-03-05 00:22:00 | INFO | fairseq.trainer | begin training epoch 155
2022-03-05 00:22:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:24:47 | INFO | train_inner | epoch 155:     65 / 97 loss=3.05, nll_loss=1.222, ppl=2.33, wps=24752, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=0.989, loss_scale=8, train_wall=234, gb_free=21, wall=39694
2022-03-05 00:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:26:14 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 12.663 | nll_loss 11.783 | ppl 3524.38 | wps 44414.9 | wpb 510.9 | bsz 1 | num_updates 14932 | best_loss 8.489
2022-03-05 00:26:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14932 updates
2022-03-05 00:26:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:26:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:26:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 155 @ 14932 updates, score 12.663) (writing took 2.8430494079366326 seconds)
2022-03-05 00:26:17 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-05 00:26:17 | INFO | train | epoch 155 | loss 3.045 | nll_loss 1.217 | ppl 2.32 | wps 24716 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14932 | lr 0.000258786 | gnorm 0.978 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39784
2022-03-05 00:26:17 | INFO | fairseq.trainer | begin training epoch 156
2022-03-05 00:26:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:29:12 | INFO | train_inner | epoch 156:     68 / 97 loss=3.041, nll_loss=1.213, ppl=2.32, wps=24748.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=0.971, loss_scale=16, train_wall=234, gb_free=21, wall=39959
2022-03-05 00:30:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:30:31 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 12.714 | nll_loss 11.838 | ppl 3662.15 | wps 44290.2 | wpb 510.9 | bsz 1 | num_updates 15029 | best_loss 8.489
2022-03-05 00:30:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15029 updates
2022-03-05 00:30:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:30:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:30:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 156 @ 15029 updates, score 12.714) (writing took 2.813059019856155 seconds)
2022-03-05 00:30:34 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-05 00:30:34 | INFO | train | epoch 156 | loss 3.041 | nll_loss 1.213 | ppl 2.32 | wps 24747.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15029 | lr 0.00025795 | gnorm 0.99 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40041
2022-03-05 00:30:34 | INFO | fairseq.trainer | begin training epoch 157
2022-03-05 00:30:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:30:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:33:39 | INFO | train_inner | epoch 157:     72 / 97 loss=3.038, nll_loss=1.209, ppl=2.31, wps=24508, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=1.001, loss_scale=16, train_wall=237, gb_free=21, wall=40226
2022-03-05 00:34:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:34:48 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 12.719 | nll_loss 11.838 | ppl 3661.07 | wps 45058.7 | wpb 510.9 | bsz 1 | num_updates 15125 | best_loss 8.489
2022-03-05 00:34:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15125 updates
2022-03-05 00:34:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:34:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:34:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 157 @ 15125 updates, score 12.719) (writing took 2.777626171708107 seconds)
2022-03-05 00:34:51 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-05 00:34:51 | INFO | train | epoch 157 | loss 3.033 | nll_loss 1.205 | ppl 2.31 | wps 24458.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15125 | lr 0.00025713 | gnorm 0.984 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40298
2022-03-05 00:34:51 | INFO | fairseq.trainer | begin training epoch 158
2022-03-05 00:34:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:36:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:38:06 | INFO | train_inner | epoch 158:     76 / 97 loss=3.031, nll_loss=1.202, ppl=2.3, wps=24525.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=0.98, loss_scale=16, train_wall=237, gb_free=21, wall=40493
2022-03-05 00:39:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:39:05 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 12.681 | nll_loss 11.803 | ppl 3572.78 | wps 43821.5 | wpb 510.9 | bsz 1 | num_updates 15221 | best_loss 8.489
2022-03-05 00:39:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15221 updates
2022-03-05 00:39:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:39:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 158 @ 15221 updates, score 12.681) (writing took 2.714151634834707 seconds)
2022-03-05 00:39:08 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-05 00:39:08 | INFO | train | epoch 158 | loss 3.029 | nll_loss 1.2 | ppl 2.3 | wps 24468 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15221 | lr 0.000256318 | gnorm 0.983 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40555
2022-03-05 00:39:08 | INFO | fairseq.trainer | begin training epoch 159
2022-03-05 00:39:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:42:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:42:33 | INFO | train_inner | epoch 159:     80 / 97 loss=3.027, nll_loss=1.199, ppl=2.3, wps=24471.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=0.974, loss_scale=8, train_wall=237, gb_free=21, wall=40761
2022-03-05 00:43:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:43:22 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 12.737 | nll_loss 11.867 | ppl 3734.46 | wps 44431.7 | wpb 510.9 | bsz 1 | num_updates 15317 | best_loss 8.489
2022-03-05 00:43:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15317 updates
2022-03-05 00:43:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:43:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 159 @ 15317 updates, score 12.737) (writing took 3.3215932035818696 seconds)
2022-03-05 00:43:26 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-05 00:43:26 | INFO | train | epoch 159 | loss 3.024 | nll_loss 1.195 | ppl 2.29 | wps 24370.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15317 | lr 0.000255513 | gnorm 0.974 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 40813
2022-03-05 00:43:26 | INFO | fairseq.trainer | begin training epoch 160
2022-03-05 00:43:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:46:59 | INFO | train_inner | epoch 160:     83 / 97 loss=3.022, nll_loss=1.193, ppl=2.29, wps=24628.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=0.978, loss_scale=8, train_wall=235, gb_free=21, wall=41026
2022-03-05 00:47:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:47:40 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 12.699 | nll_loss 11.822 | ppl 3620.02 | wps 45094.5 | wpb 510.9 | bsz 1 | num_updates 15414 | best_loss 8.489
2022-03-05 00:47:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15414 updates
2022-03-05 00:47:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:47:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:47:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 160 @ 15414 updates, score 12.699) (writing took 2.7503095166757703 seconds)
2022-03-05 00:47:43 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-05 00:47:43 | INFO | train | epoch 160 | loss 3.021 | nll_loss 1.191 | ppl 2.28 | wps 24685.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15414 | lr 0.000254708 | gnorm 0.975 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 41070
2022-03-05 00:47:43 | INFO | fairseq.trainer | begin training epoch 161
2022-03-05 00:47:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:51:24 | INFO | train_inner | epoch 161:     86 / 97 loss=3.015, nll_loss=1.185, ppl=2.27, wps=24724.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=0.966, loss_scale=16, train_wall=235, gb_free=21, wall=41291
2022-03-05 00:51:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:51:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:51:58 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 12.764 | nll_loss 11.892 | ppl 3800.84 | wps 41166.3 | wpb 510.9 | bsz 1 | num_updates 15510 | best_loss 8.489
2022-03-05 00:51:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15510 updates
2022-03-05 00:51:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 161 @ 15510 updates, score 12.764) (writing took 2.875203304924071 seconds)
2022-03-05 00:52:01 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-05 00:52:01 | INFO | train | epoch 161 | loss 3.014 | nll_loss 1.184 | ppl 2.27 | wps 24375 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15510 | lr 0.000253918 | gnorm 0.969 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 41328
2022-03-05 00:52:01 | INFO | fairseq.trainer | begin training epoch 162
2022-03-05 00:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:55:53 | INFO | train_inner | epoch 162:     90 / 97 loss=3.011, nll_loss=1.181, ppl=2.27, wps=24408.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=0.979, loss_scale=8, train_wall=237, gb_free=21, wall=41560
2022-03-05 00:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:56:15 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 12.746 | nll_loss 11.877 | ppl 3760.19 | wps 43459.6 | wpb 510.9 | bsz 1 | num_updates 15607 | best_loss 8.489
2022-03-05 00:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15607 updates
2022-03-05 00:56:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:56:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 00:56:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 162 @ 15607 updates, score 12.746) (writing took 2.6759245367720723 seconds)
2022-03-05 00:56:18 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-05 00:56:18 | INFO | train | epoch 162 | loss 3.009 | nll_loss 1.178 | ppl 2.26 | wps 24685.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15607 | lr 0.000253128 | gnorm 0.979 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 41585
2022-03-05 00:56:18 | INFO | fairseq.trainer | begin training epoch 163
2022-03-05 00:56:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:00:18 | INFO | train_inner | epoch 163:     93 / 97 loss=3.006, nll_loss=1.176, ppl=2.26, wps=24668.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15700, lr=0.000252377, gnorm=0.989, loss_scale=16, train_wall=235, gb_free=21, wall=41825
2022-03-05 01:00:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:00:33 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 12.712 | nll_loss 11.838 | ppl 3661.58 | wps 43147.7 | wpb 510.9 | bsz 1 | num_updates 15704 | best_loss 8.489
2022-03-05 01:00:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15704 updates
2022-03-05 01:00:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:00:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 163 @ 15704 updates, score 12.712) (writing took 2.6976927937939763 seconds)
2022-03-05 01:00:36 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-05 01:00:36 | INFO | train | epoch 163 | loss 3.004 | nll_loss 1.173 | ppl 2.26 | wps 24621.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15704 | lr 0.000252345 | gnorm 0.985 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 41843
2022-03-05 01:00:36 | INFO | fairseq.trainer | begin training epoch 164
2022-03-05 01:00:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:01:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:04:46 | INFO | train_inner | epoch 164:     97 / 97 loss=2.999, nll_loss=1.168, ppl=2.25, wps=24399.5, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=15800, lr=0.000251577, gnorm=0.964, loss_scale=8, train_wall=237, gb_free=21, wall=42093
2022-03-05 01:04:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:04:51 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 12.751 | nll_loss 11.88 | ppl 3769.65 | wps 45131.1 | wpb 510.9 | bsz 1 | num_updates 15800 | best_loss 8.489
2022-03-05 01:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15800 updates
2022-03-05 01:04:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 164 @ 15800 updates, score 12.751) (writing took 2.6640497148036957 seconds)
2022-03-05 01:04:54 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-05 01:04:54 | INFO | train | epoch 164 | loss 2.997 | nll_loss 1.166 | ppl 2.24 | wps 24392.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15800 | lr 0.000251577 | gnorm 0.963 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 42101
2022-03-05 01:04:54 | INFO | fairseq.trainer | begin training epoch 165
2022-03-05 01:04:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:09:08 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 12.802 | nll_loss 11.933 | ppl 3911.01 | wps 44551.4 | wpb 510.9 | bsz 1 | num_updates 15897 | best_loss 8.489
2022-03-05 01:09:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15897 updates
2022-03-05 01:09:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 165 @ 15897 updates, score 12.802) (writing took 2.6605796087533236 seconds)
2022-03-05 01:09:11 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-05 01:09:11 | INFO | train | epoch 165 | loss 2.994 | nll_loss 1.163 | ppl 2.24 | wps 24721.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15897 | lr 0.000250809 | gnorm 0.974 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42358
2022-03-05 01:09:11 | INFO | fairseq.trainer | begin training epoch 166
2022-03-05 01:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:09:19 | INFO | train_inner | epoch 166:      3 / 97 loss=2.993, nll_loss=1.162, ppl=2.24, wps=24051.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15900, lr=0.000250785, gnorm=0.973, loss_scale=16, train_wall=234, gb_free=21, wall=42366
2022-03-05 01:12:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:13:26 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 12.738 | nll_loss 11.862 | ppl 3722.59 | wps 45237.3 | wpb 510.9 | bsz 1 | num_updates 15993 | best_loss 8.489
2022-03-05 01:13:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15993 updates
2022-03-05 01:13:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:13:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 166 @ 15993 updates, score 12.738) (writing took 2.8072964791208506 seconds)
2022-03-05 01:13:28 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-05 01:13:28 | INFO | train | epoch 166 | loss 2.989 | nll_loss 1.157 | ppl 2.23 | wps 24407.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15993 | lr 0.000250055 | gnorm 0.986 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 42616
2022-03-05 01:13:29 | INFO | fairseq.trainer | begin training epoch 167
2022-03-05 01:13:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:13:47 | INFO | train_inner | epoch 167:      7 / 97 loss=2.987, nll_loss=1.155, ppl=2.23, wps=24427.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16000, lr=0.00025, gnorm=0.986, loss_scale=8, train_wall=237, gb_free=21, wall=42634
2022-03-05 01:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:17:43 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 12.745 | nll_loss 11.872 | ppl 3748.09 | wps 42951.6 | wpb 510.9 | bsz 1 | num_updates 16090 | best_loss 8.489
2022-03-05 01:17:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16090 updates
2022-03-05 01:17:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:17:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 167 @ 16090 updates, score 12.745) (writing took 2.639977812767029 seconds)
2022-03-05 01:17:46 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-05 01:17:46 | INFO | train | epoch 167 | loss 2.985 | nll_loss 1.153 | ppl 2.22 | wps 24704.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16090 | lr 0.0002493 | gnorm 0.965 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 42873
2022-03-05 01:17:46 | INFO | fairseq.trainer | begin training epoch 168
2022-03-05 01:17:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:18:11 | INFO | train_inner | epoch 168:     10 / 97 loss=2.982, nll_loss=1.15, ppl=2.22, wps=24746.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=0.964, loss_scale=8, train_wall=234, gb_free=21, wall=42898
2022-03-05 01:19:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:22:00 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 12.826 | nll_loss 11.962 | ppl 3990.16 | wps 43054.3 | wpb 510.9 | bsz 1 | num_updates 16186 | best_loss 8.489
2022-03-05 01:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16186 updates
2022-03-05 01:22:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 168 @ 16186 updates, score 12.826) (writing took 2.6692591346800327 seconds)
2022-03-05 01:22:03 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-05 01:22:03 | INFO | train | epoch 168 | loss 2.979 | nll_loss 1.146 | ppl 2.21 | wps 24430.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16186 | lr 0.000248559 | gnorm 0.965 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 43130
2022-03-05 01:22:03 | INFO | fairseq.trainer | begin training epoch 169
2022-03-05 01:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:22:39 | INFO | train_inner | epoch 169:     14 / 97 loss=2.978, nll_loss=1.146, ppl=2.21, wps=24469.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=0.962, loss_scale=8, train_wall=237, gb_free=21, wall=43166
2022-03-05 01:25:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:26:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:26:18 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 12.764 | nll_loss 11.89 | ppl 3794.96 | wps 44158 | wpb 510.9 | bsz 1 | num_updates 16282 | best_loss 8.489
2022-03-05 01:26:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16282 updates
2022-03-05 01:26:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 169 @ 16282 updates, score 12.764) (writing took 2.641157300211489 seconds)
2022-03-05 01:26:20 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-05 01:26:20 | INFO | train | epoch 169 | loss 2.975 | nll_loss 1.142 | ppl 2.21 | wps 24420.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16282 | lr 0.000247826 | gnorm 0.965 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 43387
2022-03-05 01:26:20 | INFO | fairseq.trainer | begin training epoch 170
2022-03-05 01:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:27:07 | INFO | train_inner | epoch 170:     18 / 97 loss=2.972, nll_loss=1.139, ppl=2.2, wps=24460, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=0.963, loss_scale=8, train_wall=237, gb_free=21, wall=43434
2022-03-05 01:30:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:30:36 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 12.777 | nll_loss 11.909 | ppl 3844.65 | wps 45224.1 | wpb 510.9 | bsz 1 | num_updates 16379 | best_loss 8.489
2022-03-05 01:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16379 updates
2022-03-05 01:30:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:30:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 170 @ 16379 updates, score 12.777) (writing took 2.6399062694981694 seconds)
2022-03-05 01:30:38 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-05 01:30:38 | INFO | train | epoch 170 | loss 2.97 | nll_loss 1.137 | ppl 2.2 | wps 24633.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16379 | lr 0.000247091 | gnorm 0.965 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 43645
2022-03-05 01:30:38 | INFO | fairseq.trainer | begin training epoch 171
2022-03-05 01:30:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:31:33 | INFO | train_inner | epoch 171:     21 / 97 loss=2.968, nll_loss=1.135, ppl=2.2, wps=24629.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=0.965, loss_scale=16, train_wall=236, gb_free=21, wall=43700
2022-03-05 01:34:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:34:54 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 12.762 | nll_loss 11.901 | ppl 3824.56 | wps 42550.1 | wpb 510.9 | bsz 1 | num_updates 16475 | best_loss 8.489
2022-03-05 01:34:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16475 updates
2022-03-05 01:34:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:34:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:34:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 171 @ 16475 updates, score 12.762) (writing took 2.758919115178287 seconds)
2022-03-05 01:34:57 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-05 01:34:57 | INFO | train | epoch 171 | loss 2.965 | nll_loss 1.132 | ppl 2.19 | wps 24345.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16475 | lr 0.00024637 | gnorm 0.961 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 43904
2022-03-05 01:34:57 | INFO | fairseq.trainer | begin training epoch 172
2022-03-05 01:34:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:36:01 | INFO | train_inner | epoch 172:     25 / 97 loss=2.964, nll_loss=1.131, ppl=2.19, wps=24410.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=0.964, loss_scale=8, train_wall=237, gb_free=21, wall=43968
2022-03-05 01:39:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:39:12 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 12.783 | nll_loss 11.922 | ppl 3880.49 | wps 44703.9 | wpb 510.9 | bsz 1 | num_updates 16572 | best_loss 8.489
2022-03-05 01:39:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16572 updates
2022-03-05 01:39:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:39:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 172 @ 16572 updates, score 12.783) (writing took 2.7471477780491114 seconds)
2022-03-05 01:39:14 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-05 01:39:14 | INFO | train | epoch 172 | loss 2.963 | nll_loss 1.13 | ppl 2.19 | wps 24630.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16572 | lr 0.000245648 | gnorm 0.976 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 44162
2022-03-05 01:39:15 | INFO | fairseq.trainer | begin training epoch 173
2022-03-05 01:39:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:40:27 | INFO | train_inner | epoch 173:     28 / 97 loss=2.963, nll_loss=1.13, ppl=2.19, wps=24630.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=0.972, loss_scale=8, train_wall=235, gb_free=21, wall=44234
2022-03-05 01:41:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:43:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:43:30 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 12.804 | nll_loss 11.941 | ppl 3932.15 | wps 43807.5 | wpb 510.9 | bsz 1 | num_updates 16668 | best_loss 8.489
2022-03-05 01:43:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16668 updates
2022-03-05 01:43:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:43:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:43:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 173 @ 16668 updates, score 12.804) (writing took 2.6553685031831264 seconds)
2022-03-05 01:43:32 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-05 01:43:32 | INFO | train | epoch 173 | loss 2.957 | nll_loss 1.123 | ppl 2.18 | wps 24380 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16668 | lr 0.000244939 | gnorm 0.962 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 44419
2022-03-05 01:43:32 | INFO | fairseq.trainer | begin training epoch 174
2022-03-05 01:43:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:44:55 | INFO | train_inner | epoch 174:     32 / 97 loss=2.954, nll_loss=1.12, ppl=2.17, wps=24416, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=0.967, loss_scale=8, train_wall=238, gb_free=21, wall=44502
2022-03-05 01:47:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:47:48 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 12.777 | nll_loss 11.913 | ppl 3857.3 | wps 44147.7 | wpb 510.9 | bsz 1 | num_updates 16765 | best_loss 8.489
2022-03-05 01:47:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16765 updates
2022-03-05 01:47:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:47:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 174 @ 16765 updates, score 12.777) (writing took 2.6775601180270314 seconds)
2022-03-05 01:47:50 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-05 01:47:50 | INFO | train | epoch 174 | loss 2.953 | nll_loss 1.12 | ppl 2.17 | wps 24638.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16765 | lr 0.00024423 | gnorm 0.977 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 44677
2022-03-05 01:47:50 | INFO | fairseq.trainer | begin training epoch 175
2022-03-05 01:47:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:49:21 | INFO | train_inner | epoch 175:     35 / 97 loss=2.952, nll_loss=1.117, ppl=2.17, wps=24676.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=0.973, loss_scale=16, train_wall=235, gb_free=21, wall=44768
2022-03-05 01:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:52:05 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 12.828 | nll_loss 11.968 | ppl 4004.83 | wps 43480.9 | wpb 510.9 | bsz 1 | num_updates 16861 | best_loss 8.489
2022-03-05 01:52:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16861 updates
2022-03-05 01:52:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:52:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 175 @ 16861 updates, score 12.828) (writing took 2.643386543728411 seconds)
2022-03-05 01:52:08 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-05 01:52:08 | INFO | train | epoch 175 | loss 2.949 | nll_loss 1.114 | ppl 2.16 | wps 24393.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16861 | lr 0.000243533 | gnorm 0.971 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 44935
2022-03-05 01:52:08 | INFO | fairseq.trainer | begin training epoch 176
2022-03-05 01:52:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:53:48 | INFO | train_inner | epoch 176:     39 / 97 loss=2.945, nll_loss=1.111, ppl=2.16, wps=24462.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=16900, lr=0.000243252, gnorm=0.966, loss_scale=8, train_wall=237, gb_free=21, wall=45035
2022-03-05 01:56:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:56:23 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 12.804 | nll_loss 11.943 | ppl 3938.11 | wps 44639.6 | wpb 510.9 | bsz 1 | num_updates 16958 | best_loss 8.489
2022-03-05 01:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16958 updates
2022-03-05 01:56:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:56:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 01:56:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 176 @ 16958 updates, score 12.804) (writing took 2.7540936982259154 seconds)
2022-03-05 01:56:25 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-05 01:56:25 | INFO | train | epoch 176 | loss 2.944 | nll_loss 1.109 | ppl 2.16 | wps 24672.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16958 | lr 0.000242836 | gnorm 0.956 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 45192
2022-03-05 01:56:25 | INFO | fairseq.trainer | begin training epoch 177
2022-03-05 01:56:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:58:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:58:16 | INFO | train_inner | epoch 177:     43 / 97 loss=2.943, nll_loss=1.109, ppl=2.16, wps=24457, ups=0.37, wpb=65495, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=0.958, loss_scale=8, train_wall=237, gb_free=21, wall=45303
2022-03-05 02:00:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:00:40 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 12.837 | nll_loss 11.979 | ppl 4037.12 | wps 45100.8 | wpb 510.9 | bsz 1 | num_updates 17054 | best_loss 8.489
2022-03-05 02:00:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17054 updates
2022-03-05 02:00:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:00:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 177 @ 17054 updates, score 12.837) (writing took 2.7183605367317796 seconds)
2022-03-05 02:00:43 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-05 02:00:43 | INFO | train | epoch 177 | loss 2.94 | nll_loss 1.105 | ppl 2.15 | wps 24440.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17054 | lr 0.000242151 | gnorm 0.958 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 45450
2022-03-05 02:00:43 | INFO | fairseq.trainer | begin training epoch 178
2022-03-05 02:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:02:41 | INFO | train_inner | epoch 178:     46 / 97 loss=2.935, nll_loss=1.099, ppl=2.14, wps=24710.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=0.948, loss_scale=8, train_wall=235, gb_free=21, wall=45568
2022-03-05 02:04:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:04:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:04:58 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 12.777 | nll_loss 11.912 | ppl 3852.68 | wps 42871.6 | wpb 510.9 | bsz 1 | num_updates 17150 | best_loss 8.489
2022-03-05 02:04:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17150 updates
2022-03-05 02:04:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 178 @ 17150 updates, score 12.777) (writing took 2.7917813174426556 seconds)
2022-03-05 02:05:00 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-05 02:05:00 | INFO | train | epoch 178 | loss 2.935 | nll_loss 1.1 | ppl 2.14 | wps 24395.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17150 | lr 0.000241473 | gnorm 0.953 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 45707
2022-03-05 02:05:00 | INFO | fairseq.trainer | begin training epoch 179
2022-03-05 02:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:07:09 | INFO | train_inner | epoch 179:     50 / 97 loss=2.937, nll_loss=1.102, ppl=2.15, wps=24432.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=0.961, loss_scale=8, train_wall=237, gb_free=21, wall=45836
2022-03-05 02:09:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:09:15 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 12.758 | nll_loss 11.893 | ppl 3802.19 | wps 42278.3 | wpb 510.9 | bsz 1 | num_updates 17247 | best_loss 8.489
2022-03-05 02:09:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17247 updates
2022-03-05 02:09:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:09:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:09:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 179 @ 17247 updates, score 12.758) (writing took 2.692772218026221 seconds)
2022-03-05 02:09:18 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-05 02:09:18 | INFO | train | epoch 179 | loss 2.932 | nll_loss 1.097 | ppl 2.14 | wps 24668.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17247 | lr 0.000240793 | gnorm 0.95 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 45965
2022-03-05 02:09:18 | INFO | fairseq.trainer | begin training epoch 180
2022-03-05 02:09:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:11:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:11:37 | INFO | train_inner | epoch 180:     54 / 97 loss=2.93, nll_loss=1.094, ppl=2.14, wps=24468.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=0.953, loss_scale=8, train_wall=237, gb_free=21, wall=46104
2022-03-05 02:13:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:13:32 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 12.802 | nll_loss 11.938 | ppl 3923.17 | wps 43251.8 | wpb 510.9 | bsz 1 | num_updates 17343 | best_loss 8.489
2022-03-05 02:13:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17343 updates
2022-03-05 02:13:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:13:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:13:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 180 @ 17343 updates, score 12.802) (writing took 2.6668397253379226 seconds)
2022-03-05 02:13:35 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-05 02:13:35 | INFO | train | epoch 180 | loss 2.929 | nll_loss 1.093 | ppl 2.13 | wps 24446.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17343 | lr 0.000240125 | gnorm 0.961 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 46222
2022-03-05 02:13:35 | INFO | fairseq.trainer | begin training epoch 181
2022-03-05 02:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:16:02 | INFO | train_inner | epoch 181:     57 / 97 loss=2.925, nll_loss=1.09, ppl=2.13, wps=24705.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=0.956, loss_scale=8, train_wall=235, gb_free=21, wall=46369
2022-03-05 02:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:17:50 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 12.893 | nll_loss 12.037 | ppl 4203.83 | wps 43957.3 | wpb 510.9 | bsz 1 | num_updates 17440 | best_loss 8.489
2022-03-05 02:17:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17440 updates
2022-03-05 02:17:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:17:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:17:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 181 @ 17440 updates, score 12.893) (writing took 2.72268733009696 seconds)
2022-03-05 02:17:53 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-05 02:17:53 | INFO | train | epoch 181 | loss 2.925 | nll_loss 1.089 | ppl 2.13 | wps 24659.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17440 | lr 0.000239457 | gnorm 0.954 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 46480
2022-03-05 02:17:53 | INFO | fairseq.trainer | begin training epoch 182
2022-03-05 02:17:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:20:27 | INFO | train_inner | epoch 182:     60 / 97 loss=2.924, nll_loss=1.088, ppl=2.13, wps=24705, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=0.962, loss_scale=16, train_wall=235, gb_free=21, wall=46634
2022-03-05 02:22:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:22:07 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 12.836 | nll_loss 11.977 | ppl 4031.47 | wps 43437.3 | wpb 510.9 | bsz 1 | num_updates 17537 | best_loss 8.489
2022-03-05 02:22:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17537 updates
2022-03-05 02:22:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:22:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 182 @ 17537 updates, score 12.836) (writing took 2.734010836109519 seconds)
2022-03-05 02:22:10 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-05 02:22:10 | INFO | train | epoch 182 | loss 2.921 | nll_loss 1.085 | ppl 2.12 | wps 24718.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17537 | lr 0.000238793 | gnorm 0.959 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46737
2022-03-05 02:22:10 | INFO | fairseq.trainer | begin training epoch 183
2022-03-05 02:22:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:22:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:23:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:24:57 | INFO | train_inner | epoch 183:     65 / 97 loss=2.917, nll_loss=1.08, ppl=2.11, wps=24248.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=0.95, loss_scale=8, train_wall=239, gb_free=21, wall=46904
2022-03-05 02:26:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:26:25 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 12.856 | nll_loss 12.005 | ppl 4109.25 | wps 42167.3 | wpb 510.9 | bsz 1 | num_updates 17632 | best_loss 8.489
2022-03-05 02:26:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17632 updates
2022-03-05 02:26:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:26:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 183 @ 17632 updates, score 12.856) (writing took 2.78182084672153 seconds)
2022-03-05 02:26:28 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-05 02:26:28 | INFO | train | epoch 183 | loss 2.915 | nll_loss 1.079 | ppl 2.11 | wps 24132.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 17632 | lr 0.000238149 | gnorm 0.965 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 46995
2022-03-05 02:26:28 | INFO | fairseq.trainer | begin training epoch 184
2022-03-05 02:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:29:22 | INFO | train_inner | epoch 184:     68 / 97 loss=2.915, nll_loss=1.079, ppl=2.11, wps=24684.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.968, loss_scale=16, train_wall=235, gb_free=21, wall=47170
2022-03-05 02:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:30:42 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 12.819 | nll_loss 11.957 | ppl 3974.51 | wps 43192.5 | wpb 510.9 | bsz 1 | num_updates 17729 | best_loss 8.489
2022-03-05 02:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17729 updates
2022-03-05 02:30:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:30:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:30:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 184 @ 17729 updates, score 12.819) (writing took 2.7632065396755934 seconds)
2022-03-05 02:30:45 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-05 02:30:45 | INFO | train | epoch 184 | loss 2.912 | nll_loss 1.075 | ppl 2.11 | wps 24698.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17729 | lr 0.000237497 | gnorm 0.951 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47252
2022-03-05 02:30:45 | INFO | fairseq.trainer | begin training epoch 185
2022-03-05 02:30:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:33:47 | INFO | train_inner | epoch 185:     71 / 97 loss=2.912, nll_loss=1.076, ppl=2.11, wps=24751.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=0.952, loss_scale=16, train_wall=234, gb_free=21, wall=47434
2022-03-05 02:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:34:59 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 12.76 | nll_loss 11.89 | ppl 3796.14 | wps 42565.3 | wpb 510.9 | bsz 1 | num_updates 17826 | best_loss 8.489
2022-03-05 02:34:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17826 updates
2022-03-05 02:34:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:35:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 185 @ 17826 updates, score 12.76) (writing took 2.745907791890204 seconds)
2022-03-05 02:35:02 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-05 02:35:02 | INFO | train | epoch 185 | loss 2.91 | nll_loss 1.074 | ppl 2.1 | wps 24719.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17826 | lr 0.00023685 | gnorm 0.958 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47509
2022-03-05 02:35:02 | INFO | fairseq.trainer | begin training epoch 186
2022-03-05 02:35:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:38:14 | INFO | train_inner | epoch 186:     75 / 97 loss=2.905, nll_loss=1.068, ppl=2.1, wps=24493.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17900, lr=0.00023636, gnorm=0.957, loss_scale=16, train_wall=237, gb_free=21, wall=47702
2022-03-05 02:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:39:16 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 12.844 | nll_loss 11.991 | ppl 4070.67 | wps 44912.8 | wpb 510.9 | bsz 1 | num_updates 17922 | best_loss 8.489
2022-03-05 02:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17922 updates
2022-03-05 02:39:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:39:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:39:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 186 @ 17922 updates, score 12.844) (writing took 2.742156476713717 seconds)
2022-03-05 02:39:18 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-05 02:39:18 | INFO | train | epoch 186 | loss 2.905 | nll_loss 1.068 | ppl 2.1 | wps 24492.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17922 | lr 0.000236215 | gnorm 0.958 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47765
2022-03-05 02:39:18 | INFO | fairseq.trainer | begin training epoch 187
2022-03-05 02:39:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:40:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:42:42 | INFO | train_inner | epoch 187:     79 / 97 loss=2.903, nll_loss=1.066, ppl=2.09, wps=24475.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=0.952, loss_scale=8, train_wall=237, gb_free=21, wall=47969
2022-03-05 02:43:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:43:33 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 12.791 | nll_loss 11.934 | ppl 3913.04 | wps 43076.5 | wpb 510.9 | bsz 1 | num_updates 18018 | best_loss 8.489
2022-03-05 02:43:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18018 updates
2022-03-05 02:43:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:43:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:43:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 187 @ 18018 updates, score 12.791) (writing took 2.8714039167389274 seconds)
2022-03-05 02:43:36 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-05 02:43:36 | INFO | train | epoch 187 | loss 2.899 | nll_loss 1.062 | ppl 2.09 | wps 24389.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18018 | lr 0.000235584 | gnorm 0.946 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 48023
2022-03-05 02:43:36 | INFO | fairseq.trainer | begin training epoch 188
2022-03-05 02:43:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:47:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:47:10 | INFO | train_inner | epoch 188:     83 / 97 loss=2.899, nll_loss=1.062, ppl=2.09, wps=24459.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.949, loss_scale=8, train_wall=237, gb_free=21, wall=48237
2022-03-05 02:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:47:51 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 12.814 | nll_loss 11.962 | ppl 3989.33 | wps 42441.3 | wpb 510.9 | bsz 1 | num_updates 18114 | best_loss 8.489
2022-03-05 02:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18114 updates
2022-03-05 02:47:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:47:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:47:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 188 @ 18114 updates, score 12.814) (writing took 2.8328029736876488 seconds)
2022-03-05 02:47:54 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-05 02:47:54 | INFO | train | epoch 188 | loss 2.898 | nll_loss 1.06 | ppl 2.09 | wps 24415.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18114 | lr 0.000234959 | gnorm 0.955 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 48281
2022-03-05 02:47:54 | INFO | fairseq.trainer | begin training epoch 189
2022-03-05 02:47:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:51:35 | INFO | train_inner | epoch 189:     86 / 97 loss=2.894, nll_loss=1.056, ppl=2.08, wps=24660.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.95, loss_scale=8, train_wall=235, gb_free=21, wall=48502
2022-03-05 02:52:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:52:09 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 12.836 | nll_loss 11.985 | ppl 4054.85 | wps 43664.4 | wpb 510.9 | bsz 1 | num_updates 18211 | best_loss 8.489
2022-03-05 02:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18211 updates
2022-03-05 02:52:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:52:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:52:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 189 @ 18211 updates, score 12.836) (writing took 2.849602807313204 seconds)
2022-03-05 02:52:12 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 02:52:12 | INFO | train | epoch 189 | loss 2.893 | nll_loss 1.056 | ppl 2.08 | wps 24640.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18211 | lr 0.000234333 | gnorm 0.942 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 48539
2022-03-05 02:52:12 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 02:52:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:56:01 | INFO | train_inner | epoch 190:     89 / 97 loss=2.893, nll_loss=1.056, ppl=2.08, wps=24689.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=18300, lr=0.000233762, gnorm=0.949, loss_scale=16, train_wall=235, gb_free=21, wall=48768
2022-03-05 02:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:56:26 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 12.863 | nll_loss 12.012 | ppl 4129.63 | wps 45097.1 | wpb 510.9 | bsz 1 | num_updates 18308 | best_loss 8.489
2022-03-05 02:56:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18308 updates
2022-03-05 02:56:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:56:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 02:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 190 @ 18308 updates, score 12.863) (writing took 2.7958436058834195 seconds)
2022-03-05 02:56:29 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 02:56:29 | INFO | train | epoch 190 | loss 2.891 | nll_loss 1.053 | ppl 2.08 | wps 24695.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18308 | lr 0.000233711 | gnorm 0.951 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 48796
2022-03-05 02:56:29 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 02:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:58:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:00:28 | INFO | train_inner | epoch 191:     93 / 97 loss=2.888, nll_loss=1.051, ppl=2.07, wps=24531.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18400, lr=0.000233126, gnorm=0.94, loss_scale=16, train_wall=237, gb_free=21, wall=49035
2022-03-05 03:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:00:43 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 12.867 | nll_loss 12.019 | ppl 4151.5 | wps 45037.9 | wpb 510.9 | bsz 1 | num_updates 18404 | best_loss 8.489
2022-03-05 03:00:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18404 updates
2022-03-05 03:00:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:00:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:00:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 191 @ 18404 updates, score 12.867) (writing took 2.7325088577345014 seconds)
2022-03-05 03:00:45 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 03:00:45 | INFO | train | epoch 191 | loss 2.887 | nll_loss 1.049 | ppl 2.07 | wps 24495 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18404 | lr 0.000233101 | gnorm 0.936 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 49052
2022-03-05 03:00:45 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 03:00:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:04:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:04:55 | INFO | train_inner | epoch 192:     97 / 97 loss=2.886, nll_loss=1.049, ppl=2.07, wps=24484, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=18500, lr=0.000232495, gnorm=0.952, loss_scale=16, train_wall=237, gb_free=21, wall=49302
2022-03-05 03:04:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:05:00 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 12.823 | nll_loss 11.973 | ppl 4020.11 | wps 43997.8 | wpb 510.9 | bsz 1 | num_updates 18500 | best_loss 8.489
2022-03-05 03:05:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18500 updates
2022-03-05 03:05:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:05:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:05:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 192 @ 18500 updates, score 12.823) (writing took 2.693381590768695 seconds)
2022-03-05 03:05:03 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 03:05:03 | INFO | train | epoch 192 | loss 2.884 | nll_loss 1.046 | ppl 2.07 | wps 24441.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18500 | lr 0.000232495 | gnorm 0.951 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 49310
2022-03-05 03:05:03 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 03:05:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:06:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:09:17 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 12.887 | nll_loss 12.042 | ppl 4216.33 | wps 44490.6 | wpb 510.9 | bsz 1 | num_updates 18596 | best_loss 8.489
2022-03-05 03:09:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18596 updates
2022-03-05 03:09:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:09:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:09:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 193 @ 18596 updates, score 12.887) (writing took 2.6706253364682198 seconds)
2022-03-05 03:09:19 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 03:09:19 | INFO | train | epoch 193 | loss 2.88 | nll_loss 1.042 | ppl 2.06 | wps 24503.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18596 | lr 0.000231894 | gnorm 0.944 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 49566
2022-03-05 03:09:19 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 03:09:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:09:30 | INFO | train_inner | epoch 194:      4 / 97 loss=2.878, nll_loss=1.04, ppl=2.06, wps=23848.5, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=18600, lr=0.000231869, gnorm=0.944, loss_scale=8, train_wall=236, gb_free=21, wall=49577
2022-03-05 03:12:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:13:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:13:33 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 12.902 | nll_loss 12.058 | ppl 4263.89 | wps 43552.8 | wpb 510.9 | bsz 1 | num_updates 18692 | best_loss 8.489
2022-03-05 03:13:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18692 updates
2022-03-05 03:13:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:13:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:13:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 194 @ 18692 updates, score 12.902) (writing took 2.814187495969236 seconds)
2022-03-05 03:13:36 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 03:13:36 | INFO | train | epoch 194 | loss 2.876 | nll_loss 1.038 | ppl 2.05 | wps 24502.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18692 | lr 0.000231298 | gnorm 0.94 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 49823
2022-03-05 03:13:36 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 03:13:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:13:57 | INFO | train_inner | epoch 195:      8 / 97 loss=2.875, nll_loss=1.036, ppl=2.05, wps=24531.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18700, lr=0.000231249, gnorm=0.941, loss_scale=8, train_wall=236, gb_free=21, wall=49844
2022-03-05 03:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:17:50 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 12.881 | nll_loss 12.041 | ppl 4215.11 | wps 43827.7 | wpb 510.9 | bsz 1 | num_updates 18789 | best_loss 8.489
2022-03-05 03:17:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18789 updates
2022-03-05 03:17:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:17:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:17:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 195 @ 18789 updates, score 12.881) (writing took 2.7959134224802256 seconds)
2022-03-05 03:17:53 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 03:17:53 | INFO | train | epoch 195 | loss 2.875 | nll_loss 1.037 | ppl 2.05 | wps 24709 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18789 | lr 0.0002307 | gnorm 0.955 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 50080
2022-03-05 03:17:53 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 03:17:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:18:21 | INFO | train_inner | epoch 196:     11 / 97 loss=2.874, nll_loss=1.036, ppl=2.05, wps=24749.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.949, loss_scale=16, train_wall=234, gb_free=21, wall=50108
2022-03-05 03:22:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:22:07 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 12.813 | nll_loss 11.958 | ppl 3977.71 | wps 45136.9 | wpb 510.9 | bsz 1 | num_updates 18886 | best_loss 8.489
2022-03-05 03:22:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18886 updates
2022-03-05 03:22:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:22:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 196 @ 18886 updates, score 12.813) (writing took 2.7789456993341446 seconds)
2022-03-05 03:22:10 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 03:22:10 | INFO | train | epoch 196 | loss 2.87 | nll_loss 1.031 | ppl 2.04 | wps 24741.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18886 | lr 0.000230107 | gnorm 0.933 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50337
2022-03-05 03:22:10 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 03:22:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:22:46 | INFO | train_inner | epoch 197:     14 / 97 loss=2.867, nll_loss=1.028, ppl=2.04, wps=24752.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.933, loss_scale=16, train_wall=234, gb_free=21, wall=50373
2022-03-05 03:23:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:26:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:26:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:26:24 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 12.904 | nll_loss 12.061 | ppl 4271.78 | wps 43936.6 | wpb 510.9 | bsz 1 | num_updates 18981 | best_loss 8.489
2022-03-05 03:26:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18981 updates
2022-03-05 03:26:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:26:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:26:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 197 @ 18981 updates, score 12.904) (writing took 2.733231658115983 seconds)
2022-03-05 03:26:26 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 03:26:26 | INFO | train | epoch 197 | loss 2.866 | nll_loss 1.027 | ppl 2.04 | wps 24245.4 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 18981 | lr 0.000229531 | gnorm 0.942 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 50593
2022-03-05 03:26:26 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 03:26:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:27:15 | INFO | train_inner | epoch 198:     19 / 97 loss=2.865, nll_loss=1.026, ppl=2.04, wps=24318.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.951, loss_scale=8, train_wall=239, gb_free=21, wall=50642
2022-03-05 03:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:30:40 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 12.865 | nll_loss 12.017 | ppl 4145.63 | wps 44289.5 | wpb 510.9 | bsz 1 | num_updates 19078 | best_loss 8.489
2022-03-05 03:30:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19078 updates
2022-03-05 03:30:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:30:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:30:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 198 @ 19078 updates, score 12.865) (writing took 2.7545901108533144 seconds)
2022-03-05 03:30:43 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 03:30:43 | INFO | train | epoch 198 | loss 2.864 | nll_loss 1.025 | ppl 2.03 | wps 24798.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19078 | lr 0.000228946 | gnorm 0.941 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 50850
2022-03-05 03:30:43 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 03:30:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:31:39 | INFO | train_inner | epoch 199:     22 / 97 loss=2.862, nll_loss=1.022, ppl=2.03, wps=24805.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.938, loss_scale=8, train_wall=234, gb_free=21, wall=50906
2022-03-05 03:33:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:34:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:34:56 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 12.878 | nll_loss 12.031 | ppl 4185.04 | wps 45338.8 | wpb 510.9 | bsz 1 | num_updates 19174 | best_loss 8.489
2022-03-05 03:34:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19174 updates
2022-03-05 03:34:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:34:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:34:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 199 @ 19174 updates, score 12.878) (writing took 2.714057016186416 seconds)
2022-03-05 03:34:59 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 03:34:59 | INFO | train | epoch 199 | loss 2.86 | nll_loss 1.021 | ppl 2.03 | wps 24506.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19174 | lr 0.000228372 | gnorm 0.943 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 51106
2022-03-05 03:34:59 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 03:34:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:36:06 | INFO | train_inner | epoch 200:     26 / 97 loss=2.858, nll_loss=1.019, ppl=2.03, wps=24530.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.934, loss_scale=8, train_wall=237, gb_free=21, wall=51173
2022-03-05 03:39:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:39:14 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 12.906 | nll_loss 12.068 | ppl 4294.4 | wps 43636.2 | wpb 510.9 | bsz 1 | num_updates 19271 | best_loss 8.489
2022-03-05 03:39:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19271 updates
2022-03-05 03:39:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:39:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:39:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 200 @ 19271 updates, score 12.906) (writing took 2.696817884221673 seconds)
2022-03-05 03:39:16 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 03:39:16 | INFO | train | epoch 200 | loss 2.857 | nll_loss 1.018 | ppl 2.03 | wps 24698.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19271 | lr 0.000227797 | gnorm 0.957 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51363
2022-03-05 03:39:16 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 03:39:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:40:31 | INFO | train_inner | epoch 201:     29 / 97 loss=2.857, nll_loss=1.018, ppl=2.03, wps=24755, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.957, loss_scale=16, train_wall=234, gb_free=21, wall=51438
2022-03-05 03:43:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:43:30 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 12.909 | nll_loss 12.072 | ppl 4305.98 | wps 44153 | wpb 510.9 | bsz 1 | num_updates 19368 | best_loss 8.489
2022-03-05 03:43:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19368 updates
2022-03-05 03:43:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:43:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:43:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 201 @ 19368 updates, score 12.909) (writing took 2.67960462346673 seconds)
2022-03-05 03:43:33 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 03:43:33 | INFO | train | epoch 201 | loss 2.854 | nll_loss 1.015 | ppl 2.02 | wps 24755.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19368 | lr 0.000227226 | gnorm 0.933 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51620
2022-03-05 03:43:33 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 03:43:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:44:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:44:58 | INFO | train_inner | epoch 202:     33 / 97 loss=2.851, nll_loss=1.011, ppl=2.02, wps=24531.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.936, loss_scale=8, train_wall=237, gb_free=21, wall=51705
2022-03-05 03:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:47:47 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 12.894 | nll_loss 12.05 | ppl 4240.82 | wps 44910 | wpb 510.9 | bsz 1 | num_updates 19464 | best_loss 8.489
2022-03-05 03:47:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19464 updates
2022-03-05 03:47:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:47:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:47:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 202 @ 19464 updates, score 12.894) (writing took 2.750080426223576 seconds)
2022-03-05 03:47:50 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 03:47:50 | INFO | train | epoch 202 | loss 2.85 | nll_loss 1.011 | ppl 2.01 | wps 24496 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19464 | lr 0.000226665 | gnorm 0.941 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 51877
2022-03-05 03:47:50 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 03:47:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:49:22 | INFO | train_inner | epoch 203:     36 / 97 loss=2.851, nll_loss=1.011, ppl=2.02, wps=24765.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.938, loss_scale=8, train_wall=234, gb_free=21, wall=51969
2022-03-05 03:51:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:51:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:52:04 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 12.892 | nll_loss 12.053 | ppl 4248.53 | wps 45247.7 | wpb 510.9 | bsz 1 | num_updates 19560 | best_loss 8.489
2022-03-05 03:52:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19560 updates
2022-03-05 03:52:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:52:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 203 @ 19560 updates, score 12.892) (writing took 2.682411420159042 seconds)
2022-03-05 03:52:06 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 03:52:06 | INFO | train | epoch 203 | loss 2.847 | nll_loss 1.008 | ppl 2.01 | wps 24492.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19560 | lr 0.000226108 | gnorm 0.933 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 52133
2022-03-05 03:52:06 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 03:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:53:49 | INFO | train_inner | epoch 204:     40 / 97 loss=2.845, nll_loss=1.005, ppl=2.01, wps=24552.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.936, loss_scale=8, train_wall=236, gb_free=21, wall=52236
2022-03-05 03:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:56:20 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 12.859 | nll_loss 12.014 | ppl 4136.7 | wps 45149.8 | wpb 510.9 | bsz 1 | num_updates 19657 | best_loss 8.489
2022-03-05 03:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19657 updates
2022-03-05 03:56:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:56:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 03:56:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 204 @ 19657 updates, score 12.859) (writing took 2.6504782224074006 seconds)
2022-03-05 03:56:22 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 03:56:22 | INFO | train | epoch 204 | loss 2.845 | nll_loss 1.006 | ppl 2.01 | wps 24794.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19657 | lr 0.000225549 | gnorm 0.945 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 52389
2022-03-05 03:56:22 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 03:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:58:13 | INFO | train_inner | epoch 205:     43 / 97 loss=2.845, nll_loss=1.005, ppl=2.01, wps=24809.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.946, loss_scale=16, train_wall=234, gb_free=21, wall=52500
2022-03-05 04:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:00:36 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 12.819 | nll_loss 11.97 | ppl 4010.79 | wps 45328.9 | wpb 510.9 | bsz 1 | num_updates 19754 | best_loss 8.489
2022-03-05 04:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19754 updates
2022-03-05 04:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 205 @ 19754 updates, score 12.819) (writing took 2.6610532831400633 seconds)
2022-03-05 04:00:39 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 04:00:39 | INFO | train | epoch 205 | loss 2.841 | nll_loss 1.002 | ppl 2 | wps 24777.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19754 | lr 0.000224995 | gnorm 0.935 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52646
2022-03-05 04:00:39 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 04:00:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:02:37 | INFO | train_inner | epoch 206:     46 / 97 loss=2.84, nll_loss=1, ppl=2, wps=24792.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.926, loss_scale=16, train_wall=234, gb_free=21, wall=52764
2022-03-05 04:03:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:04:53 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 12.937 | nll_loss 12.099 | ppl 4388.27 | wps 44627 | wpb 510.9 | bsz 1 | num_updates 19850 | best_loss 8.489
2022-03-05 04:04:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19850 updates
2022-03-05 04:04:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 206 @ 19850 updates, score 12.937) (writing took 2.6586171621456742 seconds)
2022-03-05 04:04:55 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 04:04:55 | INFO | train | epoch 206 | loss 2.838 | nll_loss 0.998 | ppl 2 | wps 24504.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19850 | lr 0.00022445 | gnorm 0.933 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52902
2022-03-05 04:04:55 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 04:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:07:04 | INFO | train_inner | epoch 207:     50 / 97 loss=2.837, nll_loss=0.997, ppl=2, wps=24526.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.933, loss_scale=16, train_wall=237, gb_free=21, wall=53031
2022-03-05 04:08:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:09:10 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 12.873 | nll_loss 12.026 | ppl 4170.61 | wps 45304.7 | wpb 510.9 | bsz 1 | num_updates 19946 | best_loss 8.489
2022-03-05 04:09:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19946 updates
2022-03-05 04:09:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:09:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:09:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 207 @ 19946 updates, score 12.873) (writing took 2.750684518367052 seconds)
2022-03-05 04:09:12 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 04:09:12 | INFO | train | epoch 207 | loss 2.836 | nll_loss 0.996 | ppl 1.99 | wps 24464.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19946 | lr 0.000223909 | gnorm 0.929 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53159
2022-03-05 04:09:12 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 04:09:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:10:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:11:34 | INFO | train_inner | epoch 208:     55 / 97 loss=2.834, nll_loss=0.994, ppl=1.99, wps=24277.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.92, loss_scale=8, train_wall=239, gb_free=21, wall=53301
2022-03-05 04:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:13:27 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 12.854 | nll_loss 12.01 | ppl 4125.81 | wps 44700.7 | wpb 510.9 | bsz 1 | num_updates 20042 | best_loss 8.489
2022-03-05 04:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20042 updates
2022-03-05 04:13:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:13:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:13:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 208 @ 20042 updates, score 12.854) (writing took 2.595322578214109 seconds)
2022-03-05 04:13:29 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 04:13:29 | INFO | train | epoch 208 | loss 2.832 | nll_loss 0.992 | ppl 1.99 | wps 24491.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20042 | lr 0.000223372 | gnorm 0.932 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 53416
2022-03-05 04:13:29 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 04:13:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:15:58 | INFO | train_inner | epoch 209:     58 / 97 loss=2.83, nll_loss=0.99, ppl=1.99, wps=24768.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.94, loss_scale=16, train_wall=234, gb_free=21, wall=53565
2022-03-05 04:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:17:43 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 12.9 | nll_loss 12.062 | ppl 4276.08 | wps 45274.2 | wpb 510.9 | bsz 1 | num_updates 20139 | best_loss 8.489
2022-03-05 04:17:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20139 updates
2022-03-05 04:17:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:17:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 209 @ 20139 updates, score 12.9) (writing took 2.614240100607276 seconds)
2022-03-05 04:17:46 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 04:17:46 | INFO | train | epoch 209 | loss 2.829 | nll_loss 0.988 | ppl 1.98 | wps 24757 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20139 | lr 0.000222834 | gnorm 0.935 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53673
2022-03-05 04:17:46 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 04:17:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:20:23 | INFO | train_inner | epoch 210:     61 / 97 loss=2.828, nll_loss=0.988, ppl=1.98, wps=24770.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.928, loss_scale=16, train_wall=234, gb_free=21, wall=53830
2022-03-05 04:21:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:22:00 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 12.895 | nll_loss 12.061 | ppl 4272.6 | wps 45627.5 | wpb 510.9 | bsz 1 | num_updates 20235 | best_loss 8.489
2022-03-05 04:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20235 updates
2022-03-05 04:22:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:22:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 210 @ 20235 updates, score 12.895) (writing took 2.6123583344742656 seconds)
2022-03-05 04:22:03 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 04:22:03 | INFO | train | epoch 210 | loss 2.825 | nll_loss 0.985 | ppl 1.98 | wps 24483.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20235 | lr 0.000222305 | gnorm 0.914 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53930
2022-03-05 04:22:03 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 04:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:23:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:24:53 | INFO | train_inner | epoch 211:     66 / 97 loss=2.823, nll_loss=0.983, ppl=1.98, wps=24254.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.925, loss_scale=8, train_wall=239, gb_free=21, wall=54100
2022-03-05 04:26:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:26:17 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 12.94 | nll_loss 12.109 | ppl 4418.68 | wps 44664.4 | wpb 510.9 | bsz 1 | num_updates 20331 | best_loss 8.489
2022-03-05 04:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20331 updates
2022-03-05 04:26:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 211 @ 20331 updates, score 12.94) (writing took 2.7003550147637725 seconds)
2022-03-05 04:26:20 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 04:26:20 | INFO | train | epoch 211 | loss 2.823 | nll_loss 0.983 | ppl 1.98 | wps 24432.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20331 | lr 0.000221779 | gnorm 0.933 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 54187
2022-03-05 04:26:20 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 04:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:29:17 | INFO | train_inner | epoch 212:     69 / 97 loss=2.821, nll_loss=0.981, ppl=1.97, wps=24753.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.927, loss_scale=8, train_wall=234, gb_free=21, wall=54364
2022-03-05 04:30:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:30:34 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 12.938 | nll_loss 12.104 | ppl 4403.12 | wps 45225.6 | wpb 510.9 | bsz 1 | num_updates 20427 | best_loss 8.489
2022-03-05 04:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20427 updates
2022-03-05 04:30:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:30:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:30:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 212 @ 20427 updates, score 12.938) (writing took 2.662968813441694 seconds)
2022-03-05 04:30:37 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 04:30:37 | INFO | train | epoch 212 | loss 2.82 | nll_loss 0.979 | ppl 1.97 | wps 24480.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20427 | lr 0.000221257 | gnorm 0.926 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 54444
2022-03-05 04:30:37 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 04:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:33:44 | INFO | train_inner | epoch 213:     73 / 97 loss=2.82, nll_loss=0.979, ppl=1.97, wps=24523.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.926, loss_scale=8, train_wall=237, gb_free=21, wall=54631
2022-03-05 04:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:34:51 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 12.887 | nll_loss 12.051 | ppl 4242.83 | wps 45131.5 | wpb 510.9 | bsz 1 | num_updates 20524 | best_loss 8.489
2022-03-05 04:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20524 updates
2022-03-05 04:34:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:34:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 213 @ 20524 updates, score 12.887) (writing took 2.767859765328467 seconds)
2022-03-05 04:34:53 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 04:34:53 | INFO | train | epoch 213 | loss 2.818 | nll_loss 0.977 | ppl 1.97 | wps 24734.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20524 | lr 0.000220734 | gnorm 0.924 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 54701
2022-03-05 04:34:54 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 04:34:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:38:09 | INFO | train_inner | epoch 214:     76 / 97 loss=2.817, nll_loss=0.976, ppl=1.97, wps=24770.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.93, loss_scale=16, train_wall=234, gb_free=21, wall=54896
2022-03-05 04:39:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:39:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:39:07 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 12.921 | nll_loss 12.084 | ppl 4341.39 | wps 45275.8 | wpb 510.9 | bsz 1 | num_updates 20620 | best_loss 8.489
2022-03-05 04:39:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20620 updates
2022-03-05 04:39:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 214 @ 20620 updates, score 12.921) (writing took 2.6859763227403164 seconds)
2022-03-05 04:39:10 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 04:39:10 | INFO | train | epoch 214 | loss 2.816 | nll_loss 0.975 | ppl 1.97 | wps 24517.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20620 | lr 0.000220219 | gnorm 0.929 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 54957
2022-03-05 04:39:10 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 04:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:42:36 | INFO | train_inner | epoch 215:     80 / 97 loss=2.814, nll_loss=0.974, ppl=1.96, wps=24523.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.916, loss_scale=8, train_wall=237, gb_free=21, wall=55163
2022-03-05 04:43:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:43:24 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 12.919 | nll_loss 12.083 | ppl 4339.33 | wps 44310.9 | wpb 510.9 | bsz 1 | num_updates 20717 | best_loss 8.489
2022-03-05 04:43:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20717 updates
2022-03-05 04:43:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:43:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 215 @ 20717 updates, score 12.919) (writing took 2.712613683193922 seconds)
2022-03-05 04:43:27 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 04:43:27 | INFO | train | epoch 215 | loss 2.812 | nll_loss 0.972 | ppl 1.96 | wps 24691.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20717 | lr 0.000219703 | gnorm 0.909 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 55214
2022-03-05 04:43:27 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 04:43:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:44:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:47:04 | INFO | train_inner | epoch 216:     84 / 97 loss=2.811, nll_loss=0.97, ppl=1.96, wps=24457.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.922, loss_scale=8, train_wall=237, gb_free=21, wall=55431
2022-03-05 04:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:47:42 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 12.936 | nll_loss 12.109 | ppl 4417.43 | wps 45089.2 | wpb 510.9 | bsz 1 | num_updates 20813 | best_loss 8.489
2022-03-05 04:47:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20813 updates
2022-03-05 04:47:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:47:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:47:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 216 @ 20813 updates, score 12.936) (writing took 2.72691182885319 seconds)
2022-03-05 04:47:44 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 04:47:44 | INFO | train | epoch 216 | loss 2.81 | nll_loss 0.969 | ppl 1.96 | wps 24438.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20813 | lr 0.000219196 | gnorm 0.925 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 55472
2022-03-05 04:47:44 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 04:47:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:50:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:51:31 | INFO | train_inner | epoch 217:     88 / 97 loss=2.808, nll_loss=0.967, ppl=1.96, wps=24476.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20900, lr=0.000218739, gnorm=0.922, loss_scale=8, train_wall=237, gb_free=21, wall=55698
2022-03-05 04:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:51:59 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 12.952 | nll_loss 12.124 | ppl 4463.6 | wps 44506.2 | wpb 510.9 | bsz 1 | num_updates 20909 | best_loss 8.489
2022-03-05 04:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20909 updates
2022-03-05 04:51:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:52:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:52:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 217 @ 20909 updates, score 12.952) (writing took 2.676868903450668 seconds)
2022-03-05 04:52:02 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 04:52:02 | INFO | train | epoch 217 | loss 2.807 | nll_loss 0.966 | ppl 1.95 | wps 24444.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20909 | lr 0.000218692 | gnorm 0.921 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 55729
2022-03-05 04:52:02 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 04:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:55:56 | INFO | train_inner | epoch 218:     91 / 97 loss=2.804, nll_loss=0.963, ppl=1.95, wps=24757.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21000, lr=0.000218218, gnorm=0.933, loss_scale=8, train_wall=234, gb_free=21, wall=55963
2022-03-05 04:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:56:16 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 12.928 | nll_loss 12.096 | ppl 4376.43 | wps 44060.1 | wpb 510.9 | bsz 1 | num_updates 21006 | best_loss 8.489
2022-03-05 04:56:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 21006 updates
2022-03-05 04:56:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 04:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 218 @ 21006 updates, score 12.928) (writing took 2.685270187444985 seconds)
2022-03-05 04:56:19 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 04:56:19 | INFO | train | epoch 218 | loss 2.804 | nll_loss 0.963 | ppl 1.95 | wps 24721.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21006 | lr 0.000218187 | gnorm 0.935 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 55986
2022-03-05 04:56:19 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 04:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:58:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:00:23 | INFO | train_inner | epoch 219:     95 / 97 loss=2.804, nll_loss=0.963, ppl=1.95, wps=24507.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21100, lr=0.0002177, gnorm=0.92, loss_scale=8, train_wall=237, gb_free=21, wall=56230
2022-03-05 05:00:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:00:33 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 12.877 | nll_loss 12.045 | ppl 4226.05 | wps 43250.7 | wpb 510.9 | bsz 1 | num_updates 21102 | best_loss 8.489
2022-03-05 05:00:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21102 updates
2022-03-05 05:00:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:00:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 219 @ 21102 updates, score 12.877) (writing took 2.7800044249743223 seconds)
2022-03-05 05:00:36 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 05:00:36 | INFO | train | epoch 219 | loss 2.801 | nll_loss 0.96 | ppl 1.95 | wps 24448.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21102 | lr 0.00021769 | gnorm 0.918 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 56243
2022-03-05 05:00:36 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 05:00:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:04:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:04:50 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 12.929 | nll_loss 12.096 | ppl 4378.78 | wps 44349.1 | wpb 510.9 | bsz 1 | num_updates 21199 | best_loss 8.489
2022-03-05 05:04:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21199 updates
2022-03-05 05:04:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:04:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 220 @ 21199 updates, score 12.929) (writing took 2.816836583428085 seconds)
2022-03-05 05:04:53 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 05:04:53 | INFO | train | epoch 220 | loss 2.8 | nll_loss 0.959 | ppl 1.94 | wps 24717.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21199 | lr 0.000217191 | gnorm 0.92 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56500
2022-03-05 05:04:53 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 05:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:04:56 | INFO | train_inner | epoch 221:      1 / 97 loss=2.8, nll_loss=0.959, ppl=1.94, wps=24007.3, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=21200, lr=0.000217186, gnorm=0.92, loss_scale=16, train_wall=234, gb_free=21, wall=56503
2022-03-05 05:07:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:09:07 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 12.99 | nll_loss 12.166 | ppl 4594.15 | wps 44960.3 | wpb 510.9 | bsz 1 | num_updates 21295 | best_loss 8.489
2022-03-05 05:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21295 updates
2022-03-05 05:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:09:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:09:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 221 @ 21295 updates, score 12.99) (writing took 2.8733096178621054 seconds)
2022-03-05 05:09:10 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 05:09:10 | INFO | train | epoch 221 | loss 2.796 | nll_loss 0.955 | ppl 1.94 | wps 24480.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21295 | lr 0.000216701 | gnorm 0.907 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 56757
2022-03-05 05:09:10 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 05:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:09:23 | INFO | train_inner | epoch 222:      5 / 97 loss=2.795, nll_loss=0.954, ppl=1.94, wps=24530.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21300, lr=0.000216676, gnorm=0.909, loss_scale=8, train_wall=236, gb_free=21, wall=56770
2022-03-05 05:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:13:24 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 12.962 | nll_loss 12.136 | ppl 4500.77 | wps 44352.3 | wpb 510.9 | bsz 1 | num_updates 21392 | best_loss 8.489
2022-03-05 05:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21392 updates
2022-03-05 05:13:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:13:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:13:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 222 @ 21392 updates, score 12.962) (writing took 2.765690254047513 seconds)
2022-03-05 05:13:26 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 05:13:26 | INFO | train | epoch 222 | loss 2.795 | nll_loss 0.953 | ppl 1.94 | wps 24742.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21392 | lr 0.000216209 | gnorm 0.922 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57013
2022-03-05 05:13:26 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 05:13:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:13:47 | INFO | train_inner | epoch 223:      8 / 97 loss=2.793, nll_loss=0.952, ppl=1.93, wps=24766.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.918, loss_scale=16, train_wall=234, gb_free=21, wall=57034
2022-03-05 05:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:17:40 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 12.903 | nll_loss 12.076 | ppl 4317.46 | wps 45004.2 | wpb 510.9 | bsz 1 | num_updates 21489 | best_loss 8.489
2022-03-05 05:17:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21489 updates
2022-03-05 05:17:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 223 @ 21489 updates, score 12.903) (writing took 2.69324554502964 seconds)
2022-03-05 05:17:43 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 05:17:43 | INFO | train | epoch 223 | loss 2.792 | nll_loss 0.95 | ppl 1.93 | wps 24783.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21489 | lr 0.000215721 | gnorm 0.925 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57270
2022-03-05 05:17:43 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 05:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:18:11 | INFO | train_inner | epoch 224:     11 / 97 loss=2.79, nll_loss=0.948, ppl=1.93, wps=24801.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.924, loss_scale=16, train_wall=234, gb_free=21, wall=57298
2022-03-05 05:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:21:57 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 12.874 | nll_loss 12.041 | ppl 4214.32 | wps 43976.7 | wpb 510.9 | bsz 1 | num_updates 21585 | best_loss 8.489
2022-03-05 05:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21585 updates
2022-03-05 05:21:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:21:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 224 @ 21585 updates, score 12.874) (writing took 2.6793624255806208 seconds)
2022-03-05 05:21:59 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 05:21:59 | INFO | train | epoch 224 | loss 2.788 | nll_loss 0.947 | ppl 1.93 | wps 24495.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21585 | lr 0.00021524 | gnorm 0.917 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57526
2022-03-05 05:21:59 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 05:21:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:22:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:22:41 | INFO | train_inner | epoch 225:     16 / 97 loss=2.787, nll_loss=0.945, ppl=1.93, wps=24302.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.918, loss_scale=8, train_wall=239, gb_free=21, wall=57568
2022-03-05 05:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:26:14 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 12.919 | nll_loss 12.088 | ppl 4352.91 | wps 44827.6 | wpb 510.9 | bsz 1 | num_updates 21681 | best_loss 8.489
2022-03-05 05:26:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21681 updates
2022-03-05 05:26:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:26:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 225 @ 21681 updates, score 12.919) (writing took 2.716400682926178 seconds)
2022-03-05 05:26:16 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 05:26:16 | INFO | train | epoch 225 | loss 2.786 | nll_loss 0.944 | ppl 1.92 | wps 24472.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21681 | lr 0.000214763 | gnorm 0.912 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 57783
2022-03-05 05:26:16 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 05:26:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:27:05 | INFO | train_inner | epoch 226:     19 / 97 loss=2.785, nll_loss=0.943, ppl=1.92, wps=24748, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.911, loss_scale=8, train_wall=234, gb_free=21, wall=57832
2022-03-05 05:30:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:30:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:30:30 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 12.893 | nll_loss 12.063 | ppl 4278.62 | wps 44838.7 | wpb 510.9 | bsz 1 | num_updates 21777 | best_loss 8.489
2022-03-05 05:30:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21777 updates
2022-03-05 05:30:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:30:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:30:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 226 @ 21777 updates, score 12.893) (writing took 2.7780496310442686 seconds)
2022-03-05 05:30:33 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 05:30:33 | INFO | train | epoch 226 | loss 2.783 | nll_loss 0.942 | ppl 1.92 | wps 24499.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21777 | lr 0.00021429 | gnorm 0.918 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 58040
2022-03-05 05:30:33 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 05:30:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:31:32 | INFO | train_inner | epoch 227:     23 / 97 loss=2.783, nll_loss=0.941, ppl=1.92, wps=24531.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.919, loss_scale=8, train_wall=237, gb_free=21, wall=58099
2022-03-05 05:34:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:34:47 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 12.899 | nll_loss 12.067 | ppl 4290.18 | wps 44375.6 | wpb 510.9 | bsz 1 | num_updates 21874 | best_loss 8.489
2022-03-05 05:34:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21874 updates
2022-03-05 05:34:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:34:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:34:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 227 @ 21874 updates, score 12.899) (writing took 2.6901580039411783 seconds)
2022-03-05 05:34:50 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 05:34:50 | INFO | train | epoch 227 | loss 2.783 | nll_loss 0.941 | ppl 1.92 | wps 24739.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21874 | lr 0.000213814 | gnorm 0.914 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 58297
2022-03-05 05:34:50 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 05:34:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:35:56 | INFO | train_inner | epoch 228:     26 / 97 loss=2.782, nll_loss=0.94, ppl=1.92, wps=24774.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.914, loss_scale=8, train_wall=234, gb_free=21, wall=58364
2022-03-05 05:36:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:38:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:39:04 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 12.872 | nll_loss 12.037 | ppl 4203.06 | wps 42744 | wpb 510.9 | bsz 1 | num_updates 21970 | best_loss 8.489
2022-03-05 05:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21970 updates
2022-03-05 05:39:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:39:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 228 @ 21970 updates, score 12.872) (writing took 2.739608476869762 seconds)
2022-03-05 05:39:07 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 05:39:07 | INFO | train | epoch 228 | loss 2.779 | nll_loss 0.937 | ppl 1.91 | wps 24439.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21970 | lr 0.000213346 | gnorm 0.92 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 58554
2022-03-05 05:39:07 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 05:39:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:40:24 | INFO | train_inner | epoch 229:     30 / 97 loss=2.777, nll_loss=0.935, ppl=1.91, wps=24474.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.92, loss_scale=8, train_wall=237, gb_free=21, wall=58631
2022-03-05 05:43:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:43:21 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 12.97 | nll_loss 12.143 | ppl 4523.31 | wps 44512.8 | wpb 510.9 | bsz 1 | num_updates 22067 | best_loss 8.489
2022-03-05 05:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22067 updates
2022-03-05 05:43:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:43:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:43:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 229 @ 22067 updates, score 12.97) (writing took 2.7639168221503496 seconds)
2022-03-05 05:43:24 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 05:43:24 | INFO | train | epoch 229 | loss 2.777 | nll_loss 0.936 | ppl 1.91 | wps 24740.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22067 | lr 0.000212877 | gnorm 0.926 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58811
2022-03-05 05:43:24 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 05:43:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:44:48 | INFO | train_inner | epoch 230:     33 / 97 loss=2.775, nll_loss=0.933, ppl=1.91, wps=24773.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.92, loss_scale=16, train_wall=234, gb_free=21, wall=58895
2022-03-05 05:47:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:47:38 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 12.923 | nll_loss 12.097 | ppl 4381.63 | wps 43494.8 | wpb 510.9 | bsz 1 | num_updates 22164 | best_loss 8.489
2022-03-05 05:47:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22164 updates
2022-03-05 05:47:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:47:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:47:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 230 @ 22164 updates, score 12.923) (writing took 2.6852814434096217 seconds)
2022-03-05 05:47:41 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 05:47:41 | INFO | train | epoch 230 | loss 2.773 | nll_loss 0.931 | ppl 1.91 | wps 24733.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22164 | lr 0.00021241 | gnorm 0.914 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 59068
2022-03-05 05:47:41 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 05:47:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:47:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:49:16 | INFO | train_inner | epoch 231:     37 / 97 loss=2.772, nll_loss=0.93, ppl=1.91, wps=24499.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=22200, lr=0.000212238, gnorm=0.928, loss_scale=16, train_wall=237, gb_free=21, wall=59163
2022-03-05 05:51:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:51:55 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 12.957 | nll_loss 12.133 | ppl 4490.06 | wps 44132.4 | wpb 510.9 | bsz 1 | num_updates 22260 | best_loss 8.489
2022-03-05 05:51:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22260 updates
2022-03-05 05:51:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:51:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 231 @ 22260 updates, score 12.957) (writing took 2.7273218855261803 seconds)
2022-03-05 05:51:58 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 05:51:58 | INFO | train | epoch 231 | loss 2.771 | nll_loss 0.929 | ppl 1.9 | wps 24438.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22260 | lr 0.000211952 | gnorm 0.926 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 59325
2022-03-05 05:51:58 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 05:51:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:53:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:53:43 | INFO | train_inner | epoch 232:     41 / 97 loss=2.769, nll_loss=0.927, ppl=1.9, wps=24465.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.913, loss_scale=16, train_wall=237, gb_free=21, wall=59430
2022-03-05 05:54:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:56:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:56:12 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 12.982 | nll_loss 12.16 | ppl 4575.03 | wps 43697.6 | wpb 510.9 | bsz 1 | num_updates 22355 | best_loss 8.489
2022-03-05 05:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22355 updates
2022-03-05 05:56:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:56:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 05:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 232 @ 22355 updates, score 12.982) (writing took 2.7299580546095967 seconds)
2022-03-05 05:56:15 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 05:56:15 | INFO | train | epoch 232 | loss 2.769 | nll_loss 0.927 | ppl 1.9 | wps 24174.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 22355 | lr 0.000211501 | gnorm 0.915 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 59582
2022-03-05 05:56:15 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 05:56:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:58:11 | INFO | train_inner | epoch 233:     45 / 97 loss=2.768, nll_loss=0.926, ppl=1.9, wps=24478.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.91, loss_scale=8, train_wall=237, gb_free=21, wall=59698
2022-03-05 06:00:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:00:30 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 12.941 | nll_loss 12.118 | ppl 4443.8 | wps 43900.1 | wpb 510.9 | bsz 1 | num_updates 22452 | best_loss 8.489
2022-03-05 06:00:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22452 updates
2022-03-05 06:00:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 233 @ 22452 updates, score 12.941) (writing took 2.6733248494565487 seconds)
2022-03-05 06:00:33 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 06:00:33 | INFO | train | epoch 233 | loss 2.766 | nll_loss 0.924 | ppl 1.9 | wps 24682.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22452 | lr 0.000211044 | gnorm 0.908 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 59840
2022-03-05 06:00:33 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 06:00:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:00:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:02:39 | INFO | train_inner | epoch 234:     49 / 97 loss=2.765, nll_loss=0.923, ppl=1.9, wps=24461.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.924, loss_scale=8, train_wall=237, gb_free=21, wall=59966
2022-03-05 06:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:04:47 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 12.915 | nll_loss 12.089 | ppl 4357.45 | wps 42981.3 | wpb 510.9 | bsz 1 | num_updates 22548 | best_loss 8.489
2022-03-05 06:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22548 updates
2022-03-05 06:04:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:04:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:04:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 234 @ 22548 updates, score 12.915) (writing took 2.705646821297705 seconds)
2022-03-05 06:04:50 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 06:04:50 | INFO | train | epoch 234 | loss 2.765 | nll_loss 0.923 | ppl 1.9 | wps 24428.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22548 | lr 0.000210594 | gnorm 0.921 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 60097
2022-03-05 06:04:50 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 06:04:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:07:04 | INFO | train_inner | epoch 235:     52 / 97 loss=2.763, nll_loss=0.921, ppl=1.89, wps=24715.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.91, loss_scale=16, train_wall=235, gb_free=21, wall=60231
2022-03-05 06:08:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:08:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:09:04 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 12.965 | nll_loss 12.141 | ppl 4516.36 | wps 44856.1 | wpb 510.9 | bsz 1 | num_updates 22644 | best_loss 8.489
2022-03-05 06:09:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22644 updates
2022-03-05 06:09:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:09:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 235 @ 22644 updates, score 12.965) (writing took 2.687340817414224 seconds)
2022-03-05 06:09:07 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 06:09:07 | INFO | train | epoch 235 | loss 2.762 | nll_loss 0.92 | ppl 1.89 | wps 24473.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22644 | lr 0.000210147 | gnorm 0.918 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 60354
2022-03-05 06:09:07 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 06:09:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:11:31 | INFO | train_inner | epoch 236:     56 / 97 loss=2.761, nll_loss=0.919, ppl=1.89, wps=24529.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.908, loss_scale=8, train_wall=237, gb_free=21, wall=60498
2022-03-05 06:13:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:13:21 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 12.981 | nll_loss 12.163 | ppl 4586.23 | wps 44376.1 | wpb 510.9 | bsz 1 | num_updates 22741 | best_loss 8.489
2022-03-05 06:13:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22741 updates
2022-03-05 06:13:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:13:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 236 @ 22741 updates, score 12.981) (writing took 2.790412802249193 seconds)
2022-03-05 06:13:24 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 06:13:24 | INFO | train | epoch 236 | loss 2.76 | nll_loss 0.918 | ppl 1.89 | wps 24724.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22741 | lr 0.000209698 | gnorm 0.899 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 60611
2022-03-05 06:13:24 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 06:13:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:15:55 | INFO | train_inner | epoch 237:     59 / 97 loss=2.758, nll_loss=0.916, ppl=1.89, wps=24739.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.902, loss_scale=16, train_wall=234, gb_free=21, wall=60762
2022-03-05 06:16:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:17:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:17:38 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 12.948 | nll_loss 12.13 | ppl 4482.15 | wps 44456.5 | wpb 510.9 | bsz 1 | num_updates 22837 | best_loss 8.489
2022-03-05 06:17:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22837 updates
2022-03-05 06:17:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:17:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:17:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 237 @ 22837 updates, score 12.948) (writing took 2.7090126955881715 seconds)
2022-03-05 06:17:41 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 06:17:41 | INFO | train | epoch 237 | loss 2.758 | nll_loss 0.916 | ppl 1.89 | wps 24463.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22837 | lr 0.000209257 | gnorm 0.906 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 60868
2022-03-05 06:17:41 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 06:17:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:20:23 | INFO | train_inner | epoch 238:     63 / 97 loss=2.76, nll_loss=0.918, ppl=1.89, wps=24501.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.908, loss_scale=8, train_wall=237, gb_free=21, wall=61030
2022-03-05 06:21:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:21:55 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 13.013 | nll_loss 12.201 | ppl 4709.42 | wps 44644.2 | wpb 510.9 | bsz 1 | num_updates 22934 | best_loss 8.489
2022-03-05 06:21:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22934 updates
2022-03-05 06:21:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:21:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:21:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 238 @ 22934 updates, score 13.013) (writing took 2.6772478679195046 seconds)
2022-03-05 06:21:57 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 06:21:57 | INFO | train | epoch 238 | loss 2.756 | nll_loss 0.914 | ppl 1.88 | wps 24747.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22934 | lr 0.000208814 | gnorm 0.907 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 61125
2022-03-05 06:21:58 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 06:21:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:24:47 | INFO | train_inner | epoch 239:     66 / 97 loss=2.752, nll_loss=0.909, ppl=1.88, wps=24759, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.902, loss_scale=16, train_wall=234, gb_free=21, wall=61294
2022-03-05 06:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:26:12 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 12.934 | nll_loss 12.114 | ppl 4433.84 | wps 44384.9 | wpb 510.9 | bsz 1 | num_updates 23031 | best_loss 8.489
2022-03-05 06:26:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23031 updates
2022-03-05 06:26:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:26:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:26:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 239 @ 23031 updates, score 12.934) (writing took 2.7209064373746514 seconds)
2022-03-05 06:26:15 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 06:26:15 | INFO | train | epoch 239 | loss 2.753 | nll_loss 0.911 | ppl 1.88 | wps 24707.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23031 | lr 0.000208374 | gnorm 0.904 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 61382
2022-03-05 06:26:15 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 06:26:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:27:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:29:15 | INFO | train_inner | epoch 240:     70 / 97 loss=2.753, nll_loss=0.91, ppl=1.88, wps=24488.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.906, loss_scale=8, train_wall=237, gb_free=21, wall=61562
2022-03-05 06:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:30:29 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 12.94 | nll_loss 12.118 | ppl 4446.03 | wps 45125.4 | wpb 510.9 | bsz 1 | num_updates 23127 | best_loss 8.489
2022-03-05 06:30:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23127 updates
2022-03-05 06:30:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:30:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:30:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 240 @ 23127 updates, score 12.94) (writing took 2.7336254622787237 seconds)
2022-03-05 06:30:32 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 06:30:32 | INFO | train | epoch 240 | loss 2.751 | nll_loss 0.909 | ppl 1.88 | wps 24446 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23127 | lr 0.000207941 | gnorm 0.904 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 61639
2022-03-05 06:30:32 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 06:30:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:33:40 | INFO | train_inner | epoch 241:     73 / 97 loss=2.751, nll_loss=0.909, ppl=1.88, wps=24704, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.913, loss_scale=16, train_wall=235, gb_free=21, wall=61827
2022-03-05 06:34:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:34:46 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 12.934 | nll_loss 12.116 | ppl 4438.87 | wps 45405.9 | wpb 510.9 | bsz 1 | num_updates 23224 | best_loss 8.489
2022-03-05 06:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23224 updates
2022-03-05 06:34:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:34:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 241 @ 23224 updates, score 12.934) (writing took 2.7738026846200228 seconds)
2022-03-05 06:34:49 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 06:34:49 | INFO | train | epoch 241 | loss 2.75 | nll_loss 0.908 | ppl 1.88 | wps 24696.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23224 | lr 0.000207506 | gnorm 0.913 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 61896
2022-03-05 06:34:49 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 06:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:35:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:38:07 | INFO | train_inner | epoch 242:     77 / 97 loss=2.748, nll_loss=0.906, ppl=1.87, wps=24532.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.909, loss_scale=8, train_wall=237, gb_free=21, wall=62094
2022-03-05 06:38:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:39:03 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 12.945 | nll_loss 12.123 | ppl 4461.35 | wps 45145.2 | wpb 510.9 | bsz 1 | num_updates 23320 | best_loss 8.489
2022-03-05 06:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23320 updates
2022-03-05 06:39:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 242 @ 23320 updates, score 12.945) (writing took 2.7336507868021727 seconds)
2022-03-05 06:39:06 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 06:39:06 | INFO | train | epoch 242 | loss 2.747 | nll_loss 0.905 | ppl 1.87 | wps 24501.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23320 | lr 0.000207079 | gnorm 0.911 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 62153
2022-03-05 06:39:06 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 06:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:40:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:42:34 | INFO | train_inner | epoch 243:     81 / 97 loss=2.747, nll_loss=0.906, ppl=1.87, wps=24541.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.91, loss_scale=8, train_wall=237, gb_free=21, wall=62361
2022-03-05 06:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:43:20 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 12.935 | nll_loss 12.107 | ppl 4410.75 | wps 44661.4 | wpb 510.9 | bsz 1 | num_updates 23416 | best_loss 8.489
2022-03-05 06:43:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23416 updates
2022-03-05 06:43:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 243 @ 23416 updates, score 12.935) (writing took 2.6859379168599844 seconds)
2022-03-05 06:43:22 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 06:43:22 | INFO | train | epoch 243 | loss 2.744 | nll_loss 0.902 | ppl 1.87 | wps 24499.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23416 | lr 0.000206654 | gnorm 0.905 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 62409
2022-03-05 06:43:22 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 06:43:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:46:58 | INFO | train_inner | epoch 244:     84 / 97 loss=2.743, nll_loss=0.901, ppl=1.87, wps=24765.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23500, lr=0.000206284, gnorm=0.898, loss_scale=16, train_wall=234, gb_free=21, wall=62625
2022-03-05 06:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:47:36 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 12.98 | nll_loss 12.163 | ppl 4585.83 | wps 44419.1 | wpb 510.9 | bsz 1 | num_updates 23513 | best_loss 8.489
2022-03-05 06:47:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23513 updates
2022-03-05 06:47:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:47:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:47:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 244 @ 23513 updates, score 12.98) (writing took 2.640417476184666 seconds)
2022-03-05 06:47:39 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 06:47:39 | INFO | train | epoch 244 | loss 2.742 | nll_loss 0.9 | ppl 1.87 | wps 24744.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23513 | lr 0.000206227 | gnorm 0.899 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62666
2022-03-05 06:47:39 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 06:47:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:49:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:51:25 | INFO | train_inner | epoch 245:     88 / 97 loss=2.742, nll_loss=0.899, ppl=1.87, wps=24501, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23600, lr=0.000205847, gnorm=0.91, loss_scale=8, train_wall=237, gb_free=21, wall=62892
2022-03-05 06:51:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:51:53 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 12.996 | nll_loss 12.175 | ppl 4625.47 | wps 44446.1 | wpb 510.9 | bsz 1 | num_updates 23609 | best_loss 8.489
2022-03-05 06:51:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23609 updates
2022-03-05 06:51:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:51:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:51:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 245 @ 23609 updates, score 12.996) (writing took 3.0687445513904095 seconds)
2022-03-05 06:51:56 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 06:51:56 | INFO | train | epoch 245 | loss 2.741 | nll_loss 0.898 | ppl 1.86 | wps 24418.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23609 | lr 0.000205808 | gnorm 0.907 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 62923
2022-03-05 06:51:56 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 06:51:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:55:50 | INFO | train_inner | epoch 246:     91 / 97 loss=2.739, nll_loss=0.897, ppl=1.86, wps=24707.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=0.897, loss_scale=16, train_wall=234, gb_free=21, wall=63158
2022-03-05 06:56:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:56:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:56:11 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 13.001 | nll_loss 12.192 | ppl 4679.69 | wps 43717.5 | wpb 510.9 | bsz 1 | num_updates 23705 | best_loss 8.489
2022-03-05 06:56:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23705 updates
2022-03-05 06:56:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:56:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 06:56:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 246 @ 23705 updates, score 13.001) (writing took 2.7067837780341506 seconds)
2022-03-05 06:56:14 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 06:56:14 | INFO | train | epoch 246 | loss 2.738 | nll_loss 0.895 | ppl 1.86 | wps 24456.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23705 | lr 0.00020539 | gnorm 0.9 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 63181
2022-03-05 06:56:14 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 06:56:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:00:18 | INFO | train_inner | epoch 247:     95 / 97 loss=2.737, nll_loss=0.895, ppl=1.86, wps=24521.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23800, lr=0.00020498, gnorm=0.907, loss_scale=8, train_wall=237, gb_free=21, wall=63425
2022-03-05 07:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:00:28 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 12.98 | nll_loss 12.166 | ppl 4595.72 | wps 44520.8 | wpb 510.9 | bsz 1 | num_updates 23802 | best_loss 8.489
2022-03-05 07:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23802 updates
2022-03-05 07:00:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:00:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:00:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 247 @ 23802 updates, score 12.98) (writing took 2.669768867082894 seconds)
2022-03-05 07:00:30 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 07:00:30 | INFO | train | epoch 247 | loss 2.737 | nll_loss 0.894 | ppl 1.86 | wps 24751.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23802 | lr 0.000204971 | gnorm 0.905 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 63437
2022-03-05 07:00:30 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 07:00:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:03:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:04:44 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 13.054 | nll_loss 12.248 | ppl 4864.1 | wps 44879.2 | wpb 510.9 | bsz 1 | num_updates 23898 | best_loss 8.489
2022-03-05 07:04:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23898 updates
2022-03-05 07:04:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:04:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:04:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 248 @ 23898 updates, score 13.054) (writing took 2.7765692668035626 seconds)
2022-03-05 07:04:47 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 07:04:47 | INFO | train | epoch 248 | loss 2.734 | nll_loss 0.891 | ppl 1.85 | wps 24474.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23898 | lr 0.000204559 | gnorm 0.893 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 63694
2022-03-05 07:04:47 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 07:04:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:04:52 | INFO | train_inner | epoch 249:      2 / 97 loss=2.734, nll_loss=0.892, ppl=1.86, wps=23822.1, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=23900, lr=0.000204551, gnorm=0.893, loss_scale=8, train_wall=237, gb_free=21, wall=63699
2022-03-05 07:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:09:01 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 12.931 | nll_loss 12.109 | ppl 4417.81 | wps 44022.2 | wpb 510.9 | bsz 1 | num_updates 23995 | best_loss 8.489
2022-03-05 07:09:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23995 updates
2022-03-05 07:09:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:09:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:09:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 249 @ 23995 updates, score 12.931) (writing took 2.7120254589244723 seconds)
2022-03-05 07:09:04 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 07:09:04 | INFO | train | epoch 249 | loss 2.732 | nll_loss 0.89 | ppl 1.85 | wps 24720.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23995 | lr 0.000204145 | gnorm 0.903 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 63951
2022-03-05 07:09:04 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 07:09:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:09:17 | INFO | train_inner | epoch 250:      5 / 97 loss=2.731, nll_loss=0.889, ppl=1.85, wps=24748.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.903, loss_scale=8, train_wall=234, gb_free=21, wall=63964
2022-03-05 07:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:13:18 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 13.048 | nll_loss 12.243 | ppl 4846.85 | wps 45082.6 | wpb 510.9 | bsz 1 | num_updates 24092 | best_loss 8.489
2022-03-05 07:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24092 updates
2022-03-05 07:13:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 250 @ 24092 updates, score 13.048) (writing took 2.7873119758442044 seconds)
2022-03-05 07:13:21 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 07:13:21 | INFO | train | epoch 250 | loss 2.729 | nll_loss 0.887 | ppl 1.85 | wps 24732.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24092 | lr 0.000203734 | gnorm 0.906 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64208
2022-03-05 07:13:21 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 07:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:13:41 | INFO | train_inner | epoch 251:      8 / 97 loss=2.728, nll_loss=0.885, ppl=1.85, wps=24758.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.905, loss_scale=16, train_wall=234, gb_free=21, wall=64229
2022-03-05 07:15:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:17:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:17:35 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 12.98 | nll_loss 12.167 | ppl 4597.17 | wps 44554.2 | wpb 510.9 | bsz 1 | num_updates 24188 | best_loss 8.489
2022-03-05 07:17:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24188 updates
2022-03-05 07:17:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:17:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 251 @ 24188 updates, score 12.98) (writing took 2.794089880771935 seconds)
2022-03-05 07:17:38 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 07:17:38 | INFO | train | epoch 251 | loss 2.728 | nll_loss 0.885 | ppl 1.85 | wps 24494.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24188 | lr 0.000203329 | gnorm 0.897 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64465
2022-03-05 07:17:38 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 07:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:17:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:18:11 | INFO | train_inner | epoch 252:     13 / 97 loss=2.727, nll_loss=0.885, ppl=1.85, wps=24302.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.898, loss_scale=8, train_wall=239, gb_free=21, wall=64498
2022-03-05 07:21:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:21:52 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 12.952 | nll_loss 12.139 | ppl 4509.55 | wps 45265.9 | wpb 510.9 | bsz 1 | num_updates 24284 | best_loss 8.489
2022-03-05 07:21:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24284 updates
2022-03-05 07:21:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:21:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:21:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 252 @ 24284 updates, score 12.952) (writing took 2.771923976019025 seconds)
2022-03-05 07:21:54 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 07:21:54 | INFO | train | epoch 252 | loss 2.725 | nll_loss 0.882 | ppl 1.84 | wps 24489.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24284 | lr 0.000202927 | gnorm 0.888 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 64721
2022-03-05 07:21:54 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 07:21:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:22:36 | INFO | train_inner | epoch 253:     16 / 97 loss=2.724, nll_loss=0.881, ppl=1.84, wps=24743.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.882, loss_scale=8, train_wall=234, gb_free=21, wall=64763
2022-03-05 07:26:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:26:08 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 12.881 | nll_loss 12.06 | ppl 4271.33 | wps 45119.8 | wpb 510.9 | bsz 1 | num_updates 24381 | best_loss 8.489
2022-03-05 07:26:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24381 updates
2022-03-05 07:26:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:26:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:26:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 253 @ 24381 updates, score 12.881) (writing took 2.7384140640497208 seconds)
2022-03-05 07:26:11 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 07:26:11 | INFO | train | epoch 253 | loss 2.724 | nll_loss 0.882 | ppl 1.84 | wps 24749.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24381 | lr 0.000202523 | gnorm 0.893 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64978
2022-03-05 07:26:11 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 07:26:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:27:00 | INFO | train_inner | epoch 254:     19 / 97 loss=2.723, nll_loss=0.88, ppl=1.84, wps=24783.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.896, loss_scale=16, train_wall=234, gb_free=21, wall=65027
2022-03-05 07:28:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:29:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:30:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:30:25 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 12.958 | nll_loss 12.142 | ppl 4520.59 | wps 45262 | wpb 510.9 | bsz 1 | num_updates 24476 | best_loss 8.489
2022-03-05 07:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24476 updates
2022-03-05 07:30:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:30:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 254 @ 24476 updates, score 12.958) (writing took 2.7473841663450003 seconds)
2022-03-05 07:30:28 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 07:30:28 | INFO | train | epoch 254 | loss 2.722 | nll_loss 0.879 | ppl 1.84 | wps 24228.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 24476 | lr 0.00020213 | gnorm 0.9 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 65235
2022-03-05 07:30:28 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 07:30:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:31:30 | INFO | train_inner | epoch 255:     24 / 97 loss=2.721, nll_loss=0.878, ppl=1.84, wps=24276.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.902, loss_scale=8, train_wall=239, gb_free=21, wall=65297
2022-03-05 07:34:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:34:43 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 12.977 | nll_loss 12.163 | ppl 4585.87 | wps 45133.4 | wpb 510.9 | bsz 1 | num_updates 24573 | best_loss 8.489
2022-03-05 07:34:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24573 updates
2022-03-05 07:34:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:34:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:34:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 255 @ 24573 updates, score 12.977) (writing took 2.7687943689525127 seconds)
2022-03-05 07:34:45 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 07:34:45 | INFO | train | epoch 255 | loss 2.72 | nll_loss 0.877 | ppl 1.84 | wps 24660.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24573 | lr 0.00020173 | gnorm 0.892 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 65492
2022-03-05 07:34:45 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 07:34:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:35:55 | INFO | train_inner | epoch 256:     27 / 97 loss=2.719, nll_loss=0.876, ppl=1.84, wps=24688.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.888, loss_scale=16, train_wall=235, gb_free=21, wall=65562
2022-03-05 07:38:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:38:59 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 12.924 | nll_loss 12.107 | ppl 4412.57 | wps 45396.3 | wpb 510.9 | bsz 1 | num_updates 24670 | best_loss 8.489
2022-03-05 07:38:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24670 updates
2022-03-05 07:38:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:39:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:39:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 256 @ 24670 updates, score 12.924) (writing took 2.5258480682969093 seconds)
2022-03-05 07:39:02 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 07:39:02 | INFO | train | epoch 256 | loss 2.719 | nll_loss 0.876 | ppl 1.84 | wps 24780.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24670 | lr 0.000201333 | gnorm 0.898 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 65749
2022-03-05 07:39:02 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 07:39:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:40:18 | INFO | train_inner | epoch 257:     30 / 97 loss=2.718, nll_loss=0.875, ppl=1.83, wps=24869.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.9, loss_scale=16, train_wall=234, gb_free=21, wall=65825
2022-03-05 07:40:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:42:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:43:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:43:14 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 12.983 | nll_loss 12.168 | ppl 4601.49 | wps 45335.1 | wpb 510.9 | bsz 1 | num_updates 24765 | best_loss 8.489
2022-03-05 07:43:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24765 updates
2022-03-05 07:43:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:43:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 257 @ 24765 updates, score 12.983) (writing took 2.4693169659003615 seconds)
2022-03-05 07:43:16 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 07:43:16 | INFO | train | epoch 257 | loss 2.715 | nll_loss 0.872 | ppl 1.83 | wps 24430.6 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 24765 | lr 0.000200947 | gnorm 0.891 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 66003
2022-03-05 07:43:16 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 07:43:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:44:46 | INFO | train_inner | epoch 258:     35 / 97 loss=2.713, nll_loss=0.87, ppl=1.83, wps=24491, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.89, loss_scale=8, train_wall=237, gb_free=21, wall=66093
2022-03-05 07:47:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:47:29 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 13.022 | nll_loss 12.207 | ppl 4729.34 | wps 45339.1 | wpb 510.9 | bsz 1 | num_updates 24862 | best_loss 8.489
2022-03-05 07:47:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24862 updates
2022-03-05 07:47:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:47:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:47:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 258 @ 24862 updates, score 13.022) (writing took 2.521616144105792 seconds)
2022-03-05 07:47:31 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 07:47:31 | INFO | train | epoch 258 | loss 2.715 | nll_loss 0.872 | ppl 1.83 | wps 24932 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24862 | lr 0.000200554 | gnorm 0.896 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 66258
2022-03-05 07:47:31 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 07:47:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:49:08 | INFO | train_inner | epoch 259:     38 / 97 loss=2.714, nll_loss=0.871, ppl=1.83, wps=24950.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.893, loss_scale=16, train_wall=233, gb_free=21, wall=66355
2022-03-05 07:50:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:51:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:51:44 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 13.01 | nll_loss 12.198 | ppl 4697.99 | wps 45267.3 | wpb 510.9 | bsz 1 | num_updates 24958 | best_loss 8.489
2022-03-05 07:51:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24958 updates
2022-03-05 07:51:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:51:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:51:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 259 @ 24958 updates, score 13.01) (writing took 2.6774218790233135 seconds)
2022-03-05 07:51:46 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 07:51:46 | INFO | train | epoch 259 | loss 2.713 | nll_loss 0.87 | ppl 1.83 | wps 24647.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 24958 | lr 0.000200168 | gnorm 0.899 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 66513
2022-03-05 07:51:46 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 07:51:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:53:34 | INFO | train_inner | epoch 260:     42 / 97 loss=2.713, nll_loss=0.871, ppl=1.83, wps=24681, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25000, lr=0.0002, gnorm=0.903, loss_scale=8, train_wall=235, gb_free=21, wall=66621
2022-03-05 07:55:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:55:59 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 12.962 | nll_loss 12.147 | ppl 4534.75 | wps 45172.6 | wpb 510.9 | bsz 1 | num_updates 25055 | best_loss 8.489
2022-03-05 07:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25055 updates
2022-03-05 07:55:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:56:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 07:56:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 260 @ 25055 updates, score 12.962) (writing took 2.716091603040695 seconds)
2022-03-05 07:56:01 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 07:56:01 | INFO | train | epoch 260 | loss 2.711 | nll_loss 0.869 | ppl 1.83 | wps 24911.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25055 | lr 0.00019978 | gnorm 0.897 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 66768
2022-03-05 07:56:01 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 07:56:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:57:56 | INFO | train_inner | epoch 261:     45 / 97 loss=2.709, nll_loss=0.866, ppl=1.82, wps=24924.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.894, loss_scale=16, train_wall=233, gb_free=21, wall=66883
2022-03-05 07:59:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:00:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:00:14 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 12.96 | nll_loss 12.146 | ppl 4531.64 | wps 45468.4 | wpb 510.9 | bsz 1 | num_updates 25151 | best_loss 8.489
2022-03-05 08:00:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25151 updates
2022-03-05 08:00:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:00:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:00:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 261 @ 25151 updates, score 12.96) (writing took 2.792283450253308 seconds)
2022-03-05 08:00:17 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 08:00:17 | INFO | train | epoch 261 | loss 2.708 | nll_loss 0.866 | ppl 1.82 | wps 24614 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25151 | lr 0.000199399 | gnorm 0.893 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 67024
2022-03-05 08:00:17 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 08:00:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:02:22 | INFO | train_inner | epoch 262:     49 / 97 loss=2.708, nll_loss=0.865, ppl=1.82, wps=24656.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.904, loss_scale=8, train_wall=235, gb_free=21, wall=67149
2022-03-05 08:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:04:29 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 12.951 | nll_loss 12.14 | ppl 4514.07 | wps 45267 | wpb 510.9 | bsz 1 | num_updates 25248 | best_loss 8.489
2022-03-05 08:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25248 updates
2022-03-05 08:04:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:04:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:04:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 262 @ 25248 updates, score 12.951) (writing took 2.728680005297065 seconds)
2022-03-05 08:04:32 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 08:04:32 | INFO | train | epoch 262 | loss 2.708 | nll_loss 0.865 | ppl 1.82 | wps 24873.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25248 | lr 0.000199015 | gnorm 0.911 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 67279
2022-03-05 08:04:32 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 08:04:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:06:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:06:48 | INFO | train_inner | epoch 263:     53 / 97 loss=2.707, nll_loss=0.865, ppl=1.82, wps=24661.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.896, loss_scale=8, train_wall=235, gb_free=21, wall=67415
2022-03-05 08:08:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:08:45 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 12.968 | nll_loss 12.156 | ppl 4563.58 | wps 45359.4 | wpb 510.9 | bsz 1 | num_updates 25344 | best_loss 8.489
2022-03-05 08:08:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25344 updates
2022-03-05 08:08:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:08:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:08:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 263 @ 25344 updates, score 12.968) (writing took 2.7281554201617837 seconds)
2022-03-05 08:08:47 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 08:08:47 | INFO | train | epoch 263 | loss 2.705 | nll_loss 0.862 | ppl 1.82 | wps 24636.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25344 | lr 0.000198638 | gnorm 0.89 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 67534
2022-03-05 08:08:47 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 08:08:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:11:10 | INFO | train_inner | epoch 264:     56 / 97 loss=2.703, nll_loss=0.86, ppl=1.81, wps=24924, ups=0.38, wpb=65495, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.887, loss_scale=8, train_wall=233, gb_free=21, wall=67677
2022-03-05 08:12:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:13:00 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 12.97 | nll_loss 12.16 | ppl 4577.11 | wps 45309.6 | wpb 510.9 | bsz 1 | num_updates 25441 | best_loss 8.489
2022-03-05 08:13:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25441 updates
2022-03-05 08:13:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:13:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:13:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 264 @ 25441 updates, score 12.97) (writing took 2.718501196242869 seconds)
2022-03-05 08:13:02 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 08:13:02 | INFO | train | epoch 264 | loss 2.703 | nll_loss 0.861 | ppl 1.82 | wps 24905.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25441 | lr 0.000198259 | gnorm 0.886 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 67789
2022-03-05 08:13:02 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 08:13:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:14:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:15:36 | INFO | train_inner | epoch 265:     60 / 97 loss=2.703, nll_loss=0.861, ppl=1.82, wps=24693.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.892, loss_scale=8, train_wall=235, gb_free=21, wall=67943
2022-03-05 08:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:17:15 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 12.96 | nll_loss 12.149 | ppl 4542.18 | wps 45313.1 | wpb 510.9 | bsz 1 | num_updates 25537 | best_loss 8.489
2022-03-05 08:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25537 updates
2022-03-05 08:17:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:17:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:17:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 265 @ 25537 updates, score 12.96) (writing took 2.9085632180795074 seconds)
2022-03-05 08:17:18 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 08:17:18 | INFO | train | epoch 265 | loss 2.701 | nll_loss 0.859 | ppl 1.81 | wps 24636 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25537 | lr 0.000197886 | gnorm 0.891 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 68045
2022-03-05 08:17:18 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 08:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:19:58 | INFO | train_inner | epoch 266:     63 / 97 loss=2.701, nll_loss=0.859, ppl=1.81, wps=24914.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.89, loss_scale=8, train_wall=233, gb_free=21, wall=68205
2022-03-05 08:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:21:30 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 13.026 | nll_loss 12.219 | ppl 4768.39 | wps 45271.7 | wpb 510.9 | bsz 1 | num_updates 25634 | best_loss 8.489
2022-03-05 08:21:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25634 updates
2022-03-05 08:21:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:21:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:21:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 266 @ 25634 updates, score 13.026) (writing took 6.147713351994753 seconds)
2022-03-05 08:21:36 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 08:21:36 | INFO | train | epoch 266 | loss 2.699 | nll_loss 0.857 | ppl 1.81 | wps 24571.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25634 | lr 0.000197511 | gnorm 0.89 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 68303
2022-03-05 08:21:36 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 08:21:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:24:25 | INFO | train_inner | epoch 267:     66 / 97 loss=2.698, nll_loss=0.855, ppl=1.81, wps=24591.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.886, loss_scale=16, train_wall=233, gb_free=21, wall=68472
2022-03-05 08:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:25:49 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 13.035 | nll_loss 12.234 | ppl 4817.22 | wps 45218.7 | wpb 510.9 | bsz 1 | num_updates 25731 | best_loss 8.489
2022-03-05 08:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25731 updates
2022-03-05 08:25:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:25:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 267 @ 25731 updates, score 13.035) (writing took 2.8721031975001097 seconds)
2022-03-05 08:25:52 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 08:25:52 | INFO | train | epoch 267 | loss 2.698 | nll_loss 0.855 | ppl 1.81 | wps 24876.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25731 | lr 0.000197139 | gnorm 0.887 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 68559
2022-03-05 08:25:52 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 08:25:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:26:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:26:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:28:53 | INFO | train_inner | epoch 268:     71 / 97 loss=2.698, nll_loss=0.855, ppl=1.81, wps=24432.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.898, loss_scale=8, train_wall=238, gb_free=21, wall=68740
2022-03-05 08:29:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:30:04 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 12.954 | nll_loss 12.145 | ppl 4528.95 | wps 45343.4 | wpb 510.9 | bsz 1 | num_updates 25826 | best_loss 8.489
2022-03-05 08:30:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25826 updates
2022-03-05 08:30:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 268 @ 25826 updates, score 12.954) (writing took 2.7766076400876045 seconds)
2022-03-05 08:30:07 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 08:30:07 | INFO | train | epoch 268 | loss 2.695 | nll_loss 0.853 | ppl 1.81 | wps 24381.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 25826 | lr 0.000196776 | gnorm 0.89 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 68814
2022-03-05 08:30:07 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 08:30:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:33:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:33:18 | INFO | train_inner | epoch 269:     75 / 97 loss=2.694, nll_loss=0.851, ppl=1.8, wps=24665.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.885, loss_scale=8, train_wall=236, gb_free=21, wall=69005
2022-03-05 08:34:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:34:19 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 13.007 | nll_loss 12.2 | ppl 4704.39 | wps 45264.2 | wpb 510.9 | bsz 1 | num_updates 25922 | best_loss 8.489
2022-03-05 08:34:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25922 updates
2022-03-05 08:34:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:34:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:34:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 269 @ 25922 updates, score 13.007) (writing took 2.804218364879489 seconds)
2022-03-05 08:34:22 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 08:34:22 | INFO | train | epoch 269 | loss 2.694 | nll_loss 0.851 | ppl 1.8 | wps 24623.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25922 | lr 0.000196411 | gnorm 0.895 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 69069
2022-03-05 08:34:22 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 08:34:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:37:41 | INFO | train_inner | epoch 270:     78 / 97 loss=2.693, nll_loss=0.85, ppl=1.8, wps=24914.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.888, loss_scale=8, train_wall=233, gb_free=21, wall=69268
2022-03-05 08:38:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:38:34 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 13.006 | nll_loss 12.204 | ppl 4719.03 | wps 45367.8 | wpb 510.9 | bsz 1 | num_updates 26019 | best_loss 8.489
2022-03-05 08:38:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26019 updates
2022-03-05 08:38:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:38:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 270 @ 26019 updates, score 13.006) (writing took 2.6909261662513018 seconds)
2022-03-05 08:38:37 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 08:38:37 | INFO | train | epoch 270 | loss 2.692 | nll_loss 0.849 | ppl 1.8 | wps 24901 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26019 | lr 0.000196045 | gnorm 0.879 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 69324
2022-03-05 08:38:37 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 08:38:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:39:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:42:07 | INFO | train_inner | epoch 271:     82 / 97 loss=2.692, nll_loss=0.849, ppl=1.8, wps=24666.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26100, lr=0.00019574, gnorm=0.885, loss_scale=8, train_wall=236, gb_free=21, wall=69534
2022-03-05 08:42:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:42:50 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 12.977 | nll_loss 12.169 | ppl 4605.56 | wps 45182.9 | wpb 510.9 | bsz 1 | num_updates 26115 | best_loss 8.489
2022-03-05 08:42:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26115 updates
2022-03-05 08:42:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:42:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 271 @ 26115 updates, score 12.977) (writing took 2.750678558833897 seconds)
2022-03-05 08:42:53 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 08:42:53 | INFO | train | epoch 271 | loss 2.692 | nll_loss 0.849 | ppl 1.8 | wps 24617 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26115 | lr 0.000195684 | gnorm 0.89 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 69580
2022-03-05 08:42:53 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 08:42:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:46:30 | INFO | train_inner | epoch 272:     85 / 97 loss=2.691, nll_loss=0.848, ppl=1.8, wps=24903.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26200, lr=0.000195366, gnorm=0.879, loss_scale=16, train_wall=233, gb_free=21, wall=69797
2022-03-05 08:47:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:47:05 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 13.03 | nll_loss 12.226 | ppl 4789.92 | wps 45153.1 | wpb 510.9 | bsz 1 | num_updates 26212 | best_loss 8.489
2022-03-05 08:47:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26212 updates
2022-03-05 08:47:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:47:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:47:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 272 @ 26212 updates, score 13.03) (writing took 2.683690195903182 seconds)
2022-03-05 08:47:08 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 08:47:08 | INFO | train | epoch 272 | loss 2.689 | nll_loss 0.846 | ppl 1.8 | wps 24891.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26212 | lr 0.000195321 | gnorm 0.876 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 69835
2022-03-05 08:47:08 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 08:47:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:47:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:50:55 | INFO | train_inner | epoch 273:     89 / 97 loss=2.688, nll_loss=0.846, ppl=1.8, wps=24696.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26300, lr=0.000194994, gnorm=0.892, loss_scale=8, train_wall=235, gb_free=21, wall=70062
2022-03-05 08:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:51:20 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 12.994 | nll_loss 12.187 | ppl 4662.18 | wps 45376.3 | wpb 510.9 | bsz 1 | num_updates 26308 | best_loss 8.489
2022-03-05 08:51:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26308 updates
2022-03-05 08:51:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:51:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:51:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 273 @ 26308 updates, score 12.994) (writing took 2.688274636864662 seconds)
2022-03-05 08:51:23 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 08:51:23 | INFO | train | epoch 273 | loss 2.687 | nll_loss 0.844 | ppl 1.8 | wps 24657.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26308 | lr 0.000194965 | gnorm 0.891 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 70090
2022-03-05 08:51:23 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 08:51:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:54:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:55:20 | INFO | train_inner | epoch 274:     93 / 97 loss=2.687, nll_loss=0.844, ppl=1.8, wps=24685.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.888, loss_scale=8, train_wall=235, gb_free=21, wall=70327
2022-03-05 08:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:55:35 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 12.993 | nll_loss 12.189 | ppl 4668.21 | wps 45211.1 | wpb 510.9 | bsz 1 | num_updates 26404 | best_loss 8.489
2022-03-05 08:55:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26404 updates
2022-03-05 08:55:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:55:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:55:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 274 @ 26404 updates, score 12.993) (writing took 2.686166566796601 seconds)
2022-03-05 08:55:38 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 08:55:38 | INFO | train | epoch 274 | loss 2.686 | nll_loss 0.844 | ppl 1.79 | wps 24643.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26404 | lr 0.00019461 | gnorm 0.886 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 70345
2022-03-05 08:55:38 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 08:55:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:59:43 | INFO | train_inner | epoch 275:     96 / 97 loss=2.686, nll_loss=0.844, ppl=1.79, wps=24918.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=26500, lr=0.000194257, gnorm=0.888, loss_scale=8, train_wall=233, gb_free=21, wall=70590
2022-03-05 08:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:59:50 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 12.996 | nll_loss 12.187 | ppl 4662.38 | wps 45557.8 | wpb 510.9 | bsz 1 | num_updates 26501 | best_loss 8.489
2022-03-05 08:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26501 updates
2022-03-05 08:59:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:59:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 08:59:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 275 @ 26501 updates, score 12.996) (writing took 2.6720766564831138 seconds)
2022-03-05 08:59:53 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 08:59:53 | INFO | train | epoch 275 | loss 2.685 | nll_loss 0.842 | ppl 1.79 | wps 24900.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26501 | lr 0.000194254 | gnorm 0.888 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 70600
2022-03-05 08:59:53 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 08:59:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:04:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:04:05 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 13.016 | nll_loss 12.21 | ppl 4737.32 | wps 45396.5 | wpb 510.9 | bsz 1 | num_updates 26597 | best_loss 8.489
2022-03-05 09:04:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26597 updates
2022-03-05 09:04:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:04:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:04:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 276 @ 26597 updates, score 13.016) (writing took 2.710616512224078 seconds)
2022-03-05 09:04:08 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 09:04:08 | INFO | train | epoch 276 | loss 2.682 | nll_loss 0.839 | ppl 1.79 | wps 24643.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26597 | lr 0.000193903 | gnorm 0.877 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 70855
2022-03-05 09:04:08 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 09:04:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:04:16 | INFO | train_inner | epoch 277:      3 / 97 loss=2.682, nll_loss=0.839, ppl=1.79, wps=23989.8, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=26600, lr=0.000193892, gnorm=0.877, loss_scale=8, train_wall=235, gb_free=21, wall=70863
2022-03-05 09:07:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:08:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:08:21 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 12.983 | nll_loss 12.175 | ppl 4624.64 | wps 45402.2 | wpb 510.9 | bsz 1 | num_updates 26693 | best_loss 8.489
2022-03-05 09:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26693 updates
2022-03-05 09:08:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:08:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:08:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 277 @ 26693 updates, score 12.983) (writing took 2.8351701935753226 seconds)
2022-03-05 09:08:23 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 09:08:23 | INFO | train | epoch 277 | loss 2.681 | nll_loss 0.838 | ppl 1.79 | wps 24630.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26693 | lr 0.000193554 | gnorm 0.874 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 71110
2022-03-05 09:08:23 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 09:08:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:08:41 | INFO | train_inner | epoch 278:      7 / 97 loss=2.68, nll_loss=0.837, ppl=1.79, wps=24671.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.877, loss_scale=8, train_wall=235, gb_free=21, wall=71128
2022-03-05 09:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:12:36 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 12.997 | nll_loss 12.193 | ppl 4680.77 | wps 45875.9 | wpb 510.9 | bsz 1 | num_updates 26790 | best_loss 8.489
2022-03-05 09:12:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26790 updates
2022-03-05 09:12:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:12:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:12:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 278 @ 26790 updates, score 12.997) (writing took 2.7753035137429833 seconds)
2022-03-05 09:12:39 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 09:12:39 | INFO | train | epoch 278 | loss 2.679 | nll_loss 0.837 | ppl 1.79 | wps 24883.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26790 | lr 0.000193203 | gnorm 0.89 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 71366
2022-03-05 09:12:39 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 09:12:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:13:04 | INFO | train_inner | epoch 279:     10 / 97 loss=2.679, nll_loss=0.836, ppl=1.79, wps=24903.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.889, loss_scale=8, train_wall=233, gb_free=21, wall=71391
2022-03-05 09:15:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:16:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:16:51 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 13.018 | nll_loss 12.213 | ppl 4746.14 | wps 45244.6 | wpb 510.9 | bsz 1 | num_updates 26886 | best_loss 8.489
2022-03-05 09:16:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26886 updates
2022-03-05 09:16:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:16:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:16:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 279 @ 26886 updates, score 13.018) (writing took 2.7201627464964986 seconds)
2022-03-05 09:16:54 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 09:16:54 | INFO | train | epoch 279 | loss 2.677 | nll_loss 0.834 | ppl 1.78 | wps 24635 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26886 | lr 0.000192858 | gnorm 0.868 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 71621
2022-03-05 09:16:54 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 09:16:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:17:30 | INFO | train_inner | epoch 280:     14 / 97 loss=2.675, nll_loss=0.832, ppl=1.78, wps=24678.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.865, loss_scale=8, train_wall=235, gb_free=21, wall=71657
2022-03-05 09:21:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:21:06 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 12.936 | nll_loss 12.126 | ppl 4469.64 | wps 45286.1 | wpb 510.9 | bsz 1 | num_updates 26983 | best_loss 8.489
2022-03-05 09:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26983 updates
2022-03-05 09:21:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:21:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 280 @ 26983 updates, score 12.936) (writing took 2.720498774200678 seconds)
2022-03-05 09:21:09 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 09:21:09 | INFO | train | epoch 280 | loss 2.677 | nll_loss 0.834 | ppl 1.78 | wps 24893.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26983 | lr 0.000192511 | gnorm 0.886 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 71876
2022-03-05 09:21:09 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 09:21:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:21:53 | INFO | train_inner | epoch 281:     17 / 97 loss=2.677, nll_loss=0.834, ppl=1.78, wps=24919.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.885, loss_scale=16, train_wall=233, gb_free=21, wall=71920
2022-03-05 09:22:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:25:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:25:21 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 13.004 | nll_loss 12.204 | ppl 4717.84 | wps 45287 | wpb 510.9 | bsz 1 | num_updates 27079 | best_loss 8.489
2022-03-05 09:25:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27079 updates
2022-03-05 09:25:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:25:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 281 @ 27079 updates, score 13.004) (writing took 2.64656313508749 seconds)
2022-03-05 09:25:24 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 09:25:24 | INFO | train | epoch 281 | loss 2.674 | nll_loss 0.832 | ppl 1.78 | wps 24652.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27079 | lr 0.000192169 | gnorm 0.882 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 72131
2022-03-05 09:25:24 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 09:25:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:26:18 | INFO | train_inner | epoch 282:     21 / 97 loss=2.674, nll_loss=0.831, ppl=1.78, wps=24677, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.882, loss_scale=8, train_wall=235, gb_free=21, wall=72185
2022-03-05 09:29:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:29:37 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 13.025 | nll_loss 12.224 | ppl 4784.23 | wps 45310.2 | wpb 510.9 | bsz 1 | num_updates 27176 | best_loss 8.489
2022-03-05 09:29:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27176 updates
2022-03-05 09:29:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:29:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:29:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 282 @ 27176 updates, score 13.025) (writing took 2.732282049022615 seconds)
2022-03-05 09:29:39 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 09:29:39 | INFO | train | epoch 282 | loss 2.673 | nll_loss 0.83 | ppl 1.78 | wps 24896.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27176 | lr 0.000191826 | gnorm 0.87 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72386
2022-03-05 09:29:39 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 09:29:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:29:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:30:43 | INFO | train_inner | epoch 283:     25 / 97 loss=2.672, nll_loss=0.829, ppl=1.78, wps=24701.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.872, loss_scale=8, train_wall=235, gb_free=21, wall=72450
2022-03-05 09:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:33:51 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 13.06 | nll_loss 12.272 | ppl 4947 | wps 45372.7 | wpb 510.9 | bsz 1 | num_updates 27272 | best_loss 8.489
2022-03-05 09:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27272 updates
2022-03-05 09:33:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:33:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 283 @ 27272 updates, score 13.06) (writing took 2.873103124089539 seconds)
2022-03-05 09:33:54 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 09:33:54 | INFO | train | epoch 283 | loss 2.671 | nll_loss 0.828 | ppl 1.78 | wps 24655.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27272 | lr 0.000191488 | gnorm 0.895 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 72641
2022-03-05 09:33:54 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 09:33:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:35:06 | INFO | train_inner | epoch 284:     28 / 97 loss=2.669, nll_loss=0.826, ppl=1.77, wps=24927.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.889, loss_scale=8, train_wall=233, gb_free=21, wall=72713
2022-03-05 09:37:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:38:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:38:07 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 13.052 | nll_loss 12.253 | ppl 4881.85 | wps 45403 | wpb 510.9 | bsz 1 | num_updates 27368 | best_loss 8.489
2022-03-05 09:38:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27368 updates
2022-03-05 09:38:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:38:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:38:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 284 @ 27368 updates, score 13.052) (writing took 2.84347159601748 seconds)
2022-03-05 09:38:09 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 09:38:09 | INFO | train | epoch 284 | loss 2.669 | nll_loss 0.826 | ppl 1.77 | wps 24643.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27368 | lr 0.000191152 | gnorm 0.873 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 72896
2022-03-05 09:38:09 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 09:38:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:39:31 | INFO | train_inner | epoch 285:     32 / 97 loss=2.668, nll_loss=0.825, ppl=1.77, wps=24688.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.873, loss_scale=8, train_wall=235, gb_free=21, wall=72978
2022-03-05 09:42:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:42:22 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 13.044 | nll_loss 12.248 | ppl 4863.91 | wps 45352.6 | wpb 510.9 | bsz 1 | num_updates 27465 | best_loss 8.489
2022-03-05 09:42:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27465 updates
2022-03-05 09:42:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:42:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:42:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 285 @ 27465 updates, score 13.044) (writing took 2.686722076497972 seconds)
2022-03-05 09:42:24 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 09:42:24 | INFO | train | epoch 285 | loss 2.667 | nll_loss 0.825 | ppl 1.77 | wps 24922.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27465 | lr 0.000190814 | gnorm 0.861 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 73151
2022-03-05 09:42:24 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 09:42:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:43:54 | INFO | train_inner | epoch 286:     35 / 97 loss=2.668, nll_loss=0.826, ppl=1.77, wps=24945, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27500, lr=0.000190693, gnorm=0.869, loss_scale=16, train_wall=233, gb_free=21, wall=73241
2022-03-05 09:46:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:46:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:46:37 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 13.066 | nll_loss 12.271 | ppl 4941.56 | wps 45259.4 | wpb 510.9 | bsz 1 | num_updates 27561 | best_loss 8.489
2022-03-05 09:46:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27561 updates
2022-03-05 09:46:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:46:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:46:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 286 @ 27561 updates, score 13.066) (writing took 2.49892592523247 seconds)
2022-03-05 09:46:39 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 09:46:39 | INFO | train | epoch 286 | loss 2.667 | nll_loss 0.824 | ppl 1.77 | wps 24667.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27561 | lr 0.000190481 | gnorm 0.891 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 73406
2022-03-05 09:46:39 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 09:46:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:48:19 | INFO | train_inner | epoch 287:     39 / 97 loss=2.665, nll_loss=0.823, ppl=1.77, wps=24697.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.886, loss_scale=8, train_wall=235, gb_free=21, wall=73506
2022-03-05 09:50:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:50:51 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 13.037 | nll_loss 12.238 | ppl 4831.85 | wps 45301.1 | wpb 510.9 | bsz 1 | num_updates 27658 | best_loss 8.489
2022-03-05 09:50:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27658 updates
2022-03-05 09:50:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:50:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 287 @ 27658 updates, score 13.037) (writing took 2.9265650315210223 seconds)
2022-03-05 09:50:54 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 09:50:54 | INFO | train | epoch 287 | loss 2.665 | nll_loss 0.823 | ppl 1.77 | wps 24890.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27658 | lr 0.000190147 | gnorm 0.889 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 73661
2022-03-05 09:50:54 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 09:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:51:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:52:44 | INFO | train_inner | epoch 288:     43 / 97 loss=2.665, nll_loss=0.823, ppl=1.77, wps=24679.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.89, loss_scale=8, train_wall=235, gb_free=21, wall=73771
2022-03-05 09:55:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:55:07 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 12.945 | nll_loss 12.141 | ppl 4515.58 | wps 45299.9 | wpb 510.9 | bsz 1 | num_updates 27754 | best_loss 8.489
2022-03-05 09:55:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27754 updates
2022-03-05 09:55:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:55:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 288 @ 27754 updates, score 12.945) (writing took 2.793980572372675 seconds)
2022-03-05 09:55:09 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 09:55:09 | INFO | train | epoch 288 | loss 2.663 | nll_loss 0.82 | ppl 1.77 | wps 24645.9 | ups 0.38 | wpb 65493.3 | bsz 127.9 | num_updates 27754 | lr 0.000189818 | gnorm 0.875 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 73917
2022-03-05 09:55:09 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 09:55:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:57:07 | INFO | train_inner | epoch 289:     46 / 97 loss=2.661, nll_loss=0.818, ppl=1.76, wps=24928.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.866, loss_scale=8, train_wall=233, gb_free=21, wall=74034
2022-03-05 09:57:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:59:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:59:22 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 12.983 | nll_loss 12.181 | ppl 4642.22 | wps 45454.5 | wpb 510.9 | bsz 1 | num_updates 27850 | best_loss 8.489
2022-03-05 09:59:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27850 updates
2022-03-05 09:59:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:59:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 09:59:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 289 @ 27850 updates, score 12.983) (writing took 2.668531598523259 seconds)
2022-03-05 09:59:24 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 09:59:24 | INFO | train | epoch 289 | loss 2.662 | nll_loss 0.819 | ppl 1.76 | wps 24660.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27850 | lr 0.00018949 | gnorm 0.874 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 74171
2022-03-05 09:59:24 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 09:59:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:01:32 | INFO | train_inner | epoch 290:     50 / 97 loss=2.661, nll_loss=0.818, ppl=1.76, wps=24692.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.885, loss_scale=8, train_wall=235, gb_free=21, wall=74299
2022-03-05 10:03:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:03:37 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 13.025 | nll_loss 12.225 | ppl 4786.9 | wps 45852.4 | wpb 510.9 | bsz 1 | num_updates 27947 | best_loss 8.489
2022-03-05 10:03:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27947 updates
2022-03-05 10:03:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:03:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 290 @ 27947 updates, score 13.025) (writing took 2.901125450618565 seconds)
2022-03-05 10:03:40 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 10:03:40 | INFO | train | epoch 290 | loss 2.66 | nll_loss 0.818 | ppl 1.76 | wps 24890.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27947 | lr 0.000189161 | gnorm 0.874 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 74427
2022-03-05 10:03:40 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 10:03:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:05:55 | INFO | train_inner | epoch 291:     53 / 97 loss=2.66, nll_loss=0.817, ppl=1.76, wps=24912.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.867, loss_scale=16, train_wall=233, gb_free=21, wall=74562
2022-03-05 10:06:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:07:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:07:52 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 13 | nll_loss 12.201 | ppl 4707.26 | wps 45270.6 | wpb 510.9 | bsz 1 | num_updates 28043 | best_loss 8.489
2022-03-05 10:07:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28043 updates
2022-03-05 10:07:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:07:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:07:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 291 @ 28043 updates, score 13.0) (writing took 2.8634972339496017 seconds)
2022-03-05 10:07:55 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 10:07:55 | INFO | train | epoch 291 | loss 2.659 | nll_loss 0.816 | ppl 1.76 | wps 24630.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28043 | lr 0.000188837 | gnorm 0.877 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 74682
2022-03-05 10:07:55 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 10:07:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:10:20 | INFO | train_inner | epoch 292:     57 / 97 loss=2.658, nll_loss=0.815, ppl=1.76, wps=24673.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.874, loss_scale=8, train_wall=235, gb_free=21, wall=74828
2022-03-05 10:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:12:07 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 13.023 | nll_loss 12.23 | ppl 4802.44 | wps 45310.2 | wpb 510.9 | bsz 1 | num_updates 28140 | best_loss 8.489
2022-03-05 10:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28140 updates
2022-03-05 10:12:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:12:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:12:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 292 @ 28140 updates, score 13.023) (writing took 2.8512950614094734 seconds)
2022-03-05 10:12:10 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 10:12:10 | INFO | train | epoch 292 | loss 2.657 | nll_loss 0.814 | ppl 1.76 | wps 24888.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28140 | lr 0.000188512 | gnorm 0.872 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 74937
2022-03-05 10:12:10 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 10:12:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:14:43 | INFO | train_inner | epoch 293:     60 / 97 loss=2.655, nll_loss=0.813, ppl=1.76, wps=24903.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.875, loss_scale=16, train_wall=233, gb_free=21, wall=75091
2022-03-05 10:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:16:23 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 13.032 | nll_loss 12.234 | ppl 4818.01 | wps 45378.9 | wpb 510.9 | bsz 1 | num_updates 28237 | best_loss 8.489
2022-03-05 10:16:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28237 updates
2022-03-05 10:16:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:16:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:16:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 293 @ 28237 updates, score 13.032) (writing took 2.83802804723382 seconds)
2022-03-05 10:16:25 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 10:16:25 | INFO | train | epoch 293 | loss 2.655 | nll_loss 0.813 | ppl 1.76 | wps 24882.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28237 | lr 0.000188187 | gnorm 0.876 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75193
2022-03-05 10:16:25 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 10:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:16:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:19:09 | INFO | train_inner | epoch 294:     64 / 97 loss=2.656, nll_loss=0.813, ppl=1.76, wps=24691.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.887, loss_scale=8, train_wall=235, gb_free=21, wall=75356
2022-03-05 10:20:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:20:38 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 13.05 | nll_loss 12.26 | ppl 4904 | wps 45186.1 | wpb 510.9 | bsz 1 | num_updates 28333 | best_loss 8.489
2022-03-05 10:20:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28333 updates
2022-03-05 10:20:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:20:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:20:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 294 @ 28333 updates, score 13.05) (writing took 2.7864458998665214 seconds)
2022-03-05 10:20:40 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 10:20:40 | INFO | train | epoch 294 | loss 2.654 | nll_loss 0.811 | ppl 1.75 | wps 24662.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28333 | lr 0.000187868 | gnorm 0.884 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 75447
2022-03-05 10:20:40 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 10:20:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:23:32 | INFO | train_inner | epoch 295:     67 / 97 loss=2.654, nll_loss=0.811, ppl=1.75, wps=24918.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.879, loss_scale=16, train_wall=233, gb_free=21, wall=75619
2022-03-05 10:24:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:24:53 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 13.022 | nll_loss 12.228 | ppl 4795.62 | wps 45233.4 | wpb 510.9 | bsz 1 | num_updates 28430 | best_loss 8.489
2022-03-05 10:24:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28430 updates
2022-03-05 10:24:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:24:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:24:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 295 @ 28430 updates, score 13.022) (writing took 2.794021999463439 seconds)
2022-03-05 10:24:56 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 10:24:56 | INFO | train | epoch 295 | loss 2.653 | nll_loss 0.811 | ppl 1.75 | wps 24886 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28430 | lr 0.000187548 | gnorm 0.878 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75703
2022-03-05 10:24:56 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 10:24:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:27:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:27:57 | INFO | train_inner | epoch 296:     71 / 97 loss=2.652, nll_loss=0.81, ppl=1.75, wps=24675.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.862, loss_scale=16, train_wall=235, gb_free=21, wall=75884
2022-03-05 10:29:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:29:08 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 12.989 | nll_loss 12.191 | ppl 4675.37 | wps 45239.2 | wpb 510.9 | bsz 1 | num_updates 28526 | best_loss 8.489
2022-03-05 10:29:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28526 updates
2022-03-05 10:29:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:29:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:29:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 296 @ 28526 updates, score 12.989) (writing took 2.7967610312625766 seconds)
2022-03-05 10:29:11 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 10:29:11 | INFO | train | epoch 296 | loss 2.65 | nll_loss 0.808 | ppl 1.75 | wps 24641.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28526 | lr 0.000187232 | gnorm 0.86 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75958
2022-03-05 10:29:11 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 10:29:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:32:20 | INFO | train_inner | epoch 297:     74 / 97 loss=2.649, nll_loss=0.806, ppl=1.75, wps=24919.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28600, lr=0.000186989, gnorm=0.855, loss_scale=16, train_wall=233, gb_free=21, wall=76147
2022-03-05 10:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:33:23 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 12.995 | nll_loss 12.196 | ppl 4692.17 | wps 45302.6 | wpb 510.9 | bsz 1 | num_updates 28623 | best_loss 8.489
2022-03-05 10:33:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28623 updates
2022-03-05 10:33:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:33:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:33:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 297 @ 28623 updates, score 12.995) (writing took 2.758371635340154 seconds)
2022-03-05 10:33:26 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 10:33:26 | INFO | train | epoch 297 | loss 2.65 | nll_loss 0.807 | ppl 1.75 | wps 24898.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28623 | lr 0.000186914 | gnorm 0.858 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76213
2022-03-05 10:33:26 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 10:33:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:33:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:36:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:36:48 | INFO | train_inner | epoch 298:     79 / 97 loss=2.65, nll_loss=0.808, ppl=1.75, wps=24451.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.878, loss_scale=8, train_wall=238, gb_free=21, wall=76415
2022-03-05 10:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:37:38 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 12.985 | nll_loss 12.183 | ppl 4649.15 | wps 45297 | wpb 510.9 | bsz 1 | num_updates 28718 | best_loss 8.489
2022-03-05 10:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28718 updates
2022-03-05 10:37:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:37:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 298 @ 28718 updates, score 12.985) (writing took 2.8679507076740265 seconds)
2022-03-05 10:37:41 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 10:37:41 | INFO | train | epoch 298 | loss 2.648 | nll_loss 0.806 | ppl 1.75 | wps 24382.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 28718 | lr 0.000186605 | gnorm 0.88 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 76468
2022-03-05 10:37:41 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 10:37:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:41:10 | INFO | train_inner | epoch 299:     82 / 97 loss=2.647, nll_loss=0.805, ppl=1.75, wps=24932, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28800, lr=0.000186339, gnorm=0.869, loss_scale=8, train_wall=233, gb_free=21, wall=76677
2022-03-05 10:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:41:53 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 13.037 | nll_loss 12.244 | ppl 4850.12 | wps 45525.3 | wpb 510.9 | bsz 1 | num_updates 28815 | best_loss 8.489
2022-03-05 10:41:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28815 updates
2022-03-05 10:41:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:41:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 299 @ 28815 updates, score 13.037) (writing took 2.813197623938322 seconds)
2022-03-05 10:41:56 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 10:41:56 | INFO | train | epoch 299 | loss 2.647 | nll_loss 0.805 | ppl 1.75 | wps 24916 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28815 | lr 0.00018629 | gnorm 0.864 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 76723
2022-03-05 10:41:56 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 10:41:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:45:33 | INFO | train_inner | epoch 300:     85 / 97 loss=2.646, nll_loss=0.804, ppl=1.75, wps=24924.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.866, loss_scale=16, train_wall=233, gb_free=21, wall=76940
2022-03-05 10:46:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:46:08 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 13.043 | nll_loss 12.244 | ppl 4852.01 | wps 45389.5 | wpb 510.9 | bsz 1 | num_updates 28912 | best_loss 8.489
2022-03-05 10:46:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28912 updates
2022-03-05 10:46:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:46:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:46:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 300 @ 28912 updates, score 13.043) (writing took 2.819739674217999 seconds)
2022-03-05 10:46:11 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 10:46:11 | INFO | train | epoch 300 | loss 2.646 | nll_loss 0.803 | ppl 1.75 | wps 24893.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28912 | lr 0.000185978 | gnorm 0.869 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76978
2022-03-05 10:46:11 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 10:46:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:47:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:49:58 | INFO | train_inner | epoch 301:     89 / 97 loss=2.646, nll_loss=0.804, ppl=1.75, wps=24681.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29000, lr=0.000185695, gnorm=0.869, loss_scale=8, train_wall=235, gb_free=21, wall=77205
2022-03-05 10:50:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:50:24 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 13.053 | nll_loss 12.262 | ppl 4913.15 | wps 45349.8 | wpb 510.9 | bsz 1 | num_updates 29008 | best_loss 8.489
2022-03-05 10:50:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 29008 updates
2022-03-05 10:50:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 301 @ 29008 updates, score 13.053) (writing took 2.7732924101874232 seconds)
2022-03-05 10:50:26 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 10:50:26 | INFO | train | epoch 301 | loss 2.644 | nll_loss 0.802 | ppl 1.74 | wps 24652.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29008 | lr 0.00018567 | gnorm 0.867 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 77233
2022-03-05 10:50:26 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 10:50:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:54:21 | INFO | train_inner | epoch 302:     92 / 97 loss=2.643, nll_loss=0.801, ppl=1.74, wps=24925.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29100, lr=0.000185376, gnorm=0.862, loss_scale=16, train_wall=233, gb_free=21, wall=77468
2022-03-05 10:54:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:54:39 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 13.046 | nll_loss 12.255 | ppl 4887.25 | wps 45268.6 | wpb 510.9 | bsz 1 | num_updates 29105 | best_loss 8.489
2022-03-05 10:54:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29105 updates
2022-03-05 10:54:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:54:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:54:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 302 @ 29105 updates, score 13.046) (writing took 2.7833510106429458 seconds)
2022-03-05 10:54:41 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 10:54:41 | INFO | train | epoch 302 | loss 2.643 | nll_loss 0.801 | ppl 1.74 | wps 24899.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29105 | lr 0.00018536 | gnorm 0.864 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77489
2022-03-05 10:54:41 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 10:54:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:56:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:58:46 | INFO | train_inner | epoch 303:     96 / 97 loss=2.642, nll_loss=0.8, ppl=1.74, wps=24690.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29200, lr=0.000185058, gnorm=0.857, loss_scale=8, train_wall=235, gb_free=21, wall=77733
2022-03-05 10:58:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:58:54 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 13.059 | nll_loss 12.266 | ppl 4925.95 | wps 45195.4 | wpb 510.9 | bsz 1 | num_updates 29201 | best_loss 8.489
2022-03-05 10:58:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29201 updates
2022-03-05 10:58:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:58:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 10:58:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 303 @ 29201 updates, score 13.059) (writing took 2.664456815458834 seconds)
2022-03-05 10:58:56 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 10:58:56 | INFO | train | epoch 303 | loss 2.641 | nll_loss 0.798 | ppl 1.74 | wps 24660.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29201 | lr 0.000185055 | gnorm 0.855 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 77743
2022-03-05 10:58:56 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 10:58:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:02:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:03:09 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 13.071 | nll_loss 12.285 | ppl 4990.01 | wps 45273.1 | wpb 510.9 | bsz 1 | num_updates 29297 | best_loss 8.489
2022-03-05 11:03:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29297 updates
2022-03-05 11:03:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:03:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:03:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 304 @ 29297 updates, score 13.071) (writing took 2.813327278010547 seconds)
2022-03-05 11:03:12 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 11:03:12 | INFO | train | epoch 304 | loss 2.641 | nll_loss 0.799 | ppl 1.74 | wps 24627.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29297 | lr 0.000184752 | gnorm 0.877 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 77999
2022-03-05 11:03:12 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 11:03:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:03:19 | INFO | train_inner | epoch 305:      3 / 97 loss=2.641, nll_loss=0.799, ppl=1.74, wps=23973.9, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=29300, lr=0.000184742, gnorm=0.877, loss_scale=8, train_wall=235, gb_free=21, wall=78006
2022-03-05 11:07:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:07:24 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 12.947 | nll_loss 12.142 | ppl 4520.86 | wps 45315.5 | wpb 510.9 | bsz 1 | num_updates 29394 | best_loss 8.489
2022-03-05 11:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29394 updates
2022-03-05 11:07:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:07:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:07:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 305 @ 29394 updates, score 12.947) (writing took 2.6312776170670986 seconds)
2022-03-05 11:07:27 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 11:07:27 | INFO | train | epoch 305 | loss 2.639 | nll_loss 0.797 | ppl 1.74 | wps 24924.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29394 | lr 0.000184447 | gnorm 0.875 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 78254
2022-03-05 11:07:27 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 11:07:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:07:42 | INFO | train_inner | epoch 306:      6 / 97 loss=2.638, nll_loss=0.796, ppl=1.74, wps=24947, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.874, loss_scale=8, train_wall=233, gb_free=21, wall=78269
2022-03-05 11:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:11:39 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 13.071 | nll_loss 12.276 | ppl 4958.33 | wps 45312.5 | wpb 510.9 | bsz 1 | num_updates 29491 | best_loss 8.489
2022-03-05 11:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29491 updates
2022-03-05 11:11:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:11:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 306 @ 29491 updates, score 13.071) (writing took 2.8649216182529926 seconds)
2022-03-05 11:11:42 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 11:11:42 | INFO | train | epoch 306 | loss 2.636 | nll_loss 0.794 | ppl 1.73 | wps 24896.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29491 | lr 0.000184143 | gnorm 0.864 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78509
2022-03-05 11:11:42 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 11:11:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:12:05 | INFO | train_inner | epoch 307:      9 / 97 loss=2.636, nll_loss=0.793, ppl=1.73, wps=24919.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.865, loss_scale=16, train_wall=233, gb_free=21, wall=78532
2022-03-05 11:13:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:15:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:15:54 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 13.062 | nll_loss 12.275 | ppl 4956.11 | wps 45168.1 | wpb 510.9 | bsz 1 | num_updates 29587 | best_loss 8.489
2022-03-05 11:15:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29587 updates
2022-03-05 11:15:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:15:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:15:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 307 @ 29587 updates, score 13.062) (writing took 2.733431374654174 seconds)
2022-03-05 11:15:57 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 11:15:57 | INFO | train | epoch 307 | loss 2.635 | nll_loss 0.793 | ppl 1.73 | wps 24645.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29587 | lr 0.000183844 | gnorm 0.862 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78764
2022-03-05 11:15:57 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 11:15:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:16:30 | INFO | train_inner | epoch 308:     13 / 97 loss=2.635, nll_loss=0.793, ppl=1.73, wps=24676.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.862, loss_scale=16, train_wall=235, gb_free=21, wall=78797
2022-03-05 11:18:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:20:09 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 12.983 | nll_loss 12.187 | ppl 4662.01 | wps 45327.6 | wpb 510.9 | bsz 1 | num_updates 29683 | best_loss 8.489
2022-03-05 11:20:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29683 updates
2022-03-05 11:20:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:20:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 308 @ 29683 updates, score 12.983) (writing took 2.7615056540817022 seconds)
2022-03-05 11:20:12 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 11:20:12 | INFO | train | epoch 308 | loss 2.634 | nll_loss 0.792 | ppl 1.73 | wps 24634.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29683 | lr 0.000183546 | gnorm 0.863 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 79019
2022-03-05 11:20:12 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 11:20:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:20:55 | INFO | train_inner | epoch 309:     17 / 97 loss=2.633, nll_loss=0.791, ppl=1.73, wps=24687, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.859, loss_scale=8, train_wall=235, gb_free=21, wall=79063
2022-03-05 11:24:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:24:24 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 13.025 | nll_loss 12.229 | ppl 4799.58 | wps 45234.2 | wpb 510.9 | bsz 1 | num_updates 29780 | best_loss 8.489
2022-03-05 11:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 29780 updates
2022-03-05 11:24:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:24:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt
2022-03-05 11:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.08_#1/checkpoint_last.pt (epoch 309 @ 29780 updates, score 13.025) (writing took 2.745648347772658 seconds)
2022-03-05 11:24:27 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-03-05 11:24:27 | INFO | train | epoch 309 | loss 2.634 | nll_loss 0.792 | ppl 1.73 | wps 24902 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29780 | lr 0.000183247 | gnorm 0.859 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 79274
2022-03-05 11:24:27 | INFO | fairseq.trainer | begin training epoch 310
2022-03-05 11:24:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:25:18 | INFO | train_inner | epoch 310:     20 / 97 loss=2.633, nll_loss=0.791, ppl=1.73, wps=24923.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29800, lr=0.000183186, gnorm=0.86, loss_scale=16, train_wall=233, gb_free=21, wall=79325
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4216, in multi_head_attention_forward
    k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
