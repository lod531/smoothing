Sender: LSF System <lsfadmin@eu-g3-012>
Subject: Job 208722424: <de_dropout_0.3_jelinek_0.03_0.02_0.95_#1> in cluster <euler> Done

Job <de_dropout_0.3_jelinek_0.03_0.02_0.95_#1> was submitted from host <eu-login-24> by user <andriusb> in cluster <euler> at Tue Mar 15 16:18:16 2022
Job was executed on host(s) <eu-g3-012>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 16:32:50 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 16:32:50 2022
Terminated at Wed Mar 16 13:35:11 2022
Results reported at Wed Mar 16 13:35:11 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/de --save-dir /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.03,0.02, 0.95)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   75674.64 sec.
    Max Memory :                                 3853 MB
    Average Memory :                             2484.72 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16147.00 MB
    Max Swap :                                   576 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   75740 sec.
    Turnaround time :                            76615 sec.

The output (if any) follows:

2022-03-15 16:33:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/de', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.03,0.02, 0.95)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 16:33:06 | INFO | fairseq.tasks.language_modeling | dictionary: 34528 types
2022-03-15 16:33:07 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
Calculating frequency stats:
  0%|          | 0/45920 [00:00<?, ?it/s]  0%|          | 57/45920 [00:00<01:22, 554.35it/s]  0%|          | 140/45920 [00:00<01:07, 674.68it/s]  0%|          | 208/45920 [00:00<01:12, 634.14it/s]  1%|          | 272/45920 [00:00<01:25, 534.28it/s]  1%|          | 328/45920 [00:00<01:24, 538.44it/s]  1%|          | 416/45920 [00:00<01:10, 641.68it/s]  1%|          | 483/45920 [00:00<01:12, 623.27it/s]  1%|          | 567/45920 [00:00<01:06, 685.99it/s]  1%|▏         | 638/45920 [00:01<01:08, 662.67it/s]  2%|▏         | 706/45920 [00:01<01:29, 507.13it/s]  2%|▏         | 763/45920 [00:01<01:27, 518.14it/s]  2%|▏         | 826/45920 [00:01<01:22, 543.54it/s]  2%|▏         | 884/45920 [00:01<01:22, 548.31it/s]  2%|▏         | 964/45920 [00:01<01:13, 615.74it/s]  2%|▏         | 1039/45920 [00:01<01:09, 647.19it/s]  2%|▏         | 1114/45920 [00:01<01:06, 675.57it/s]  3%|▎         | 1205/45920 [00:01<01:00, 741.96it/s]  3%|▎         | 1281/45920 [00:02<01:01, 727.64it/s]  3%|▎         | 1358/45920 [00:02<01:00, 738.89it/s]  3%|▎         | 1433/45920 [00:02<01:00, 740.21it/s]  3%|▎         | 1521/45920 [00:02<00:57, 778.81it/s]  3%|▎         | 1600/45920 [00:02<00:56, 778.42it/s]  4%|▎         | 1679/45920 [00:02<01:08, 649.59it/s]  4%|▍         | 1765/45920 [00:02<01:03, 698.18it/s]  4%|▍         | 1849/45920 [00:02<00:59, 735.46it/s]  4%|▍         | 1926/45920 [00:02<00:59, 735.35it/s]  4%|▍         | 2007/45920 [00:03<00:58, 755.97it/s]  5%|▍         | 2085/45920 [00:03<01:00, 719.61it/s]  5%|▍         | 2159/45920 [00:03<01:04, 675.38it/s]  5%|▍         | 2228/45920 [00:03<01:05, 672.17it/s]  5%|▌         | 2297/45920 [00:03<01:11, 606.86it/s]  5%|▌         | 2369/45920 [00:03<01:08, 632.13it/s]  5%|▌         | 2434/45920 [00:03<01:08, 630.40it/s]  5%|▌         | 2515/45920 [00:03<01:04, 678.06it/s]  6%|▌         | 2584/45920 [00:03<01:07, 640.41it/s]  6%|▌         | 2658/45920 [00:04<01:04, 667.23it/s]  6%|▌         | 2740/45920 [00:04<01:00, 708.99it/s]  6%|▌         | 2812/45920 [00:04<01:04, 663.53it/s]  6%|▋         | 2909/45920 [00:04<00:57, 746.35it/s]  7%|▋         | 2986/45920 [00:04<00:57, 745.51it/s]  7%|▋         | 3062/45920 [00:04<01:06, 642.33it/s]  7%|▋         | 3130/45920 [00:04<01:08, 627.40it/s]  7%|▋         | 3195/45920 [00:04<01:08, 628.11it/s]  7%|▋         | 3275/45920 [00:04<01:03, 671.66it/s]  7%|▋         | 3344/45920 [00:05<01:04, 658.77it/s]  7%|▋         | 3411/45920 [00:05<01:04, 658.25it/s]  8%|▊         | 3478/45920 [00:05<01:06, 635.86it/s]  8%|▊         | 3543/45920 [00:05<01:06, 632.70it/s]  8%|▊         | 3607/45920 [00:05<01:14, 571.25it/s]  8%|▊         | 3666/45920 [00:05<01:15, 560.79it/s]  8%|▊         | 3723/45920 [00:05<01:15, 560.81it/s]  8%|▊         | 3792/45920 [00:05<01:20, 525.70it/s]  8%|▊         | 3861/45920 [00:05<01:14, 568.09it/s]  9%|▊         | 3925/45920 [00:06<01:11, 586.85it/s]  9%|▊         | 3985/45920 [00:06<01:17, 543.28it/s]  9%|▉         | 4048/45920 [00:06<01:13, 565.99it/s]  9%|▉         | 4106/45920 [00:06<01:17, 537.40it/s]  9%|▉         | 4161/45920 [00:06<01:21, 513.55it/s]  9%|▉         | 4232/45920 [00:06<01:13, 565.76it/s]  9%|▉         | 4290/45920 [00:06<01:27, 473.54it/s]  9%|▉         | 4344/45920 [00:06<01:25, 488.71it/s] 10%|▉         | 4420/45920 [00:07<01:14, 558.79it/s] 10%|▉         | 4498/45920 [00:07<01:06, 618.61it/s] 10%|▉         | 4579/45920 [00:07<01:01, 667.42it/s] 10%|█         | 4648/45920 [00:07<01:11, 575.03it/s] 10%|█         | 4731/45920 [00:07<01:04, 638.33it/s] 10%|█         | 4812/45920 [00:07<01:02, 657.49it/s] 11%|█         | 4889/45920 [00:07<01:00, 677.32it/s] 11%|█         | 4970/45920 [00:07<00:58, 702.57it/s] 11%|█         | 5050/45920 [00:07<00:56, 724.66it/s] 11%|█         | 5124/45920 [00:08<01:03, 642.08it/s] 11%|█▏        | 5191/45920 [00:08<01:07, 604.40it/s] 11%|█▏        | 5254/45920 [00:08<01:06, 608.50it/s] 12%|█▏        | 5317/45920 [00:08<01:06, 610.54it/s] 12%|█▏        | 5404/45920 [00:08<00:59, 682.15it/s] 12%|█▏        | 5474/45920 [00:08<01:03, 635.90it/s] 12%|█▏        | 5539/45920 [00:08<01:03, 637.00it/s] 12%|█▏        | 5604/45920 [00:08<01:05, 613.62it/s] 12%|█▏        | 5667/45920 [00:08<01:07, 598.63it/s] 12%|█▏        | 5737/45920 [00:09<01:04, 623.42it/s] 13%|█▎        | 5800/45920 [00:09<01:08, 584.57it/s] 13%|█▎        | 5888/45920 [00:09<01:00, 663.79it/s] 13%|█▎        | 5956/45920 [00:09<01:05, 606.72it/s] 13%|█▎        | 6023/45920 [00:09<01:04, 623.05it/s] 13%|█▎        | 6087/45920 [00:09<01:08, 577.45it/s] 13%|█▎        | 6147/45920 [00:09<01:08, 582.34it/s] 14%|█▎        | 6218/45920 [00:09<01:07, 588.56it/s] 14%|█▎        | 6291/45920 [00:09<01:03, 626.57it/s] 14%|█▍        | 6364/45920 [00:10<01:00, 654.85it/s] 14%|█▍        | 6453/45920 [00:10<00:54, 717.87it/s] 14%|█▍        | 6526/45920 [00:10<00:56, 700.95it/s] 14%|█▍        | 6597/45920 [00:10<01:03, 617.96it/s] 15%|█▍        | 6661/45920 [00:10<01:03, 620.30it/s] 15%|█▍        | 6726/45920 [00:10<01:02, 626.03it/s] 15%|█▍        | 6802/45920 [00:10<00:59, 662.35it/s] 15%|█▍        | 6870/45920 [00:10<01:00, 645.28it/s] 15%|█▌        | 6947/45920 [00:10<00:57, 673.08it/s] 15%|█▌        | 7036/45920 [00:11<00:53, 730.94it/s] 15%|█▌        | 7114/45920 [00:11<00:52, 744.71it/s] 16%|█▌        | 7189/45920 [00:11<00:54, 709.48it/s] 16%|█▌        | 7261/45920 [00:11<00:57, 671.52it/s] 16%|█▌        | 7329/45920 [00:11<01:02, 616.77it/s] 16%|█▌        | 7392/45920 [00:11<01:02, 614.15it/s] 16%|█▋        | 7463/45920 [00:11<01:00, 638.84it/s] 16%|█▋        | 7528/45920 [00:11<01:02, 612.99it/s] 17%|█▋        | 7590/45920 [00:12<01:17, 494.52it/s] 17%|█▋        | 7644/45920 [00:12<01:39, 385.92it/s] 17%|█▋        | 7720/45920 [00:12<01:22, 463.43it/s] 17%|█▋        | 7774/45920 [00:12<01:24, 453.26it/s] 17%|█▋        | 7849/45920 [00:12<01:13, 518.90it/s] 17%|█▋        | 7929/45920 [00:12<01:04, 588.31it/s] 17%|█▋        | 7995/45920 [00:12<01:03, 600.17it/s] 18%|█▊        | 8059/45920 [00:12<01:03, 595.78it/s] 18%|█▊        | 8132/45920 [00:13<00:59, 631.79it/s] 18%|█▊        | 8198/45920 [00:13<01:02, 601.48it/s] 18%|█▊        | 8278/45920 [00:13<00:58, 648.12it/s] 18%|█▊        | 8358/45920 [00:13<00:54, 689.40it/s] 18%|█▊        | 8437/45920 [00:13<00:52, 716.79it/s] 19%|█▊        | 8510/45920 [00:13<01:00, 614.40it/s] 19%|█▊        | 8586/45920 [00:13<00:57, 651.95it/s] 19%|█▉        | 8656/45920 [00:13<00:56, 663.74it/s] 19%|█▉        | 8725/45920 [00:13<01:01, 607.89it/s] 19%|█▉        | 8800/45920 [00:14<00:57, 644.90it/s] 19%|█▉        | 8867/45920 [00:14<00:57, 639.77it/s] 19%|█▉        | 8933/45920 [00:14<01:02, 588.74it/s] 20%|█▉        | 9009/45920 [00:14<00:58, 632.85it/s] 20%|█▉        | 9091/45920 [00:14<00:54, 679.09it/s] 20%|█▉        | 9161/45920 [00:14<00:58, 631.09it/s] 20%|██        | 9230/45920 [00:14<00:56, 643.92it/s] 20%|██        | 9298/45920 [00:14<00:56, 653.37it/s] 20%|██        | 9372/45920 [00:14<00:54, 676.24it/s] 21%|██        | 9441/45920 [00:15<00:56, 645.88it/s] 21%|██        | 9515/45920 [00:15<00:54, 663.44it/s] 21%|██        | 9582/45920 [00:15<01:01, 594.43it/s] 21%|██        | 9644/45920 [00:15<01:03, 568.48it/s] 21%|██        | 9702/45920 [00:15<01:04, 565.87it/s] 21%|██▏       | 9767/45920 [00:15<01:01, 587.62it/s] 21%|██▏       | 9829/45920 [00:15<01:00, 596.35it/s] 22%|██▏       | 9890/45920 [00:15<01:04, 560.14it/s] 22%|██▏       | 9966/45920 [00:15<00:58, 613.89it/s] 22%|██▏       | 10029/45920 [00:16<01:00, 591.61it/s] 22%|██▏       | 10115/45920 [00:16<00:53, 663.45it/s] 22%|██▏       | 10183/45920 [00:16<01:03, 565.78it/s] 22%|██▏       | 10257/45920 [00:16<00:58, 609.63it/s] 23%|██▎       | 10346/45920 [00:16<00:52, 679.54it/s] 23%|██▎       | 10417/45920 [00:16<00:54, 657.36it/s] 23%|██▎       | 10509/45920 [00:16<00:54, 644.39it/s] 23%|██▎       | 10584/45920 [00:16<00:53, 666.34it/s] 23%|██▎       | 10653/45920 [00:17<00:58, 602.74it/s] 23%|██▎       | 10733/45920 [00:17<00:53, 652.76it/s] 24%|██▎       | 10808/45920 [00:17<00:51, 677.78it/s] 24%|██▎       | 10878/45920 [00:17<00:58, 596.50it/s] 24%|██▍       | 10941/45920 [00:17<00:58, 602.81it/s] 24%|██▍       | 11018/45920 [00:17<00:54, 642.91it/s] 24%|██▍       | 11085/45920 [00:17<00:57, 610.62it/s] 24%|██▍       | 11156/45920 [00:17<00:54, 634.02it/s] 24%|██▍       | 11228/45920 [00:17<00:53, 642.52it/s] 25%|██▍       | 11315/45920 [00:18<00:49, 693.03it/s] 25%|██▍       | 11386/45920 [00:18<00:58, 585.49it/s] 25%|██▍       | 11467/45920 [00:18<00:53, 639.35it/s] 25%|██▌       | 11535/45920 [00:18<00:56, 609.75it/s] 25%|██▌       | 11628/45920 [00:18<00:52, 656.12it/s] 25%|██▌       | 11696/45920 [00:18<01:00, 568.79it/s] 26%|██▌       | 11756/45920 [00:18<01:07, 509.40it/s] 26%|██▌       | 11810/45920 [00:18<01:07, 506.09it/s] 26%|██▌       | 11875/45920 [00:19<01:02, 540.56it/s] 26%|██▌       | 11937/45920 [00:19<01:01, 556.35it/s] 26%|██▌       | 11999/45920 [00:19<01:00, 564.18it/s] 26%|██▋       | 12064/45920 [00:19<00:57, 586.72it/s] 26%|██▋       | 12124/45920 [00:19<01:09, 488.55it/s] 27%|██▋       | 12198/45920 [00:19<01:01, 549.43it/s] 27%|██▋       | 12263/45920 [00:19<00:58, 574.51it/s] 27%|██▋       | 12346/45920 [00:19<00:52, 643.84it/s] 27%|██▋       | 12433/45920 [00:19<00:47, 703.97it/s] 27%|██▋       | 12506/45920 [00:20<00:47, 709.17it/s] 27%|██▋       | 12579/45920 [00:20<00:48, 687.31it/s] 28%|██▊       | 12657/45920 [00:20<00:46, 710.47it/s] 28%|██▊       | 12729/45920 [00:20<00:49, 673.44it/s] 28%|██▊       | 12798/45920 [00:20<00:59, 555.83it/s] 28%|██▊       | 12890/45920 [00:20<00:51, 644.85it/s] 28%|██▊       | 12960/45920 [00:20<00:53, 619.23it/s] 28%|██▊       | 13026/45920 [00:20<00:52, 629.32it/s] 29%|██▊       | 13098/45920 [00:21<00:50, 648.27it/s] 29%|██▊       | 13180/45920 [00:21<00:47, 695.16it/s] 29%|██▉       | 13252/45920 [00:21<00:47, 688.97it/s] 29%|██▉       | 13330/45920 [00:21<00:45, 714.27it/s] 29%|██▉       | 13409/45920 [00:21<00:44, 735.02it/s] 29%|██▉       | 13484/45920 [00:21<00:50, 645.37it/s] 30%|██▉       | 13551/45920 [00:21<00:50, 642.10it/s] 30%|██▉       | 13622/45920 [00:21<00:50, 644.26it/s] 30%|██▉       | 13688/45920 [00:21<00:51, 626.08it/s] 30%|██▉       | 13759/45920 [00:22<00:49, 646.23it/s] 30%|███       | 13846/45920 [00:22<00:45, 705.96it/s] 30%|███       | 13918/45920 [00:22<00:46, 690.67it/s] 30%|███       | 13988/45920 [00:22<00:50, 634.98it/s] 31%|███       | 14053/45920 [00:22<00:50, 633.80it/s] 31%|███       | 14118/45920 [00:22<00:59, 530.84it/s] 31%|███       | 14184/45920 [00:22<00:56, 562.38it/s] 31%|███       | 14244/45920 [00:22<01:00, 525.12it/s] 31%|███       | 14322/45920 [00:22<00:53, 587.92it/s] 31%|███▏      | 14384/45920 [00:23<00:55, 566.98it/s] 31%|███▏      | 14449/45920 [00:23<00:53, 588.90it/s] 32%|███▏      | 14518/45920 [00:23<00:51, 611.26it/s] 32%|███▏      | 14581/45920 [00:23<00:58, 531.23it/s] 32%|███▏      | 14661/45920 [00:23<00:52, 598.61it/s] 32%|███▏      | 14724/45920 [00:23<00:55, 559.72it/s] 32%|███▏      | 14783/45920 [00:23<00:56, 548.99it/s] 32%|███▏      | 14851/45920 [00:23<00:53, 581.69it/s] 33%|███▎      | 14925/45920 [00:23<00:50, 619.12it/s] 33%|███▎      | 14989/45920 [00:24<00:49, 621.09it/s] 33%|███▎      | 15053/45920 [00:24<00:52, 592.55it/s] 33%|███▎      | 15114/45920 [00:24<01:00, 507.92it/s] 33%|███▎      | 15193/45920 [00:24<00:53, 576.71it/s] 33%|███▎      | 15268/45920 [00:24<00:52, 579.20it/s] 33%|███▎      | 15331/45920 [00:24<00:51, 590.88it/s] 34%|███▎      | 15392/45920 [00:24<00:56, 542.91it/s] 34%|███▎      | 15448/45920 [00:24<00:56, 538.06it/s] 34%|███▍      | 15522/45920 [00:25<00:51, 592.03it/s] 34%|███▍      | 15604/45920 [00:25<00:46, 647.00it/s] 34%|███▍      | 15670/45920 [00:25<00:51, 584.53it/s] 34%|███▍      | 15749/45920 [00:25<00:47, 637.49it/s] 34%|███▍      | 15815/45920 [00:25<00:51, 586.07it/s] 35%|███▍      | 15876/45920 [00:25<00:53, 560.03it/s] 35%|███▍      | 15943/45920 [00:25<00:51, 578.90it/s] 35%|███▍      | 16009/45920 [00:25<00:49, 600.42it/s] 35%|███▌      | 16086/45920 [00:25<00:46, 647.25it/s] 35%|███▌      | 16152/45920 [00:26<00:49, 604.86it/s] 35%|███▌      | 16226/45920 [00:26<00:46, 635.84it/s] 35%|███▌      | 16291/45920 [00:26<01:00, 489.15it/s] 36%|███▌      | 16373/45920 [00:26<00:52, 565.69it/s] 36%|███▌      | 16439/45920 [00:26<00:50, 587.79it/s] 36%|███▌      | 16504/45920 [00:26<00:50, 582.55it/s] 36%|███▌      | 16566/45920 [00:26<00:52, 561.74it/s] 36%|███▋      | 16655/45920 [00:26<00:45, 646.66it/s] 36%|███▋      | 16736/45920 [00:27<00:42, 686.81it/s] 37%|███▋      | 16807/45920 [00:27<00:50, 580.54it/s] 37%|███▋      | 16870/45920 [00:27<00:51, 561.87it/s] 37%|███▋      | 16930/45920 [00:27<00:57, 506.38it/s] 37%|███▋      | 16984/45920 [00:27<00:59, 489.92it/s] 37%|███▋      | 17087/45920 [00:27<00:46, 623.94it/s] 37%|███▋      | 17174/45920 [00:27<00:41, 688.09it/s] 38%|███▊      | 17247/45920 [00:27<00:42, 680.35it/s] 38%|███▊      | 17318/45920 [00:28<00:43, 658.88it/s] 38%|███▊      | 17386/45920 [00:28<00:42, 664.40it/s] 38%|███▊      | 17468/45920 [00:28<00:40, 707.93it/s] 38%|███▊      | 17557/45920 [00:28<00:37, 759.49it/s] 38%|███▊      | 17635/45920 [00:28<00:38, 738.29it/s] 39%|███▊      | 17710/45920 [00:28<00:39, 714.33it/s] 39%|███▊      | 17783/45920 [00:28<00:40, 695.49it/s] 39%|███▉      | 17854/45920 [00:28<00:41, 676.55it/s] 39%|███▉      | 17923/45920 [00:28<00:44, 626.19it/s] 39%|███▉      | 17987/45920 [00:29<00:48, 579.39it/s] 39%|███▉      | 18047/45920 [00:29<00:47, 583.57it/s] 39%|███▉      | 18135/45920 [00:29<00:41, 663.68it/s] 40%|███▉      | 18210/45920 [00:29<00:40, 685.15it/s] 40%|███▉      | 18289/45920 [00:29<00:38, 714.03it/s] 40%|███▉      | 18365/45920 [00:29<00:37, 725.48it/s] 40%|████      | 18439/45920 [00:29<00:40, 677.66it/s] 40%|████      | 18508/45920 [00:29<00:47, 577.44it/s] 40%|████      | 18570/45920 [00:29<00:46, 587.61it/s] 41%|████      | 18632/45920 [00:30<01:02, 438.33it/s] 41%|████      | 18707/45920 [00:30<00:53, 505.30it/s] 41%|████      | 18776/45920 [00:30<00:49, 548.23it/s] 41%|████      | 18862/45920 [00:30<00:43, 626.62it/s] 41%|████      | 18939/45920 [00:30<00:40, 663.35it/s] 41%|████▏     | 19024/45920 [00:30<00:37, 712.41it/s] 42%|████▏     | 19099/45920 [00:30<00:37, 708.39it/s] 42%|████▏     | 19173/45920 [00:30<00:40, 658.13it/s] 42%|████▏     | 19242/45920 [00:31<00:41, 649.63it/s] 42%|████▏     | 19309/45920 [00:31<00:41, 646.21it/s] 42%|████▏     | 19375/45920 [00:31<00:41, 632.51it/s] 42%|████▏     | 19446/45920 [00:31<00:40, 648.58it/s] 42%|████▏     | 19512/45920 [00:31<00:41, 631.23it/s] 43%|████▎     | 19605/45920 [00:31<00:36, 712.60it/s] 43%|████▎     | 19678/45920 [00:31<00:37, 702.54it/s] 43%|████▎     | 19749/45920 [00:31<00:38, 680.41it/s] 43%|████▎     | 19824/45920 [00:31<00:37, 695.83it/s] 43%|████▎     | 19894/45920 [00:32<00:43, 597.81it/s] 43%|████▎     | 19970/45920 [00:32<00:40, 639.43it/s] 44%|████▎     | 20048/45920 [00:32<00:38, 671.76it/s] 44%|████▍     | 20118/45920 [00:32<00:44, 580.90it/s] 44%|████▍     | 20206/45920 [00:32<00:39, 656.09it/s] 44%|████▍     | 20276/45920 [00:32<00:39, 645.86it/s] 44%|████▍     | 20350/45920 [00:32<00:38, 664.29it/s] 44%|████▍     | 20426/45920 [00:32<00:37, 684.48it/s] 45%|████▍     | 20496/45920 [00:32<00:38, 661.73it/s] 45%|████▍     | 20564/45920 [00:33<00:40, 627.53it/s] 45%|████▍     | 20628/45920 [00:33<00:42, 588.66it/s] 45%|████▌     | 20711/45920 [00:33<00:38, 652.51it/s] 45%|████▌     | 20797/45920 [00:33<00:35, 708.63it/s] 45%|████▌     | 20870/45920 [00:33<00:40, 615.21it/s] 46%|████▌     | 20942/45920 [00:33<00:38, 641.10it/s] 46%|████▌     | 21009/45920 [00:33<00:38, 645.74it/s] 46%|████▌     | 21076/45920 [00:33<00:47, 527.45it/s] 46%|████▌     | 21151/45920 [00:34<00:42, 578.70it/s] 46%|████▌     | 21214/45920 [00:34<00:43, 571.86it/s] 46%|████▋     | 21281/45920 [00:34<00:41, 596.50it/s] 46%|████▋     | 21344/45920 [00:34<00:44, 553.17it/s] 47%|████▋     | 21427/45920 [00:34<00:39, 622.44it/s] 47%|████▋     | 21492/45920 [00:34<00:40, 596.57it/s] 47%|████▋     | 21579/45920 [00:34<00:36, 669.26it/s] 47%|████▋     | 21649/45920 [00:34<00:38, 622.56it/s] 47%|████▋     | 21714/45920 [00:34<00:40, 593.97it/s] 47%|████▋     | 21789/45920 [00:35<00:43, 560.95it/s] 48%|████▊     | 21848/45920 [00:35<00:42, 566.12it/s] 48%|████▊     | 21906/45920 [00:35<00:43, 550.40it/s] 48%|████▊     | 21978/45920 [00:35<00:43, 553.04it/s] 48%|████▊     | 22034/45920 [00:35<00:44, 538.56it/s] 48%|████▊     | 22100/45920 [00:35<00:42, 558.60it/s] 48%|████▊     | 22189/45920 [00:35<00:36, 645.45it/s] 49%|████▊     | 22280/45920 [00:35<00:32, 718.84it/s] 49%|████▊     | 22354/45920 [00:35<00:34, 686.78it/s] 49%|████▉     | 22440/45920 [00:36<00:31, 734.25it/s] 49%|████▉     | 22515/45920 [00:36<00:32, 719.66it/s] 49%|████▉     | 22588/45920 [00:36<00:36, 642.62it/s] 49%|████▉     | 22656/45920 [00:36<00:35, 649.42it/s] 49%|████▉     | 22723/45920 [00:36<00:37, 619.05it/s] 50%|████▉     | 22807/45920 [00:36<00:34, 678.59it/s] 50%|████▉     | 22887/45920 [00:36<00:32, 711.05it/s] 50%|█████     | 22960/45920 [00:36<00:32, 704.27it/s] 50%|█████     | 23061/45920 [00:36<00:28, 790.48it/s] 50%|█████     | 23142/45920 [00:37<00:32, 691.73it/s] 51%|█████     | 23215/45920 [00:37<00:34, 660.97it/s] 51%|█████     | 23286/45920 [00:37<00:33, 671.42it/s] 51%|█████     | 23362/45920 [00:37<00:32, 692.96it/s] 51%|█████     | 23433/45920 [00:37<00:35, 632.25it/s] 51%|█████     | 23498/45920 [00:37<00:38, 586.25it/s] 51%|█████▏    | 23560/45920 [00:37<00:40, 549.00it/s] 51%|█████▏    | 23619/45920 [00:37<00:39, 558.90it/s] 52%|█████▏    | 23682/45920 [00:38<00:38, 577.51it/s] 52%|█████▏    | 23758/45920 [00:38<00:35, 625.39it/s] 52%|█████▏    | 23822/45920 [00:38<00:35, 622.84it/s] 52%|█████▏    | 23894/45920 [00:38<00:33, 648.62it/s] 52%|█████▏    | 23960/45920 [00:38<00:36, 599.75it/s] 52%|█████▏    | 24039/45920 [00:38<00:33, 651.70it/s] 53%|█████▎    | 24110/45920 [00:38<00:32, 664.67it/s] 53%|█████▎    | 24178/45920 [00:38<00:39, 556.69it/s] 53%|█████▎    | 24238/45920 [00:39<00:40, 529.09it/s] 53%|█████▎    | 24304/45920 [00:39<00:38, 561.70it/s] 53%|█████▎    | 24372/45920 [00:39<00:36, 591.07it/s] 53%|█████▎    | 24444/45920 [00:39<00:34, 621.53it/s] 53%|█████▎    | 24518/45920 [00:39<00:32, 653.45it/s] 54%|█████▎    | 24596/45920 [00:39<00:30, 689.23it/s] 54%|█████▍    | 24683/45920 [00:39<00:28, 738.97it/s] 54%|█████▍    | 24758/45920 [00:39<00:30, 700.70it/s] 54%|█████▍    | 24830/45920 [00:39<00:31, 679.66it/s] 54%|█████▍    | 24899/45920 [00:39<00:31, 664.67it/s] 54%|█████▍    | 24966/45920 [00:40<00:33, 634.19it/s] 55%|█████▍    | 25030/45920 [00:40<00:37, 559.47it/s] 55%|█████▍    | 25093/45920 [00:40<00:36, 577.10it/s] 55%|█████▍    | 25160/45920 [00:40<00:34, 599.02it/s] 55%|█████▍    | 25222/45920 [00:40<00:34, 592.03it/s] 55%|█████▌    | 25305/45920 [00:40<00:31, 657.58it/s] 55%|█████▌    | 25372/45920 [00:40<00:31, 657.56it/s] 55%|█████▌    | 25449/45920 [00:40<00:29, 683.51it/s] 56%|█████▌    | 25530/45920 [00:40<00:28, 719.93it/s] 56%|█████▌    | 25603/45920 [00:41<00:30, 661.70it/s] 56%|█████▌    | 25671/45920 [00:41<00:34, 590.47it/s] 56%|█████▌    | 25738/45920 [00:41<00:33, 610.41it/s] 56%|█████▌    | 25815/45920 [00:41<00:30, 653.42it/s] 56%|█████▋    | 25887/45920 [00:41<00:29, 669.68it/s] 57%|█████▋    | 25965/45920 [00:41<00:28, 696.91it/s] 57%|█████▋    | 26036/45920 [00:41<00:28, 692.14it/s] 57%|█████▋    | 26106/45920 [00:41<00:31, 623.79it/s] 57%|█████▋    | 26171/45920 [00:41<00:32, 602.92it/s] 57%|█████▋    | 26233/45920 [00:42<00:34, 575.36it/s] 57%|█████▋    | 26308/45920 [00:42<00:31, 618.71it/s] 57%|█████▋    | 26373/45920 [00:42<00:31, 627.04it/s] 58%|█████▊    | 26442/45920 [00:42<00:30, 643.25it/s] 58%|█████▊    | 26508/45920 [00:42<00:30, 642.54it/s] 58%|█████▊    | 26573/45920 [00:42<00:30, 631.71it/s] 58%|█████▊    | 26646/45920 [00:42<00:29, 657.79it/s] 58%|█████▊    | 26713/45920 [00:42<00:30, 635.91it/s] 58%|█████▊    | 26798/45920 [00:42<00:27, 695.50it/s] 59%|█████▊    | 26869/45920 [00:43<00:29, 649.89it/s] 59%|█████▊    | 26935/45920 [00:43<00:32, 586.89it/s] 59%|█████▉    | 27001/45920 [00:43<00:31, 604.75it/s] 59%|█████▉    | 27084/45920 [00:43<00:28, 659.73it/s] 59%|█████▉    | 27152/45920 [00:43<00:39, 471.37it/s] 59%|█████▉    | 27208/45920 [00:43<00:38, 482.43it/s] 59%|█████▉    | 27279/45920 [00:43<00:36, 517.00it/s] 60%|█████▉    | 27368/45920 [00:43<00:30, 607.91it/s] 60%|█████▉    | 27435/45920 [00:44<00:29, 621.16it/s] 60%|█████▉    | 27502/45920 [00:44<00:30, 602.52it/s] 60%|██████    | 27566/45920 [00:44<00:31, 581.91it/s] 60%|██████    | 27639/45920 [00:44<00:29, 615.11it/s] 60%|██████    | 27703/45920 [00:44<00:29, 609.34it/s] 60%|██████    | 27766/45920 [00:44<00:29, 614.62it/s] 61%|██████    | 27829/45920 [00:44<00:31, 575.72it/s] 61%|██████    | 27888/45920 [00:44<00:31, 567.93it/s] 61%|██████    | 27946/45920 [00:45<00:37, 485.12it/s] 61%|██████    | 28028/45920 [00:45<00:31, 567.37it/s] 61%|██████    | 28096/45920 [00:45<00:29, 596.37it/s] 61%|██████▏   | 28159/45920 [00:45<00:31, 567.89it/s] 61%|██████▏   | 28230/45920 [00:45<00:29, 604.98it/s] 62%|██████▏   | 28301/45920 [00:45<00:28, 619.04it/s] 62%|██████▏   | 28372/45920 [00:45<00:27, 644.05it/s] 62%|██████▏   | 28445/45920 [00:45<00:26, 667.82it/s] 62%|██████▏   | 28516/45920 [00:45<00:25, 673.19it/s] 62%|██████▏   | 28585/45920 [00:45<00:25, 677.08it/s] 62%|██████▏   | 28654/45920 [00:46<00:28, 602.64it/s] 63%|██████▎   | 28729/45920 [00:46<00:26, 641.32it/s] 63%|██████▎   | 28799/45920 [00:46<00:26, 656.39it/s] 63%|██████▎   | 28866/45920 [00:46<00:26, 648.77it/s] 63%|██████▎   | 28935/45920 [00:46<00:25, 658.06it/s] 63%|██████▎   | 29025/45920 [00:46<00:23, 727.50it/s] 63%|██████▎   | 29099/45920 [00:46<00:24, 689.87it/s] 64%|██████▎   | 29169/45920 [00:46<00:25, 656.50it/s] 64%|██████▎   | 29236/45920 [00:46<00:25, 649.90it/s] 64%|██████▍   | 29302/45920 [00:47<00:31, 527.01it/s] 64%|██████▍   | 29359/45920 [00:47<00:31, 520.50it/s] 64%|██████▍   | 29427/45920 [00:47<00:29, 560.62it/s] 64%|██████▍   | 29504/45920 [00:47<00:26, 614.21it/s] 64%|██████▍   | 29568/45920 [00:47<00:26, 616.77it/s] 65%|██████▍   | 29637/45920 [00:47<00:25, 635.07it/s] 65%|██████▍   | 29704/45920 [00:47<00:25, 644.32it/s] 65%|██████▍   | 29770/45920 [00:47<00:25, 635.22it/s] 65%|██████▍   | 29835/45920 [00:47<00:26, 617.53it/s] 65%|██████▌   | 29898/45920 [00:48<00:27, 589.09it/s] 65%|██████▌   | 29982/45920 [00:48<00:24, 655.79it/s] 65%|██████▌   | 30049/45920 [00:48<00:25, 612.42it/s] 66%|██████▌   | 30127/45920 [00:48<00:24, 649.89it/s] 66%|██████▌   | 30200/45920 [00:48<00:25, 609.21it/s] 66%|██████▌   | 30263/45920 [00:48<00:26, 593.77it/s] 66%|██████▌   | 30334/45920 [00:48<00:25, 622.17it/s] 66%|██████▌   | 30398/45920 [00:48<00:25, 614.09it/s] 66%|██████▋   | 30481/45920 [00:48<00:22, 671.75it/s] 67%|██████▋   | 30559/45920 [00:49<00:21, 702.22it/s] 67%|██████▋   | 30630/45920 [00:49<00:22, 693.18it/s] 67%|██████▋   | 30700/45920 [00:49<00:22, 674.22it/s] 67%|██████▋   | 30774/45920 [00:49<00:21, 692.85it/s] 67%|██████▋   | 30850/45920 [00:49<00:21, 709.97it/s] 67%|██████▋   | 30922/45920 [00:49<00:23, 651.52it/s] 67%|██████▋   | 30992/45920 [00:49<00:22, 663.23it/s] 68%|██████▊   | 31080/45920 [00:49<00:20, 723.38it/s] 68%|██████▊   | 31157/45920 [00:49<00:20, 736.60it/s] 68%|██████▊   | 31232/45920 [00:50<00:20, 704.28it/s] 68%|██████▊   | 31304/45920 [00:50<00:21, 691.37it/s] 68%|██████▊   | 31379/45920 [00:50<00:20, 703.60it/s] 69%|██████▊   | 31463/45920 [00:50<00:19, 740.75it/s] 69%|██████▊   | 31538/45920 [00:50<00:29, 494.96it/s] 69%|██████▉   | 31611/45920 [00:50<00:26, 540.69it/s] 69%|██████▉   | 31675/45920 [00:50<00:26, 540.30it/s] 69%|██████▉   | 31736/45920 [00:50<00:26, 539.72it/s] 69%|██████▉   | 31797/45920 [00:51<00:27, 518.18it/s] 69%|██████▉   | 31869/45920 [00:51<00:24, 565.30it/s] 70%|██████▉   | 31929/45920 [00:51<00:24, 563.77it/s] 70%|██████▉   | 32004/45920 [00:51<00:22, 612.14it/s] 70%|██████▉   | 32085/45920 [00:51<00:20, 665.17it/s] 70%|███████   | 32154/45920 [00:51<00:21, 650.22it/s] 70%|███████   | 32221/45920 [00:51<00:21, 634.85it/s] 70%|███████   | 32307/45920 [00:51<00:19, 693.69it/s] 71%|███████   | 32378/45920 [00:51<00:20, 651.79it/s] 71%|███████   | 32446/45920 [00:52<00:20, 651.37it/s] 71%|███████   | 32537/45920 [00:52<00:18, 722.86it/s] 71%|███████   | 32611/45920 [00:52<00:19, 688.52it/s] 71%|███████   | 32691/45920 [00:52<00:18, 714.22it/s] 71%|███████▏  | 32764/45920 [00:52<00:22, 577.09it/s] 72%|███████▏  | 32837/45920 [00:52<00:21, 613.46it/s] 72%|███████▏  | 32919/45920 [00:52<00:19, 665.54it/s] 72%|███████▏  | 32990/45920 [00:52<00:19, 662.77it/s] 72%|███████▏  | 33068/45920 [00:53<00:18, 693.41it/s] 72%|███████▏  | 33140/45920 [00:53<00:19, 647.92it/s] 72%|███████▏  | 33219/45920 [00:53<00:18, 684.89it/s] 73%|███████▎  | 33297/45920 [00:53<00:17, 709.78it/s] 73%|███████▎  | 33373/45920 [00:53<00:17, 723.54it/s] 73%|███████▎  | 33447/45920 [00:53<00:17, 727.50it/s] 73%|███████▎  | 33522/45920 [00:53<00:17, 725.85it/s] 73%|███████▎  | 33596/45920 [00:53<00:17, 716.79it/s] 73%|███████▎  | 33669/45920 [00:53<00:17, 709.97it/s] 73%|███████▎  | 33741/45920 [00:53<00:18, 641.29it/s] 74%|███████▎  | 33811/45920 [00:54<00:18, 652.40it/s] 74%|███████▍  | 33890/45920 [00:54<00:17, 688.32it/s] 74%|███████▍  | 33960/45920 [00:54<00:17, 666.24it/s] 74%|███████▍  | 34041/45920 [00:54<00:16, 706.38it/s] 74%|███████▍  | 34113/45920 [00:54<00:17, 691.30it/s] 74%|███████▍  | 34183/45920 [00:54<00:19, 613.09it/s] 75%|███████▍  | 34247/45920 [00:54<00:19, 613.64it/s] 75%|███████▍  | 34310/45920 [00:54<00:19, 589.67it/s] 75%|███████▍  | 34390/45920 [00:54<00:17, 645.80it/s] 75%|███████▌  | 34456/45920 [00:55<00:19, 596.25it/s] 75%|███████▌  | 34533/45920 [00:55<00:17, 637.72it/s] 75%|███████▌  | 34627/45920 [00:55<00:15, 713.54it/s] 76%|███████▌  | 34700/45920 [00:55<00:17, 648.71it/s] 76%|███████▌  | 34791/45920 [00:55<00:15, 717.98it/s] 76%|███████▌  | 34866/45920 [00:55<00:15, 708.24it/s] 76%|███████▌  | 34939/45920 [00:55<00:17, 629.81it/s] 76%|███████▌  | 35005/45920 [00:55<00:18, 582.05it/s] 76%|███████▋  | 35066/45920 [00:56<00:19, 563.71it/s] 76%|███████▋  | 35124/45920 [00:56<00:20, 537.59it/s] 77%|███████▋  | 35208/45920 [00:56<00:17, 612.80it/s] 77%|███████▋  | 35277/45920 [00:56<00:16, 631.74it/s] 77%|███████▋  | 35362/45920 [00:56<00:15, 690.61it/s] 77%|███████▋  | 35433/45920 [00:56<00:17, 583.58it/s] 77%|███████▋  | 35509/45920 [00:56<00:16, 617.14it/s] 77%|███████▋  | 35574/45920 [00:56<00:17, 598.72it/s] 78%|███████▊  | 35660/45920 [00:56<00:15, 667.46it/s] 78%|███████▊  | 35730/45920 [00:57<00:16, 636.84it/s] 78%|███████▊  | 35796/45920 [00:57<00:16, 605.65it/s] 78%|███████▊  | 35871/45920 [00:57<00:15, 629.95it/s] 78%|███████▊  | 35943/45920 [00:57<00:15, 652.20it/s] 78%|███████▊  | 36012/45920 [00:57<00:14, 661.88it/s] 79%|███████▊  | 36080/45920 [00:57<00:14, 666.14it/s] 79%|███████▊  | 36148/45920 [00:57<00:15, 644.64it/s] 79%|███████▉  | 36213/45920 [00:57<00:15, 634.08it/s] 79%|███████▉  | 36288/45920 [00:57<00:14, 666.94it/s] 79%|███████▉  | 36356/45920 [00:58<00:15, 598.60it/s] 79%|███████▉  | 36418/45920 [00:58<00:16, 589.08it/s] 79%|███████▉  | 36492/45920 [00:58<00:14, 629.95it/s] 80%|███████▉  | 36557/45920 [00:58<00:15, 622.47it/s] 80%|███████▉  | 36621/45920 [00:58<00:15, 613.14it/s] 80%|███████▉  | 36683/45920 [00:58<00:16, 577.07it/s] 80%|████████  | 36751/45920 [00:58<00:15, 597.83it/s] 80%|████████  | 36821/45920 [00:58<00:14, 619.85it/s] 80%|████████  | 36884/45920 [00:59<00:15, 574.54it/s] 80%|████████  | 36950/45920 [00:59<00:15, 592.15it/s] 81%|████████  | 37010/45920 [00:59<00:15, 585.28it/s] 81%|████████  | 37094/45920 [00:59<00:13, 654.78it/s] 81%|████████  | 37169/45920 [00:59<00:12, 680.05it/s] 81%|████████  | 37239/45920 [00:59<00:12, 682.99it/s] 81%|████████  | 37308/45920 [00:59<00:12, 679.55it/s] 81%|████████▏ | 37377/45920 [00:59<00:12, 674.62it/s] 82%|████████▏ | 37445/45920 [00:59<00:14, 593.80it/s] 82%|████████▏ | 37509/45920 [00:59<00:13, 604.01it/s] 82%|████████▏ | 37576/45920 [01:00<00:13, 621.27it/s] 82%|████████▏ | 37640/45920 [01:00<00:13, 609.68it/s] 82%|████████▏ | 37704/45920 [01:00<00:13, 616.86it/s] 82%|████████▏ | 37788/45920 [01:00<00:11, 680.70it/s] 82%|████████▏ | 37858/45920 [01:00<00:11, 684.80it/s] 83%|████████▎ | 37936/45920 [01:00<00:11, 711.07it/s] 83%|████████▎ | 38008/45920 [01:00<00:13, 601.68it/s] 83%|████████▎ | 38072/45920 [01:00<00:13, 578.48it/s] 83%|████████▎ | 38133/45920 [01:00<00:13, 571.93it/s] 83%|████████▎ | 38210/45920 [01:01<00:12, 624.76it/s] 83%|████████▎ | 38275/45920 [01:01<00:12, 608.36it/s] 83%|████████▎ | 38339/45920 [01:01<00:12, 615.62it/s] 84%|████████▎ | 38403/45920 [01:01<00:12, 619.67it/s] 84%|████████▍ | 38481/45920 [01:01<00:11, 664.28it/s] 84%|████████▍ | 38556/45920 [01:01<00:10, 683.24it/s] 84%|████████▍ | 38625/45920 [01:01<00:12, 588.93it/s] 84%|████████▍ | 38687/45920 [01:01<00:13, 555.69it/s] 84%|████████▍ | 38745/45920 [01:01<00:12, 553.60it/s] 84%|████████▍ | 38802/45920 [01:02<00:13, 541.38it/s] 85%|████████▍ | 38880/45920 [01:02<00:11, 604.82it/s] 85%|████████▍ | 38942/45920 [01:02<00:12, 559.21it/s] 85%|████████▍ | 39015/45920 [01:02<00:11, 601.69it/s] 85%|████████▌ | 39077/45920 [01:02<00:11, 603.22it/s] 85%|████████▌ | 39156/45920 [01:02<00:10, 652.17it/s] 85%|████████▌ | 39223/45920 [01:02<00:11, 608.41it/s] 86%|████████▌ | 39291/45920 [01:02<00:10, 626.36it/s] 86%|████████▌ | 39355/45920 [01:03<00:11, 547.85it/s] 86%|████████▌ | 39413/45920 [01:03<00:11, 552.44it/s] 86%|████████▌ | 39492/45920 [01:03<00:10, 615.18it/s] 86%|████████▌ | 39556/45920 [01:03<00:10, 612.62it/s] 86%|████████▋ | 39623/45920 [01:03<00:10, 625.62it/s] 86%|████████▋ | 39687/45920 [01:03<00:11, 534.26it/s] 87%|████████▋ | 39751/45920 [01:03<00:11, 560.34it/s] 87%|████████▋ | 39827/45920 [01:03<00:10, 576.31it/s] 87%|████████▋ | 39906/45920 [01:03<00:09, 632.16it/s] 87%|████████▋ | 39996/45920 [01:04<00:08, 702.96it/s] 87%|████████▋ | 40069/45920 [01:04<00:08, 655.24it/s] 87%|████████▋ | 40138/45920 [01:04<00:08, 660.83it/s] 88%|████████▊ | 40210/45920 [01:04<00:08, 673.96it/s] 88%|████████▊ | 40279/45920 [01:04<00:09, 605.05it/s] 88%|████████▊ | 40342/45920 [01:04<00:10, 554.98it/s] 88%|████████▊ | 40412/45920 [01:04<00:09, 585.71it/s] 88%|████████▊ | 40478/45920 [01:04<00:09, 603.84it/s] 88%|████████▊ | 40553/45920 [01:04<00:08, 625.57it/s] 89%|████████▊ | 40646/45920 [01:05<00:07, 709.46it/s] 89%|████████▊ | 40719/45920 [01:05<00:07, 691.95it/s] 89%|████████▉ | 40796/45920 [01:05<00:07, 713.48it/s] 89%|████████▉ | 40869/45920 [01:05<00:08, 616.51it/s] 89%|████████▉ | 40941/45920 [01:05<00:07, 641.26it/s] 89%|████████▉ | 41009/45920 [01:05<00:07, 650.73it/s] 89%|████████▉ | 41085/45920 [01:05<00:07, 679.18it/s] 90%|████████▉ | 41155/45920 [01:05<00:07, 634.44it/s] 90%|████████▉ | 41220/45920 [01:05<00:07, 631.49it/s] 90%|████████▉ | 41286/45920 [01:06<00:07, 637.64it/s] 90%|█████████ | 41351/45920 [01:06<00:08, 565.24it/s] 90%|█████████ | 41419/45920 [01:06<00:07, 592.23it/s] 90%|█████████ | 41485/45920 [01:06<00:07, 609.55it/s] 90%|█████████ | 41548/45920 [01:06<00:07, 586.22it/s] 91%|█████████ | 41633/45920 [01:06<00:06, 658.78it/s] 91%|█████████ | 41714/45920 [01:06<00:06, 700.25it/s] 91%|█████████ | 41787/45920 [01:06<00:05, 708.71it/s] 91%|█████████ | 41879/45920 [01:06<00:05, 770.11it/s] 91%|█████████▏| 41967/45920 [01:07<00:04, 792.61it/s] 92%|█████████▏| 42047/45920 [01:07<00:04, 776.50it/s] 92%|█████████▏| 42126/45920 [01:07<00:06, 607.50it/s] 92%|█████████▏| 42193/45920 [01:07<00:06, 598.94it/s] 92%|█████████▏| 42273/45920 [01:07<00:05, 644.95it/s] 92%|█████████▏| 42342/45920 [01:07<00:05, 627.44it/s] 92%|█████████▏| 42412/45920 [01:07<00:05, 646.51it/s] 93%|█████████▎| 42490/45920 [01:07<00:05, 682.36it/s] 93%|█████████▎| 42560/45920 [01:07<00:05, 657.20it/s] 93%|█████████▎| 42628/45920 [01:08<00:05, 650.79it/s] 93%|█████████▎| 42694/45920 [01:08<00:04, 652.75it/s] 93%|█████████▎| 42760/45920 [01:08<00:04, 651.48it/s] 93%|█████████▎| 42826/45920 [01:08<00:04, 642.06it/s] 93%|█████████▎| 42899/45920 [01:08<00:04, 666.04it/s] 94%|█████████▎| 42966/45920 [01:08<00:05, 589.14it/s] 94%|█████████▎| 43027/45920 [01:08<00:04, 583.79it/s] 94%|█████████▍| 43092/45920 [01:08<00:04, 600.14it/s] 94%|█████████▍| 43161/45920 [01:08<00:04, 624.95it/s] 94%|█████████▍| 43225/45920 [01:09<00:04, 604.57it/s] 94%|█████████▍| 43287/45920 [01:09<00:04, 572.80it/s] 94%|█████████▍| 43365/45920 [01:09<00:04, 627.44it/s] 95%|█████████▍| 43429/45920 [01:09<00:04, 620.47it/s] 95%|█████████▍| 43492/45920 [01:09<00:04, 590.72it/s] 95%|█████████▍| 43561/45920 [01:09<00:03, 615.85it/s] 95%|█████████▌| 43629/45920 [01:09<00:03, 633.92it/s] 95%|█████████▌| 43710/45920 [01:09<00:03, 683.96it/s] 95%|█████████▌| 43779/45920 [01:09<00:03, 643.51it/s] 95%|█████████▌| 43845/45920 [01:10<00:03, 605.22it/s] 96%|█████████▌| 43917/45920 [01:10<00:03, 635.97it/s] 96%|█████████▌| 43985/45920 [01:10<00:02, 647.93it/s] 96%|█████████▌| 44051/45920 [01:10<00:02, 637.92it/s] 96%|█████████▌| 44125/45920 [01:10<00:02, 665.53it/s] 96%|█████████▌| 44193/45920 [01:10<00:03, 492.90it/s] 96%|█████████▋| 44263/45920 [01:10<00:03, 541.14it/s] 97%|█████████▋| 44335/45920 [01:10<00:02, 585.50it/s] 97%|█████████▋| 44404/45920 [01:11<00:02, 611.50it/s] 97%|█████████▋| 44472/45920 [01:11<00:02, 629.30it/s] 97%|█████████▋| 44541/45920 [01:11<00:02, 641.35it/s] 97%|█████████▋| 44608/45920 [01:11<00:02, 624.40it/s] 97%|█████████▋| 44672/45920 [01:11<00:02, 543.21it/s] 97%|█████████▋| 44739/45920 [01:11<00:02, 572.58it/s] 98%|█████████▊| 44799/45920 [01:11<00:01, 566.87it/s] 98%|█████████▊| 44858/45920 [01:11<00:02, 526.50it/s] 98%|█████████▊| 44916/45920 [01:11<00:01, 539.45it/s] 98%|█████████▊| 44972/45920 [01:12<00:01, 533.16it/s] 98%|█████████▊| 45027/45920 [01:12<00:01, 520.22it/s] 98%|█████████▊| 45080/45920 [01:12<00:01, 483.40it/s] 98%|█████████▊| 45146/45920 [01:12<00:01, 527.46it/s] 98%|█████████▊| 45218/45920 [01:12<00:01, 577.64it/s] 99%|█████████▊| 45277/45920 [01:12<00:01, 558.22it/s] 99%|█████████▊| 45337/45920 [01:12<00:01, 569.50it/s] 99%|█████████▉| 45411/45920 [01:12<00:00, 616.59it/s] 99%|█████████▉| 45487/45920 [01:12<00:00, 636.42it/s] 99%|█████████▉| 45565/45920 [01:13<00:00, 672.65it/s] 99%|█████████▉| 45633/45920 [01:13<00:00, 537.42it/s]100%|█████████▉| 45699/45920 [01:13<00:00, 565.62it/s]100%|█████████▉| 45778/45920 [01:13<00:00, 623.34it/s]100%|█████████▉| 45844/45920 [01:13<00:00, 632.30it/s]100%|█████████▉| 45910/45920 [01:13<00:00, 515.63it/s]100%|██████████| 45920/45920 [01:13<00:00, 623.10it/s]

gathering stats for n=1
  0%|          | 0/45920 [00:00<?, ?it/s]  0%|          | 209/45920 [00:00<00:22, 2041.14it/s]  1%|          | 443/45920 [00:00<00:20, 2214.04it/s]  1%|▏         | 678/45920 [00:00<00:20, 2177.63it/s]  2%|▏         | 897/45920 [00:00<00:21, 2069.32it/s]  3%|▎         | 1160/45920 [00:00<00:19, 2254.95it/s]  3%|▎         | 1424/45920 [00:00<00:18, 2380.82it/s]  4%|▎         | 1678/45920 [00:00<00:18, 2420.18it/s]  4%|▍         | 1947/45920 [00:00<00:17, 2498.78it/s]  5%|▍         | 2198/45920 [00:00<00:17, 2461.20it/s]  5%|▌         | 2445/45920 [00:01<00:17, 2423.30it/s]  6%|▌         | 2693/45920 [00:01<00:17, 2440.07it/s]  6%|▋         | 2952/45920 [00:01<00:17, 2481.80it/s]  7%|▋         | 3201/45920 [00:01<00:18, 2364.04it/s]  7%|▋         | 3443/45920 [00:01<00:17, 2379.54it/s]  8%|▊         | 3682/45920 [00:01<00:18, 2248.10it/s]  9%|▊         | 3909/45920 [00:01<00:18, 2230.11it/s]  9%|▉         | 4134/45920 [00:01<00:19, 2109.91it/s]  9%|▉         | 4347/45920 [00:01<00:20, 2028.49it/s] 10%|█         | 4600/45920 [00:02<00:19, 2165.68it/s] 11%|█         | 4852/45920 [00:02<00:18, 2264.44it/s] 11%|█         | 5081/45920 [00:02<00:18, 2265.01it/s] 12%|█▏        | 5310/45920 [00:02<00:18, 2255.93it/s] 12%|█▏        | 5544/45920 [00:02<00:17, 2269.75it/s] 13%|█▎        | 5772/45920 [00:02<00:17, 2260.61it/s] 13%|█▎        | 5999/45920 [00:02<00:17, 2249.36it/s] 14%|█▎        | 6225/45920 [00:02<00:17, 2219.33it/s] 14%|█▍        | 6488/45920 [00:02<00:16, 2338.01it/s] 15%|█▍        | 6723/45920 [00:02<00:16, 2331.11it/s] 15%|█▌        | 6977/45920 [00:03<00:16, 2392.19it/s] 16%|█▌        | 7242/45920 [00:03<00:15, 2463.34it/s] 16%|█▋        | 7489/45920 [00:03<00:16, 2343.95it/s] 17%|█▋        | 7725/45920 [00:03<00:18, 2028.63it/s] 17%|█▋        | 7994/45920 [00:03<00:17, 2198.77it/s] 18%|█▊        | 8222/45920 [00:03<00:17, 2197.74it/s] 18%|█▊        | 8479/45920 [00:03<00:16, 2297.08it/s] 19%|█▉        | 8714/45920 [00:03<00:16, 2246.52it/s] 19%|█▉        | 8942/45920 [00:03<00:16, 2225.55it/s] 20%|██        | 9190/45920 [00:04<00:15, 2297.45it/s] 21%|██        | 9429/45920 [00:04<00:15, 2316.24it/s] 21%|██        | 9663/45920 [00:04<00:15, 2268.86it/s] 22%|██▏       | 9892/45920 [00:04<00:16, 2219.94it/s] 22%|██▏       | 10127/45920 [00:04<00:16, 2231.77it/s] 23%|██▎       | 10353/45920 [00:04<00:15, 2237.82it/s] 23%|██▎       | 10599/45920 [00:04<00:15, 2302.47it/s] 24%|██▎       | 10838/45920 [00:04<00:15, 2327.67it/s] 24%|██▍       | 11072/45920 [00:04<00:15, 2245.06it/s] 25%|██▍       | 11321/45920 [00:04<00:14, 2315.36it/s] 25%|██▌       | 11554/45920 [00:05<00:15, 2283.41it/s] 26%|██▌       | 11783/45920 [00:05<00:15, 2145.59it/s] 26%|██▌       | 12000/45920 [00:05<00:15, 2128.53it/s] 27%|██▋       | 12223/45920 [00:05<00:15, 2157.06it/s] 27%|██▋       | 12493/45920 [00:05<00:14, 2311.19it/s] 28%|██▊       | 12727/45920 [00:05<00:14, 2319.41it/s] 28%|██▊       | 12960/45920 [00:05<00:14, 2268.82it/s] 29%|██▉       | 13210/45920 [00:05<00:14, 2334.81it/s] 29%|██▉       | 13466/45920 [00:05<00:13, 2398.67it/s] 30%|██▉       | 13707/45920 [00:06<00:13, 2314.71it/s] 30%|███       | 13945/45920 [00:06<00:13, 2332.65it/s] 31%|███       | 14180/45920 [00:06<00:14, 2213.57it/s] 31%|███▏      | 14405/45920 [00:06<00:14, 2201.84it/s] 32%|███▏      | 14627/45920 [00:06<00:14, 2132.25it/s] 32%|███▏      | 14842/45920 [00:06<00:14, 2124.27it/s] 33%|███▎      | 15061/45920 [00:06<00:14, 2141.93it/s] 33%|███▎      | 15276/45920 [00:06<00:14, 2137.81it/s] 34%|███▎      | 15491/45920 [00:06<00:14, 2127.43it/s] 34%|███▍      | 15714/45920 [00:06<00:14, 2157.19it/s] 35%|███▍      | 15943/45920 [00:07<00:13, 2167.04it/s] 35%|███▌      | 16165/45920 [00:07<00:13, 2179.81it/s] 36%|███▌      | 16394/45920 [00:07<00:13, 2210.65it/s] 36%|███▌      | 16616/45920 [00:07<00:13, 2186.18it/s] 37%|███▋      | 16835/45920 [00:07<00:13, 2149.56it/s] 37%|███▋      | 17051/45920 [00:07<00:13, 2137.88it/s] 38%|███▊      | 17308/45920 [00:07<00:12, 2252.88it/s] 38%|███▊      | 17573/45920 [00:07<00:11, 2369.71it/s] 39%|███▉      | 17811/45920 [00:07<00:11, 2368.36it/s] 39%|███▉      | 18049/45920 [00:07<00:12, 2234.67it/s] 40%|███▉      | 18326/45920 [00:08<00:11, 2385.24it/s] 40%|████      | 18567/45920 [00:08<00:12, 2270.39it/s] 41%|████      | 18797/45920 [00:08<00:12, 2142.66it/s] 42%|████▏     | 19073/45920 [00:08<00:11, 2306.67it/s] 42%|████▏     | 19307/45920 [00:08<00:11, 2308.63it/s] 43%|████▎     | 19541/45920 [00:08<00:11, 2293.20it/s] 43%|████▎     | 19797/45920 [00:08<00:11, 2368.93it/s] 44%|████▎     | 20039/45920 [00:08<00:10, 2381.82it/s] 44%|████▍     | 20279/45920 [00:08<00:11, 2317.61it/s] 45%|████▍     | 20512/45920 [00:09<00:11, 2281.84it/s] 45%|████▌     | 20798/45920 [00:09<00:10, 2424.10it/s] 46%|████▌     | 21042/45920 [00:09<00:10, 2316.43it/s] 46%|████▋     | 21275/45920 [00:09<00:11, 2236.05it/s] 47%|████▋     | 21500/45920 [00:09<00:11, 2187.50it/s] 47%|████▋     | 21744/45920 [00:09<00:10, 2256.52it/s] 48%|████▊     | 21971/45920 [00:09<00:10, 2209.29it/s] 48%|████▊     | 22202/45920 [00:09<00:10, 2237.95it/s] 49%|████▉     | 22460/45920 [00:09<00:10, 2336.18it/s] 49%|████▉     | 22695/45920 [00:10<00:10, 2289.97it/s] 50%|█████     | 22963/45920 [00:10<00:09, 2400.67it/s] 51%|█████     | 23204/45920 [00:10<00:09, 2372.73it/s] 51%|█████     | 23446/45920 [00:10<00:09, 2386.43it/s] 52%|█████▏    | 23686/45920 [00:10<00:10, 2196.58it/s] 52%|█████▏    | 23917/45920 [00:10<00:09, 2226.45it/s] 53%|█████▎    | 24143/45920 [00:10<00:09, 2218.52it/s] 53%|█████▎    | 24367/45920 [00:10<00:09, 2190.07it/s] 54%|█████▎    | 24642/45920 [00:10<00:09, 2348.32it/s] 54%|█████▍    | 24885/45920 [00:10<00:08, 2370.17it/s] 55%|█████▍    | 25124/45920 [00:11<00:08, 2334.60it/s] 55%|█████▌    | 25374/45920 [00:11<00:08, 2382.78it/s] 56%|█████▌    | 25620/45920 [00:11<00:08, 2403.60it/s] 56%|█████▋    | 25874/45920 [00:11<00:08, 2424.75it/s] 57%|█████▋    | 26117/45920 [00:11<00:08, 2390.49it/s] 57%|█████▋    | 26357/45920 [00:11<00:08, 2368.07it/s] 58%|█████▊    | 26595/45920 [00:11<00:08, 2361.15it/s] 58%|█████▊    | 26846/45920 [00:11<00:07, 2402.59it/s] 59%|█████▉    | 27087/45920 [00:11<00:07, 2396.27it/s] 60%|█████▉    | 27327/45920 [00:12<00:08, 2192.56it/s] 60%|██████    | 27553/45920 [00:12<00:08, 2203.64it/s] 61%|██████    | 27784/45920 [00:12<00:08, 2219.60it/s] 61%|██████    | 28008/45920 [00:12<00:08, 2130.42it/s] 61%|██████▏   | 28225/45920 [00:12<00:08, 2128.96it/s] 62%|██████▏   | 28459/45920 [00:12<00:07, 2188.10it/s] 62%|██████▏   | 28692/45920 [00:12<00:07, 2228.95it/s] 63%|██████▎   | 28926/45920 [00:12<00:07, 2257.88it/s] 64%|██████▎   | 29161/45920 [00:12<00:07, 2284.67it/s] 64%|██████▍   | 29391/45920 [00:12<00:07, 2157.46it/s] 65%|██████▍   | 29637/45920 [00:13<00:07, 2240.54it/s] 65%|██████▌   | 29863/45920 [00:13<00:07, 2233.61it/s] 66%|██████▌   | 30100/45920 [00:13<00:06, 2270.96it/s] 66%|██████▌   | 30328/45920 [00:13<00:06, 2252.48it/s] 67%|██████▋   | 30570/45920 [00:13<00:06, 2300.58it/s] 67%|██████▋   | 30808/45920 [00:13<00:06, 2322.43it/s] 68%|██████▊   | 31082/45920 [00:13<00:06, 2445.58it/s] 68%|██████▊   | 31327/45920 [00:13<00:05, 2434.95it/s] 69%|██████▉   | 31571/45920 [00:13<00:06, 2236.78it/s] 69%|██████▉   | 31798/45920 [00:14<00:06, 2158.24it/s] 70%|██████▉   | 32026/45920 [00:14<00:06, 2191.05it/s] 70%|███████   | 32298/45920 [00:14<00:05, 2341.08it/s] 71%|███████   | 32538/45920 [00:14<00:05, 2344.86it/s] 71%|███████▏  | 32775/45920 [00:14<00:05, 2294.55it/s] 72%|███████▏  | 33029/45920 [00:14<00:05, 2361.72it/s] 72%|███████▏  | 33272/45920 [00:14<00:05, 2380.93it/s] 73%|███████▎  | 33536/45920 [00:14<00:05, 2451.65it/s] 74%|███████▎  | 33782/45920 [00:14<00:04, 2444.66it/s] 74%|███████▍  | 34027/45920 [00:14<00:04, 2416.67it/s] 75%|███████▍  | 34270/45920 [00:15<00:05, 2291.89it/s] 75%|███████▌  | 34505/45920 [00:15<00:04, 2306.72it/s] 76%|███████▌  | 34763/45920 [00:15<00:04, 2385.37it/s] 76%|███████▌  | 35003/45920 [00:15<00:04, 2284.86it/s] 77%|███████▋  | 35233/45920 [00:15<00:04, 2207.47it/s] 77%|███████▋  | 35456/45920 [00:15<00:04, 2208.44it/s] 78%|███████▊  | 35703/45920 [00:15<00:04, 2283.27it/s] 78%|███████▊  | 35933/45920 [00:15<00:04, 2275.82it/s] 79%|███████▉  | 36180/45920 [00:15<00:04, 2331.85it/s] 79%|███████▉  | 36414/45920 [00:15<00:04, 2215.53it/s] 80%|███████▉  | 36659/45920 [00:16<00:04, 2282.05it/s] 80%|████████  | 36889/45920 [00:16<00:04, 2218.62it/s] 81%|████████  | 37142/45920 [00:16<00:03, 2305.29it/s] 81%|████████▏ | 37374/45920 [00:16<00:03, 2302.73it/s] 82%|████████▏ | 37606/45920 [00:16<00:03, 2281.99it/s] 82%|████████▏ | 37844/45920 [00:16<00:03, 2310.16it/s] 83%|████████▎ | 38076/45920 [00:16<00:03, 2240.37it/s] 83%|████████▎ | 38301/45920 [00:16<00:03, 2230.23it/s] 84%|████████▍ | 38546/45920 [00:16<00:03, 2294.13it/s] 84%|████████▍ | 38776/45920 [00:17<00:03, 2154.27it/s] 85%|████████▍ | 38994/45920 [00:17<00:03, 2156.87it/s] 85%|████████▌ | 39215/45920 [00:17<00:03, 2171.77it/s] 86%|████████▌ | 39434/45920 [00:17<00:03, 2128.90it/s] 86%|████████▋ | 39648/45920 [00:17<00:02, 2107.33it/s] 87%|████████▋ | 39879/45920 [00:17<00:02, 2165.46it/s] 87%|████████▋ | 40116/45920 [00:17<00:02, 2215.60it/s] 88%|████████▊ | 40339/45920 [00:17<00:02, 2176.41it/s] 88%|████████▊ | 40570/45920 [00:17<00:02, 2214.20it/s] 89%|████████▉ | 40820/45920 [00:17<00:02, 2282.44it/s] 89%|████████▉ | 41064/45920 [00:18<00:02, 2316.90it/s] 90%|████████▉ | 41296/45920 [00:18<00:02, 2276.89it/s] 90%|█████████ | 41524/45920 [00:18<00:02, 2182.47it/s] 91%|█████████ | 41813/45920 [00:18<00:01, 2382.43it/s] 92%|█████████▏| 42087/45920 [00:18<00:01, 2475.28it/s] 92%|█████████▏| 42336/45920 [00:18<00:01, 2332.18it/s] 93%|█████████▎| 42581/45920 [00:18<00:01, 2363.87it/s] 93%|█████████▎| 42820/45920 [00:18<00:01, 2347.95it/s] 94%|█████████▍| 43057/45920 [00:18<00:01, 2255.08it/s] 94%|█████████▍| 43301/45920 [00:19<00:01, 2303.01it/s] 95%|█████████▍| 43533/45920 [00:19<00:01, 2262.96it/s] 95%|█████████▌| 43783/45920 [00:19<00:00, 2328.81it/s] 96%|█████████▌| 44039/45920 [00:19<00:00, 2395.23it/s] 96%|█████████▋| 44280/45920 [00:19<00:00, 2293.96it/s] 97%|█████████▋| 44535/45920 [00:19<00:00, 2350.38it/s] 98%|█████████▊| 44772/45920 [00:19<00:00, 2265.73it/s] 98%|█████████▊| 45000/45920 [00:19<00:00, 2210.30it/s] 98%|█████████▊| 45222/45920 [00:19<00:00, 2141.57it/s] 99%|█████████▉| 45452/45920 [00:20<00:00, 2185.19it/s] 99%|█████████▉| 45672/45920 [00:20<00:00, 2174.00it/s]100%|█████████▉| 45900/45920 [00:20<00:00, 2154.90it/s]100%|██████████| 45920/45920 [00:20<00:00, 2270.86it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 55.39it/s]2022-03-15 16:34:47 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(34528, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=34528, bias=False)
  )
)
2022-03-15 16:34:47 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 16:34:47 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 16:34:47 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 16:34:47 | INFO | fairseq_cli.train | num. shared model params: 36,592,640 (num. trained: 36,592,640)
2022-03-15 16:34:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 16:34:47 | INFO | fairseq.data.data_utils | loaded 2,294 examples from: data-bin/de/valid
2022-03-15 16:34:47 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 16:34:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:47 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 16:34:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:47 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 16:34:47 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 16:34:47 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-15 16:34:47 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-15 16:34:47 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 16:34:47 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
2022-03-15 16:34:47 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 16:34:47 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 16:34:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 16:34:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:35:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 16:35:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 16:39:36 | INFO | train_inner | epoch 001:    104 / 392 loss=14.717, ppl=26939.3, wps=24606.4, ups=0.38, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.13, loss_scale=8, train_wall=266, gb_free=9.6, wall=289
2022-03-15 16:44:02 | INFO | train_inner | epoch 001:    204 / 392 loss=13.191, ppl=9353.06, wps=24586.7, ups=0.38, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.714, loss_scale=16, train_wall=245, gb_free=9.6, wall=555
2022-03-15 16:48:29 | INFO | train_inner | epoch 001:    304 / 392 loss=12.264, ppl=4919.09, wps=24549.6, ups=0.37, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.451, loss_scale=32, train_wall=245, gb_free=9.6, wall=822
2022-03-15 16:52:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 16:52:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.626 | ppl 3161.18 | wps 37414.3 | wpb 511.9 | bsz 1 | num_updates 388
2022-03-15 16:52:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-15 16:52:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 16:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 16:52:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.626) (writing took 2.1615057345479727 seconds)
2022-03-15 16:52:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 16:52:59 | INFO | train | epoch 001 | loss 13.033 | ppl 8381.85 | wps 23717.9 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 388 | lr 4.85903e-05 | gnorm 0.937 | loss_scale 64 | train_wall 971 | gb_free 9.6 | wall 1092
KL Stats: Epoch 1 Divergences: Uniform: 0.6897268616116728 Unigram: 0.8955753872658476
2022-03-15 16:52:59 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 16:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 16:53:31 | INFO | train_inner | epoch 002:     12 / 392 loss=11.787, ppl=3534.58, wps=21526.4, ups=0.33, wpb=65022.4, bsz=127, num_updates=400, lr=5.009e-05, gnorm=0.389, loss_scale=64, train_wall=244, gb_free=9.6, wall=1124
2022-03-15 16:56:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:58:00 | INFO | train_inner | epoch 002:    113 / 392 loss=11.554, ppl=3006.35, wps=24359.6, ups=0.37, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.375, loss_scale=32, train_wall=247, gb_free=9.6, wall=1393
2022-03-15 17:02:26 | INFO | train_inner | epoch 002:    213 / 392 loss=11.296, ppl=2514.45, wps=24637, ups=0.38, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.376, loss_scale=64, train_wall=244, gb_free=9.6, wall=1659
2022-03-15 17:06:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:06:55 | INFO | train_inner | epoch 002:    314 / 392 loss=10.998, ppl=2045.65, wps=24404.9, ups=0.37, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.44, loss_scale=32, train_wall=247, gb_free=9.6, wall=1928
2022-03-15 17:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:10:55 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.478 | ppl 1426.62 | wps 36891.8 | wpb 511.9 | bsz 1 | num_updates 778 | best_loss 10.478
2022-03-15 17:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 778 updates
2022-03-15 17:10:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 17:10:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 17:10:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 2 @ 778 updates, score 10.478) (writing took 1.9175271596759558 seconds)
2022-03-15 17:10:57 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 17:10:57 | INFO | train | epoch 002 | loss 11.188 | ppl 2333.06 | wps 23671.1 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 778 | lr 9.73306e-05 | gnorm 0.404 | loss_scale 32 | train_wall 956 | gb_free 9.6 | wall 2170
KL Stats: Epoch 2 Divergences: Uniform: 1.4120559627336071 Unigram: 0.5888220427745491
2022-03-15 17:10:57 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 17:10:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:11:56 | INFO | train_inner | epoch 003:     22 / 392 loss=10.713, ppl=1678.24, wps=21622.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=800, lr=0.00010008, gnorm=0.437, loss_scale=32, train_wall=242, gb_free=9.6, wall=2229
2022-03-15 17:13:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:16:24 | INFO | train_inner | epoch 003:    123 / 392 loss=10.464, ppl=1412.12, wps=24419.6, ups=0.37, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.492, loss_scale=32, train_wall=246, gb_free=9.6, wall=2497
2022-03-15 17:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:20:53 | INFO | train_inner | epoch 003:    224 / 392 loss=10.255, ppl=1221.67, wps=24397.8, ups=0.37, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.499, loss_scale=32, train_wall=247, gb_free=9.6, wall=2766
2022-03-15 17:25:18 | INFO | train_inner | epoch 003:    324 / 392 loss=10.065, ppl=1071.36, wps=24668.7, ups=0.38, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.562, loss_scale=64, train_wall=244, gb_free=9.6, wall=3031
2022-03-15 17:25:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:28:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:28:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.708 | ppl 836.29 | wps 37379.9 | wpb 511.9 | bsz 1 | num_updates 1167 | best_loss 9.708
2022-03-15 17:28:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1167 updates
2022-03-15 17:28:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 17:28:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 17:28:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 3 @ 1167 updates, score 9.708) (writing took 2.1905964808538556 seconds)
2022-03-15 17:28:54 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 17:28:54 | INFO | train | epoch 003 | loss 10.223 | ppl 1195.02 | wps 23629 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 1167 | lr 0.000145946 | gnorm 0.524 | loss_scale 32 | train_wall 955 | gb_free 9.6 | wall 3247
KL Stats: Epoch 3 Divergences: Uniform: 1.8994364350746977 Unigram: 1.3629487825067865
2022-03-15 17:28:54 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 17:28:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:30:22 | INFO | train_inner | epoch 004:     33 / 392 loss=9.894, ppl=951.71, wps=21433.6, ups=0.33, wpb=65025.8, bsz=127, num_updates=1200, lr=0.00015007, gnorm=0.571, loss_scale=32, train_wall=245, gb_free=9.6, wall=3335
2022-03-15 17:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:34:50 | INFO | train_inner | epoch 004:    134 / 392 loss=9.738, ppl=854.02, wps=24394.7, ups=0.37, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.61, loss_scale=32, train_wall=247, gb_free=9.6, wall=3603
2022-03-15 17:39:16 | INFO | train_inner | epoch 004:    234 / 392 loss=9.603, ppl=777.74, wps=24685.8, ups=0.38, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.632, loss_scale=32, train_wall=244, gb_free=9.6, wall=3869
2022-03-15 17:40:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:43:44 | INFO | train_inner | epoch 004:    335 / 392 loss=9.474, ppl=710.97, wps=24428.7, ups=0.37, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.646, loss_scale=32, train_wall=247, gb_free=9.6, wall=4137
2022-03-15 17:46:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:46:48 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.134 | ppl 561.73 | wps 37580.3 | wpb 511.9 | bsz 1 | num_updates 1557 | best_loss 9.134
2022-03-15 17:46:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1557 updates
2022-03-15 17:46:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 17:46:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 17:46:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 4 @ 1557 updates, score 9.134) (writing took 2.1869153594598174 seconds)
2022-03-15 17:46:50 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 17:46:50 | INFO | train | epoch 004 | loss 9.59 | ppl 770.46 | wps 23700.3 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 1557 | lr 0.000194686 | gnorm 0.629 | loss_scale 64 | train_wall 955 | gb_free 9.6 | wall 4323
KL Stats: Epoch 4 Divergences: Uniform: 2.2955604056493693 Unigram: 1.7765314070671026
2022-03-15 17:46:50 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 17:46:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:47:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:48:47 | INFO | train_inner | epoch 005:     44 / 392 loss=9.325, ppl=641.5, wps=21498, ups=0.33, wpb=65025.8, bsz=127, num_updates=1600, lr=0.00020006, gnorm=0.683, loss_scale=32, train_wall=244, gb_free=9.6, wall=4440
2022-03-15 17:53:13 | INFO | train_inner | epoch 005:    144 / 392 loss=9.184, ppl=581.63, wps=24630.1, ups=0.38, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.669, loss_scale=32, train_wall=244, gb_free=9.6, wall=4706
2022-03-15 17:55:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:57:42 | INFO | train_inner | epoch 005:    245 / 392 loss=9.053, ppl=531.05, wps=24330.3, ups=0.37, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.699, loss_scale=32, train_wall=247, gb_free=9.6, wall=4975
2022-03-15 18:01:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:02:13 | INFO | train_inner | epoch 005:    346 / 392 loss=8.929, ppl=487.4, wps=24175.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.705, loss_scale=32, train_wall=249, gb_free=9.6, wall=5246
2022-03-15 18:04:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:04:49 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.611 | ppl 390.96 | wps 37125 | wpb 511.9 | bsz 1 | num_updates 1946 | best_loss 8.611
2022-03-15 18:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1946 updates
2022-03-15 18:04:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:04:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:04:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 5 @ 1946 updates, score 8.611) (writing took 1.9479014268144965 seconds)
2022-03-15 18:04:51 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 18:04:51 | INFO | train | epoch 005 | loss 9.053 | ppl 531.14 | wps 23540.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 1946 | lr 0.000243301 | gnorm 0.69 | loss_scale 32 | train_wall 959 | gb_free 9.6 | wall 5404
KL Stats: Epoch 5 Divergences: Uniform: 2.5982267783983963 Unigram: 2.0555095250193185
2022-03-15 18:04:51 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 18:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:07:14 | INFO | train_inner | epoch 006:     54 / 392 loss=8.786, ppl=441.42, wps=21594.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=2000, lr=0.00025005, gnorm=0.678, loss_scale=32, train_wall=243, gb_free=9.6, wall=5547
2022-03-15 18:08:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:11:43 | INFO | train_inner | epoch 006:    155 / 392 loss=8.667, ppl=406.45, wps=24356.6, ups=0.37, wpb=65532.7, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.682, loss_scale=32, train_wall=247, gb_free=9.6, wall=5816
2022-03-15 18:14:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:16:12 | INFO | train_inner | epoch 006:    256 / 392 loss=8.568, ppl=379.58, wps=24358.9, ups=0.37, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.675, loss_scale=32, train_wall=247, gb_free=9.6, wall=6085
2022-03-15 18:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:20:42 | INFO | train_inner | epoch 006:    357 / 392 loss=8.472, ppl=355.06, wps=24301.2, ups=0.37, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.651, loss_scale=16, train_wall=248, gb_free=9.6, wall=6355
2022-03-15 18:22:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:22:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.217 | ppl 297.65 | wps 37571.7 | wpb 511.9 | bsz 1 | num_updates 2335 | best_loss 8.217
2022-03-15 18:22:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2335 updates
2022-03-15 18:22:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:22:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:22:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 6 @ 2335 updates, score 8.217) (writing took 2.724989639595151 seconds)
2022-03-15 18:22:50 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 18:22:50 | INFO | train | epoch 006 | loss 8.581 | ppl 382.96 | wps 23569.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 2335 | lr 0.000291917 | gnorm 0.674 | loss_scale 32 | train_wall 958 | gb_free 9.6 | wall 6483
KL Stats: Epoch 6 Divergences: Uniform: 2.8808766586925807 Unigram: 2.2852641335927815
2022-03-15 18:22:50 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 18:22:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:25:43 | INFO | train_inner | epoch 007:     65 / 392 loss=8.367, ppl=330.12, wps=21618.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=2400, lr=0.00030004, gnorm=0.676, loss_scale=32, train_wall=242, gb_free=9.6, wall=6656
2022-03-15 18:28:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:30:11 | INFO | train_inner | epoch 007:    166 / 392 loss=8.27, ppl=308.76, wps=24418.5, ups=0.37, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.632, loss_scale=32, train_wall=246, gb_free=9.6, wall=6924
2022-03-15 18:34:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:34:41 | INFO | train_inner | epoch 007:    267 / 392 loss=8.2, ppl=293.99, wps=24322, ups=0.37, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.651, loss_scale=32, train_wall=248, gb_free=9.6, wall=7194
2022-03-15 18:35:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:39:13 | INFO | train_inner | epoch 007:    368 / 392 loss=8.126, ppl=279.33, wps=24083.4, ups=0.37, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.631, loss_scale=16, train_wall=250, gb_free=9.6, wall=7466
2022-03-15 18:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:40:49 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.916 | ppl 241.56 | wps 37141.1 | wpb 511.9 | bsz 1 | num_updates 2724 | best_loss 7.916
2022-03-15 18:40:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2724 updates
2022-03-15 18:40:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:40:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:40:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 7 @ 2724 updates, score 7.916) (writing took 1.7821020940318704 seconds)
2022-03-15 18:40:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 18:40:51 | INFO | train | epoch 007 | loss 8.216 | ppl 297.27 | wps 23539.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 2724 | lr 0.000340532 | gnorm 0.644 | loss_scale 16 | train_wall 959 | gb_free 9.6 | wall 7564
KL Stats: Epoch 7 Divergences: Uniform: 3.098862996427875 Unigram: 2.4577046747143485
2022-03-15 18:40:51 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 18:40:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:43:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:44:16 | INFO | train_inner | epoch 008:     77 / 392 loss=8.021, ppl=259.81, wps=21439.1, ups=0.33, wpb=65025.8, bsz=127, num_updates=2800, lr=0.00035003, gnorm=0.634, loss_scale=16, train_wall=245, gb_free=9.6, wall=7769
2022-03-15 18:48:42 | INFO | train_inner | epoch 008:    177 / 392 loss=7.969, ppl=250.58, wps=24600.7, ups=0.38, wpb=65532.7, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.608, loss_scale=32, train_wall=245, gb_free=9.6, wall=8036
2022-03-15 18:49:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:53:12 | INFO | train_inner | epoch 008:    278 / 392 loss=7.903, ppl=239.29, wps=24338, ups=0.37, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.61, loss_scale=16, train_wall=247, gb_free=9.6, wall=8305
2022-03-15 18:57:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:57:41 | INFO | train_inner | epoch 008:    379 / 392 loss=7.855, ppl=231.57, wps=24349.3, ups=0.37, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.61, loss_scale=16, train_wall=247, gb_free=9.6, wall=8574
2022-03-15 18:58:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:58:48 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.662 | ppl 202.53 | wps 37263.4 | wpb 511.9 | bsz 1 | num_updates 3113 | best_loss 7.662
2022-03-15 18:58:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3113 updates
2022-03-15 18:58:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:58:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 18:58:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 8 @ 3113 updates, score 7.662) (writing took 1.817522936500609 seconds)
2022-03-15 18:58:50 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 18:58:50 | INFO | train | epoch 008 | loss 7.924 | ppl 242.92 | wps 23576.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 3113 | lr 0.000389147 | gnorm 0.613 | loss_scale 16 | train_wall 958 | gb_free 9.6 | wall 8643
KL Stats: Epoch 8 Divergences: Uniform: 3.252470012545323 Unigram: 2.5759347233597443
2022-03-15 18:58:50 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 18:58:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:02:43 | INFO | train_inner | epoch 009:     87 / 392 loss=7.74, ppl=213.71, wps=21504, ups=0.33, wpb=65029.1, bsz=127, num_updates=3200, lr=0.00040002, gnorm=0.622, loss_scale=16, train_wall=244, gb_free=9.6, wall=8876
2022-03-15 19:03:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:07:14 | INFO | train_inner | epoch 009:    188 / 392 loss=7.706, ppl=208.83, wps=24205.3, ups=0.37, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.6, loss_scale=16, train_wall=249, gb_free=9.6, wall=9147
2022-03-15 19:09:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:11:43 | INFO | train_inner | epoch 009:    289 / 392 loss=7.656, ppl=201.73, wps=24359, ups=0.37, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.595, loss_scale=16, train_wall=247, gb_free=9.6, wall=9416
2022-03-15 19:15:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:16:10 | INFO | train_inner | epoch 009:    390 / 392 loss=7.594, ppl=193.17, wps=24586.5, ups=0.38, wpb=65532.7, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.592, loss_scale=16, train_wall=245, gb_free=9.6, wall=9683
2022-03-15 19:16:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:16:47 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.4 | ppl 168.93 | wps 37564.1 | wpb 511.9 | bsz 1 | num_updates 3502 | best_loss 7.4
2022-03-15 19:16:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3502 updates
2022-03-15 19:16:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 19:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 19:16:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 9 @ 3502 updates, score 7.4) (writing took 1.9434697329998016 seconds)
2022-03-15 19:16:49 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 19:16:49 | INFO | train | epoch 009 | loss 7.669 | ppl 203.56 | wps 23577.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 3502 | lr 0.000437762 | gnorm 0.603 | loss_scale 16 | train_wall 958 | gb_free 9.6 | wall 9722
KL Stats: Epoch 9 Divergences: Uniform: 3.364373229102651 Unigram: 2.673739532054973
2022-03-15 19:16:49 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 19:16:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:21:08 | INFO | train_inner | epoch 010:     98 / 392 loss=7.48, ppl=178.48, wps=21772.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=3600, lr=0.00045001, gnorm=0.603, loss_scale=16, train_wall=241, gb_free=9.6, wall=9981
2022-03-15 19:21:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:25:38 | INFO | train_inner | epoch 010:    199 / 392 loss=7.451, ppl=174.94, wps=24265.3, ups=0.37, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.608, loss_scale=16, train_wall=248, gb_free=9.6, wall=10251
2022-03-15 19:28:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:30:05 | INFO | train_inner | epoch 010:    300 / 392 loss=7.41, ppl=170.03, wps=24564.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.603, loss_scale=16, train_wall=245, gb_free=9.6, wall=10518
2022-03-15 19:34:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:34:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.2 | ppl 147.08 | wps 37600.3 | wpb 511.9 | bsz 1 | num_updates 3891 | best_loss 7.2
2022-03-15 19:34:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3891 updates
2022-03-15 19:34:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 19:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 19:34:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 10 @ 3891 updates, score 7.2) (writing took 1.9205608647316694 seconds)
2022-03-15 19:34:42 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 19:34:42 | INFO | train | epoch 010 | loss 7.429 | ppl 172.28 | wps 23708.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 3891 | lr 0.000486378 | gnorm 0.603 | loss_scale 16 | train_wall 952 | gb_free 9.6 | wall 10795
KL Stats: Epoch 10 Divergences: Uniform: 3.467307817145128 Unigram: 2.7607927141900555
2022-03-15 19:34:42 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 19:34:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:35:06 | INFO | train_inner | epoch 011:      9 / 392 loss=7.365, ppl=164.8, wps=21600.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=3900, lr=0.000487503, gnorm=0.61, loss_scale=16, train_wall=243, gb_free=9.6, wall=10819
2022-03-15 19:39:31 | INFO | train_inner | epoch 011:    109 / 392 loss=7.266, ppl=153.92, wps=24794.9, ups=0.38, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.595, loss_scale=16, train_wall=243, gb_free=9.6, wall=11084
2022-03-15 19:42:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:43:58 | INFO | train_inner | epoch 011:    210 / 392 loss=7.239, ppl=151.05, wps=24549.2, ups=0.37, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.585, loss_scale=16, train_wall=245, gb_free=9.6, wall=11351
2022-03-15 19:47:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:48:25 | INFO | train_inner | epoch 011:    311 / 392 loss=7.218, ppl=148.84, wps=24544.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.587, loss_scale=16, train_wall=245, gb_free=9.6, wall=11618
2022-03-15 19:51:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:52:31 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.045 | ppl 132.01 | wps 37550.2 | wpb 511.9 | bsz 1 | num_updates 4281 | best_loss 7.045
2022-03-15 19:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4281 updates
2022-03-15 19:52:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 19:52:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 19:52:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 11 @ 4281 updates, score 7.045) (writing took 1.889348584227264 seconds)
2022-03-15 19:52:33 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 19:52:33 | INFO | train | epoch 011 | loss 7.23 | ppl 150.13 | wps 23825.8 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 4281 | lr 0.000483312 | gnorm 0.59 | loss_scale 16 | train_wall 950 | gb_free 9.6 | wall 11866
KL Stats: Epoch 11 Divergences: Uniform: 3.5383312919643934 Unigram: 2.8223196105447017
2022-03-15 19:52:33 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 19:52:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:53:23 | INFO | train_inner | epoch 012:     19 / 392 loss=7.17, ppl=143.98, wps=21766.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=4300, lr=0.000482243, gnorm=0.584, loss_scale=16, train_wall=241, gb_free=9.6, wall=11916
2022-03-15 19:54:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:57:50 | INFO | train_inner | epoch 012:    120 / 392 loss=7.093, ppl=136.56, wps=24537.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.566, loss_scale=16, train_wall=245, gb_free=9.6, wall=12183
2022-03-15 20:00:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:02:18 | INFO | train_inner | epoch 012:    221 / 392 loss=7.08, ppl=135.33, wps=24529.5, ups=0.37, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.569, loss_scale=16, train_wall=245, gb_free=9.6, wall=12451
2022-03-15 20:06:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:06:45 | INFO | train_inner | epoch 012:    322 / 392 loss=7.054, ppl=132.85, wps=24547.1, ups=0.37, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.553, loss_scale=16, train_wall=245, gb_free=9.6, wall=12718
2022-03-15 20:09:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:10:23 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.925 | ppl 121.5 | wps 37537.3 | wpb 511.9 | bsz 1 | num_updates 4670 | best_loss 6.925
2022-03-15 20:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4670 updates
2022-03-15 20:10:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 20:10:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 20:10:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 12 @ 4670 updates, score 6.925) (writing took 1.753373465500772 seconds)
2022-03-15 20:10:25 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 20:10:25 | INFO | train | epoch 012 | loss 7.071 | ppl 134.44 | wps 23728 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 4670 | lr 0.000462745 | gnorm 0.564 | loss_scale 16 | train_wall 951 | gb_free 9.6 | wall 12938
KL Stats: Epoch 12 Divergences: Uniform: 3.5985441332664676 Unigram: 2.875162732328533
2022-03-15 20:10:25 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 20:10:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:11:45 | INFO | train_inner | epoch 013:     30 / 392 loss=7.014, ppl=129.28, wps=21667.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=4700, lr=0.000461266, gnorm=0.564, loss_scale=16, train_wall=242, gb_free=9.6, wall=13018
2022-03-15 20:13:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:16:12 | INFO | train_inner | epoch 013:    131 / 392 loss=6.967, ppl=125.13, wps=24550.3, ups=0.37, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.555, loss_scale=16, train_wall=245, gb_free=9.6, wall=13285
2022-03-15 20:20:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:20:39 | INFO | train_inner | epoch 013:    232 / 392 loss=6.948, ppl=123.45, wps=24549, ups=0.37, wpb=65532.7, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.549, loss_scale=16, train_wall=245, gb_free=9.6, wall=13552
2022-03-15 20:25:03 | INFO | train_inner | epoch 013:    332 / 392 loss=6.946, ppl=123.26, wps=24801.3, ups=0.38, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.56, loss_scale=16, train_wall=243, gb_free=9.6, wall=13816
2022-03-15 20:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:28:14 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.836 | ppl 114.21 | wps 37564.3 | wpb 511.9 | bsz 1 | num_updates 5060 | best_loss 6.836
2022-03-15 20:28:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5060 updates
2022-03-15 20:28:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 20:28:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 20:28:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 13 @ 5060 updates, score 6.836) (writing took 1.8018806306645274 seconds)
2022-03-15 20:28:16 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 20:28:16 | INFO | train | epoch 013 | loss 6.95 | ppl 123.61 | wps 23831 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 5060 | lr 0.000444554 | gnorm 0.555 | loss_scale 32 | train_wall 950 | gb_free 9.6 | wall 14009
KL Stats: Epoch 13 Divergences: Uniform: 3.642430552810933 Unigram: 2.916267237481073
2022-03-15 20:28:16 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 20:28:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:29:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:30:04 | INFO | train_inner | epoch 014:     41 / 392 loss=6.895, ppl=119, wps=21581.4, ups=0.33, wpb=65025.8, bsz=127, num_updates=5100, lr=0.000442807, gnorm=0.554, loss_scale=16, train_wall=243, gb_free=9.6, wall=14117
2022-03-15 20:34:28 | INFO | train_inner | epoch 014:    141 / 392 loss=6.845, ppl=114.97, wps=24808.4, ups=0.38, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.54, loss_scale=16, train_wall=243, gb_free=9.6, wall=14381
2022-03-15 20:35:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:38:56 | INFO | train_inner | epoch 014:    242 / 392 loss=6.861, ppl=116.24, wps=24497.2, ups=0.37, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.545, loss_scale=16, train_wall=246, gb_free=9.6, wall=14649
2022-03-15 20:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:43:22 | INFO | train_inner | epoch 014:    343 / 392 loss=6.855, ppl=115.79, wps=24576.9, ups=0.38, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.54, loss_scale=16, train_wall=245, gb_free=9.6, wall=14916
2022-03-15 20:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:46:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.76 | ppl 108.36 | wps 36847.2 | wpb 511.9 | bsz 1 | num_updates 5449 | best_loss 6.76
2022-03-15 20:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5449 updates
2022-03-15 20:46:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 20:46:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 20:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 14 @ 5449 updates, score 6.76) (writing took 1.7683227276429534 seconds)
2022-03-15 20:46:07 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 20:46:07 | INFO | train | epoch 014 | loss 6.852 | ppl 115.53 | wps 23740.6 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 5449 | lr 0.000428392 | gnorm 0.543 | loss_scale 16 | train_wall 950 | gb_free 9.6 | wall 15080
KL Stats: Epoch 14 Divergences: Uniform: 3.67860311797364 Unigram: 2.9484888936287588
2022-03-15 20:46:07 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 20:46:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:48:25 | INFO | train_inner | epoch 015:     51 / 392 loss=6.803, ppl=111.7, wps=21524.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=5500, lr=0.000426401, gnorm=0.541, loss_scale=32, train_wall=243, gb_free=9.6, wall=15218
2022-03-15 20:48:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:52:56 | INFO | train_inner | epoch 015:    152 / 392 loss=6.777, ppl=109.67, wps=24128.3, ups=0.37, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.54, loss_scale=16, train_wall=249, gb_free=9.6, wall=15489
2022-03-15 20:55:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:57:28 | INFO | train_inner | epoch 015:    253 / 392 loss=6.776, ppl=109.61, wps=24136.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.538, loss_scale=16, train_wall=249, gb_free=9.6, wall=15761
2022-03-15 21:01:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:02:00 | INFO | train_inner | epoch 015:    354 / 392 loss=6.772, ppl=109.27, wps=24081.9, ups=0.37, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.531, loss_scale=16, train_wall=250, gb_free=9.6, wall=16033
2022-03-15 21:03:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:04:15 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.715 | ppl 105.06 | wps 36888.5 | wpb 511.9 | bsz 1 | num_updates 5838 | best_loss 6.715
2022-03-15 21:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5838 updates
2022-03-15 21:04:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:04:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:04:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 15 @ 5838 updates, score 6.715) (writing took 2.062972373329103 seconds)
2022-03-15 21:04:17 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 21:04:17 | INFO | train | epoch 015 | loss 6.772 | ppl 109.29 | wps 23343.5 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 5838 | lr 0.000413874 | gnorm 0.538 | loss_scale 16 | train_wall 966 | gb_free 9.6 | wall 16170
KL Stats: Epoch 15 Divergences: Uniform: 3.7118002621436825 Unigram: 2.9830474681194743
2022-03-15 21:04:17 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 21:04:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:07:04 | INFO | train_inner | epoch 016:     62 / 392 loss=6.72, ppl=105.43, wps=21396.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=5900, lr=0.000411693, gnorm=0.536, loss_scale=16, train_wall=245, gb_free=9.6, wall=16337
2022-03-15 21:08:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:11:32 | INFO | train_inner | epoch 016:    163 / 392 loss=6.706, ppl=104.38, wps=24420.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.54, loss_scale=16, train_wall=246, gb_free=9.6, wall=16605
2022-03-15 21:14:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:16:00 | INFO | train_inner | epoch 016:    264 / 392 loss=6.709, ppl=104.61, wps=24420, ups=0.37, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.536, loss_scale=16, train_wall=246, gb_free=9.6, wall=16874
2022-03-15 21:20:35 | INFO | train_inner | epoch 016:    364 / 392 loss=6.698, ppl=103.82, wps=23835, ups=0.36, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.534, loss_scale=32, train_wall=252, gb_free=9.6, wall=17148
2022-03-15 21:20:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:21:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:22:26 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.666 | ppl 101.53 | wps 36041.3 | wpb 511.9 | bsz 1 | num_updates 6227 | best_loss 6.666
2022-03-15 21:22:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6227 updates
2022-03-15 21:22:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:22:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:22:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 16 @ 6227 updates, score 6.666) (writing took 1.9598077395930886 seconds)
2022-03-15 21:22:28 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 21:22:28 | INFO | train | epoch 016 | loss 6.704 | ppl 104.25 | wps 23332.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 6227 | lr 0.000400738 | gnorm 0.537 | loss_scale 16 | train_wall 966 | gb_free 9.6 | wall 17261
KL Stats: Epoch 16 Divergences: Uniform: 3.737125370860267 Unigram: 3.006081524225678
2022-03-15 21:22:28 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 21:22:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:25:44 | INFO | train_inner | epoch 017:     73 / 392 loss=6.657, ppl=100.88, wps=21102.6, ups=0.32, wpb=65025.8, bsz=127, num_updates=6300, lr=0.00039841, gnorm=0.535, loss_scale=16, train_wall=248, gb_free=9.6, wall=17457
2022-03-15 21:27:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:30:13 | INFO | train_inner | epoch 017:    174 / 392 loss=6.647, ppl=100.22, wps=24348.9, ups=0.37, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.538, loss_scale=16, train_wall=247, gb_free=9.6, wall=17726
2022-03-15 21:33:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:34:43 | INFO | train_inner | epoch 017:    275 / 392 loss=6.647, ppl=100.25, wps=24277.9, ups=0.37, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.531, loss_scale=16, train_wall=248, gb_free=9.6, wall=17996
2022-03-15 21:39:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:39:13 | INFO | train_inner | epoch 017:    376 / 392 loss=6.651, ppl=100.47, wps=24221.9, ups=0.37, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.532, loss_scale=16, train_wall=249, gb_free=9.6, wall=18266
2022-03-15 21:39:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:40:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.62 | ppl 98.34 | wps 36861.7 | wpb 511.9 | bsz 1 | num_updates 6616 | best_loss 6.62
2022-03-15 21:40:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6616 updates
2022-03-15 21:40:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:40:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 17 @ 6616 updates, score 6.62) (writing took 2.2269936855882406 seconds)
2022-03-15 21:40:31 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 21:40:31 | INFO | train | epoch 017 | loss 6.646 | ppl 100.15 | wps 23477.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 6616 | lr 0.000388779 | gnorm 0.534 | loss_scale 16 | train_wall 961 | gb_free 9.6 | wall 18344
KL Stats: Epoch 17 Divergences: Uniform: 3.759736442168081 Unigram: 3.0260258925480708
2022-03-15 21:40:31 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 21:40:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:44:16 | INFO | train_inner | epoch 018:     84 / 392 loss=6.588, ppl=96.18, wps=21486.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=6700, lr=0.000386334, gnorm=0.542, loss_scale=16, train_wall=244, gb_free=9.6, wall=18569
2022-03-15 21:46:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:48:46 | INFO | train_inner | epoch 018:    185 / 392 loss=6.595, ppl=96.66, wps=24299.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.528, loss_scale=16, train_wall=248, gb_free=9.6, wall=18839
2022-03-15 21:52:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:53:15 | INFO | train_inner | epoch 018:    286 / 392 loss=6.601, ppl=97.05, wps=24324.3, ups=0.37, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.53, loss_scale=16, train_wall=247, gb_free=9.6, wall=19108
2022-03-15 21:57:42 | INFO | train_inner | epoch 018:    386 / 392 loss=6.603, ppl=97.23, wps=24502.7, ups=0.37, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.532, loss_scale=16, train_wall=246, gb_free=9.6, wall=19376
2022-03-15 21:57:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:58:32 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.592 | ppl 96.46 | wps 36576.1 | wpb 511.9 | bsz 1 | num_updates 7006 | best_loss 6.592
2022-03-15 21:58:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7006 updates
2022-03-15 21:58:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 21:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 18 @ 7006 updates, score 6.592) (writing took 1.9178404966369271 seconds)
2022-03-15 21:58:34 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 21:58:34 | INFO | train | epoch 018 | loss 6.595 | ppl 96.66 | wps 23562.8 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 7006 | lr 0.000377803 | gnorm 0.532 | loss_scale 16 | train_wall 960 | gb_free 9.6 | wall 19427
KL Stats: Epoch 18 Divergences: Uniform: 3.779534868653007 Unigram: 3.0460543682887877
2022-03-15 21:58:34 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 21:58:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:58:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:02:48 | INFO | train_inner | epoch 019:     95 / 392 loss=6.533, ppl=92.62, wps=21253.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=7100, lr=0.000375293, gnorm=0.53, loss_scale=16, train_wall=247, gb_free=9.6, wall=19682
2022-03-15 22:04:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:07:17 | INFO | train_inner | epoch 019:    196 / 392 loss=6.55, ppl=93.71, wps=24416.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.535, loss_scale=16, train_wall=246, gb_free=9.6, wall=19950
2022-03-15 22:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:11:48 | INFO | train_inner | epoch 019:    297 / 392 loss=6.557, ppl=94.18, wps=24158.4, ups=0.37, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.526, loss_scale=16, train_wall=249, gb_free=9.6, wall=20221
2022-03-15 22:16:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:16:38 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.551 | ppl 93.78 | wps 36213.4 | wpb 511.9 | bsz 1 | num_updates 7395 | best_loss 6.551
2022-03-15 22:16:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7395 updates
2022-03-15 22:16:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 22:16:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 22:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 19 @ 7395 updates, score 6.551) (writing took 1.9732744134962559 seconds)
2022-03-15 22:16:40 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-15 22:16:40 | INFO | train | epoch 019 | loss 6.55 | ppl 93.69 | wps 23434.6 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 7395 | lr 0.000367732 | gnorm 0.53 | loss_scale 16 | train_wall 962 | gb_free 9.6 | wall 20513
KL Stats: Epoch 19 Divergences: Uniform: 3.7970221885249265 Unigram: 3.0630817808125377
2022-03-15 22:16:40 | INFO | fairseq.trainer | begin training epoch 20
2022-03-15 22:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:16:53 | INFO | train_inner | epoch 020:      5 / 392 loss=6.556, ppl=94.1, wps=21315.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=7400, lr=0.000367607, gnorm=0.533, loss_scale=16, train_wall=246, gb_free=9.6, wall=20526
2022-03-15 22:17:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:21:27 | INFO | train_inner | epoch 020:    106 / 392 loss=6.503, ppl=90.73, wps=23945.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.528, loss_scale=16, train_wall=251, gb_free=9.6, wall=20800
2022-03-15 22:23:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:25:57 | INFO | train_inner | epoch 020:    207 / 392 loss=6.507, ppl=90.92, wps=24238.1, ups=0.37, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.532, loss_scale=16, train_wall=248, gb_free=9.6, wall=21070
2022-03-15 22:29:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:30:26 | INFO | train_inner | epoch 020:    308 / 392 loss=6.517, ppl=91.58, wps=24381.2, ups=0.37, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.528, loss_scale=16, train_wall=247, gb_free=9.6, wall=21339
2022-03-15 22:34:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:34:42 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.529 | ppl 92.35 | wps 37207.1 | wpb 511.9 | bsz 1 | num_updates 7784 | best_loss 6.529
2022-03-15 22:34:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7784 updates
2022-03-15 22:34:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 22:34:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 22:34:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 20 @ 7784 updates, score 6.529) (writing took 1.89887641556561 seconds)
2022-03-15 22:34:44 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-15 22:34:44 | INFO | train | epoch 020 | loss 6.51 | ppl 91.15 | wps 23456.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 7784 | lr 0.000358425 | gnorm 0.532 | loss_scale 16 | train_wall 962 | gb_free 9.6 | wall 21597
KL Stats: Epoch 20 Divergences: Uniform: 3.8144546213481734 Unigram: 3.0788034614151414
2022-03-15 22:34:44 | INFO | fairseq.trainer | begin training epoch 21
2022-03-15 22:34:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:35:27 | INFO | train_inner | epoch 021:     16 / 392 loss=6.507, ppl=90.96, wps=21608.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=7800, lr=0.000358057, gnorm=0.544, loss_scale=16, train_wall=243, gb_free=9.6, wall=21640
2022-03-15 22:35:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:39:56 | INFO | train_inner | epoch 021:    117 / 392 loss=6.463, ppl=88.23, wps=24359.9, ups=0.37, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.525, loss_scale=16, train_wall=247, gb_free=9.6, wall=21909
2022-03-15 22:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:44:25 | INFO | train_inner | epoch 021:    218 / 392 loss=6.463, ppl=88.25, wps=24329.8, ups=0.37, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.531, loss_scale=16, train_wall=247, gb_free=9.6, wall=22178
2022-03-15 22:48:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:48:58 | INFO | train_inner | epoch 021:    319 / 392 loss=6.481, ppl=89.31, wps=24053.3, ups=0.37, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.521, loss_scale=16, train_wall=250, gb_free=9.6, wall=22451
2022-03-15 22:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:52:45 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.501 | ppl 90.6 | wps 37248 | wpb 511.9 | bsz 1 | num_updates 8173 | best_loss 6.501
2022-03-15 22:52:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8173 updates
2022-03-15 22:52:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 22:52:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 22:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 21 @ 8173 updates, score 6.501) (writing took 1.9436134025454521 seconds)
2022-03-15 22:52:47 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-15 22:52:47 | INFO | train | epoch 021 | loss 6.473 | ppl 88.83 | wps 23501.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 8173 | lr 0.000349791 | gnorm 0.53 | loss_scale 16 | train_wall 961 | gb_free 9.6 | wall 22680
KL Stats: Epoch 21 Divergences: Uniform: 3.8306380346808377 Unigram: 3.0920439674131286
2022-03-15 22:52:47 | INFO | fairseq.trainer | begin training epoch 22
2022-03-15 22:52:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:53:59 | INFO | train_inner | epoch 022:     27 / 392 loss=6.469, ppl=88.59, wps=21604, ups=0.33, wpb=65025.8, bsz=127, num_updates=8200, lr=0.000349215, gnorm=0.538, loss_scale=16, train_wall=243, gb_free=9.6, wall=22752
2022-03-15 22:55:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:58:28 | INFO | train_inner | epoch 022:    128 / 392 loss=6.428, ppl=86.08, wps=24368.3, ups=0.37, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.523, loss_scale=16, train_wall=247, gb_free=9.6, wall=23021
2022-03-15 23:01:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:02:56 | INFO | train_inner | epoch 022:    229 / 392 loss=6.448, ppl=87.3, wps=24417, ups=0.37, wpb=65532.7, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.532, loss_scale=16, train_wall=247, gb_free=9.6, wall=23289
2022-03-15 23:07:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:07:27 | INFO | train_inner | epoch 022:    330 / 392 loss=6.448, ppl=87.33, wps=24220.5, ups=0.37, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.527, loss_scale=16, train_wall=248, gb_free=9.6, wall=23560
2022-03-15 23:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:10:44 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.478 | ppl 89.16 | wps 37364.8 | wpb 511.9 | bsz 1 | num_updates 8562 | best_loss 6.478
2022-03-15 23:10:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8562 updates
2022-03-15 23:10:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 23:10:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 23:10:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 22 @ 8562 updates, score 6.478) (writing took 2.123334778472781 seconds)
2022-03-15 23:10:46 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-15 23:10:46 | INFO | train | epoch 022 | loss 6.441 | ppl 86.86 | wps 23574.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 8562 | lr 0.000341753 | gnorm 0.526 | loss_scale 16 | train_wall 957 | gb_free 9.6 | wall 23759
KL Stats: Epoch 22 Divergences: Uniform: 3.8454066620072975 Unigram: 3.1078312430261605
2022-03-15 23:10:46 | INFO | fairseq.trainer | begin training epoch 23
2022-03-15 23:10:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:12:27 | INFO | train_inner | epoch 023:     38 / 392 loss=6.424, ppl=85.87, wps=21644.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=8600, lr=0.000340997, gnorm=0.526, loss_scale=16, train_wall=242, gb_free=9.6, wall=23860
2022-03-15 23:13:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:16:56 | INFO | train_inner | epoch 023:    139 / 392 loss=6.401, ppl=84.52, wps=24389.5, ups=0.37, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.524, loss_scale=16, train_wall=247, gb_free=9.6, wall=24129
2022-03-15 23:19:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:21:24 | INFO | train_inner | epoch 023:    240 / 392 loss=6.413, ppl=85.2, wps=24437, ups=0.37, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.529, loss_scale=16, train_wall=246, gb_free=9.6, wall=24397
2022-03-15 23:25:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:25:52 | INFO | train_inner | epoch 023:    341 / 392 loss=6.421, ppl=85.7, wps=24424.8, ups=0.37, wpb=65532.7, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.524, loss_scale=16, train_wall=247, gb_free=9.6, wall=24665
2022-03-15 23:28:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:28:40 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.461 | ppl 88.08 | wps 37396.1 | wpb 511.9 | bsz 1 | num_updates 8951 | best_loss 6.461
2022-03-15 23:28:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 8951 updates
2022-03-15 23:28:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 23:28:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 23:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 23 @ 8951 updates, score 6.461) (writing took 1.8766710050404072 seconds)
2022-03-15 23:28:42 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-15 23:28:42 | INFO | train | epoch 023 | loss 6.411 | ppl 85.11 | wps 23649.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 8951 | lr 0.000334244 | gnorm 0.529 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 24835
KL Stats: Epoch 23 Divergences: Uniform: 3.8566121986038233 Unigram: 3.118025119399472
2022-03-15 23:28:42 | INFO | fairseq.trainer | begin training epoch 24
2022-03-15 23:28:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:30:52 | INFO | train_inner | epoch 024:     49 / 392 loss=6.401, ppl=84.5, wps=21708, ups=0.33, wpb=65029.1, bsz=127, num_updates=9000, lr=0.000333333, gnorm=0.535, loss_scale=16, train_wall=242, gb_free=9.6, wall=24965
2022-03-15 23:31:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:35:20 | INFO | train_inner | epoch 024:    150 / 392 loss=6.373, ppl=82.89, wps=24472.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.534, loss_scale=16, train_wall=246, gb_free=9.6, wall=25233
2022-03-15 23:37:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:39:47 | INFO | train_inner | epoch 024:    251 / 392 loss=6.39, ppl=83.87, wps=24485.8, ups=0.37, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.525, loss_scale=16, train_wall=246, gb_free=9.6, wall=25500
2022-03-15 23:43:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:44:15 | INFO | train_inner | epoch 024:    352 / 392 loss=6.391, ppl=83.93, wps=24474.7, ups=0.37, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.532, loss_scale=16, train_wall=246, gb_free=9.6, wall=25768
2022-03-15 23:46:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:46:34 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.446 | ppl 87.2 | wps 37336.7 | wpb 511.9 | bsz 1 | num_updates 9340 | best_loss 6.446
2022-03-15 23:46:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9340 updates
2022-03-15 23:46:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 23:46:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-15 23:46:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 24 @ 9340 updates, score 6.446) (writing took 1.8506504092365503 seconds)
2022-03-15 23:46:36 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-15 23:46:36 | INFO | train | epoch 024 | loss 6.383 | ppl 83.49 | wps 23687.5 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 9340 | lr 0.00032721 | gnorm 0.532 | loss_scale 16 | train_wall 953 | gb_free 9.6 | wall 25909
KL Stats: Epoch 24 Divergences: Uniform: 3.8713427082816434 Unigram: 3.13146881676718
2022-03-15 23:46:36 | INFO | fairseq.trainer | begin training epoch 25
2022-03-15 23:46:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:49:16 | INFO | train_inner | epoch 025:     60 / 392 loss=6.355, ppl=81.84, wps=21635.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=9400, lr=0.000326164, gnorm=0.537, loss_scale=16, train_wall=243, gb_free=9.6, wall=26069
2022-03-15 23:50:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 23:53:47 | INFO | train_inner | epoch 025:    162 / 392 loss=6.352, ppl=81.67, wps=24136.3, ups=0.37, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.533, loss_scale=8, train_wall=249, gb_free=9.6, wall=26340
2022-03-15 23:58:13 | INFO | train_inner | epoch 025:    262 / 392 loss=6.367, ppl=82.55, wps=24676.6, ups=0.38, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.536, loss_scale=16, train_wall=244, gb_free=9.6, wall=26606
2022-03-16 00:02:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:02:41 | INFO | train_inner | epoch 025:    363 / 392 loss=6.367, ppl=82.54, wps=24417, ups=0.37, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.523, loss_scale=16, train_wall=247, gb_free=9.6, wall=26874
2022-03-16 00:03:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:04:31 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.423 | ppl 85.83 | wps 37340.6 | wpb 511.9 | bsz 1 | num_updates 9729 | best_loss 6.423
2022-03-16 00:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9729 updates
2022-03-16 00:04:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:04:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:04:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 25 @ 9729 updates, score 6.423) (writing took 1.933690369129181 seconds)
2022-03-16 00:04:33 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-16 00:04:33 | INFO | train | epoch 025 | loss 6.358 | ppl 82.02 | wps 23627.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 9729 | lr 0.000320602 | gnorm 0.531 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 26986
KL Stats: Epoch 25 Divergences: Uniform: 3.8821881274314234 Unigram: 3.141694786360461
2022-03-16 00:04:33 | INFO | fairseq.trainer | begin training epoch 26
2022-03-16 00:04:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:07:42 | INFO | train_inner | epoch 026:     71 / 392 loss=6.328, ppl=80.36, wps=21649.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=9800, lr=0.000319438, gnorm=0.531, loss_scale=16, train_wall=242, gb_free=9.6, wall=27175
2022-03-16 00:08:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:12:10 | INFO | train_inner | epoch 026:    172 / 392 loss=6.329, ppl=80.37, wps=24414.1, ups=0.37, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.524, loss_scale=16, train_wall=246, gb_free=9.6, wall=27443
2022-03-16 00:14:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:16:39 | INFO | train_inner | epoch 026:    273 / 392 loss=6.338, ppl=80.89, wps=24387.9, ups=0.37, wpb=65532.7, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.529, loss_scale=16, train_wall=247, gb_free=9.6, wall=27712
2022-03-16 00:19:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 00:21:07 | INFO | train_inner | epoch 026:    374 / 392 loss=6.35, ppl=81.55, wps=24386.7, ups=0.37, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.529, loss_scale=8, train_wall=247, gb_free=9.6, wall=27981
2022-03-16 00:21:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:22:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.411 | ppl 85.1 | wps 37197.4 | wpb 511.9 | bsz 1 | num_updates 10118 | best_loss 6.411
2022-03-16 00:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10118 updates
2022-03-16 00:22:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:22:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:22:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 26 @ 10118 updates, score 6.411) (writing took 1.7653580764308572 seconds)
2022-03-16 00:22:30 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 00:22:30 | INFO | train | epoch 026 | loss 6.334 | ppl 80.66 | wps 23618.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 10118 | lr 0.000314378 | gnorm 0.528 | loss_scale 8 | train_wall 956 | gb_free 9.6 | wall 28063
KL Stats: Epoch 26 Divergences: Uniform: 3.893764496315686 Unigram: 3.1513038580858646
2022-03-16 00:22:30 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 00:22:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:26:08 | INFO | train_inner | epoch 027:     82 / 392 loss=6.299, ppl=78.76, wps=21615.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=10200, lr=0.000313112, gnorm=0.538, loss_scale=16, train_wall=243, gb_free=9.6, wall=28281
2022-03-16 00:27:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 00:30:36 | INFO | train_inner | epoch 027:    183 / 392 loss=6.304, ppl=78.99, wps=24455, ups=0.37, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.532, loss_scale=8, train_wall=246, gb_free=9.6, wall=28549
2022-03-16 00:35:02 | INFO | train_inner | epoch 027:    283 / 392 loss=6.323, ppl=80.08, wps=24690.8, ups=0.38, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.534, loss_scale=16, train_wall=244, gb_free=9.6, wall=28815
2022-03-16 00:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:39:30 | INFO | train_inner | epoch 027:    384 / 392 loss=6.331, ppl=80.49, wps=24449, ups=0.37, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.541, loss_scale=16, train_wall=246, gb_free=9.6, wall=29083
2022-03-16 00:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:40:24 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.399 | ppl 84.37 | wps 37322.6 | wpb 511.9 | bsz 1 | num_updates 10508 | best_loss 6.399
2022-03-16 00:40:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10508 updates
2022-03-16 00:40:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:40:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:40:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 27 @ 10508 updates, score 6.399) (writing took 2.0659383982419968 seconds)
2022-03-16 00:40:26 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 00:40:26 | INFO | train | epoch 027 | loss 6.313 | ppl 79.48 | wps 23712.4 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 10508 | lr 0.000308489 | gnorm 0.537 | loss_scale 16 | train_wall 954 | gb_free 9.6 | wall 29139
KL Stats: Epoch 27 Divergences: Uniform: 3.9029687860872677 Unigram: 3.1617615887176447
2022-03-16 00:40:26 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 00:40:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:44:30 | INFO | train_inner | epoch 028:     92 / 392 loss=6.273, ppl=77.34, wps=21627.3, ups=0.33, wpb=65025.8, bsz=127, num_updates=10600, lr=0.000307148, gnorm=0.531, loss_scale=16, train_wall=242, gb_free=9.6, wall=29383
2022-03-16 00:45:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:49:02 | INFO | train_inner | epoch 028:    193 / 392 loss=6.284, ppl=77.93, wps=24137.7, ups=0.37, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.535, loss_scale=16, train_wall=249, gb_free=9.6, wall=29655
2022-03-16 00:51:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:53:34 | INFO | train_inner | epoch 028:    294 / 392 loss=6.305, ppl=79.05, wps=24095.1, ups=0.37, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.532, loss_scale=16, train_wall=250, gb_free=9.6, wall=29927
2022-03-16 00:57:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:57:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:58:27 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.388 | ppl 83.75 | wps 37248.2 | wpb 511.9 | bsz 1 | num_updates 10897 | best_loss 6.388
2022-03-16 00:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10897 updates
2022-03-16 00:58:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:58:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 00:58:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 28 @ 10897 updates, score 6.388) (writing took 1.8947010887786746 seconds)
2022-03-16 00:58:29 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 00:58:29 | INFO | train | epoch 028 | loss 6.292 | ppl 78.35 | wps 23481.1 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 10897 | lr 0.000302933 | gnorm 0.534 | loss_scale 16 | train_wall 962 | gb_free 9.6 | wall 30222
KL Stats: Epoch 28 Divergences: Uniform: 3.913795249978654 Unigram: 3.1696524834737954
2022-03-16 00:58:29 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 00:58:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:58:37 | INFO | train_inner | epoch 029:      3 / 392 loss=6.305, ppl=79.08, wps=21435.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=10900, lr=0.000302891, gnorm=0.541, loss_scale=16, train_wall=245, gb_free=9.6, wall=30230
2022-03-16 01:03:03 | INFO | train_inner | epoch 029:    103 / 392 loss=6.246, ppl=75.91, wps=24647.3, ups=0.38, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.53, loss_scale=16, train_wall=244, gb_free=9.6, wall=30496
2022-03-16 01:03:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:07:31 | INFO | train_inner | epoch 029:    204 / 392 loss=6.27, ppl=77.15, wps=24433.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.53, loss_scale=16, train_wall=246, gb_free=9.6, wall=30764
2022-03-16 01:09:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:12:00 | INFO | train_inner | epoch 029:    305 / 392 loss=6.281, ppl=77.74, wps=24437.3, ups=0.37, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.527, loss_scale=16, train_wall=246, gb_free=9.6, wall=31033
2022-03-16 01:15:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:16:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.37 | ppl 82.71 | wps 37403.8 | wpb 511.9 | bsz 1 | num_updates 11287 | best_loss 6.37
2022-03-16 01:16:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11287 updates
2022-03-16 01:16:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 01:16:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 01:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 29 @ 11287 updates, score 6.37) (writing took 1.8522207215428352 seconds)
2022-03-16 01:16:26 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 01:16:26 | INFO | train | epoch 029 | loss 6.273 | ppl 77.31 | wps 23678.3 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 11287 | lr 0.000297653 | gnorm 0.533 | loss_scale 32 | train_wall 956 | gb_free 9.6 | wall 31300
KL Stats: Epoch 29 Divergences: Uniform: 3.921850393362427 Unigram: 3.177868605887718
2022-03-16 01:16:27 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 01:16:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:16:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:17:04 | INFO | train_inner | epoch 030:     14 / 392 loss=6.289, ppl=78.18, wps=21375.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=11300, lr=0.000297482, gnorm=0.542, loss_scale=16, train_wall=246, gb_free=9.6, wall=31337
2022-03-16 01:21:29 | INFO | train_inner | epoch 030:    114 / 392 loss=6.24, ppl=75.57, wps=24667.5, ups=0.38, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.527, loss_scale=16, train_wall=244, gb_free=9.6, wall=31603
2022-03-16 01:22:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:25:58 | INFO | train_inner | epoch 030:    215 / 392 loss=6.251, ppl=76.17, wps=24431.3, ups=0.37, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.544, loss_scale=16, train_wall=246, gb_free=9.6, wall=31871
2022-03-16 01:28:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:29:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:30:29 | INFO | train_inner | epoch 030:    317 / 392 loss=6.266, ppl=76.98, wps=24180.8, ups=0.37, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.521, loss_scale=8, train_wall=249, gb_free=9.6, wall=32142
2022-03-16 01:33:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:34:21 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.358 | ppl 82.01 | wps 37196.7 | wpb 511.9 | bsz 1 | num_updates 11675 | best_loss 6.358
2022-03-16 01:34:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11675 updates
2022-03-16 01:34:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 01:34:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 01:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 30 @ 11675 updates, score 6.358) (writing took 1.8630421133711934 seconds)
2022-03-16 01:34:23 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 01:34:23 | INFO | train | epoch 030 | loss 6.254 | ppl 76.34 | wps 23578.3 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 11675 | lr 0.000292666 | gnorm 0.531 | loss_scale 8 | train_wall 955 | gb_free 9.6 | wall 32376
KL Stats: Epoch 30 Divergences: Uniform: 3.930175060641588 Unigram: 3.1847543340109143
2022-03-16 01:34:23 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 01:34:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:35:29 | INFO | train_inner | epoch 031:     25 / 392 loss=6.251, ppl=76.17, wps=21627.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=11700, lr=0.000292353, gnorm=0.535, loss_scale=8, train_wall=242, gb_free=9.6, wall=32442
2022-03-16 01:39:55 | INFO | train_inner | epoch 031:    125 / 392 loss=6.214, ppl=74.21, wps=24643.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.529, loss_scale=16, train_wall=244, gb_free=9.6, wall=32708
2022-03-16 01:41:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:44:23 | INFO | train_inner | epoch 031:    226 / 392 loss=6.251, ppl=76.18, wps=24448.6, ups=0.37, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.527, loss_scale=16, train_wall=246, gb_free=9.6, wall=32976
2022-03-16 01:46:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:48:51 | INFO | train_inner | epoch 031:    327 / 392 loss=6.244, ppl=75.77, wps=24454, ups=0.37, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.527, loss_scale=8, train_wall=246, gb_free=9.6, wall=33244
2022-03-16 01:51:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:52:16 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.345 | ppl 81.26 | wps 37387.9 | wpb 511.9 | bsz 1 | num_updates 12065 | best_loss 6.345
2022-03-16 01:52:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12065 updates
2022-03-16 01:52:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 01:52:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 01:52:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 31 @ 12065 updates, score 6.345) (writing took 1.9935497315600514 seconds)
2022-03-16 01:52:18 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 01:52:18 | INFO | train | epoch 031 | loss 6.238 | ppl 75.46 | wps 23713.2 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 12065 | lr 0.000287896 | gnorm 0.531 | loss_scale 8 | train_wall 954 | gb_free 9.6 | wall 33452
KL Stats: Epoch 31 Divergences: Uniform: 3.937761429009514 Unigram: 3.192153205389783
2022-03-16 01:52:18 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 01:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:53:52 | INFO | train_inner | epoch 032:     35 / 392 loss=6.239, ppl=75.54, wps=21663.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=12100, lr=0.00028748, gnorm=0.546, loss_scale=16, train_wall=242, gb_free=9.6, wall=33545
2022-03-16 01:58:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:58:20 | INFO | train_inner | epoch 032:    136 / 392 loss=6.209, ppl=73.96, wps=24408.5, ups=0.37, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.534, loss_scale=16, train_wall=247, gb_free=9.6, wall=33813
2022-03-16 01:58:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:02:49 | INFO | train_inner | epoch 032:    237 / 392 loss=6.224, ppl=74.73, wps=24402.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.533, loss_scale=8, train_wall=247, gb_free=9.6, wall=34082
2022-03-16 02:07:15 | INFO | train_inner | epoch 032:    337 / 392 loss=6.225, ppl=74.82, wps=24643.4, ups=0.38, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.529, loss_scale=16, train_wall=244, gb_free=9.6, wall=34348
2022-03-16 02:09:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:10:13 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.339 | ppl 80.95 | wps 37315.3 | wpb 511.9 | bsz 1 | num_updates 12455 | best_loss 6.339
2022-03-16 02:10:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12455 updates
2022-03-16 02:10:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 02:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 02:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 32 @ 12455 updates, score 6.339) (writing took 1.8419191781431437 seconds)
2022-03-16 02:10:15 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 02:10:15 | INFO | train | epoch 032 | loss 6.222 | ppl 74.65 | wps 23689.7 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 12455 | lr 0.000283353 | gnorm 0.536 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 34528
KL Stats: Epoch 32 Divergences: Uniform: 3.9485442100641213 Unigram: 3.2008884799525363
2022-03-16 02:10:15 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 02:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:10:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:12:18 | INFO | train_inner | epoch 033:     46 / 392 loss=6.217, ppl=74.38, wps=21453.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=12500, lr=0.000282843, gnorm=0.541, loss_scale=16, train_wall=245, gb_free=9.6, wall=34651
2022-03-16 02:16:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:16:46 | INFO | train_inner | epoch 033:    147 / 392 loss=6.191, ppl=73.04, wps=24400.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.532, loss_scale=16, train_wall=247, gb_free=9.6, wall=34919
2022-03-16 02:21:12 | INFO | train_inner | epoch 033:    247 / 392 loss=6.214, ppl=74.23, wps=24640.8, ups=0.38, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.53, loss_scale=16, train_wall=244, gb_free=9.6, wall=35185
2022-03-16 02:22:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:25:41 | INFO | train_inner | epoch 033:    348 / 392 loss=6.215, ppl=74.27, wps=24411.3, ups=0.37, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.538, loss_scale=16, train_wall=247, gb_free=9.6, wall=35454
2022-03-16 02:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:28:10 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.331 | ppl 80.51 | wps 37423.7 | wpb 511.9 | bsz 1 | num_updates 12844 | best_loss 6.331
2022-03-16 02:28:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12844 updates
2022-03-16 02:28:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 02:28:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 02:28:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 33 @ 12844 updates, score 6.331) (writing took 1.8732425533235073 seconds)
2022-03-16 02:28:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 02:28:12 | INFO | train | epoch 033 | loss 6.206 | ppl 73.82 | wps 23626.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 12844 | lr 0.000279029 | gnorm 0.535 | loss_scale 16 | train_wall 956 | gb_free 9.6 | wall 35605
KL Stats: Epoch 33 Divergences: Uniform: 3.957221502133051 Unigram: 3.2084291520119357
2022-03-16 02:28:12 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 02:28:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:28:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:30:44 | INFO | train_inner | epoch 034:     57 / 392 loss=6.193, ppl=73.15, wps=21460.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=12900, lr=0.000278423, gnorm=0.541, loss_scale=16, train_wall=245, gb_free=9.6, wall=35757
2022-03-16 02:35:10 | INFO | train_inner | epoch 034:    157 / 392 loss=6.181, ppl=72.58, wps=24587.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.53, loss_scale=32, train_wall=245, gb_free=9.6, wall=36023
2022-03-16 02:35:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:39:38 | INFO | train_inner | epoch 034:    258 / 392 loss=6.198, ppl=73.41, wps=24439.3, ups=0.37, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.526, loss_scale=16, train_wall=246, gb_free=9.6, wall=36291
2022-03-16 02:41:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:44:07 | INFO | train_inner | epoch 034:    359 / 392 loss=6.206, ppl=73.8, wps=24438.9, ups=0.37, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.537, loss_scale=16, train_wall=246, gb_free=9.6, wall=36560
2022-03-16 02:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:46:07 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.318 | ppl 79.76 | wps 37346.4 | wpb 511.9 | bsz 1 | num_updates 13233 | best_loss 6.318
2022-03-16 02:46:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13233 updates
2022-03-16 02:46:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 02:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 02:46:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 34 @ 13233 updates, score 6.318) (writing took 1.8053509518504143 seconds)
2022-03-16 02:46:09 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 02:46:09 | INFO | train | epoch 034 | loss 6.192 | ppl 73.11 | wps 23634 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 13233 | lr 0.000274898 | gnorm 0.533 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 36682
KL Stats: Epoch 34 Divergences: Uniform: 3.962445461444555 Unigram: 3.213867661575654
2022-03-16 02:46:09 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 02:46:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:47:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:49:12 | INFO | train_inner | epoch 035:     68 / 392 loss=6.169, ppl=71.95, wps=21291, ups=0.33, wpb=65029.1, bsz=127, num_updates=13300, lr=0.000274204, gnorm=0.546, loss_scale=16, train_wall=247, gb_free=9.6, wall=36865
2022-03-16 02:53:41 | INFO | train_inner | epoch 035:    168 / 392 loss=6.168, ppl=71.9, wps=24326.8, ups=0.37, wpb=65532.7, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.524, loss_scale=32, train_wall=247, gb_free=9.6, wall=37134
2022-03-16 02:53:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:56:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:58:13 | INFO | train_inner | epoch 035:    270 / 392 loss=6.193, ppl=73.18, wps=24142.6, ups=0.37, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.537, loss_scale=8, train_wall=249, gb_free=9.6, wall=37406
2022-03-16 03:02:39 | INFO | train_inner | epoch 035:    370 / 392 loss=6.193, ppl=73.18, wps=24638, ups=0.38, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.531, loss_scale=16, train_wall=244, gb_free=9.6, wall=37672
2022-03-16 03:02:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:03:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:04:10 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.308 | ppl 79.24 | wps 37137.9 | wpb 511.9 | bsz 1 | num_updates 13621 | best_loss 6.308
2022-03-16 03:04:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13621 updates
2022-03-16 03:04:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 03:04:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 03:04:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 35 @ 13621 updates, score 6.308) (writing took 1.8497037505730987 seconds)
2022-03-16 03:04:12 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 03:04:12 | INFO | train | epoch 035 | loss 6.179 | ppl 72.47 | wps 23425.3 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 13621 | lr 0.000270954 | gnorm 0.535 | loss_scale 8 | train_wall 961 | gb_free 9.6 | wall 37765
KL Stats: Epoch 35 Divergences: Uniform: 3.9695571139455743 Unigram: 3.219829990739516
2022-03-16 03:04:12 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 03:04:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:07:42 | INFO | train_inner | epoch 036:     79 / 392 loss=6.145, ppl=70.79, wps=21445.8, ups=0.33, wpb=65025.8, bsz=127, num_updates=13700, lr=0.000270172, gnorm=0.545, loss_scale=8, train_wall=245, gb_free=9.6, wall=37975
2022-03-16 03:12:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:12:10 | INFO | train_inner | epoch 036:    180 / 392 loss=6.164, ppl=71.7, wps=24439.4, ups=0.37, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.538, loss_scale=8, train_wall=246, gb_free=9.6, wall=38243
2022-03-16 03:16:36 | INFO | train_inner | epoch 036:    280 / 392 loss=6.174, ppl=72.2, wps=24677.8, ups=0.38, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.528, loss_scale=8, train_wall=244, gb_free=9.6, wall=38509
2022-03-16 03:21:03 | INFO | train_inner | epoch 036:    380 / 392 loss=6.186, ppl=72.81, wps=24566.8, ups=0.37, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.535, loss_scale=16, train_wall=245, gb_free=9.6, wall=38776
2022-03-16 03:21:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:22:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.31 | ppl 79.34 | wps 37318.6 | wpb 511.9 | bsz 1 | num_updates 14012 | best_loss 6.308
2022-03-16 03:22:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14012 updates
2022-03-16 03:22:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 03:22:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 03:22:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 36 @ 14012 updates, score 6.31) (writing took 0.838182121515274 seconds)
2022-03-16 03:22:08 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 03:22:08 | INFO | train | epoch 036 | loss 6.166 | ppl 71.81 | wps 23754.3 | ups 0.36 | wpb 65405.5 | bsz 127.7 | num_updates 14012 | lr 0.000267147 | gnorm 0.536 | loss_scale 16 | train_wall 956 | gb_free 9.6 | wall 38842
KL Stats: Epoch 36 Divergences: Uniform: 3.978533960585479 Unigram: 3.2289918357281673
2022-03-16 03:22:09 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 03:22:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:24:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:26:05 | INFO | train_inner | epoch 037:     89 / 392 loss=6.137, ppl=70.36, wps=21493.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=14100, lr=0.000266312, gnorm=0.542, loss_scale=16, train_wall=245, gb_free=9.6, wall=39078
2022-03-16 03:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:29:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:30:36 | INFO | train_inner | epoch 037:    191 / 392 loss=6.147, ppl=70.86, wps=24203.3, ups=0.37, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.541, loss_scale=8, train_wall=249, gb_free=9.6, wall=39349
2022-03-16 03:35:01 | INFO | train_inner | epoch 037:    291 / 392 loss=6.162, ppl=71.6, wps=24674.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.536, loss_scale=8, train_wall=244, gb_free=9.6, wall=39615
2022-03-16 03:39:27 | INFO | train_inner | epoch 037:    391 / 392 loss=6.176, ppl=72.29, wps=24665.6, ups=0.38, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.538, loss_scale=16, train_wall=244, gb_free=9.6, wall=39880
2022-03-16 03:39:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:40:02 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.301 | ppl 78.83 | wps 37250.2 | wpb 511.9 | bsz 1 | num_updates 14401 | best_loss 6.301
2022-03-16 03:40:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14401 updates
2022-03-16 03:40:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 03:40:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 03:40:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 37 @ 14401 updates, score 6.301) (writing took 2.0035379277542233 seconds)
2022-03-16 03:40:04 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 03:40:04 | INFO | train | epoch 037 | loss 6.154 | ppl 71.22 | wps 23645.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 14401 | lr 0.000263514 | gnorm 0.539 | loss_scale 16 | train_wall 954 | gb_free 9.6 | wall 39918
KL Stats: Epoch 37 Divergences: Uniform: 3.9846838370259494 Unigram: 3.233536600458963
2022-03-16 03:40:04 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 03:40:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:41:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:44:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:44:33 | INFO | train_inner | epoch 038:    101 / 392 loss=6.12, ppl=69.56, wps=21263.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=14500, lr=0.000262613, gnorm=0.551, loss_scale=8, train_wall=247, gb_free=9.6, wall=40186
2022-03-16 03:48:59 | INFO | train_inner | epoch 038:    201 / 392 loss=6.137, ppl=70.36, wps=24654.3, ups=0.38, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.536, loss_scale=8, train_wall=244, gb_free=9.6, wall=40452
2022-03-16 03:53:25 | INFO | train_inner | epoch 038:    301 / 392 loss=6.153, ppl=71.15, wps=24637.5, ups=0.38, wpb=65532.7, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.535, loss_scale=16, train_wall=244, gb_free=9.6, wall=40718
2022-03-16 03:54:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:57:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:58:03 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.295 | ppl 78.52 | wps 36837.5 | wpb 511.9 | bsz 1 | num_updates 14790 | best_loss 6.295
2022-03-16 03:58:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14790 updates
2022-03-16 03:58:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 03:58:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 03:58:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 38 @ 14790 updates, score 6.295) (writing took 1.9257505852729082 seconds)
2022-03-16 03:58:05 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 03:58:05 | INFO | train | epoch 038 | loss 6.142 | ppl 70.62 | wps 23549.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 14790 | lr 0.000260025 | gnorm 0.54 | loss_scale 8 | train_wall 958 | gb_free 9.6 | wall 40998
KL Stats: Epoch 38 Divergences: Uniform: 3.991249696835109 Unigram: 3.238796718330719
2022-03-16 03:58:05 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 03:58:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:58:32 | INFO | train_inner | epoch 039:     10 / 392 loss=6.153, ppl=71.17, wps=21176.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=14800, lr=0.000259938, gnorm=0.541, loss_scale=8, train_wall=248, gb_free=9.6, wall=41025
2022-03-16 04:02:58 | INFO | train_inner | epoch 039:    110 / 392 loss=6.112, ppl=69.17, wps=24657.8, ups=0.38, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.539, loss_scale=16, train_wall=244, gb_free=9.6, wall=41291
2022-03-16 04:06:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:07:26 | INFO | train_inner | epoch 039:    211 / 392 loss=6.126, ppl=69.82, wps=24428.9, ups=0.37, wpb=65532.7, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.532, loss_scale=16, train_wall=246, gb_free=9.6, wall=41559
2022-03-16 04:11:51 | INFO | train_inner | epoch 039:    311 / 392 loss=6.144, ppl=70.72, wps=24682.6, ups=0.38, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.539, loss_scale=16, train_wall=244, gb_free=9.6, wall=41824
2022-03-16 04:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:15:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:15:59 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.279 | ppl 77.67 | wps 37393.9 | wpb 511.9 | bsz 1 | num_updates 15180 | best_loss 6.279
2022-03-16 04:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15180 updates
2022-03-16 04:15:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 04:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 04:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 39 @ 15180 updates, score 6.279) (writing took 1.8994457088410854 seconds)
2022-03-16 04:16:01 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 04:16:01 | INFO | train | epoch 039 | loss 6.132 | ppl 70.11 | wps 23702.4 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 15180 | lr 0.000256664 | gnorm 0.537 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 42074
KL Stats: Epoch 39 Divergences: Uniform: 3.996002081420207 Unigram: 3.2429966977147218
2022-03-16 04:16:01 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 04:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:16:54 | INFO | train_inner | epoch 040:     20 / 392 loss=6.136, ppl=70.32, wps=21471.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=15200, lr=0.000256495, gnorm=0.541, loss_scale=16, train_wall=245, gb_free=9.6, wall=42127
2022-03-16 04:18:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:21:23 | INFO | train_inner | epoch 040:    121 / 392 loss=6.102, ppl=68.71, wps=24403.2, ups=0.37, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.541, loss_scale=16, train_wall=247, gb_free=9.6, wall=42396
2022-03-16 04:24:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:25:51 | INFO | train_inner | epoch 040:    222 / 392 loss=6.12, ppl=69.54, wps=24414.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.536, loss_scale=16, train_wall=247, gb_free=9.6, wall=42664
2022-03-16 04:27:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:30:21 | INFO | train_inner | epoch 040:    323 / 392 loss=6.132, ppl=70.16, wps=24283.3, ups=0.37, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.536, loss_scale=8, train_wall=248, gb_free=9.6, wall=42934
2022-03-16 04:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:33:57 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.279 | ppl 77.64 | wps 37326.9 | wpb 511.9 | bsz 1 | num_updates 15569 | best_loss 6.279
2022-03-16 04:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15569 updates
2022-03-16 04:33:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 04:33:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 04:33:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 40 @ 15569 updates, score 6.279) (writing took 1.8373181875795126 seconds)
2022-03-16 04:33:59 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 04:33:59 | INFO | train | epoch 040 | loss 6.121 | ppl 69.6 | wps 23599.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 15569 | lr 0.000253437 | gnorm 0.54 | loss_scale 16 | train_wall 956 | gb_free 9.6 | wall 43152
KL Stats: Epoch 40 Divergences: Uniform: 4.00378949880754 Unigram: 3.2490188371047983
2022-03-16 04:33:59 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 04:33:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:35:22 | INFO | train_inner | epoch 041:     31 / 392 loss=6.123, ppl=69.71, wps=21645.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=15600, lr=0.000253185, gnorm=0.547, loss_scale=16, train_wall=242, gb_free=9.6, wall=43235
2022-03-16 04:39:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:39:50 | INFO | train_inner | epoch 041:    132 / 392 loss=6.101, ppl=68.65, wps=24386.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.541, loss_scale=16, train_wall=247, gb_free=9.6, wall=43503
2022-03-16 04:44:16 | INFO | train_inner | epoch 041:    232 / 392 loss=6.112, ppl=69.16, wps=24645.9, ups=0.38, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.538, loss_scale=16, train_wall=244, gb_free=9.6, wall=43769
2022-03-16 04:45:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:48:44 | INFO | train_inner | epoch 041:    333 / 392 loss=6.124, ppl=69.73, wps=24464.8, ups=0.37, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.531, loss_scale=16, train_wall=246, gb_free=9.6, wall=44037
2022-03-16 04:50:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:51:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:51:53 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.275 | ppl 77.41 | wps 37376.3 | wpb 511.9 | bsz 1 | num_updates 15958 | best_loss 6.275
2022-03-16 04:51:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 15958 updates
2022-03-16 04:51:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 04:51:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 04:51:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 41 @ 15958 updates, score 6.275) (writing took 1.8818728001788259 seconds)
2022-03-16 04:51:55 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 04:51:55 | INFO | train | epoch 041 | loss 6.111 | ppl 69.13 | wps 23645.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 15958 | lr 0.000250329 | gnorm 0.539 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 44228
KL Stats: Epoch 41 Divergences: Uniform: 4.010350408581807 Unigram: 3.2545083825693255
2022-03-16 04:51:55 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 04:51:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:52:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:53:49 | INFO | train_inner | epoch 042:     43 / 392 loss=6.103, ppl=68.75, wps=21309.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=16000, lr=0.00025, gnorm=0.547, loss_scale=8, train_wall=247, gb_free=9.6, wall=44342
2022-03-16 04:58:15 | INFO | train_inner | epoch 042:    143 / 392 loss=6.089, ppl=68.06, wps=24695.6, ups=0.38, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.54, loss_scale=16, train_wall=244, gb_free=9.6, wall=44608
2022-03-16 05:02:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:02:43 | INFO | train_inner | epoch 042:    244 / 392 loss=6.1, ppl=68.57, wps=24388.7, ups=0.37, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.536, loss_scale=8, train_wall=247, gb_free=9.6, wall=44876
2022-03-16 05:07:12 | INFO | train_inner | epoch 042:    344 / 392 loss=6.119, ppl=69.52, wps=24433.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.536, loss_scale=8, train_wall=246, gb_free=9.6, wall=45145
2022-03-16 05:09:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:09:55 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.272 | ppl 77.28 | wps 36249.2 | wpb 511.9 | bsz 1 | num_updates 16348 | best_loss 6.272
2022-03-16 05:09:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16348 updates
2022-03-16 05:09:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 05:09:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 05:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 42 @ 16348 updates, score 6.272) (writing took 2.04282611515373 seconds)
2022-03-16 05:09:57 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 05:09:57 | INFO | train | epoch 042 | loss 6.101 | ppl 68.65 | wps 23576 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 16348 | lr 0.000247325 | gnorm 0.54 | loss_scale 16 | train_wall 959 | gb_free 9.6 | wall 45310
KL Stats: Epoch 42 Divergences: Uniform: 4.018552910537953 Unigram: 3.2607557965248724
2022-03-16 05:09:57 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 05:09:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:12:16 | INFO | train_inner | epoch 043:     52 / 392 loss=6.091, ppl=68.19, wps=21383.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=16400, lr=0.000246932, gnorm=0.543, loss_scale=16, train_wall=245, gb_free=9.6, wall=45449
2022-03-16 05:14:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:16:44 | INFO | train_inner | epoch 043:    153 / 392 loss=6.081, ppl=67.7, wps=24456.2, ups=0.37, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.545, loss_scale=16, train_wall=246, gb_free=9.6, wall=45717
2022-03-16 05:20:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:21:12 | INFO | train_inner | epoch 043:    254 / 392 loss=6.096, ppl=68.4, wps=24424.2, ups=0.37, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.541, loss_scale=16, train_wall=246, gb_free=9.6, wall=45985
2022-03-16 05:25:38 | INFO | train_inner | epoch 043:    354 / 392 loss=6.108, ppl=68.99, wps=24676.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.534, loss_scale=16, train_wall=244, gb_free=9.6, wall=46251
2022-03-16 05:26:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:27:51 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.261 | ppl 76.68 | wps 37310.3 | wpb 511.9 | bsz 1 | num_updates 16737 | best_loss 6.261
2022-03-16 05:27:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16737 updates
2022-03-16 05:27:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 05:27:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 05:27:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 43 @ 16737 updates, score 6.261) (writing took 1.935846752487123 seconds)
2022-03-16 05:27:53 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 05:27:53 | INFO | train | epoch 043 | loss 6.092 | ppl 68.22 | wps 23648 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 16737 | lr 0.000244434 | gnorm 0.543 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 46386
KL Stats: Epoch 43 Divergences: Uniform: 4.0217435107761474 Unigram: 3.2641657030262676
2022-03-16 05:27:53 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 05:27:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:30:41 | INFO | train_inner | epoch 044:     63 / 392 loss=6.076, ppl=67.47, wps=21461.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=16800, lr=0.000243975, gnorm=0.552, loss_scale=16, train_wall=245, gb_free=9.6, wall=46554
2022-03-16 05:32:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:35:09 | INFO | train_inner | epoch 044:    164 / 392 loss=6.067, ppl=67.02, wps=24419.8, ups=0.37, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.543, loss_scale=16, train_wall=246, gb_free=9.6, wall=46822
2022-03-16 05:39:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:39:37 | INFO | train_inner | epoch 044:    265 / 392 loss=6.097, ppl=68.45, wps=24398.6, ups=0.37, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.536, loss_scale=16, train_wall=247, gb_free=9.6, wall=47091
2022-03-16 05:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:44:06 | INFO | train_inner | epoch 044:    366 / 392 loss=6.092, ppl=68.22, wps=24439.7, ups=0.37, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.543, loss_scale=8, train_wall=246, gb_free=9.6, wall=47359
2022-03-16 05:45:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:45:47 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.262 | ppl 76.76 | wps 37441.3 | wpb 511.9 | bsz 1 | num_updates 17126 | best_loss 6.261
2022-03-16 05:45:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17126 updates
2022-03-16 05:45:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 05:45:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 05:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 44 @ 17126 updates, score 6.262) (writing took 0.8877645283937454 seconds)
2022-03-16 05:45:48 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 05:45:48 | INFO | train | epoch 044 | loss 6.082 | ppl 67.76 | wps 23666.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 17126 | lr 0.000241642 | gnorm 0.543 | loss_scale 8 | train_wall 955 | gb_free 9.6 | wall 47461
KL Stats: Epoch 44 Divergences: Uniform: 4.027359455641304 Unigram: 3.270351063895917
2022-03-16 05:45:48 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 05:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:49:05 | INFO | train_inner | epoch 045:     74 / 392 loss=6.063, ppl=66.87, wps=21739.6, ups=0.33, wpb=65025.8, bsz=127, num_updates=17200, lr=0.000241121, gnorm=0.543, loss_scale=16, train_wall=242, gb_free=9.6, wall=47658
2022-03-16 05:53:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:53:33 | INFO | train_inner | epoch 045:    175 / 392 loss=6.061, ppl=66.78, wps=24422, ups=0.37, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.546, loss_scale=8, train_wall=246, gb_free=9.6, wall=47926
2022-03-16 05:57:59 | INFO | train_inner | epoch 045:    275 / 392 loss=6.086, ppl=67.93, wps=24675.7, ups=0.38, wpb=65532.7, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.541, loss_scale=8, train_wall=244, gb_free=9.6, wall=48192
2022-03-16 06:02:24 | INFO | train_inner | epoch 045:    375 / 392 loss=6.093, ppl=68.26, wps=24683, ups=0.38, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.532, loss_scale=16, train_wall=244, gb_free=9.6, wall=48457
2022-03-16 06:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:03:42 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.257 | ppl 76.48 | wps 37328.7 | wpb 511.9 | bsz 1 | num_updates 17517 | best_loss 6.257
2022-03-16 06:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17517 updates
2022-03-16 06:03:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:03:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:03:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 45 @ 17517 updates, score 6.257) (writing took 1.9646746776998043 seconds)
2022-03-16 06:03:44 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 06:03:44 | INFO | train | epoch 045 | loss 6.075 | ppl 67.4 | wps 23770 | ups 0.36 | wpb 65405.5 | bsz 127.7 | num_updates 17517 | lr 0.00023893 | gnorm 0.54 | loss_scale 16 | train_wall 955 | gb_free 9.6 | wall 48537
KL Stats: Epoch 45 Divergences: Uniform: 4.033422970361796 Unigram: 3.2736029804844926
2022-03-16 06:03:44 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 06:03:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:05:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:07:27 | INFO | train_inner | epoch 046:     84 / 392 loss=6.052, ppl=66.33, wps=21467.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=17600, lr=0.000238366, gnorm=0.548, loss_scale=16, train_wall=244, gb_free=9.6, wall=48760
2022-03-16 06:11:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:11:55 | INFO | train_inner | epoch 046:    185 / 392 loss=6.064, ppl=66.89, wps=24422.6, ups=0.37, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.546, loss_scale=16, train_wall=246, gb_free=9.6, wall=49029
2022-03-16 06:16:21 | INFO | train_inner | epoch 046:    285 / 392 loss=6.072, ppl=67.27, wps=24684.8, ups=0.38, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.536, loss_scale=16, train_wall=244, gb_free=9.6, wall=49294
2022-03-16 06:17:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:20:51 | INFO | train_inner | epoch 046:    386 / 392 loss=6.079, ppl=67.62, wps=24251.1, ups=0.37, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.541, loss_scale=16, train_wall=248, gb_free=9.6, wall=49564
2022-03-16 06:21:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:21:40 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.25 | ppl 76.09 | wps 36722 | wpb 511.9 | bsz 1 | num_updates 17906 | best_loss 6.25
2022-03-16 06:21:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 17906 updates
2022-03-16 06:21:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:21:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:21:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 46 @ 17906 updates, score 6.25) (writing took 1.8157099410891533 seconds)
2022-03-16 06:21:42 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 06:21:42 | INFO | train | epoch 046 | loss 6.066 | ppl 66.98 | wps 23592.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 17906 | lr 0.00023632 | gnorm 0.543 | loss_scale 16 | train_wall 956 | gb_free 9.6 | wall 49615
KL Stats: Epoch 46 Divergences: Uniform: 4.039111554733135 Unigram: 3.2779376464993275
2022-03-16 06:21:42 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 06:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:24:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:25:59 | INFO | train_inner | epoch 047:     95 / 392 loss=6.037, ppl=65.68, wps=21130.1, ups=0.32, wpb=65029.1, bsz=127, num_updates=18000, lr=0.000235702, gnorm=0.542, loss_scale=16, train_wall=249, gb_free=9.6, wall=49872
2022-03-16 06:30:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:30:33 | INFO | train_inner | epoch 047:    196 / 392 loss=6.051, ppl=66.28, wps=23944.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.543, loss_scale=16, train_wall=251, gb_free=9.6, wall=50146
2022-03-16 06:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:35:02 | INFO | train_inner | epoch 047:    297 / 392 loss=6.067, ppl=67.03, wps=24363, ups=0.37, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.545, loss_scale=8, train_wall=247, gb_free=9.6, wall=50415
2022-03-16 06:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:39:46 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.248 | ppl 75.99 | wps 37341.2 | wpb 511.9 | bsz 1 | num_updates 18295 | best_loss 6.248
2022-03-16 06:39:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18295 updates
2022-03-16 06:39:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:39:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:39:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 47 @ 18295 updates, score 6.248) (writing took 1.8308781832456589 seconds)
2022-03-16 06:39:48 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 06:39:48 | INFO | train | epoch 047 | loss 6.058 | ppl 66.63 | wps 23427.1 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 18295 | lr 0.000233794 | gnorm 0.545 | loss_scale 16 | train_wall 964 | gb_free 9.6 | wall 50701
KL Stats: Epoch 47 Divergences: Uniform: 4.044305498671101 Unigram: 3.2832766284769552
2022-03-16 06:39:48 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 06:39:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:40:02 | INFO | train_inner | epoch 048:      5 / 392 loss=6.077, ppl=67.49, wps=21672.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=18300, lr=0.000233762, gnorm=0.555, loss_scale=16, train_wall=242, gb_free=9.6, wall=50715
2022-03-16 06:44:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:44:33 | INFO | train_inner | epoch 048:    106 / 392 loss=6.03, ppl=65.34, wps=24135.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.538, loss_scale=16, train_wall=249, gb_free=9.6, wall=50986
2022-03-16 06:48:59 | INFO | train_inner | epoch 048:    206 / 392 loss=6.045, ppl=66.03, wps=24655.2, ups=0.38, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.536, loss_scale=16, train_wall=244, gb_free=9.6, wall=51252
2022-03-16 06:50:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:53:27 | INFO | train_inner | epoch 048:    307 / 392 loss=6.063, ppl=66.84, wps=24451.6, ups=0.37, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.546, loss_scale=16, train_wall=246, gb_free=9.6, wall=51520
2022-03-16 06:56:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:56:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:57:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:57:45 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.246 | ppl 75.9 | wps 37411.8 | wpb 511.9 | bsz 1 | num_updates 18683 | best_loss 6.246
2022-03-16 06:57:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18683 updates
2022-03-16 06:57:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:57:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 06:57:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 48 @ 18683 updates, score 6.246) (writing took 1.9475569427013397 seconds)
2022-03-16 06:57:47 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 06:57:47 | INFO | train | epoch 048 | loss 6.05 | ppl 66.28 | wps 23527.4 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 18683 | lr 0.000231354 | gnorm 0.54 | loss_scale 8 | train_wall 957 | gb_free 9.6 | wall 51780
KL Stats: Epoch 48 Divergences: Uniform: 4.049558838449178 Unigram: 3.2876249714068964
2022-03-16 06:57:47 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 06:57:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:58:32 | INFO | train_inner | epoch 049:     17 / 392 loss=6.06, ppl=66.71, wps=21309.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=18700, lr=0.000231249, gnorm=0.542, loss_scale=8, train_wall=247, gb_free=9.6, wall=51825
2022-03-16 07:02:58 | INFO | train_inner | epoch 049:    117 / 392 loss=6.023, ppl=65.04, wps=24666.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.543, loss_scale=16, train_wall=244, gb_free=9.6, wall=52091
2022-03-16 07:07:23 | INFO | train_inner | epoch 049:    217 / 392 loss=6.043, ppl=65.92, wps=24681.4, ups=0.38, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.542, loss_scale=16, train_wall=244, gb_free=9.6, wall=52356
2022-03-16 07:08:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:08:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:11:54 | INFO | train_inner | epoch 049:    319 / 392 loss=6.053, ppl=66.4, wps=24184.9, ups=0.37, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.539, loss_scale=8, train_wall=249, gb_free=9.6, wall=52627
2022-03-16 07:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:15:41 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.239 | ppl 75.51 | wps 37262.2 | wpb 511.9 | bsz 1 | num_updates 19073 | best_loss 6.239
2022-03-16 07:15:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19073 updates
2022-03-16 07:15:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 07:15:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 07:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 49 @ 19073 updates, score 6.239) (writing took 1.896452596411109 seconds)
2022-03-16 07:15:43 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 07:15:43 | INFO | train | epoch 049 | loss 6.043 | ppl 65.93 | wps 23708.6 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 19073 | lr 0.000228976 | gnorm 0.543 | loss_scale 16 | train_wall 954 | gb_free 9.6 | wall 52856
KL Stats: Epoch 49 Divergences: Uniform: 4.0544545900317175 Unigram: 3.291199800125262
2022-03-16 07:15:43 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 07:15:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:16:55 | INFO | train_inner | epoch 050:     27 / 392 loss=6.044, ppl=65.97, wps=21661.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=19100, lr=0.000228814, gnorm=0.543, loss_scale=16, train_wall=242, gb_free=9.6, wall=52928
2022-03-16 07:20:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:21:20 | INFO | train_inner | epoch 050:    128 / 392 loss=6.015, ppl=64.68, wps=24661.2, ups=0.38, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.545, loss_scale=16, train_wall=244, gb_free=9.6, wall=53193
2022-03-16 07:25:43 | INFO | train_inner | epoch 050:    228 / 392 loss=6.035, ppl=65.58, wps=24906.3, ups=0.38, wpb=65532.7, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.544, loss_scale=16, train_wall=242, gb_free=9.6, wall=53457
2022-03-16 07:27:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:28:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:30:12 | INFO | train_inner | epoch 050:    330 / 392 loss=6.051, ppl=66.28, wps=24430.4, ups=0.37, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.54, loss_scale=8, train_wall=246, gb_free=9.6, wall=53725
2022-03-16 07:32:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:33:27 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.24 | ppl 75.59 | wps 37673.7 | wpb 511.9 | bsz 1 | num_updates 19462 | best_loss 6.239
2022-03-16 07:33:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19462 updates
2022-03-16 07:33:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 07:33:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 07:33:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 50 @ 19462 updates, score 6.24) (writing took 0.9020371306687593 seconds)
2022-03-16 07:33:28 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 07:33:28 | INFO | train | epoch 050 | loss 6.036 | ppl 65.62 | wps 23888.6 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 19462 | lr 0.000226676 | gnorm 0.547 | loss_scale 8 | train_wall 945 | gb_free 9.6 | wall 53921
KL Stats: Epoch 50 Divergences: Uniform: 4.060070548097753 Unigram: 3.2964617784346766
2022-03-16 07:33:28 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 07:33:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:35:08 | INFO | train_inner | epoch 051:     38 / 392 loss=6.043, ppl=65.95, wps=21941.8, ups=0.34, wpb=65029.1, bsz=127, num_updates=19500, lr=0.000226455, gnorm=0.554, loss_scale=16, train_wall=240, gb_free=9.6, wall=54021
2022-03-16 07:39:32 | INFO | train_inner | epoch 051:    138 / 392 loss=6.011, ppl=64.5, wps=24878.1, ups=0.38, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.542, loss_scale=16, train_wall=242, gb_free=9.6, wall=54285
2022-03-16 07:40:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:43:57 | INFO | train_inner | epoch 051:    239 / 392 loss=6.028, ppl=65.27, wps=24685.9, ups=0.38, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.544, loss_scale=16, train_wall=244, gb_free=9.6, wall=54550
2022-03-16 07:46:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:48:23 | INFO | train_inner | epoch 051:    340 / 392 loss=6.049, ppl=66.22, wps=24666.1, ups=0.38, wpb=65532.7, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.542, loss_scale=16, train_wall=244, gb_free=9.6, wall=54816
2022-03-16 07:50:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:51:12 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.233 | ppl 75.22 | wps 37640.9 | wpb 511.9 | bsz 1 | num_updates 19851 | best_loss 6.233
2022-03-16 07:51:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 19851 updates
2022-03-16 07:51:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 07:51:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 07:51:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 51 @ 19851 updates, score 6.233) (writing took 1.800231502391398 seconds)
2022-03-16 07:51:14 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 07:51:14 | INFO | train | epoch 051 | loss 6.03 | ppl 65.33 | wps 23869.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 19851 | lr 0.000224444 | gnorm 0.546 | loss_scale 8 | train_wall 945 | gb_free 9.6 | wall 54987
KL Stats: Epoch 51 Divergences: Uniform: 4.064298642495403 Unigram: 3.298707018213389
2022-03-16 07:51:14 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 07:51:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:53:23 | INFO | train_inner | epoch 052:     49 / 392 loss=6.027, ppl=65.2, wps=21667.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=19900, lr=0.000224168, gnorm=0.56, loss_scale=8, train_wall=242, gb_free=9.6, wall=55116
2022-03-16 07:57:46 | INFO | train_inner | epoch 052:    149 / 392 loss=6.007, ppl=64.32, wps=24862, ups=0.38, wpb=65532.7, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.54, loss_scale=16, train_wall=242, gb_free=9.6, wall=55379
2022-03-16 08:02:10 | INFO | train_inner | epoch 052:    249 / 392 loss=6.024, ppl=65.06, wps=24899.8, ups=0.38, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.543, loss_scale=32, train_wall=242, gb_free=9.6, wall=55643
2022-03-16 08:02:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:06:35 | INFO | train_inner | epoch 052:    350 / 392 loss=6.041, ppl=65.86, wps=24667.4, ups=0.38, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.549, loss_scale=16, train_wall=244, gb_free=9.6, wall=55908
2022-03-16 08:08:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:08:58 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.228 | ppl 74.95 | wps 37565.2 | wpb 511.9 | bsz 1 | num_updates 20241 | best_loss 6.228
2022-03-16 08:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20241 updates
2022-03-16 08:08:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 08:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 08:09:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 52 @ 20241 updates, score 6.228) (writing took 1.8058395171537995 seconds)
2022-03-16 08:09:00 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 08:09:00 | INFO | train | epoch 052 | loss 6.023 | ppl 65.01 | wps 23921.5 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 20241 | lr 0.000222272 | gnorm 0.546 | loss_scale 16 | train_wall 946 | gb_free 9.6 | wall 56053
KL Stats: Epoch 52 Divergences: Uniform: 4.069371332218055 Unigram: 3.302272993296811
2022-03-16 08:09:00 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 08:09:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:11:36 | INFO | train_inner | epoch 053:     59 / 392 loss=6.006, ppl=64.28, wps=21655.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=20300, lr=0.000221948, gnorm=0.549, loss_scale=16, train_wall=242, gb_free=9.6, wall=56209
2022-03-16 08:14:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:16:01 | INFO | train_inner | epoch 053:    160 / 392 loss=6.008, ppl=64.36, wps=24643.3, ups=0.38, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.553, loss_scale=16, train_wall=244, gb_free=9.6, wall=56475
2022-03-16 08:20:25 | INFO | train_inner | epoch 053:    260 / 392 loss=6.018, ppl=64.8, wps=24894.7, ups=0.38, wpb=65532.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.544, loss_scale=32, train_wall=242, gb_free=9.6, wall=56738
2022-03-16 08:20:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:24:51 | INFO | train_inner | epoch 053:    361 / 392 loss=6.034, ppl=65.53, wps=24658.2, ups=0.38, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.552, loss_scale=16, train_wall=244, gb_free=9.6, wall=57004
2022-03-16 08:26:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:26:44 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.227 | ppl 74.92 | wps 37578 | wpb 511.9 | bsz 1 | num_updates 20630 | best_loss 6.227
2022-03-16 08:26:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20630 updates
2022-03-16 08:26:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 08:26:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 08:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 53 @ 20630 updates, score 6.227) (writing took 1.874721241183579 seconds)
2022-03-16 08:26:46 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 08:26:46 | INFO | train | epoch 053 | loss 6.016 | ppl 64.72 | wps 23861.5 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 20630 | lr 0.000220166 | gnorm 0.551 | loss_scale 16 | train_wall 945 | gb_free 9.6 | wall 57119
KL Stats: Epoch 53 Divergences: Uniform: 4.073722229270984 Unigram: 3.305950460702061
2022-03-16 08:26:46 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 08:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:29:51 | INFO | train_inner | epoch 054:     70 / 392 loss=5.995, ppl=63.79, wps=21651.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=20700, lr=0.000219793, gnorm=0.55, loss_scale=16, train_wall=242, gb_free=9.6, wall=57304
2022-03-16 08:32:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:34:17 | INFO | train_inner | epoch 054:    171 / 392 loss=6.004, ppl=64.18, wps=24633.2, ups=0.38, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.549, loss_scale=16, train_wall=244, gb_free=9.6, wall=57570
2022-03-16 08:37:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:38:45 | INFO | train_inner | epoch 054:    272 / 392 loss=6.018, ppl=64.78, wps=24473.8, ups=0.37, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.547, loss_scale=8, train_wall=246, gb_free=9.6, wall=57838
2022-03-16 08:43:11 | INFO | train_inner | epoch 054:    372 / 392 loss=6.027, ppl=65.23, wps=24644.5, ups=0.38, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.545, loss_scale=16, train_wall=244, gb_free=9.6, wall=58104
2022-03-16 08:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:44:36 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.226 | ppl 74.83 | wps 37591.7 | wpb 511.9 | bsz 1 | num_updates 21020 | best_loss 6.226
2022-03-16 08:44:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21020 updates
2022-03-16 08:44:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 08:44:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 08:44:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 54 @ 21020 updates, score 6.226) (writing took 1.8310557203367352 seconds)
2022-03-16 08:44:38 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 08:44:38 | INFO | train | epoch 054 | loss 6.01 | ppl 64.43 | wps 23810.9 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 21020 | lr 0.000218114 | gnorm 0.547 | loss_scale 16 | train_wall 950 | gb_free 9.6 | wall 58191
KL Stats: Epoch 54 Divergences: Uniform: 4.079042396480752 Unigram: 3.3113192870288604
2022-03-16 08:44:38 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 08:44:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:48:08 | INFO | train_inner | epoch 055:     80 / 392 loss=5.986, ppl=63.37, wps=21840.4, ups=0.34, wpb=65029.1, bsz=127, num_updates=21100, lr=0.0002177, gnorm=0.547, loss_scale=16, train_wall=240, gb_free=9.6, wall=58401
2022-03-16 08:49:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:52:34 | INFO | train_inner | epoch 055:    181 / 392 loss=5.997, ppl=63.85, wps=24637.2, ups=0.38, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.544, loss_scale=16, train_wall=244, gb_free=9.6, wall=58667
2022-03-16 08:55:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:57:02 | INFO | train_inner | epoch 055:    282 / 392 loss=6.012, ppl=64.55, wps=24497.5, ups=0.37, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.547, loss_scale=16, train_wall=246, gb_free=9.6, wall=58935
2022-03-16 09:01:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:01:30 | INFO | train_inner | epoch 055:    383 / 392 loss=6.018, ppl=64.79, wps=24413.8, ups=0.37, wpb=65532.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.54, loss_scale=16, train_wall=246, gb_free=9.6, wall=59203
2022-03-16 09:01:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:02:26 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.217 | ppl 74.36 | wps 37712.8 | wpb 511.9 | bsz 1 | num_updates 21409 | best_loss 6.217
2022-03-16 09:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21409 updates
2022-03-16 09:02:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:02:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:02:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 55 @ 21409 updates, score 6.217) (writing took 1.7640607003122568 seconds)
2022-03-16 09:02:28 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 09:02:28 | INFO | train | epoch 055 | loss 6.003 | ppl 64.15 | wps 23768.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 21409 | lr 0.000216123 | gnorm 0.545 | loss_scale 16 | train_wall 950 | gb_free 9.6 | wall 59261
KL Stats: Epoch 55 Divergences: Uniform: 4.082021494351093 Unigram: 3.312327828845918
2022-03-16 09:02:28 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 09:02:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:06:28 | INFO | train_inner | epoch 056:     91 / 392 loss=5.973, ppl=62.81, wps=21854.9, ups=0.34, wpb=65029.1, bsz=127, num_updates=21500, lr=0.000215666, gnorm=0.553, loss_scale=16, train_wall=240, gb_free=9.6, wall=59501
2022-03-16 09:07:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:10:54 | INFO | train_inner | epoch 056:    192 / 392 loss=5.993, ppl=63.71, wps=24643.2, ups=0.38, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.55, loss_scale=16, train_wall=244, gb_free=9.6, wall=59767
2022-03-16 09:12:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:15:20 | INFO | train_inner | epoch 056:    293 / 392 loss=6.012, ppl=64.53, wps=24600.4, ups=0.38, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.548, loss_scale=8, train_wall=245, gb_free=9.6, wall=60033
2022-03-16 09:18:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:19:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:20:20 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.216 | ppl 74.35 | wps 36979.1 | wpb 511.9 | bsz 1 | num_updates 21798 | best_loss 6.216
2022-03-16 09:20:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21798 updates
2022-03-16 09:20:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:20:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:20:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 56 @ 21798 updates, score 6.216) (writing took 1.9002639958634973 seconds)
2022-03-16 09:20:22 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 09:20:22 | INFO | train | epoch 056 | loss 5.998 | ppl 63.89 | wps 23695.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 21798 | lr 0.000214186 | gnorm 0.551 | loss_scale 8 | train_wall 952 | gb_free 9.6 | wall 60335
KL Stats: Epoch 56 Divergences: Uniform: 4.085681494495841 Unigram: 3.317135464229179
2022-03-16 09:20:22 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 09:20:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:20:27 | INFO | train_inner | epoch 057:      2 / 392 loss=6.017, ppl=64.76, wps=21176.4, ups=0.33, wpb=65025.8, bsz=127, num_updates=21800, lr=0.000214176, gnorm=0.556, loss_scale=8, train_wall=248, gb_free=9.6, wall=60340
2022-03-16 09:24:55 | INFO | train_inner | epoch 057:    102 / 392 loss=5.964, ppl=62.43, wps=24478.8, ups=0.37, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.549, loss_scale=8, train_wall=246, gb_free=9.6, wall=60608
2022-03-16 09:29:24 | INFO | train_inner | epoch 057:    202 / 392 loss=5.989, ppl=63.53, wps=24385, ups=0.37, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.551, loss_scale=16, train_wall=247, gb_free=9.6, wall=60877
2022-03-16 09:30:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:33:56 | INFO | train_inner | epoch 057:    303 / 392 loss=6.011, ppl=64.51, wps=24104.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.554, loss_scale=16, train_wall=250, gb_free=9.6, wall=61149
2022-03-16 09:37:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:37:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:38:27 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.216 | ppl 74.35 | wps 36952.7 | wpb 511.9 | bsz 1 | num_updates 22188 | best_loss 6.216
2022-03-16 09:38:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22188 updates
2022-03-16 09:38:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:38:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:38:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 57 @ 22188 updates, score 6.216) (writing took 1.7802483905106783 seconds)
2022-03-16 09:38:29 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 09:38:29 | INFO | train | epoch 057 | loss 5.992 | ppl 63.65 | wps 23458.1 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 22188 | lr 0.000212296 | gnorm 0.551 | loss_scale 16 | train_wall 964 | gb_free 9.6 | wall 61422
KL Stats: Epoch 57 Divergences: Uniform: 4.090726750648082 Unigram: 3.320390768880875
2022-03-16 09:38:29 | INFO | fairseq.trainer | begin training epoch 58
2022-03-16 09:38:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:39:01 | INFO | train_inner | epoch 058:     12 / 392 loss=6, ppl=64.01, wps=21264, ups=0.33, wpb=65029.1, bsz=127, num_updates=22200, lr=0.000212238, gnorm=0.549, loss_scale=16, train_wall=247, gb_free=9.6, wall=61455
2022-03-16 09:39:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:43:32 | INFO | train_inner | epoch 058:    113 / 392 loss=5.96, ppl=62.24, wps=24214.2, ups=0.37, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.547, loss_scale=8, train_wall=248, gb_free=9.6, wall=61725
2022-03-16 09:48:00 | INFO | train_inner | epoch 058:    213 / 392 loss=5.984, ppl=63.3, wps=24495.7, ups=0.37, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.546, loss_scale=16, train_wall=246, gb_free=9.6, wall=61993
2022-03-16 09:51:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:52:29 | INFO | train_inner | epoch 058:    314 / 392 loss=5.997, ppl=63.88, wps=24325.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.55, loss_scale=16, train_wall=247, gb_free=9.6, wall=62262
2022-03-16 09:55:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:56:26 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.213 | ppl 74.2 | wps 37631.9 | wpb 511.9 | bsz 1 | num_updates 22578 | best_loss 6.213
2022-03-16 09:56:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22578 updates
2022-03-16 09:56:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:56:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 09:56:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 58 @ 22578 updates, score 6.213) (writing took 2.015572370029986 seconds)
2022-03-16 09:56:28 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-16 09:56:28 | INFO | train | epoch 058 | loss 5.986 | ppl 63.4 | wps 23636.6 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 22578 | lr 0.000210454 | gnorm 0.549 | loss_scale 16 | train_wall 957 | gb_free 9.6 | wall 62501
KL Stats: Epoch 58 Divergences: Uniform: 4.092648882840586 Unigram: 3.322934007322037
2022-03-16 09:56:28 | INFO | fairseq.trainer | begin training epoch 59
2022-03-16 09:56:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:57:26 | INFO | train_inner | epoch 059:     22 / 392 loss=6.003, ppl=64.13, wps=21873.7, ups=0.34, wpb=65029.1, bsz=127, num_updates=22600, lr=0.000210352, gnorm=0.559, loss_scale=32, train_wall=240, gb_free=9.6, wall=62559
2022-03-16 09:57:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:01:57 | INFO | train_inner | epoch 059:    123 / 392 loss=5.965, ppl=62.48, wps=24223.4, ups=0.37, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.551, loss_scale=16, train_wall=248, gb_free=9.6, wall=62830
2022-03-16 10:03:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:05:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:06:30 | INFO | train_inner | epoch 059:    225 / 392 loss=5.981, ppl=63.18, wps=23974.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.563, loss_scale=8, train_wall=251, gb_free=9.6, wall=63103
2022-03-16 10:10:58 | INFO | train_inner | epoch 059:    325 / 392 loss=5.996, ppl=63.83, wps=24459.2, ups=0.37, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.544, loss_scale=8, train_wall=246, gb_free=9.6, wall=63371
2022-03-16 10:13:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:14:31 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.21 | ppl 74.03 | wps 36946.2 | wpb 511.9 | bsz 1 | num_updates 22967 | best_loss 6.21
2022-03-16 10:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 22967 updates
2022-03-16 10:14:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 10:14:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 10:14:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 59 @ 22967 updates, score 6.21) (writing took 2.0803567552939057 seconds)
2022-03-16 10:14:33 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-16 10:14:33 | INFO | train | epoch 059 | loss 5.982 | ppl 63.19 | wps 23453.6 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 22967 | lr 0.000208664 | gnorm 0.552 | loss_scale 16 | train_wall 962 | gb_free 9.6 | wall 63586
KL Stats: Epoch 59 Divergences: Uniform: 4.098708476528343 Unigram: 3.327767708617972
2022-03-16 10:14:33 | INFO | fairseq.trainer | begin training epoch 60
2022-03-16 10:14:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:16:00 | INFO | train_inner | epoch 060:     33 / 392 loss=5.973, ppl=62.79, wps=21547.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=23000, lr=0.000208514, gnorm=0.549, loss_scale=16, train_wall=243, gb_free=9.6, wall=63673
2022-03-16 10:18:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:20:30 | INFO | train_inner | epoch 060:    134 / 392 loss=5.961, ppl=62.29, wps=24255.9, ups=0.37, wpb=65532.7, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.547, loss_scale=16, train_wall=248, gb_free=9.6, wall=63943
2022-03-16 10:24:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:25:01 | INFO | train_inner | epoch 060:    235 / 392 loss=5.982, ppl=63.21, wps=24177.4, ups=0.37, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.547, loss_scale=16, train_wall=249, gb_free=9.6, wall=64214
2022-03-16 10:29:24 | INFO | train_inner | epoch 060:    335 / 392 loss=5.99, ppl=63.56, wps=24903.4, ups=0.38, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.538, loss_scale=16, train_wall=242, gb_free=9.6, wall=64477
2022-03-16 10:29:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:31:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:32:27 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.21 | ppl 74.04 | wps 37583.3 | wpb 511.9 | bsz 1 | num_updates 23355 | best_loss 6.21
2022-03-16 10:32:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23355 updates
2022-03-16 10:32:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 10:32:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 10:32:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 60 @ 23355 updates, score 6.21) (writing took 1.8861647536978126 seconds)
2022-03-16 10:32:28 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-16 10:32:28 | INFO | train | epoch 060 | loss 5.976 | ppl 62.94 | wps 23599.5 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 23355 | lr 0.000206924 | gnorm 0.546 | loss_scale 8 | train_wall 954 | gb_free 9.6 | wall 64661
KL Stats: Epoch 60 Divergences: Uniform: 4.1027120945898305 Unigram: 3.3303697076454846
2022-03-16 10:32:28 | INFO | fairseq.trainer | begin training epoch 61
2022-03-16 10:32:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:34:30 | INFO | train_inner | epoch 061:     45 / 392 loss=5.965, ppl=62.49, wps=21303.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=23400, lr=0.000206725, gnorm=0.552, loss_scale=8, train_wall=247, gb_free=9.6, wall=64783
2022-03-16 10:39:14 | INFO | train_inner | epoch 061:    145 / 392 loss=5.957, ppl=62.11, wps=23020.6, ups=0.35, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.551, loss_scale=16, train_wall=261, gb_free=9.6, wall=65067
2022-03-16 10:44:00 | INFO | train_inner | epoch 061:    245 / 392 loss=5.974, ppl=62.84, wps=22937.9, ups=0.35, wpb=65532.7, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.552, loss_scale=16, train_wall=262, gb_free=9.6, wall=65353
2022-03-16 10:44:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:48:48 | INFO | train_inner | epoch 061:    346 / 392 loss=5.985, ppl=63.34, wps=22744.6, ups=0.35, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.543, loss_scale=16, train_wall=264, gb_free=9.6, wall=65641
2022-03-16 10:50:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:51:34 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.2 | ppl 73.5 | wps 34508.4 | wpb 511.9 | bsz 1 | num_updates 23746 | best_loss 6.2
2022-03-16 10:51:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23746 updates
2022-03-16 10:51:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 10:51:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 10:51:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 61 @ 23746 updates, score 6.2) (writing took 1.8494109008461237 seconds)
2022-03-16 10:51:36 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-16 10:51:36 | INFO | train | epoch 061 | loss 5.972 | ppl 62.75 | wps 22288.8 | ups 0.34 | wpb 65405.5 | bsz 127.7 | num_updates 23746 | lr 0.000205213 | gnorm 0.552 | loss_scale 32 | train_wall 1017 | gb_free 9.6 | wall 65809
KL Stats: Epoch 61 Divergences: Uniform: 4.1039770943018095 Unigram: 3.332202287617013
2022-03-16 10:51:36 | INFO | fairseq.trainer | begin training epoch 62
2022-03-16 10:51:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:51:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:54:15 | INFO | train_inner | epoch 062:     56 / 392 loss=5.973, ppl=62.8, wps=19876, ups=0.31, wpb=65029.1, bsz=127, num_updates=23800, lr=0.00020498, gnorm=0.567, loss_scale=8, train_wall=264, gb_free=9.6, wall=65968
2022-03-16 10:58:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:58:47 | INFO | train_inner | epoch 062:    157 / 392 loss=5.954, ppl=61.98, wps=24130.1, ups=0.37, wpb=65532.7, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.547, loss_scale=8, train_wall=249, gb_free=9.6, wall=66240
2022-03-16 11:03:13 | INFO | train_inner | epoch 062:    257 / 392 loss=5.962, ppl=62.32, wps=24634.1, ups=0.38, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.547, loss_scale=8, train_wall=244, gb_free=9.6, wall=66506
2022-03-16 11:07:45 | INFO | train_inner | epoch 062:    357 / 392 loss=5.987, ppl=63.43, wps=24129.6, ups=0.37, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.55, loss_scale=16, train_wall=249, gb_free=9.6, wall=66778
2022-03-16 11:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:09:51 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.205 | ppl 73.78 | wps 36933.6 | wpb 511.9 | bsz 1 | num_updates 24135 | best_loss 6.2
2022-03-16 11:09:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24135 updates
2022-03-16 11:09:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 11:09:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 11:09:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 62 @ 24135 updates, score 6.205) (writing took 0.844300452619791 seconds)
2022-03-16 11:09:52 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-16 11:09:52 | INFO | train | epoch 062 | loss 5.966 | ppl 62.52 | wps 23211 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 24135 | lr 0.000203552 | gnorm 0.551 | loss_scale 16 | train_wall 973 | gb_free 9.6 | wall 66905
KL Stats: Epoch 62 Divergences: Uniform: 4.108450769065705 Unigram: 3.335129424346378
2022-03-16 11:09:52 | INFO | fairseq.trainer | begin training epoch 63
2022-03-16 11:09:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:10:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:12:48 | INFO | train_inner | epoch 063:     66 / 392 loss=5.949, ppl=61.77, wps=21393.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=24200, lr=0.000203279, gnorm=0.552, loss_scale=16, train_wall=246, gb_free=9.6, wall=67082
2022-03-16 11:15:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:17:22 | INFO | train_inner | epoch 063:    167 / 392 loss=5.956, ppl=62.07, wps=23971, ups=0.37, wpb=65532.7, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.547, loss_scale=16, train_wall=251, gb_free=9.6, wall=67355
2022-03-16 11:22:06 | INFO | train_inner | epoch 063:    267 / 392 loss=5.97, ppl=62.66, wps=23101.2, ups=0.35, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.555, loss_scale=32, train_wall=260, gb_free=9.6, wall=67639
2022-03-16 11:22:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:26:53 | INFO | train_inner | epoch 063:    368 / 392 loss=5.977, ppl=63.01, wps=22823.6, ups=0.35, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.561, loss_scale=16, train_wall=264, gb_free=9.6, wall=67926
2022-03-16 11:27:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:28:36 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.205 | ppl 73.8 | wps 34453.5 | wpb 511.9 | bsz 1 | num_updates 24524 | best_loss 6.2
2022-03-16 11:28:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24524 updates
2022-03-16 11:28:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 11:28:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 11:28:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 63 @ 24524 updates, score 6.205) (writing took 0.8631565338000655 seconds)
2022-03-16 11:28:37 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-16 11:28:37 | INFO | train | epoch 063 | loss 5.962 | ppl 62.32 | wps 22616.8 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 24524 | lr 0.000201932 | gnorm 0.554 | loss_scale 16 | train_wall 997 | gb_free 9.6 | wall 68030
KL Stats: Epoch 63 Divergences: Uniform: 4.114012503334296 Unigram: 3.3399957019183417
2022-03-16 11:28:37 | INFO | fairseq.trainer | begin training epoch 64
2022-03-16 11:28:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:29:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:29:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:32:14 | INFO | train_inner | epoch 064:     78 / 392 loss=5.942, ppl=61.48, wps=20254.6, ups=0.31, wpb=65025.8, bsz=127, num_updates=24600, lr=0.000201619, gnorm=0.561, loss_scale=8, train_wall=259, gb_free=9.6, wall=68247
2022-03-16 11:35:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:36:39 | INFO | train_inner | epoch 064:    179 / 392 loss=5.943, ppl=61.51, wps=24694.1, ups=0.38, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.569, loss_scale=8, train_wall=244, gb_free=9.6, wall=68512
2022-03-16 11:41:03 | INFO | train_inner | epoch 064:    279 / 392 loss=5.964, ppl=62.41, wps=24804.3, ups=0.38, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.557, loss_scale=8, train_wall=243, gb_free=9.6, wall=68776
2022-03-16 11:45:31 | INFO | train_inner | epoch 064:    379 / 392 loss=5.978, ppl=63.04, wps=24474, ups=0.37, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.548, loss_scale=16, train_wall=246, gb_free=9.6, wall=69044
2022-03-16 11:46:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:46:39 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.198 | ppl 73.42 | wps 36910.3 | wpb 511.9 | bsz 1 | num_updates 24913 | best_loss 6.198
2022-03-16 11:46:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 24913 updates
2022-03-16 11:46:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 11:46:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 11:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 64 @ 24913 updates, score 6.198) (writing took 1.9424675991758704 seconds)
2022-03-16 11:46:41 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-16 11:46:41 | INFO | train | epoch 064 | loss 5.956 | ppl 62.08 | wps 23470.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 24913 | lr 0.000200349 | gnorm 0.56 | loss_scale 16 | train_wall 961 | gb_free 9.6 | wall 69114
KL Stats: Epoch 64 Divergences: Uniform: 4.115346202602391 Unigram: 3.3409435918284953
2022-03-16 11:46:41 | INFO | fairseq.trainer | begin training epoch 65
2022-03-16 11:46:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:47:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:50:38 | INFO | train_inner | epoch 065:     88 / 392 loss=5.939, ppl=61.35, wps=21191.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=25000, lr=0.0002, gnorm=0.556, loss_scale=16, train_wall=248, gb_free=9.6, wall=69351
2022-03-16 11:53:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:55:11 | INFO | train_inner | epoch 065:    189 / 392 loss=5.94, ppl=61.41, wps=23999.4, ups=0.37, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.55, loss_scale=16, train_wall=251, gb_free=9.6, wall=69624
2022-03-16 11:59:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:59:39 | INFO | train_inner | epoch 065:    290 / 392 loss=5.959, ppl=62.19, wps=24491.2, ups=0.37, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.553, loss_scale=16, train_wall=246, gb_free=9.6, wall=69892
2022-03-16 12:04:02 | INFO | train_inner | epoch 065:    390 / 392 loss=5.975, ppl=62.88, wps=24885.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.553, loss_scale=16, train_wall=242, gb_free=9.6, wall=70155
2022-03-16 12:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:04:40 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.199 | ppl 73.45 | wps 37565.9 | wpb 511.9 | bsz 1 | num_updates 25302 | best_loss 6.198
2022-03-16 12:04:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25302 updates
2022-03-16 12:04:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 12:04:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 12:04:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 65 @ 25302 updates, score 6.199) (writing took 0.941779900342226 seconds)
2022-03-16 12:04:41 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-16 12:04:41 | INFO | train | epoch 065 | loss 5.952 | ppl 61.92 | wps 23564.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 25302 | lr 0.000198803 | gnorm 0.553 | loss_scale 16 | train_wall 959 | gb_free 9.6 | wall 70194
KL Stats: Epoch 65 Divergences: Uniform: 4.119950255868558 Unigram: 3.3444277591936675
2022-03-16 12:04:41 | INFO | fairseq.trainer | begin training epoch 66
2022-03-16 12:04:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:05:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:09:07 | INFO | train_inner | epoch 066:     99 / 392 loss=5.925, ppl=60.74, wps=21353, ups=0.33, wpb=65029.1, bsz=127, num_updates=25400, lr=0.000198419, gnorm=0.552, loss_scale=16, train_wall=247, gb_free=9.6, wall=70460
2022-03-16 12:11:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:13:54 | INFO | train_inner | epoch 066:    200 / 392 loss=5.949, ppl=61.78, wps=22826.4, ups=0.35, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.555, loss_scale=8, train_wall=263, gb_free=9.6, wall=70747
2022-03-16 12:18:32 | INFO | train_inner | epoch 066:    300 / 392 loss=5.959, ppl=62.19, wps=23554.9, ups=0.36, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.548, loss_scale=16, train_wall=255, gb_free=9.6, wall=71025
2022-03-16 12:20:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:22:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:23:07 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.195 | ppl 73.24 | wps 37510.5 | wpb 511.9 | bsz 1 | num_updates 25691 | best_loss 6.195
2022-03-16 12:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25691 updates
2022-03-16 12:23:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 12:23:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 12:23:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 66 @ 25691 updates, score 6.195) (writing took 1.9532755808904767 seconds)
2022-03-16 12:23:09 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-16 12:23:09 | INFO | train | epoch 066 | loss 5.947 | ppl 61.71 | wps 22950.3 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 25691 | lr 0.000197292 | gnorm 0.553 | loss_scale 8 | train_wall 984 | gb_free 9.6 | wall 71302
KL Stats: Epoch 66 Divergences: Uniform: 4.121771326761331 Unigram: 3.346077555361335
2022-03-16 12:23:09 | INFO | fairseq.trainer | begin training epoch 67
2022-03-16 12:23:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:23:33 | INFO | train_inner | epoch 067:      9 / 392 loss=5.955, ppl=62.05, wps=21596.3, ups=0.33, wpb=65025.8, bsz=127, num_updates=25700, lr=0.000197257, gnorm=0.565, loss_scale=8, train_wall=243, gb_free=9.6, wall=71326
2022-03-16 12:28:00 | INFO | train_inner | epoch 067:    109 / 392 loss=5.922, ppl=60.64, wps=24512.9, ups=0.37, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.551, loss_scale=16, train_wall=245, gb_free=9.6, wall=71593
2022-03-16 12:32:32 | INFO | train_inner | epoch 067:    209 / 392 loss=5.94, ppl=61.4, wps=24079.7, ups=0.37, wpb=65532.7, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.548, loss_scale=16, train_wall=250, gb_free=9.6, wall=71866
2022-03-16 12:33:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:37:01 | INFO | train_inner | epoch 067:    310 / 392 loss=5.954, ppl=61.99, wps=24429.1, ups=0.37, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.551, loss_scale=16, train_wall=246, gb_free=9.6, wall=72134
2022-03-16 12:37:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:41:09 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.183 | ppl 72.68 | wps 37558.7 | wpb 511.9 | bsz 1 | num_updates 26081 | best_loss 6.183
2022-03-16 12:41:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26081 updates
2022-03-16 12:41:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 12:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt
2022-03-16 12:41:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_best.pt (epoch 67 @ 26081 updates, score 6.183) (writing took 1.960978128015995 seconds)
2022-03-16 12:41:11 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-16 12:41:11 | INFO | train | epoch 067 | loss 5.944 | ppl 61.54 | wps 23576.9 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 26081 | lr 0.000195811 | gnorm 0.554 | loss_scale 8 | train_wall 960 | gb_free 9.6 | wall 72384
KL Stats: Epoch 67 Divergences: Uniform: 4.123796256225797 Unigram: 3.3469435677839647
2022-03-16 12:41:11 | INFO | fairseq.trainer | begin training epoch 68
2022-03-16 12:41:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:42:01 | INFO | train_inner | epoch 068:     19 / 392 loss=5.952, ppl=61.9, wps=21634.3, ups=0.33, wpb=65025.8, bsz=127, num_updates=26100, lr=0.00019574, gnorm=0.562, loss_scale=8, train_wall=243, gb_free=9.6, wall=72434
2022-03-16 12:46:26 | INFO | train_inner | epoch 068:    119 / 392 loss=5.923, ppl=60.66, wps=24805.6, ups=0.38, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.552, loss_scale=16, train_wall=243, gb_free=9.6, wall=72699
2022-03-16 12:49:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:50:52 | INFO | train_inner | epoch 068:    220 / 392 loss=5.933, ppl=61.11, wps=24633.4, ups=0.38, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.542, loss_scale=16, train_wall=244, gb_free=9.6, wall=72965
2022-03-16 12:55:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:55:18 | INFO | train_inner | epoch 068:    321 / 392 loss=5.947, ppl=61.69, wps=24605.2, ups=0.38, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.552, loss_scale=16, train_wall=245, gb_free=9.6, wall=73231
2022-03-16 12:58:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:58:57 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.196 | ppl 73.3 | wps 37656.6 | wpb 511.9 | bsz 1 | num_updates 26471 | best_loss 6.183
2022-03-16 12:58:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26471 updates
2022-03-16 12:58:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 12:58:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 12:58:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 68 @ 26471 updates, score 6.196) (writing took 0.8144230553880334 seconds)
2022-03-16 12:58:58 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-16 12:58:58 | INFO | train | epoch 068 | loss 5.939 | ppl 61.33 | wps 23906.5 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 26471 | lr 0.000194364 | gnorm 0.549 | loss_scale 16 | train_wall 948 | gb_free 9.6 | wall 73451
KL Stats: Epoch 68 Divergences: Uniform: 4.128850326111149 Unigram: 3.3532012031047254
2022-03-16 12:58:58 | INFO | fairseq.trainer | begin training epoch 69
2022-03-16 12:58:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:59:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 13:00:17 | INFO | train_inner | epoch 069:     30 / 392 loss=5.947, ppl=61.68, wps=21716.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=26500, lr=0.000194257, gnorm=0.555, loss_scale=8, train_wall=243, gb_free=9.6, wall=73530
2022-03-16 13:04:44 | INFO | train_inner | epoch 069:    130 / 392 loss=5.919, ppl=60.52, wps=24569.4, ups=0.37, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.56, loss_scale=16, train_wall=245, gb_free=9.6, wall=73797
2022-03-16 13:09:13 | INFO | train_inner | epoch 069:    230 / 392 loss=5.929, ppl=60.92, wps=24399, ups=0.37, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.553, loss_scale=16, train_wall=246, gb_free=9.6, wall=74066
2022-03-16 13:10:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:13:44 | INFO | train_inner | epoch 069:    331 / 392 loss=5.954, ppl=62.01, wps=24127.3, ups=0.37, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.553, loss_scale=16, train_wall=249, gb_free=9.6, wall=74337
2022-03-16 13:16:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:17:01 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.188 | ppl 72.91 | wps 36982.8 | wpb 511.9 | bsz 1 | num_updates 26861 | best_loss 6.183
2022-03-16 13:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 26861 updates
2022-03-16 13:17:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 13:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 13:17:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 69 @ 26861 updates, score 6.188) (writing took 0.8451634282246232 seconds)
2022-03-16 13:17:01 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-16 13:17:01 | INFO | train | epoch 069 | loss 5.935 | ppl 61.18 | wps 23546.6 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 26861 | lr 0.000192947 | gnorm 0.556 | loss_scale 32 | train_wall 961 | gb_free 9.6 | wall 74534
KL Stats: Epoch 69 Divergences: Uniform: 4.130737946330902 Unigram: 3.3545751693031423
2022-03-16 13:17:01 | INFO | fairseq.trainer | begin training epoch 70
2022-03-16 13:17:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:17:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:18:50 | INFO | train_inner | epoch 070:     40 / 392 loss=5.935, ppl=61.18, wps=21302.3, ups=0.33, wpb=65025.8, bsz=127, num_updates=26900, lr=0.000192807, gnorm=0.556, loss_scale=16, train_wall=247, gb_free=9.6, wall=74643
2022-03-16 13:23:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:23:21 | INFO | train_inner | epoch 070:    141 / 392 loss=5.92, ppl=60.56, wps=24164.6, ups=0.37, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.557, loss_scale=16, train_wall=249, gb_free=9.6, wall=74914
2022-03-16 13:27:49 | INFO | train_inner | epoch 070:    241 / 392 loss=5.936, ppl=61.21, wps=24470.5, ups=0.37, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.556, loss_scale=16, train_wall=246, gb_free=9.6, wall=75182
2022-03-16 13:29:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:32:20 | INFO | train_inner | epoch 070:    342 / 392 loss=5.942, ppl=61.49, wps=24173.8, ups=0.37, wpb=65532.7, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.545, loss_scale=16, train_wall=249, gb_free=9.6, wall=75453
2022-03-16 13:34:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:35:08 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.192 | ppl 73.1 | wps 36448 | wpb 511.9 | bsz 1 | num_updates 27250 | best_loss 6.183
2022-03-16 13:35:08 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 13:35:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27250 updates
2022-03-16 13:35:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 13:35:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt
2022-03-16 13:35:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.03_0.02_0.95/checkpoint_last.pt (epoch 70 @ 27250 updates, score 6.192) (writing took 0.9318695263937116 seconds)
2022-03-16 13:35:09 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-16 13:35:09 | INFO | train | epoch 070 | loss 5.931 | ppl 61.01 | wps 23394.1 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 27250 | lr 0.000191565 | gnorm 0.554 | loss_scale 16 | train_wall 965 | gb_free 9.6 | wall 75622
2022-03-16 13:35:09 | INFO | fairseq_cli.train | done training in 75621.8 seconds
