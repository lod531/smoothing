Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 207132259: <w103_size_0.125_fp16_label_smoothing_0.04_#1> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.04_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:21:13 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:21:42 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:21:42 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.04 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   93739.70 sec.
    Max Memory :                                 6809 MB
    Average Memory :                             3078.11 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13191.00 MB
    Max Swap :                                   2510 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   93820 sec.
    Turnaround time :                            93849 sec.

The output (if any) follows:

2022-03-04 09:21:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.04, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:21:50 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-04 09:21:52 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-04 09:21:52 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:21:52 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:21:52 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 09:21:52 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-04 09:21:52 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:21:52 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-04 09:21:55 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:21:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:21:55 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-04 09:21:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:21:55 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:21:55 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:21:55 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 09:21:55 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 09:21:55 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:21:55 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-04 09:21:55 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:21:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:22:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:22:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:22:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:24:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:27:46 | INFO | train_inner | epoch 001:    105 / 196 loss=16.431, nll_loss=16.35, ppl=83529.2, wps=20492.9, ups=0.31, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.403, loss_scale=4, train_wall=327, gb_free=19.9, wall=351
2022-03-04 09:32:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:32:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.228 | nll_loss 13.01 | ppl 8250.5 | wps 40239.9 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-04 09:32:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-04 09:32:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 09:32:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 09:32:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.228) (writing took 7.097902400884777 seconds)
2022-03-04 09:32:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:32:46 | INFO | train | epoch 001 | loss 15.372 | nll_loss 15.247 | ppl 38888.5 | wps 20166.4 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.495 | loss_scale 8 | train_wall 594 | gb_free 19.9 | wall 651
2022-03-04 09:32:46 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:32:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:33:15 | INFO | train_inner | epoch 002:      9 / 196 loss=14.117, nll_loss=13.94, ppl=15712.2, wps=19903.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.472, loss_scale=8, train_wall=293, gb_free=19.9, wall=680
2022-03-04 09:38:30 | INFO | train_inner | epoch 002:    109 / 196 loss=12.236, nll_loss=11.968, ppl=4006.98, wps=20757.8, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.912, loss_scale=16, train_wall=293, gb_free=19.9, wall=996
2022-03-04 09:43:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:43:10 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.705 | nll_loss 10.321 | ppl 1279.23 | wps 39739.6 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.705
2022-03-04 09:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-04 09:43:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 09:43:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 09:43:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.705) (writing took 7.148620946099982 seconds)
2022-03-04 09:43:17 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 09:43:17 | INFO | train | epoch 002 | loss 11.748 | nll_loss 11.446 | ppl 2790.35 | wps 20343.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.777 | loss_scale 16 | train_wall 573 | gb_free 19.9 | wall 1282
2022-03-04 09:43:17 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 09:43:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:43:58 | INFO | train_inner | epoch 003:     13 / 196 loss=10.985, nll_loss=10.631, ppl=1585.35, wps=19952.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.558, loss_scale=16, train_wall=292, gb_free=19.9, wall=1323
2022-03-04 09:49:14 | INFO | train_inner | epoch 003:    113 / 196 loss=10.447, nll_loss=10.037, ppl=1050.34, wps=20764.4, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.484, loss_scale=32, train_wall=293, gb_free=19.9, wall=1639
2022-03-04 09:52:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:53:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:53:40 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.016 | nll_loss 9.567 | ppl 758.66 | wps 40572.7 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.016
2022-03-04 09:53:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-04 09:53:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 09:53:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 09:53:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 3 @ 582 updates, score 10.016) (writing took 7.205842706141993 seconds)
2022-03-04 09:53:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 09:53:47 | INFO | train | epoch 003 | loss 10.342 | nll_loss 9.923 | ppl 970.66 | wps 20243.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.519 | loss_scale 32 | train_wall 573 | gb_free 19.9 | wall 1912
2022-03-04 09:53:47 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 09:53:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:54:44 | INFO | train_inner | epoch 004:     18 / 196 loss=10.131, nll_loss=9.693, ppl=827.57, wps=19773.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.593, loss_scale=32, train_wall=295, gb_free=19.9, wall=1969
2022-03-04 09:59:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:00:03 | INFO | train_inner | epoch 004:    119 / 196 loss=9.855, nll_loss=9.397, ppl=674.36, wps=20542.6, ups=0.31, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.606, loss_scale=32, train_wall=296, gb_free=19.9, wall=2288
2022-03-04 10:01:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:04:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:04:17 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.553 | nll_loss 9.077 | ppl 540.09 | wps 38055.7 | wpb 510.9 | bsz 1 | num_updates 776 | best_loss 9.553
2022-03-04 10:04:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 776 updates
2022-03-04 10:04:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 4 @ 776 updates, score 9.553) (writing took 7.278593801893294 seconds)
2022-03-04 10:04:24 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 10:04:24 | INFO | train | epoch 004 | loss 9.796 | nll_loss 9.334 | ppl 645.38 | wps 19936.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 776 | lr 9.70806e-05 | gnorm 0.656 | loss_scale 16 | train_wall 578 | gb_free 19.9 | wall 2549
2022-03-04 10:04:24 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 10:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:05:42 | INFO | train_inner | epoch 005:     24 / 196 loss=9.637, nll_loss=9.165, ppl=573.89, wps=19300.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.745, loss_scale=16, train_wall=301, gb_free=19.9, wall=2627
2022-03-04 10:11:05 | INFO | train_inner | epoch 005:    124 / 196 loss=9.399, nll_loss=8.91, ppl=481.14, wps=20258.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.825, loss_scale=32, train_wall=299, gb_free=19.9, wall=2951
2022-03-04 10:14:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:15:03 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.163 | nll_loss 8.656 | ppl 403.4 | wps 37697.3 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 9.163
2022-03-04 10:15:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-04 10:15:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 5 @ 972 updates, score 9.163) (writing took 7.274041818920523 seconds)
2022-03-04 10:15:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:15:11 | INFO | train | epoch 005 | loss 9.36 | nll_loss 8.869 | ppl 467.6 | wps 19843.9 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.827 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 3196
2022-03-04 10:15:11 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:15:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:16:45 | INFO | train_inner | epoch 006:     29 / 196 loss=9.211, nll_loss=8.711, ppl=419.02, wps=19275.4, ups=0.29, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.874, loss_scale=32, train_wall=301, gb_free=19.9, wall=3290
2022-03-04 10:22:08 | INFO | train_inner | epoch 006:    129 / 196 loss=9.02, nll_loss=8.508, ppl=364.07, wps=20258.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.889, loss_scale=32, train_wall=299, gb_free=19.9, wall=3613
2022-03-04 10:22:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:25:50 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.854 | nll_loss 8.328 | ppl 321.39 | wps 37991.5 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.854
2022-03-04 10:25:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-04 10:25:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:25:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:25:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.854) (writing took 7.276182648027316 seconds)
2022-03-04 10:25:57 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 10:25:57 | INFO | train | epoch 006 | loss 8.995 | nll_loss 8.481 | ppl 357.4 | wps 19648.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.91 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 3842
2022-03-04 10:25:57 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 10:25:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:27:47 | INFO | train_inner | epoch 007:     34 / 196 loss=8.865, nll_loss=8.343, ppl=324.78, wps=19283.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.908, loss_scale=32, train_wall=301, gb_free=19.9, wall=3952
2022-03-04 10:29:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:33:14 | INFO | train_inner | epoch 007:    135 / 196 loss=8.709, nll_loss=8.177, ppl=289.44, wps=20046.7, ups=0.31, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.924, loss_scale=32, train_wall=302, gb_free=19.9, wall=4279
2022-03-04 10:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:36:36 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.604 | nll_loss 8.058 | ppl 266.55 | wps 37553.3 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.604
2022-03-04 10:36:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-04 10:36:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:36:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:36:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 7 @ 1361 updates, score 8.604) (writing took 7.033797767013311 seconds)
2022-03-04 10:36:43 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 10:36:43 | INFO | train | epoch 007 | loss 8.689 | nll_loss 8.157 | ppl 285.37 | wps 19744 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.924 | loss_scale 32 | train_wall 586 | gb_free 19.9 | wall 4488
2022-03-04 10:36:43 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 10:36:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:37:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:38:53 | INFO | train_inner | epoch 008:     40 / 196 loss=8.559, nll_loss=8.019, ppl=259.35, wps=19288.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.919, loss_scale=32, train_wall=301, gb_free=19.9, wall=4618
2022-03-04 10:44:16 | INFO | train_inner | epoch 008:    140 / 196 loss=8.423, nll_loss=7.873, ppl=234.5, wps=20267.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.934, loss_scale=64, train_wall=299, gb_free=19.9, wall=4941
2022-03-04 10:46:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:47:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:47:22 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.368 | nll_loss 7.807 | ppl 224 | wps 37497.9 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 8.368
2022-03-04 10:47:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-04 10:47:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:47:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 8 @ 1555 updates, score 8.368) (writing took 6.98044208297506 seconds)
2022-03-04 10:47:29 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 10:47:29 | INFO | train | epoch 008 | loss 8.414 | nll_loss 7.864 | ppl 232.98 | wps 19654.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 0.916 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 5134
2022-03-04 10:47:29 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 10:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:49:55 | INFO | train_inner | epoch 009:     45 / 196 loss=8.284, nll_loss=7.726, ppl=211.68, wps=19301.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.904, loss_scale=32, train_wall=301, gb_free=19.9, wall=5280
2022-03-04 10:54:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:55:22 | INFO | train_inner | epoch 009:    146 / 196 loss=8.156, nll_loss=7.59, ppl=192.72, wps=20054.1, ups=0.31, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.926, loss_scale=32, train_wall=302, gb_free=19.9, wall=5607
2022-03-04 10:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:58:08 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.175 | nll_loss 7.6 | ppl 193.99 | wps 38019.9 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 8.175
2022-03-04 10:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1750 updates
2022-03-04 10:58:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:58:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 10:58:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 9 @ 1750 updates, score 8.175) (writing took 6.784057438140735 seconds)
2022-03-04 10:58:15 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 10:58:15 | INFO | train | epoch 009 | loss 8.156 | nll_loss 7.59 | ppl 192.73 | wps 19763.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1750 | lr 0.000218806 | gnorm 0.924 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 5780
2022-03-04 10:58:15 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 10:58:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:00:57 | INFO | train_inner | epoch 010:     50 / 196 loss=8.037, nll_loss=7.463, ppl=176.48, wps=19493.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.919, loss_scale=32, train_wall=298, gb_free=19.9, wall=5942
2022-03-04 11:01:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:06:23 | INFO | train_inner | epoch 010:    151 / 196 loss=7.915, nll_loss=7.334, ppl=161.3, wps=20065.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.897, loss_scale=32, train_wall=302, gb_free=19.9, wall=6269
2022-03-04 11:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:08:54 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.996 | nll_loss 7.403 | ppl 169.27 | wps 37851.8 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 7.996
2022-03-04 11:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-04 11:08:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:08:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 10 @ 1945 updates, score 7.996) (writing took 6.614015084225684 seconds)
2022-03-04 11:09:01 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 11:09:01 | INFO | train | epoch 010 | loss 7.915 | nll_loss 7.333 | ppl 161.27 | wps 19766.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.898 | loss_scale 64 | train_wall 585 | gb_free 19.9 | wall 6426
2022-03-04 11:09:01 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 11:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:09:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:12:02 | INFO | train_inner | epoch 011:     56 / 196 loss=7.782, nll_loss=7.193, ppl=146.3, wps=19323.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.888, loss_scale=32, train_wall=301, gb_free=19.9, wall=6607
2022-03-04 11:16:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:17:28 | INFO | train_inner | epoch 011:    157 / 196 loss=7.687, nll_loss=7.091, ppl=136.29, wps=20067.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.885, loss_scale=32, train_wall=302, gb_free=19.9, wall=6933
2022-03-04 11:19:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:19:40 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.854 | nll_loss 7.245 | ppl 151.72 | wps 37921.9 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 7.854
2022-03-04 11:19:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2139 updates
2022-03-04 11:19:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 11 @ 2139 updates, score 7.854) (writing took 6.594208464026451 seconds)
2022-03-04 11:19:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 11:19:46 | INFO | train | epoch 011 | loss 7.69 | nll_loss 7.094 | ppl 136.59 | wps 19665.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2139 | lr 0.000267422 | gnorm 0.885 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 7071
2022-03-04 11:19:46 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 11:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:23:04 | INFO | train_inner | epoch 012:     61 / 196 loss=7.567, nll_loss=6.964, ppl=124.81, wps=19492.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.877, loss_scale=32, train_wall=298, gb_free=19.9, wall=7269
2022-03-04 11:23:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:28:30 | INFO | train_inner | epoch 012:    162 / 196 loss=7.473, nll_loss=6.863, ppl=116.39, wps=20077.3, ups=0.31, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.878, loss_scale=32, train_wall=302, gb_free=19.9, wall=7595
2022-03-04 11:30:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:30:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.73 | nll_loss 7.114 | ppl 138.51 | wps 37965.9 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.73
2022-03-04 11:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-04 11:30:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:30:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.73) (writing took 6.681429716059938 seconds)
2022-03-04 11:30:32 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 11:30:32 | INFO | train | epoch 012 | loss 7.482 | nll_loss 6.872 | ppl 117.14 | wps 19771 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.865 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 7717
2022-03-04 11:30:32 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 11:30:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:31:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:34:08 | INFO | train_inner | epoch 013:     67 / 196 loss=7.348, nll_loss=6.73, ppl=106.15, wps=19316.2, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.849, loss_scale=32, train_wall=301, gb_free=19.9, wall=7934
2022-03-04 11:39:32 | INFO | train_inner | epoch 013:    167 / 196 loss=7.293, nll_loss=6.67, ppl=101.84, wps=20268.7, ups=0.31, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.866, loss_scale=64, train_wall=299, gb_free=19.9, wall=8257
2022-03-04 11:39:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:41:11 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.634 | nll_loss 7.012 | ppl 129.1 | wps 37605.5 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 7.634
2022-03-04 11:41:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2528 updates
2022-03-04 11:41:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:41:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:41:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 13 @ 2528 updates, score 7.634) (writing took 6.734424178022891 seconds)
2022-03-04 11:41:17 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 11:41:17 | INFO | train | epoch 013 | loss 7.289 | nll_loss 6.667 | ppl 101.6 | wps 19664.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2528 | lr 0.000316037 | gnorm 0.856 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 8363
2022-03-04 11:41:17 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 11:41:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:45:10 | INFO | train_inner | epoch 014:     72 / 196 loss=7.156, nll_loss=6.525, ppl=92.1, wps=19313.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.835, loss_scale=32, train_wall=301, gb_free=19.9, wall=8595
2022-03-04 11:46:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:50:37 | INFO | train_inner | epoch 014:    173 / 196 loss=7.107, nll_loss=6.472, ppl=88.77, wps=20077.5, ups=0.31, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.835, loss_scale=16, train_wall=302, gb_free=19.9, wall=8922
2022-03-04 11:51:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:51:56 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.548 | nll_loss 6.917 | ppl 120.87 | wps 37998.1 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 7.548
2022-03-04 11:51:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2723 updates
2022-03-04 11:51:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:51:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 14 @ 2723 updates, score 7.548) (writing took 6.746482251910493 seconds)
2022-03-04 11:52:03 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 11:52:03 | INFO | train | epoch 014 | loss 7.112 | nll_loss 6.478 | ppl 89.12 | wps 19775.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2723 | lr 0.000340407 | gnorm 0.848 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 9008
2022-03-04 11:52:03 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 11:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:56:12 | INFO | train_inner | epoch 015:     77 / 196 loss=6.979, nll_loss=6.336, ppl=80.77, wps=19500, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.841, loss_scale=32, train_wall=298, gb_free=19.9, wall=9257
2022-03-04 12:00:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:01:38 | INFO | train_inner | epoch 015:    178 / 196 loss=6.951, nll_loss=6.305, ppl=79.08, wps=20065.9, ups=0.31, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.836, loss_scale=32, train_wall=302, gb_free=19.9, wall=9584
2022-03-04 12:02:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:02:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:02:42 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.493 | nll_loss 6.865 | ppl 116.55 | wps 37921.6 | wpb 510.9 | bsz 1 | num_updates 2917 | best_loss 7.493
2022-03-04 12:02:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2917 updates
2022-03-04 12:02:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:02:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 15 @ 2917 updates, score 7.493) (writing took 6.6325709340162575 seconds)
2022-03-04 12:02:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 12:02:48 | INFO | train | epoch 015 | loss 6.948 | nll_loss 6.302 | ppl 78.9 | wps 19671.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2917 | lr 0.000364652 | gnorm 0.842 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 9653
2022-03-04 12:02:48 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 12:02:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:07:17 | INFO | train_inner | epoch 016:     83 / 196 loss=6.82, nll_loss=6.166, ppl=71.82, wps=19313.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.843, loss_scale=16, train_wall=301, gb_free=19.9, wall=9922
2022-03-04 12:09:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:12:44 | INFO | train_inner | epoch 016:    184 / 196 loss=6.802, nll_loss=6.146, ppl=70.82, wps=20051.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.807, loss_scale=16, train_wall=302, gb_free=19.9, wall=10249
2022-03-04 12:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:13:27 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.452 | nll_loss 6.817 | ppl 112.75 | wps 37900.6 | wpb 510.9 | bsz 1 | num_updates 3112 | best_loss 7.452
2022-03-04 12:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3112 updates
2022-03-04 12:13:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 16 @ 3112 updates, score 7.452) (writing took 6.633233533008024 seconds)
2022-03-04 12:13:34 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 12:13:34 | INFO | train | epoch 016 | loss 6.796 | nll_loss 6.14 | ppl 70.53 | wps 19760.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3112 | lr 0.000389022 | gnorm 0.808 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 10299
2022-03-04 12:13:34 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 12:13:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:18:19 | INFO | train_inner | epoch 017:     88 / 196 loss=6.656, nll_loss=5.992, ppl=63.63, wps=19518.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.815, loss_scale=32, train_wall=298, gb_free=19.9, wall=10584
2022-03-04 12:23:42 | INFO | train_inner | epoch 017:    188 / 196 loss=6.666, nll_loss=6.001, ppl=64.03, wps=20255.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.807, loss_scale=32, train_wall=299, gb_free=19.9, wall=10907
2022-03-04 12:23:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:24:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:24:13 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.455 | nll_loss 6.823 | ppl 113.22 | wps 38137.3 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 7.452
2022-03-04 12:24:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3307 updates
2022-03-04 12:24:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 12:24:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 12:24:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 17 @ 3307 updates, score 7.455) (writing took 3.1431154359597713 seconds)
2022-03-04 12:24:16 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 12:24:16 | INFO | train | epoch 017 | loss 6.655 | nll_loss 5.989 | ppl 63.52 | wps 19878 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3307 | lr 0.000413392 | gnorm 0.817 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 10941
2022-03-04 12:24:16 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 12:24:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:29:17 | INFO | train_inner | epoch 018:     93 / 196 loss=6.519, nll_loss=5.845, ppl=57.46, wps=19509.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.831, loss_scale=32, train_wall=301, gb_free=19.9, wall=11242
2022-03-04 12:31:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:32:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:34:47 | INFO | train_inner | epoch 018:    195 / 196 loss=6.533, nll_loss=5.858, ppl=58, wps=19877.6, ups=0.3, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.796, loss_scale=16, train_wall=305, gb_free=19.9, wall=11572
2022-03-04 12:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:34:55 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.453 | nll_loss 6.793 | ppl 110.93 | wps 38131 | wpb 510.9 | bsz 1 | num_updates 3501 | best_loss 7.452
2022-03-04 12:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3501 updates
2022-03-04 12:34:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 12:34:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 12:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 18 @ 3501 updates, score 7.453) (writing took 3.049899114994332 seconds)
2022-03-04 12:34:58 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 12:34:58 | INFO | train | epoch 018 | loss 6.52 | nll_loss 5.845 | ppl 57.49 | wps 19777.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 3501 | lr 0.000437637 | gnorm 0.813 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 11583
2022-03-04 12:34:58 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 12:34:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:40:18 | INFO | train_inner | epoch 019:     99 / 196 loss=6.372, nll_loss=5.688, ppl=51.54, wps=19739.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.831, loss_scale=32, train_wall=298, gb_free=19.9, wall=11903
2022-03-04 12:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:45:37 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.44 | nll_loss 6.79 | ppl 110.67 | wps 37727.1 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 7.44
2022-03-04 12:45:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3697 updates
2022-03-04 12:45:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:45:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 19 @ 3697 updates, score 7.44) (writing took 6.964481154922396 seconds)
2022-03-04 12:45:44 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 12:45:44 | INFO | train | epoch 019 | loss 6.396 | nll_loss 5.712 | ppl 52.42 | wps 19865.5 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 3697 | lr 0.000462133 | gnorm 0.825 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 12229
2022-03-04 12:45:44 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 12:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:45:54 | INFO | train_inner | epoch 020:      3 / 196 loss=6.417, nll_loss=5.733, ppl=53.21, wps=19475.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.819, loss_scale=32, train_wall=298, gb_free=19.9, wall=12239
2022-03-04 12:46:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:51:20 | INFO | train_inner | epoch 020:    104 / 196 loss=6.246, nll_loss=5.553, ppl=46.94, wps=20048.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.808, loss_scale=32, train_wall=302, gb_free=19.9, wall=12566
2022-03-04 12:52:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:56:23 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.457 | nll_loss 6.808 | ppl 112.05 | wps 37907.7 | wpb 510.9 | bsz 1 | num_updates 3891 | best_loss 7.44
2022-03-04 12:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3891 updates
2022-03-04 12:56:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 12:56:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 12:56:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 20 @ 3891 updates, score 7.457) (writing took 3.0833003830630332 seconds)
2022-03-04 12:56:26 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 12:56:26 | INFO | train | epoch 020 | loss 6.276 | nll_loss 5.583 | ppl 47.94 | wps 19761.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 3891 | lr 0.000486378 | gnorm 0.825 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 12871
2022-03-04 12:56:26 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 12:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:56:55 | INFO | train_inner | epoch 021:      9 / 196 loss=6.29, nll_loss=5.597, ppl=48.41, wps=19512.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.838, loss_scale=16, train_wall=301, gb_free=19.9, wall=12901
2022-03-04 13:02:19 | INFO | train_inner | epoch 021:    109 / 196 loss=6.132, nll_loss=5.43, ppl=43.12, wps=20249.1, ups=0.31, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.791, loss_scale=32, train_wall=299, gb_free=19.9, wall=13224
2022-03-04 13:04:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:07:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.446 | nll_loss 6.801 | ppl 111.49 | wps 37741.4 | wpb 510.9 | bsz 1 | num_updates 4086 | best_loss 7.44
2022-03-04 13:07:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4086 updates
2022-03-04 13:07:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:07:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 21 @ 4086 updates, score 7.446) (writing took 3.1160397690255195 seconds)
2022-03-04 13:07:09 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 13:07:09 | INFO | train | epoch 021 | loss 6.159 | nll_loss 5.458 | ppl 43.97 | wps 19871.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4086 | lr 0.00049471 | gnorm 0.789 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 13514
2022-03-04 13:07:09 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 13:07:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:07:54 | INFO | train_inner | epoch 022:     14 / 196 loss=6.167, nll_loss=5.466, ppl=44.19, wps=19524.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.791, loss_scale=16, train_wall=301, gb_free=19.9, wall=13559
2022-03-04 13:13:17 | INFO | train_inner | epoch 022:    114 / 196 loss=6.014, nll_loss=5.304, ppl=39.5, wps=20254.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.79, loss_scale=32, train_wall=299, gb_free=19.9, wall=13883
2022-03-04 13:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:17:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:17:48 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.488 | nll_loss 6.84 | ppl 114.59 | wps 37911.3 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 7.44
2022-03-04 13:17:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-04 13:17:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:17:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:17:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 22 @ 4281 updates, score 7.488) (writing took 3.078273592982441 seconds)
2022-03-04 13:17:51 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 13:17:51 | INFO | train | epoch 022 | loss 6.037 | nll_loss 5.328 | ppl 40.16 | wps 19873.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.787 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 14156
2022-03-04 13:17:51 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 13:17:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:18:52 | INFO | train_inner | epoch 023:     19 / 196 loss=6.033, nll_loss=5.322, ppl=40.01, wps=19532.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.787, loss_scale=16, train_wall=301, gb_free=19.9, wall=14217
2022-03-04 13:24:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:24:18 | INFO | train_inner | epoch 023:    120 / 196 loss=5.899, nll_loss=5.18, ppl=36.26, wps=20083.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.77, loss_scale=16, train_wall=302, gb_free=19.9, wall=14544
2022-03-04 13:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:28:29 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.498 | nll_loss 6.837 | ppl 114.29 | wps 37477.6 | wpb 510.9 | bsz 1 | num_updates 4476 | best_loss 7.44
2022-03-04 13:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4476 updates
2022-03-04 13:28:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:28:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 23 @ 4476 updates, score 7.498) (writing took 3.15340408612974 seconds)
2022-03-04 13:28:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 13:28:32 | INFO | train | epoch 023 | loss 5.92 | nll_loss 5.203 | ppl 36.83 | wps 19885.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4476 | lr 0.000472667 | gnorm 0.77 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 14798
2022-03-04 13:28:32 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 13:28:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:29:50 | INFO | train_inner | epoch 024:     24 / 196 loss=5.906, nll_loss=5.187, ppl=36.43, wps=19707.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.746, loss_scale=16, train_wall=298, gb_free=19.9, wall=14875
2022-03-04 13:31:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:35:17 | INFO | train_inner | epoch 024:    125 / 196 loss=5.797, nll_loss=5.071, ppl=33.6, wps=20050.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.766, loss_scale=16, train_wall=302, gb_free=19.9, wall=15202
2022-03-04 13:38:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:39:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:39:12 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.541 | nll_loss 6.886 | ppl 118.25 | wps 37712.6 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 7.44
2022-03-04 13:39:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-04 13:39:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:39:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:39:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 24 @ 4670 updates, score 7.541) (writing took 3.2064121081493795 seconds)
2022-03-04 13:39:15 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 13:39:15 | INFO | train | epoch 024 | loss 5.81 | nll_loss 5.084 | ppl 33.92 | wps 19768.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.762 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 15440
2022-03-04 13:39:15 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 13:39:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:40:52 | INFO | train_inner | epoch 025:     30 / 196 loss=5.794, nll_loss=5.067, ppl=33.52, wps=19513.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.764, loss_scale=16, train_wall=301, gb_free=19.9, wall=15537
2022-03-04 13:46:15 | INFO | train_inner | epoch 025:    130 / 196 loss=5.7, nll_loss=4.966, ppl=31.25, wps=20263.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.759, loss_scale=32, train_wall=299, gb_free=19.9, wall=15860
2022-03-04 13:49:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:49:54 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.564 | nll_loss 6.911 | ppl 120.32 | wps 38173.9 | wpb 510.9 | bsz 1 | num_updates 4866 | best_loss 7.44
2022-03-04 13:49:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4866 updates
2022-03-04 13:49:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:49:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:49:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 25 @ 4866 updates, score 7.564) (writing took 3.239125435007736 seconds)
2022-03-04 13:49:57 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 13:49:57 | INFO | train | epoch 025 | loss 5.709 | nll_loss 4.976 | ppl 31.47 | wps 19968.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4866 | lr 0.000453329 | gnorm 0.761 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 16082
2022-03-04 13:49:57 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 13:49:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:51:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:51:50 | INFO | train_inner | epoch 026:     35 / 196 loss=5.684, nll_loss=4.949, ppl=30.88, wps=19508.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.761, loss_scale=16, train_wall=301, gb_free=19.9, wall=16196
2022-03-04 13:57:14 | INFO | train_inner | epoch 026:    135 / 196 loss=5.602, nll_loss=4.861, ppl=29.05, wps=20259.2, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.755, loss_scale=16, train_wall=299, gb_free=19.9, wall=16519
2022-03-04 14:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:00:36 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.591 | nll_loss 6.914 | ppl 120.6 | wps 38182 | wpb 510.9 | bsz 1 | num_updates 5061 | best_loss 7.44
2022-03-04 14:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5061 updates
2022-03-04 14:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 26 @ 5061 updates, score 7.591) (writing took 3.1250033979304135 seconds)
2022-03-04 14:00:39 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 14:00:39 | INFO | train | epoch 026 | loss 5.611 | nll_loss 4.87 | ppl 29.24 | wps 19874.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5061 | lr 0.00044451 | gnorm 0.75 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 16724
2022-03-04 14:00:39 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 14:00:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:01:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:02:49 | INFO | train_inner | epoch 027:     40 / 196 loss=5.574, nll_loss=4.831, ppl=28.46, wps=19516, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.742, loss_scale=16, train_wall=301, gb_free=19.9, wall=16854
2022-03-04 14:08:12 | INFO | train_inner | epoch 027:    140 / 196 loss=5.52, nll_loss=4.772, ppl=27.31, wps=20253.7, ups=0.31, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.769, loss_scale=16, train_wall=299, gb_free=19.9, wall=17178
2022-03-04 14:10:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:11:18 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.662 | nll_loss 6.998 | ppl 127.86 | wps 37945.1 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 7.44
2022-03-04 14:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5255 updates
2022-03-04 14:11:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:11:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:11:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 27 @ 5255 updates, score 7.662) (writing took 3.1220806429628283 seconds)
2022-03-04 14:11:21 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 14:11:21 | INFO | train | epoch 027 | loss 5.52 | nll_loss 4.772 | ppl 27.33 | wps 19770.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 5255 | lr 0.000436228 | gnorm 0.767 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 17367
2022-03-04 14:11:21 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 14:11:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:13:47 | INFO | train_inner | epoch 028:     45 / 196 loss=5.477, nll_loss=4.726, ppl=26.47, wps=19524.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.768, loss_scale=16, train_wall=301, gb_free=19.9, wall=17512
2022-03-04 14:17:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:19:14 | INFO | train_inner | epoch 028:    146 / 196 loss=5.44, nll_loss=4.685, ppl=25.73, wps=20055.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.772, loss_scale=16, train_wall=302, gb_free=19.9, wall=17839
2022-03-04 14:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:22:01 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.669 | nll_loss 7 | ppl 128.04 | wps 38122.5 | wpb 510.9 | bsz 1 | num_updates 5450 | best_loss 7.44
2022-03-04 14:22:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5450 updates
2022-03-04 14:22:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:22:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:22:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 28 @ 5450 updates, score 7.669) (writing took 3.1492622839286923 seconds)
2022-03-04 14:22:04 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 14:22:04 | INFO | train | epoch 028 | loss 5.434 | nll_loss 4.679 | ppl 25.61 | wps 19866.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5450 | lr 0.000428353 | gnorm 0.762 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 18009
2022-03-04 14:22:04 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 14:22:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:24:46 | INFO | train_inner | epoch 029:     50 / 196 loss=5.382, nll_loss=4.623, ppl=24.65, wps=19704.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.767, loss_scale=16, train_wall=298, gb_free=19.9, wall=18171
2022-03-04 14:25:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:30:12 | INFO | train_inner | epoch 029:    151 / 196 loss=5.358, nll_loss=4.596, ppl=24.19, wps=20062.8, ups=0.31, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.755, loss_scale=16, train_wall=302, gb_free=19.9, wall=18497
2022-03-04 14:32:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:32:43 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.744 | nll_loss 7.063 | ppl 133.68 | wps 37826.1 | wpb 510.9 | bsz 1 | num_updates 5645 | best_loss 7.44
2022-03-04 14:32:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5645 updates
2022-03-04 14:32:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:32:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:32:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 29 @ 5645 updates, score 7.744) (writing took 3.134906272171065 seconds)
2022-03-04 14:32:46 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 14:32:46 | INFO | train | epoch 029 | loss 5.351 | nll_loss 4.59 | ppl 24.08 | wps 19875.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5645 | lr 0.000420889 | gnorm 0.761 | loss_scale 32 | train_wall 585 | gb_free 19.9 | wall 18651
2022-03-04 14:32:46 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 14:32:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:32:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:35:47 | INFO | train_inner | epoch 030:     56 / 196 loss=5.307, nll_loss=4.542, ppl=23.3, wps=19516.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.766, loss_scale=16, train_wall=301, gb_free=19.9, wall=18832
2022-03-04 14:41:11 | INFO | train_inner | epoch 030:    156 / 196 loss=5.282, nll_loss=4.515, ppl=22.87, wps=20265.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.765, loss_scale=32, train_wall=299, gb_free=19.9, wall=19156
2022-03-04 14:41:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:43:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:43:25 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.791 | nll_loss 7.13 | ppl 140.05 | wps 37853.2 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 7.44
2022-03-04 14:43:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-04 14:43:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:43:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:43:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.791) (writing took 3.1639207960106432 seconds)
2022-03-04 14:43:28 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 14:43:28 | INFO | train | epoch 030 | loss 5.273 | nll_loss 4.506 | ppl 22.71 | wps 19776.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.771 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 19293
2022-03-04 14:43:28 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 14:43:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:46:45 | INFO | train_inner | epoch 031:     61 / 196 loss=5.212, nll_loss=4.441, ppl=21.72, wps=19532.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.779, loss_scale=16, train_wall=301, gb_free=19.9, wall=19490
2022-03-04 14:49:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:52:12 | INFO | train_inner | epoch 031:    162 / 196 loss=5.219, nll_loss=4.447, ppl=21.81, wps=20068.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.785, loss_scale=16, train_wall=302, gb_free=19.9, wall=19817
2022-03-04 14:53:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:54:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:54:07 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.825 | nll_loss 7.17 | ppl 144 | wps 37771 | wpb 510.9 | bsz 1 | num_updates 6033 | best_loss 7.44
2022-03-04 14:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6033 updates
2022-03-04 14:54:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:54:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 31 @ 6033 updates, score 7.825) (writing took 3.1390319999773055 seconds)
2022-03-04 14:54:10 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 14:54:10 | INFO | train | epoch 031 | loss 5.199 | nll_loss 4.426 | ppl 21.5 | wps 19781.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 6033 | lr 0.00040713 | gnorm 0.784 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 19935
2022-03-04 14:54:10 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 14:54:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:57:47 | INFO | train_inner | epoch 032:     67 / 196 loss=5.131, nll_loss=4.353, ppl=20.43, wps=19528.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.79, loss_scale=8, train_wall=301, gb_free=19.9, wall=20152
2022-03-04 15:03:10 | INFO | train_inner | epoch 032:    167 / 196 loss=5.15, nll_loss=4.372, ppl=20.71, wps=20276.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.779, loss_scale=16, train_wall=299, gb_free=19.9, wall=20475
2022-03-04 15:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:04:49 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.886 | nll_loss 7.219 | ppl 148.97 | wps 38109.9 | wpb 510.9 | bsz 1 | num_updates 6229 | best_loss 7.44
2022-03-04 15:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6229 updates
2022-03-04 15:04:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 32 @ 6229 updates, score 7.886) (writing took 3.1724547899793833 seconds)
2022-03-04 15:04:52 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 15:04:52 | INFO | train | epoch 032 | loss 5.131 | nll_loss 4.352 | ppl 20.42 | wps 19984.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6229 | lr 0.000400674 | gnorm 0.788 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 20577
2022-03-04 15:04:52 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 15:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:07:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:08:45 | INFO | train_inner | epoch 033:     72 / 196 loss=5.059, nll_loss=4.275, ppl=19.36, wps=19522.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.786, loss_scale=16, train_wall=301, gb_free=19.9, wall=20810
2022-03-04 15:14:08 | INFO | train_inner | epoch 033:    172 / 196 loss=5.089, nll_loss=4.306, ppl=19.78, wps=20259.3, ups=0.31, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.79, loss_scale=16, train_wall=299, gb_free=19.9, wall=21133
2022-03-04 15:14:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:15:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:15:31 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.923 | nll_loss 7.259 | ppl 153.2 | wps 37813 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 7.44
2022-03-04 15:15:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6423 updates
2022-03-04 15:15:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:15:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 33 @ 6423 updates, score 7.923) (writing took 3.151748850941658 seconds)
2022-03-04 15:15:34 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 15:15:34 | INFO | train | epoch 033 | loss 5.061 | nll_loss 4.277 | ppl 19.38 | wps 19773.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 6423 | lr 0.000394576 | gnorm 0.79 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 21219
2022-03-04 15:15:34 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 15:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:17:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:19:46 | INFO | train_inner | epoch 034:     78 / 196 loss=4.989, nll_loss=4.2, ppl=18.38, wps=19337.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.818, loss_scale=8, train_wall=304, gb_free=19.9, wall=21471
2022-03-04 15:25:09 | INFO | train_inner | epoch 034:    178 / 196 loss=5.026, nll_loss=4.237, ppl=18.86, wps=20266.4, ups=0.31, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.794, loss_scale=16, train_wall=299, gb_free=19.9, wall=21795
2022-03-04 15:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:26:13 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.951 | nll_loss 7.28 | ppl 155.39 | wps 37787.9 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 7.44
2022-03-04 15:26:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-04 15:26:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:26:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 34 @ 6618 updates, score 7.951) (writing took 3.108711200999096 seconds)
2022-03-04 15:26:16 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 15:26:16 | INFO | train | epoch 034 | loss 4.998 | nll_loss 4.208 | ppl 18.48 | wps 19882.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.801 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 21861
2022-03-04 15:26:16 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 15:26:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:30:41 | INFO | train_inner | epoch 035:     82 / 196 loss=4.913, nll_loss=4.117, ppl=17.35, wps=19709.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.8, loss_scale=16, train_wall=298, gb_free=19.9, wall=22126
2022-03-04 15:31:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:36:08 | INFO | train_inner | epoch 035:    183 / 196 loss=4.973, nll_loss=4.179, ppl=18.12, wps=20059.9, ups=0.31, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.827, loss_scale=16, train_wall=302, gb_free=19.9, wall=22453
2022-03-04 15:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:36:55 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.021 | nll_loss 7.351 | ppl 163.25 | wps 37887.1 | wpb 510.9 | bsz 1 | num_updates 6813 | best_loss 7.44
2022-03-04 15:36:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6813 updates
2022-03-04 15:36:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:36:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 35 @ 6813 updates, score 8.021) (writing took 3.1724280300550163 seconds)
2022-03-04 15:36:58 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 15:36:58 | INFO | train | epoch 035 | loss 4.937 | nll_loss 4.142 | ppl 17.65 | wps 19871 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6813 | lr 0.000383116 | gnorm 0.818 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 22503
2022-03-04 15:36:58 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 15:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:38:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:41:43 | INFO | train_inner | epoch 036:     88 / 196 loss=4.85, nll_loss=4.049, ppl=16.55, wps=19514.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.812, loss_scale=16, train_wall=301, gb_free=19.9, wall=22788
2022-03-04 15:45:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:47:09 | INFO | train_inner | epoch 036:    189 / 196 loss=4.922, nll_loss=4.125, ppl=17.45, wps=20073.6, ups=0.31, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.817, loss_scale=16, train_wall=302, gb_free=19.9, wall=23114
2022-03-04 15:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:47:37 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.083 | nll_loss 7.419 | ppl 171.13 | wps 38031.1 | wpb 510.9 | bsz 1 | num_updates 7007 | best_loss 7.44
2022-03-04 15:47:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7007 updates
2022-03-04 15:47:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:47:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:47:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 36 @ 7007 updates, score 8.083) (writing took 3.1680260521825403 seconds)
2022-03-04 15:47:40 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 15:47:40 | INFO | train | epoch 036 | loss 4.878 | nll_loss 4.078 | ppl 16.89 | wps 19776 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 7007 | lr 0.000377776 | gnorm 0.812 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 23145
2022-03-04 15:47:40 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 15:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:52:39 | INFO | train_inner | epoch 037:     93 / 196 loss=4.785, nll_loss=3.978, ppl=15.76, wps=19803.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.825, loss_scale=16, train_wall=297, gb_free=19.9, wall=23444
2022-03-04 15:53:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:57:59 | INFO | train_inner | epoch 037:    194 / 196 loss=4.867, nll_loss=4.064, ppl=16.73, wps=20495.1, ups=0.31, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.824, loss_scale=16, train_wall=297, gb_free=19.9, wall=23764
2022-03-04 15:58:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:58:10 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.147 | nll_loss 7.488 | ppl 179.49 | wps 39846.1 | wpb 510.9 | bsz 1 | num_updates 7202 | best_loss 7.44
2022-03-04 15:58:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7202 updates
2022-03-04 15:58:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:58:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:58:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 37 @ 7202 updates, score 8.147) (writing took 3.1226618399377912 seconds)
2022-03-04 15:58:13 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 15:58:13 | INFO | train | epoch 037 | loss 4.822 | nll_loss 4.018 | ppl 16.2 | wps 20153.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7202 | lr 0.000372626 | gnorm 0.825 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 23778
2022-03-04 15:58:13 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 15:58:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:00:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:03:27 | INFO | train_inner | epoch 038:     99 / 196 loss=4.721, nll_loss=3.909, ppl=15.03, wps=19958.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.824, loss_scale=16, train_wall=296, gb_free=19.9, wall=24092
2022-03-04 16:04:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:08:38 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.167 | nll_loss 7.505 | ppl 181.6 | wps 40286.4 | wpb 510.9 | bsz 1 | num_updates 7396 | best_loss 7.44
2022-03-04 16:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7396 updates
2022-03-04 16:08:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:08:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 38 @ 7396 updates, score 8.167) (writing took 3.2480820219498128 seconds)
2022-03-04 16:08:41 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 16:08:41 | INFO | train | epoch 038 | loss 4.768 | nll_loss 3.958 | ppl 15.55 | wps 20216.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7396 | lr 0.000367707 | gnorm 0.826 | loss_scale 8 | train_wall 574 | gb_free 19.9 | wall 24406
2022-03-04 16:08:41 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 16:08:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:08:54 | INFO | train_inner | epoch 039:      4 / 196 loss=4.811, nll_loss=4.004, ppl=16.04, wps=19960.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.833, loss_scale=8, train_wall=296, gb_free=19.9, wall=24419
2022-03-04 16:12:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:14:14 | INFO | train_inner | epoch 039:    105 / 196 loss=4.67, nll_loss=3.854, ppl=14.46, wps=20509.6, ups=0.31, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.854, loss_scale=8, train_wall=296, gb_free=19.9, wall=24739
2022-03-04 16:19:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:19:06 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.207 | nll_loss 7.545 | ppl 186.73 | wps 39950.6 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 7.44
2022-03-04 16:19:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-04 16:19:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:19:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:19:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 39 @ 7591 updates, score 8.207) (writing took 3.2994863789062947 seconds)
2022-03-04 16:19:09 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 16:19:09 | INFO | train | epoch 039 | loss 4.716 | nll_loss 3.902 | ppl 14.95 | wps 20314.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.843 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 25035
2022-03-04 16:19:10 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 16:19:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:19:38 | INFO | train_inner | epoch 040:      9 / 196 loss=4.752, nll_loss=3.941, ppl=15.36, wps=20144.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.836, loss_scale=16, train_wall=293, gb_free=19.9, wall=25063
2022-03-04 16:24:54 | INFO | train_inner | epoch 040:    109 / 196 loss=4.631, nll_loss=3.812, ppl=14.04, wps=20715.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.823, loss_scale=16, train_wall=294, gb_free=19.9, wall=25379
2022-03-04 16:26:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:26:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:29:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:29:34 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.286 | nll_loss 7.624 | ppl 197.28 | wps 40104 | wpb 510.9 | bsz 1 | num_updates 7785 | best_loss 7.44
2022-03-04 16:29:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7785 updates
2022-03-04 16:29:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:29:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:29:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 40 @ 7785 updates, score 8.286) (writing took 3.292488040169701 seconds)
2022-03-04 16:29:37 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:29:37 | INFO | train | epoch 040 | loss 4.666 | nll_loss 3.849 | ppl 14.41 | wps 20222 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7785 | lr 0.000358402 | gnorm 0.841 | loss_scale 8 | train_wall 574 | gb_free 19.9 | wall 25662
2022-03-04 16:29:37 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:29:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:30:25 | INFO | train_inner | epoch 041:     15 / 196 loss=4.688, nll_loss=3.872, ppl=14.64, wps=19778.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.854, loss_scale=8, train_wall=298, gb_free=19.9, wall=25710
2022-03-04 16:33:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:35:44 | INFO | train_inner | epoch 041:    116 / 196 loss=4.582, nll_loss=3.758, ppl=13.53, wps=20534.9, ups=0.31, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.839, loss_scale=8, train_wall=296, gb_free=19.9, wall=26029
2022-03-04 16:40:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:40:06 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.342 | nll_loss 7.677 | ppl 204.64 | wps 37944.4 | wpb 510.9 | bsz 1 | num_updates 7980 | best_loss 7.44
2022-03-04 16:40:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7980 updates
2022-03-04 16:40:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:40:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:40:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 41 @ 7980 updates, score 8.342) (writing took 3.395145701011643 seconds)
2022-03-04 16:40:10 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:40:10 | INFO | train | epoch 041 | loss 4.619 | nll_loss 3.797 | ppl 13.9 | wps 20179.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7980 | lr 0.000353996 | gnorm 0.857 | loss_scale 8 | train_wall 577 | gb_free 19.9 | wall 26295
2022-03-04 16:40:10 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:40:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:41:15 | INFO | train_inner | epoch 042:     20 / 196 loss=4.642, nll_loss=3.822, ppl=14.14, wps=19775.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.876, loss_scale=16, train_wall=297, gb_free=19.9, wall=26360
2022-03-04 16:42:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:46:41 | INFO | train_inner | epoch 042:    121 / 196 loss=4.546, nll_loss=3.719, ppl=13.17, wps=20066.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.869, loss_scale=8, train_wall=302, gb_free=19.9, wall=26686
2022-03-04 16:49:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:50:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:50:49 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.395 | nll_loss 7.721 | ppl 210.95 | wps 37923.4 | wpb 510.9 | bsz 1 | num_updates 8174 | best_loss 7.44
2022-03-04 16:50:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8174 updates
2022-03-04 16:50:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:50:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:50:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 42 @ 8174 updates, score 8.395) (writing took 3.256461277138442 seconds)
2022-03-04 16:50:52 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 16:50:52 | INFO | train | epoch 042 | loss 4.572 | nll_loss 3.747 | ppl 13.43 | wps 19774.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 8174 | lr 0.00034977 | gnorm 0.866 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 26937
2022-03-04 16:50:52 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 16:50:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:52:16 | INFO | train_inner | epoch 043:     26 / 196 loss=4.583, nll_loss=3.758, ppl=13.53, wps=19510.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.864, loss_scale=8, train_wall=301, gb_free=19.9, wall=27021
2022-03-04 16:56:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:57:43 | INFO | train_inner | epoch 043:    127 / 196 loss=4.512, nll_loss=3.682, ppl=12.83, wps=20062.6, ups=0.31, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.865, loss_scale=8, train_wall=302, gb_free=19.9, wall=27348
2022-03-04 17:01:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:01:31 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.421 | nll_loss 7.75 | ppl 215.32 | wps 37594.7 | wpb 510.9 | bsz 1 | num_updates 8369 | best_loss 7.44
2022-03-04 17:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8369 updates
2022-03-04 17:01:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 43 @ 8369 updates, score 8.421) (writing took 3.2853443790227175 seconds)
2022-03-04 17:01:34 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 17:01:34 | INFO | train | epoch 043 | loss 4.53 | nll_loss 3.701 | ppl 13.01 | wps 19872.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8369 | lr 0.000345671 | gnorm 0.871 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 27579
2022-03-04 17:01:34 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 17:01:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:03:14 | INFO | train_inner | epoch 044:     31 / 196 loss=4.53, nll_loss=3.7, ppl=13, wps=19713.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.881, loss_scale=8, train_wall=298, gb_free=19.9, wall=27679
2022-03-04 17:03:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:08:41 | INFO | train_inner | epoch 044:    132 / 196 loss=4.473, nll_loss=3.639, ppl=12.46, wps=20077.3, ups=0.31, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.871, loss_scale=8, train_wall=301, gb_free=19.9, wall=28006
2022-03-04 17:12:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:12:13 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.503 | nll_loss 7.847 | ppl 230.17 | wps 38199.6 | wpb 510.9 | bsz 1 | num_updates 8564 | best_loss 7.44
2022-03-04 17:12:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8564 updates
2022-03-04 17:12:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:12:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:12:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 44 @ 8564 updates, score 8.503) (writing took 3.1958077428862453 seconds)
2022-03-04 17:12:16 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 17:12:16 | INFO | train | epoch 044 | loss 4.487 | nll_loss 3.654 | ppl 12.59 | wps 19885.2 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8564 | lr 0.000341713 | gnorm 0.874 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 28221
2022-03-04 17:12:16 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 17:12:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:12:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:14:16 | INFO | train_inner | epoch 045:     37 / 196 loss=4.489, nll_loss=3.656, ppl=12.6, wps=19522.9, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.881, loss_scale=8, train_wall=301, gb_free=19.9, wall=28341
2022-03-04 17:19:39 | INFO | train_inner | epoch 045:    137 / 196 loss=4.439, nll_loss=3.602, ppl=12.14, wps=20270.3, ups=0.31, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.876, loss_scale=16, train_wall=299, gb_free=19.9, wall=28664
2022-03-04 17:22:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:22:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:22:55 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.539 | nll_loss 7.881 | ppl 235.7 | wps 37862 | wpb 510.9 | bsz 1 | num_updates 8758 | best_loss 7.44
2022-03-04 17:22:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8758 updates
2022-03-04 17:22:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:22:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:22:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 45 @ 8758 updates, score 8.539) (writing took 3.2902286301832646 seconds)
2022-03-04 17:22:58 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 17:22:58 | INFO | train | epoch 045 | loss 4.446 | nll_loss 3.61 | ppl 12.21 | wps 19779.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 8758 | lr 0.000337907 | gnorm 0.884 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 28863
2022-03-04 17:22:58 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 17:22:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:25:14 | INFO | train_inner | epoch 046:     42 / 196 loss=4.43, nll_loss=3.592, ppl=12.06, wps=19527.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.897, loss_scale=8, train_wall=301, gb_free=19.9, wall=28999
2022-03-04 17:29:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:30:40 | INFO | train_inner | epoch 046:    143 / 196 loss=4.404, nll_loss=3.565, ppl=11.83, wps=20076.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.876, loss_scale=8, train_wall=302, gb_free=19.9, wall=29325
2022-03-04 17:33:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:33:36 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.585 | nll_loss 7.925 | ppl 243.06 | wps 37866.8 | wpb 510.9 | bsz 1 | num_updates 8953 | best_loss 7.44
2022-03-04 17:33:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8953 updates
2022-03-04 17:33:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:33:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:33:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 46 @ 8953 updates, score 8.585) (writing took 3.1841480401344597 seconds)
2022-03-04 17:33:40 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 17:33:40 | INFO | train | epoch 046 | loss 4.408 | nll_loss 3.569 | ppl 11.86 | wps 19884 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8953 | lr 0.000334207 | gnorm 0.887 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 29505
2022-03-04 17:33:40 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 17:33:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:36:12 | INFO | train_inner | epoch 047:     47 / 196 loss=4.391, nll_loss=3.55, ppl=11.71, wps=19707.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.895, loss_scale=8, train_wall=298, gb_free=19.9, wall=29657
2022-03-04 17:37:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:41:38 | INFO | train_inner | epoch 047:    148 / 196 loss=4.373, nll_loss=3.531, ppl=11.56, wps=20081.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.903, loss_scale=8, train_wall=302, gb_free=19.9, wall=29983
2022-03-04 17:44:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:44:18 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.622 | nll_loss 7.954 | ppl 247.99 | wps 38534.5 | wpb 510.9 | bsz 1 | num_updates 9148 | best_loss 7.44
2022-03-04 17:44:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9148 updates
2022-03-04 17:44:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:44:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:44:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 47 @ 9148 updates, score 8.622) (writing took 3.1555842640809715 seconds)
2022-03-04 17:44:21 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 17:44:21 | INFO | train | epoch 047 | loss 4.369 | nll_loss 3.527 | ppl 11.53 | wps 19891.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9148 | lr 0.000330626 | gnorm 0.897 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 30146
2022-03-04 17:44:21 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 17:44:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:44:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:47:13 | INFO | train_inner | epoch 048:     53 / 196 loss=4.348, nll_loss=3.503, ppl=11.34, wps=19542.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.897, loss_scale=8, train_wall=301, gb_free=19.9, wall=30318
2022-03-04 17:51:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:52:39 | INFO | train_inner | epoch 048:    154 / 196 loss=4.337, nll_loss=3.491, ppl=11.25, wps=20076.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.902, loss_scale=8, train_wall=302, gb_free=19.9, wall=30644
2022-03-04 17:54:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:55:00 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.664 | nll_loss 7.994 | ppl 254.9 | wps 38058.4 | wpb 510.9 | bsz 1 | num_updates 9342 | best_loss 7.44
2022-03-04 17:55:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9342 updates
2022-03-04 17:55:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:55:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:55:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 48 @ 9342 updates, score 8.664) (writing took 3.132826816989109 seconds)
2022-03-04 17:55:03 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 17:55:03 | INFO | train | epoch 048 | loss 4.333 | nll_loss 3.487 | ppl 11.21 | wps 19789.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 9342 | lr 0.000327175 | gnorm 0.903 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 30788
2022-03-04 17:55:03 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 17:55:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:58:10 | INFO | train_inner | epoch 049:     58 / 196 loss=4.302, nll_loss=3.454, ppl=10.96, wps=19722.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.912, loss_scale=8, train_wall=298, gb_free=19.9, wall=30976
2022-03-04 17:58:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:03:37 | INFO | train_inner | epoch 049:    159 / 196 loss=4.31, nll_loss=3.462, ppl=11.02, wps=20075.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.903, loss_scale=8, train_wall=302, gb_free=19.9, wall=31302
2022-03-04 18:05:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:05:41 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.746 | nll_loss 8.083 | ppl 271.23 | wps 37967.9 | wpb 510.9 | bsz 1 | num_updates 9537 | best_loss 7.44
2022-03-04 18:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9537 updates
2022-03-04 18:05:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 49 @ 9537 updates, score 8.746) (writing took 3.142959523946047 seconds)
2022-03-04 18:05:45 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 18:05:45 | INFO | train | epoch 049 | loss 4.298 | nll_loss 3.449 | ppl 10.92 | wps 19887.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 9537 | lr 0.000323813 | gnorm 0.907 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 31430
2022-03-04 18:05:45 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 18:05:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:07:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:09:11 | INFO | train_inner | epoch 050:     64 / 196 loss=4.262, nll_loss=3.41, ppl=10.63, wps=19537.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.911, loss_scale=8, train_wall=301, gb_free=19.9, wall=31637
2022-03-04 18:14:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:14:38 | INFO | train_inner | epoch 050:    165 / 196 loss=4.287, nll_loss=3.436, ppl=10.83, wps=20082.6, ups=0.31, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.937, loss_scale=8, train_wall=302, gb_free=19.9, wall=31963
2022-03-04 18:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:16:23 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.769 | nll_loss 8.112 | ppl 276.68 | wps 37808.6 | wpb 510.9 | bsz 1 | num_updates 9731 | best_loss 7.44
2022-03-04 18:16:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9731 updates
2022-03-04 18:16:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:16:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 50 @ 9731 updates, score 8.769) (writing took 3.1879674340598285 seconds)
2022-03-04 18:16:26 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 18:16:26 | INFO | train | epoch 050 | loss 4.264 | nll_loss 3.412 | ppl 10.65 | wps 19786 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 9731 | lr 0.000320569 | gnorm 0.925 | loss_scale 8 | train_wall 585 | gb_free 19.9 | wall 32071
2022-03-04 18:16:26 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 18:16:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:20:09 | INFO | train_inner | epoch 051:     69 / 196 loss=4.216, nll_loss=3.361, ppl=10.28, wps=19712.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.917, loss_scale=8, train_wall=298, gb_free=19.9, wall=32294
2022-03-04 18:25:33 | INFO | train_inner | epoch 051:    169 / 196 loss=4.253, nll_loss=3.4, ppl=10.56, wps=20265.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.924, loss_scale=16, train_wall=299, gb_free=19.9, wall=32618
2022-03-04 18:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:27:05 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.827 | nll_loss 8.171 | ppl 288.28 | wps 38230.8 | wpb 510.9 | bsz 1 | num_updates 9927 | best_loss 7.44
2022-03-04 18:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9927 updates
2022-03-04 18:27:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:27:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 51 @ 9927 updates, score 8.827) (writing took 3.182854645187035 seconds)
2022-03-04 18:27:08 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 18:27:08 | INFO | train | epoch 051 | loss 4.232 | nll_loss 3.377 | ppl 10.39 | wps 19986.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 9927 | lr 0.000317388 | gnorm 0.925 | loss_scale 16 | train_wall 585 | gb_free 19.9 | wall 32713
2022-03-04 18:27:08 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 18:27:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:28:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:28:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:31:10 | INFO | train_inner | epoch 052:     75 / 196 loss=4.19, nll_loss=3.332, ppl=10.07, wps=19364.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.938, loss_scale=8, train_wall=304, gb_free=19.9, wall=32955
2022-03-04 18:36:27 | INFO | train_inner | epoch 052:    175 / 196 loss=4.226, nll_loss=3.371, ppl=10.35, wps=20693.5, ups=0.32, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.934, loss_scale=16, train_wall=294, gb_free=19.9, wall=33272
2022-03-04 18:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:37:38 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.833 | nll_loss 8.167 | ppl 287.47 | wps 39817.3 | wpb 510.9 | bsz 1 | num_updates 10121 | best_loss 7.44
2022-03-04 18:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10121 updates
2022-03-04 18:37:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:37:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 52 @ 10121 updates, score 8.833) (writing took 3.0203909270931035 seconds)
2022-03-04 18:37:41 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 18:37:41 | INFO | train | epoch 052 | loss 4.199 | nll_loss 3.342 | ppl 10.14 | wps 20054.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10121 | lr 0.000314332 | gnorm 0.937 | loss_scale 16 | train_wall 579 | gb_free 19.9 | wall 33346
2022-03-04 18:37:41 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 18:37:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:40:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:41:55 | INFO | train_inner | epoch 053:     80 / 196 loss=4.142, nll_loss=3.281, ppl=9.72, wps=19896.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.934, loss_scale=8, train_wall=297, gb_free=19.9, wall=33601
2022-03-04 18:47:17 | INFO | train_inner | epoch 053:    180 / 196 loss=4.207, nll_loss=3.35, ppl=10.2, wps=20379.7, ups=0.31, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.93, loss_scale=8, train_wall=298, gb_free=19.9, wall=33922
2022-03-04 18:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:48:13 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.88 | nll_loss 8.219 | ppl 298.03 | wps 38456 | wpb 510.9 | bsz 1 | num_updates 10316 | best_loss 7.44
2022-03-04 18:48:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10316 updates
2022-03-04 18:48:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:48:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:48:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 53 @ 10316 updates, score 8.88) (writing took 3.0597707049455494 seconds)
2022-03-04 18:48:16 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 18:48:16 | INFO | train | epoch 053 | loss 4.169 | nll_loss 3.309 | ppl 9.91 | wps 20091.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10316 | lr 0.000311347 | gnorm 0.933 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 33982
2022-03-04 18:48:16 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 18:48:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:48:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:52:50 | INFO | train_inner | epoch 054:     85 / 196 loss=4.112, nll_loss=3.248, ppl=9.5, wps=19655.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.978, loss_scale=8, train_wall=300, gb_free=19.9, wall=34255
2022-03-04 18:56:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:58:14 | INFO | train_inner | epoch 054:    186 / 196 loss=4.174, nll_loss=3.314, ppl=9.95, wps=20183.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.951, loss_scale=8, train_wall=301, gb_free=19.9, wall=34579
2022-03-04 18:58:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:58:51 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.936 | nll_loss 8.27 | ppl 308.73 | wps 38471 | wpb 510.9 | bsz 1 | num_updates 10510 | best_loss 7.44
2022-03-04 18:58:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10510 updates
2022-03-04 18:58:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:58:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:58:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 54 @ 10510 updates, score 8.936) (writing took 3.054774973075837 seconds)
2022-03-04 18:58:54 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 18:58:54 | INFO | train | epoch 054 | loss 4.139 | nll_loss 3.276 | ppl 9.69 | wps 19901.8 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 10510 | lr 0.00030846 | gnorm 0.965 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 34619
2022-03-04 18:58:54 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 18:58:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:03:44 | INFO | train_inner | epoch 055:     90 / 196 loss=4.074, nll_loss=3.207, ppl=9.23, wps=19849.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.946, loss_scale=16, train_wall=297, gb_free=19.9, wall=34909
2022-03-04 19:04:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:09:08 | INFO | train_inner | epoch 055:    191 / 196 loss=4.155, nll_loss=3.293, ppl=9.8, wps=20194.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.968, loss_scale=8, train_wall=300, gb_free=19.9, wall=35233
2022-03-04 19:09:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:29 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.007 | nll_loss 8.349 | ppl 325.98 | wps 38295.9 | wpb 510.9 | bsz 1 | num_updates 10705 | best_loss 7.44
2022-03-04 19:09:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10705 updates
2022-03-04 19:09:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:09:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:09:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 55 @ 10705 updates, score 9.007) (writing took 2.993579857982695 seconds)
2022-03-04 19:09:32 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 19:09:32 | INFO | train | epoch 055 | loss 4.112 | nll_loss 3.247 | ppl 9.49 | wps 20011.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10705 | lr 0.000305638 | gnorm 0.958 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 35257
2022-03-04 19:09:32 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 19:09:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:14:41 | INFO | train_inner | epoch 056:     96 / 196 loss=4.04, nll_loss=3.17, ppl=9, wps=19655.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.965, loss_scale=8, train_wall=300, gb_free=19.9, wall=35566
2022-03-04 19:20:01 | INFO | train_inner | epoch 056:    196 / 196 loss=4.132, nll_loss=3.268, ppl=9.63, wps=20399.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10900, lr=0.000302891, gnorm=0.96, loss_scale=8, train_wall=297, gb_free=19.9, wall=35886
2022-03-04 19:20:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:20:07 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.05 | nll_loss 8.396 | ppl 336.76 | wps 38574.4 | wpb 510.9 | bsz 1 | num_updates 10900 | best_loss 7.44
2022-03-04 19:20:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10900 updates
2022-03-04 19:20:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:20:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:20:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 56 @ 10900 updates, score 9.05) (writing took 2.9519223859533668 seconds)
2022-03-04 19:20:10 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 19:20:10 | INFO | train | epoch 056 | loss 4.083 | nll_loss 3.216 | ppl 9.29 | wps 20014.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10900 | lr 0.000302891 | gnorm 0.961 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 35895
2022-03-04 19:20:10 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 19:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:21:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:25:34 | INFO | train_inner | epoch 057:    101 / 196 loss=4.009, nll_loss=3.136, ppl=8.79, wps=19667, ups=0.3, wpb=65532.4, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.953, loss_scale=8, train_wall=301, gb_free=19.9, wall=36219
2022-03-04 19:29:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:30:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:30:45 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.083 | nll_loss 8.429 | ppl 344.59 | wps 38648 | wpb 510.9 | bsz 1 | num_updates 11094 | best_loss 7.44
2022-03-04 19:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11094 updates
2022-03-04 19:30:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:30:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:30:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 57 @ 11094 updates, score 9.083) (writing took 3.0155289620161057 seconds)
2022-03-04 19:30:48 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 19:30:48 | INFO | train | epoch 057 | loss 4.055 | nll_loss 3.186 | ppl 9.1 | wps 19901.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 11094 | lr 0.000300231 | gnorm 0.953 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 36533
2022-03-04 19:30:48 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 19:30:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:31:07 | INFO | train_inner | epoch 058:      6 / 196 loss=4.093, nll_loss=3.226, ppl=9.36, wps=19642.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.951, loss_scale=8, train_wall=300, gb_free=19.9, wall=36552
2022-03-04 19:36:28 | INFO | train_inner | epoch 058:    106 / 196 loss=3.992, nll_loss=3.117, ppl=8.68, wps=20404.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.965, loss_scale=16, train_wall=297, gb_free=19.9, wall=36873
2022-03-04 19:40:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:41:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:41:22 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.115 | nll_loss 8.451 | ppl 349.96 | wps 38354.5 | wpb 510.9 | bsz 1 | num_updates 11289 | best_loss 7.44
2022-03-04 19:41:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11289 updates
2022-03-04 19:41:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:41:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:41:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 58 @ 11289 updates, score 9.115) (writing took 2.9791706618852913 seconds)
2022-03-04 19:41:25 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 19:41:25 | INFO | train | epoch 058 | loss 4.031 | nll_loss 3.159 | ppl 8.93 | wps 20015.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11289 | lr 0.000297627 | gnorm 0.963 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 37170
2022-03-04 19:41:25 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 19:41:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:42:01 | INFO | train_inner | epoch 059:     11 / 196 loss=4.065, nll_loss=3.196, ppl=9.16, wps=19659.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.962, loss_scale=8, train_wall=300, gb_free=19.9, wall=37206
2022-03-04 19:47:22 | INFO | train_inner | epoch 059:    111 / 196 loss=3.973, nll_loss=3.096, ppl=8.55, wps=20391.9, ups=0.31, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.971, loss_scale=8, train_wall=298, gb_free=19.9, wall=37527
2022-03-04 19:49:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:52:00 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.185 | nll_loss 8.526 | ppl 368.65 | wps 38378.4 | wpb 510.9 | bsz 1 | num_updates 11484 | best_loss 7.44
2022-03-04 19:52:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11484 updates
2022-03-04 19:52:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:52:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 59 @ 11484 updates, score 9.185) (writing took 2.9728447019588202 seconds)
2022-03-04 19:52:03 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 19:52:03 | INFO | train | epoch 059 | loss 4.005 | nll_loss 3.131 | ppl 8.76 | wps 20011.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11484 | lr 0.000295089 | gnorm 0.973 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 37808
2022-03-04 19:52:03 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 19:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:52:55 | INFO | train_inner | epoch 060:     16 / 196 loss=4.03, nll_loss=3.158, ppl=8.92, wps=19661.3, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.984, loss_scale=8, train_wall=300, gb_free=19.9, wall=37860
2022-03-04 19:58:16 | INFO | train_inner | epoch 060:    116 / 196 loss=3.951, nll_loss=3.072, ppl=8.41, wps=20404.5, ups=0.31, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.99, loss_scale=16, train_wall=297, gb_free=19.9, wall=38181
2022-03-04 20:02:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:02:38 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.226 | nll_loss 8.574 | ppl 381.15 | wps 38326.4 | wpb 510.9 | bsz 1 | num_updates 11680 | best_loss 7.44
2022-03-04 20:02:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11680 updates
2022-03-04 20:02:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:02:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:02:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 60 @ 11680 updates, score 9.226) (writing took 2.945033944910392 seconds)
2022-03-04 20:02:41 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 20:02:41 | INFO | train | epoch 060 | loss 3.981 | nll_loss 3.105 | ppl 8.6 | wps 20114.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 11680 | lr 0.000292603 | gnorm 0.995 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 38446
2022-03-04 20:02:41 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 20:02:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:03:48 | INFO | train_inner | epoch 061:     21 / 196 loss=4.002, nll_loss=3.127, ppl=8.74, wps=19657.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=1, loss_scale=8, train_wall=300, gb_free=19.9, wall=38513
2022-03-04 20:09:09 | INFO | train_inner | epoch 061:    121 / 196 loss=3.929, nll_loss=3.049, ppl=8.28, wps=20407.5, ups=0.31, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.978, loss_scale=8, train_wall=297, gb_free=19.9, wall=38835
2022-03-04 20:13:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:13:15 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.256 | nll_loss 8.597 | ppl 387.08 | wps 38257.7 | wpb 510.9 | bsz 1 | num_updates 11875 | best_loss 7.44
2022-03-04 20:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11875 updates
2022-03-04 20:13:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:13:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:13:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 61 @ 11875 updates, score 9.256) (writing took 2.9649096271023154 seconds)
2022-03-04 20:13:18 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 20:13:18 | INFO | train | epoch 061 | loss 3.956 | nll_loss 3.078 | ppl 8.44 | wps 20023.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11875 | lr 0.000290191 | gnorm 0.983 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 39083
2022-03-04 20:13:18 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 20:13:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:13:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:14:42 | INFO | train_inner | epoch 062:     26 / 196 loss=3.974, nll_loss=3.097, ppl=8.56, wps=19668.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.987, loss_scale=8, train_wall=300, gb_free=19.9, wall=39167
2022-03-04 20:20:03 | INFO | train_inner | epoch 062:    126 / 196 loss=3.911, nll_loss=3.03, ppl=8.17, wps=20413.7, ups=0.31, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.987, loss_scale=8, train_wall=297, gb_free=19.9, wall=39488
2022-03-04 20:21:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:23:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:23:53 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.259 | nll_loss 8.604 | ppl 389.04 | wps 38464.7 | wpb 510.9 | bsz 1 | num_updates 12069 | best_loss 7.44
2022-03-04 20:23:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12069 updates
2022-03-04 20:23:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:23:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:23:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 62 @ 12069 updates, score 9.259) (writing took 2.9597975409124047 seconds)
2022-03-04 20:23:56 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 20:23:56 | INFO | train | epoch 062 | loss 3.934 | nll_loss 3.054 | ppl 8.3 | wps 19921.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 12069 | lr 0.000287849 | gnorm 0.992 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 39721
2022-03-04 20:23:56 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 20:23:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:25:35 | INFO | train_inner | epoch 063:     31 / 196 loss=3.942, nll_loss=3.062, ppl=8.35, wps=19662.1, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.985, loss_scale=8, train_wall=300, gb_free=19.9, wall=39820
2022-03-04 20:30:57 | INFO | train_inner | epoch 063:    131 / 196 loss=3.897, nll_loss=3.014, ppl=8.08, wps=20388.2, ups=0.31, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.977, loss_scale=16, train_wall=298, gb_free=19.9, wall=40142
2022-03-04 20:31:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:34:30 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.302 | nll_loss 8.645 | ppl 400.2 | wps 38345.1 | wpb 510.9 | bsz 1 | num_updates 12264 | best_loss 7.44
2022-03-04 20:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12264 updates
2022-03-04 20:34:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:34:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:34:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 63 @ 12264 updates, score 9.302) (writing took 2.9822353690396994 seconds)
2022-03-04 20:34:33 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 20:34:33 | INFO | train | epoch 063 | loss 3.911 | nll_loss 3.029 | ppl 8.17 | wps 20012.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12264 | lr 0.000285551 | gnorm 0.981 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 40358
2022-03-04 20:34:33 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 20:34:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:36:29 | INFO | train_inner | epoch 064:     36 / 196 loss=3.916, nll_loss=3.034, ppl=8.19, wps=19662.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.995, loss_scale=8, train_wall=300, gb_free=19.9, wall=40474
2022-03-04 20:41:50 | INFO | train_inner | epoch 064:    136 / 196 loss=3.881, nll_loss=2.997, ppl=7.98, wps=20393.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.999, loss_scale=16, train_wall=297, gb_free=19.9, wall=40796
2022-03-04 20:42:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:45:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:45:08 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.351 | nll_loss 8.696 | ppl 414.64 | wps 38296 | wpb 510.9 | bsz 1 | num_updates 12459 | best_loss 7.44
2022-03-04 20:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12459 updates
2022-03-04 20:45:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:45:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:45:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 64 @ 12459 updates, score 9.351) (writing took 2.9626605971716344 seconds)
2022-03-04 20:45:11 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 20:45:11 | INFO | train | epoch 064 | loss 3.891 | nll_loss 3.007 | ppl 8.04 | wps 20008 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12459 | lr 0.000283308 | gnorm 0.998 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 40996
2022-03-04 20:45:11 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 20:45:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:47:23 | INFO | train_inner | epoch 065:     41 / 196 loss=3.893, nll_loss=3.009, ppl=8.05, wps=19656.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=1.002, loss_scale=8, train_wall=300, gb_free=19.9, wall=41128
2022-03-04 20:50:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:52:47 | INFO | train_inner | epoch 065:    142 / 196 loss=3.864, nll_loss=2.979, ppl=7.88, wps=20207.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=12600, lr=0.000281718, gnorm=1.011, loss_scale=8, train_wall=300, gb_free=19.9, wall=41452
2022-03-04 20:55:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:55:46 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.385 | nll_loss 8.735 | ppl 426.03 | wps 38379.1 | wpb 510.9 | bsz 1 | num_updates 12654 | best_loss 7.44
2022-03-04 20:55:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12654 updates
2022-03-04 20:55:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:55:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 65 @ 12654 updates, score 9.385) (writing took 2.958895959891379 seconds)
2022-03-04 20:55:49 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 20:55:49 | INFO | train | epoch 065 | loss 3.869 | nll_loss 2.984 | ppl 7.91 | wps 20020.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12654 | lr 0.000281116 | gnorm 1.007 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 41634
2022-03-04 20:55:49 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 20:55:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:58:17 | INFO | train_inner | epoch 066:     46 / 196 loss=3.861, nll_loss=2.975, ppl=7.86, wps=19848.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.986, loss_scale=16, train_wall=297, gb_free=19.9, wall=41782
2022-03-04 20:58:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:03:41 | INFO | train_inner | epoch 066:    147 / 196 loss=3.844, nll_loss=2.956, ppl=7.76, wps=20206.8, ups=0.31, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=1, loss_scale=8, train_wall=300, gb_free=19.9, wall=42106
2022-03-04 21:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:06:23 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.437 | nll_loss 8.785 | ppl 441.01 | wps 38373.8 | wpb 510.9 | bsz 1 | num_updates 12849 | best_loss 7.44
2022-03-04 21:06:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12849 updates
2022-03-04 21:06:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 66 @ 12849 updates, score 9.437) (writing took 2.99106782884337 seconds)
2022-03-04 21:06:26 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 21:06:26 | INFO | train | epoch 066 | loss 3.848 | nll_loss 2.961 | ppl 7.78 | wps 20011.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12849 | lr 0.000278975 | gnorm 0.991 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 42271
2022-03-04 21:06:26 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 21:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:08:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:09:14 | INFO | train_inner | epoch 067:     52 / 196 loss=3.837, nll_loss=2.949, ppl=7.72, wps=19653.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=1.002, loss_scale=8, train_wall=300, gb_free=19.9, wall=42439
2022-03-04 21:14:35 | INFO | train_inner | epoch 067:    152 / 196 loss=3.837, nll_loss=2.949, ppl=7.72, wps=20397.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=1.02, loss_scale=8, train_wall=297, gb_free=19.9, wall=42760
2022-03-04 21:15:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:17:01 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.469 | nll_loss 8.818 | ppl 451.16 | wps 38343.9 | wpb 510.9 | bsz 1 | num_updates 13043 | best_loss 7.44
2022-03-04 21:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13043 updates
2022-03-04 21:17:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:17:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:17:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 67 @ 13043 updates, score 9.469) (writing took 2.9677695869468153 seconds)
2022-03-04 21:17:04 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 21:17:04 | INFO | train | epoch 067 | loss 3.828 | nll_loss 2.94 | ppl 7.67 | wps 19906.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 13043 | lr 0.000276893 | gnorm 1.016 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 42909
2022-03-04 21:17:04 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 21:17:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:20:07 | INFO | train_inner | epoch 068:     57 / 196 loss=3.812, nll_loss=2.922, ppl=7.58, wps=19659.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=1.006, loss_scale=8, train_wall=300, gb_free=19.9, wall=43092
2022-03-04 21:22:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:25:32 | INFO | train_inner | epoch 068:    158 / 196 loss=3.816, nll_loss=2.926, ppl=7.6, wps=20196.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=1.008, loss_scale=8, train_wall=301, gb_free=19.9, wall=43417
2022-03-04 21:27:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:27:39 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.504 | nll_loss 8.858 | ppl 463.92 | wps 38207.1 | wpb 510.9 | bsz 1 | num_updates 13238 | best_loss 7.44
2022-03-04 21:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13238 updates
2022-03-04 21:27:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:27:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:27:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 68 @ 13238 updates, score 9.504) (writing took 3.0110527689103037 seconds)
2022-03-04 21:27:42 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 21:27:42 | INFO | train | epoch 068 | loss 3.809 | nll_loss 2.918 | ppl 7.56 | wps 20014.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13238 | lr 0.000274846 | gnorm 1.012 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 43547
2022-03-04 21:27:42 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 21:27:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:30:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:31:04 | INFO | train_inner | epoch 069:     63 / 196 loss=3.785, nll_loss=2.893, ppl=7.43, wps=19669.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=1.026, loss_scale=8, train_wall=300, gb_free=19.9, wall=43749
2022-03-04 21:36:25 | INFO | train_inner | epoch 069:    163 / 196 loss=3.805, nll_loss=2.914, ppl=7.54, wps=20405.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=1.026, loss_scale=8, train_wall=297, gb_free=19.9, wall=44070
2022-03-04 21:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:38:16 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.534 | nll_loss 8.883 | ppl 472.18 | wps 38372.5 | wpb 510.9 | bsz 1 | num_updates 13433 | best_loss 7.44
2022-03-04 21:38:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13433 updates
2022-03-04 21:38:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:38:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:38:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 69 @ 13433 updates, score 9.534) (writing took 2.9798431461676955 seconds)
2022-03-04 21:38:19 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 21:38:19 | INFO | train | epoch 069 | loss 3.79 | nll_loss 2.898 | ppl 7.45 | wps 20023.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13433 | lr 0.000272843 | gnorm 1.027 | loss_scale 16 | train_wall 582 | gb_free 19.9 | wall 44184
2022-03-04 21:38:19 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 21:38:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:40:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:41:58 | INFO | train_inner | epoch 070:     68 / 196 loss=3.761, nll_loss=2.866, ppl=7.29, wps=19668, ups=0.3, wpb=65367, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=1.027, loss_scale=8, train_wall=300, gb_free=19.9, wall=44403
2022-03-04 21:47:19 | INFO | train_inner | epoch 070:    168 / 196 loss=3.794, nll_loss=2.902, ppl=7.48, wps=20399.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=1.024, loss_scale=16, train_wall=298, gb_free=19.9, wall=44724
2022-03-04 21:47:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:48:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:48:54 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.567 | nll_loss 8.916 | ppl 483.1 | wps 38799.8 | wpb 510.9 | bsz 1 | num_updates 13627 | best_loss 7.44
2022-03-04 21:48:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13627 updates
2022-03-04 21:48:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:48:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:48:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 70 @ 13627 updates, score 9.567) (writing took 2.9789622251410037 seconds)
2022-03-04 21:48:57 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 21:48:57 | INFO | train | epoch 070 | loss 3.77 | nll_loss 2.877 | ppl 7.34 | wps 19917 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 13627 | lr 0.000270894 | gnorm 1.02 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 44822
2022-03-04 21:48:57 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 21:48:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:52:51 | INFO | train_inner | epoch 071:     73 / 196 loss=3.731, nll_loss=2.834, ppl=7.13, wps=19665, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=1.013, loss_scale=8, train_wall=300, gb_free=19.9, wall=45056
2022-03-04 21:57:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:58:16 | INFO | train_inner | epoch 071:    174 / 196 loss=3.78, nll_loss=2.887, ppl=7.4, wps=20196.5, ups=0.31, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=1.014, loss_scale=8, train_wall=301, gb_free=19.9, wall=45381
2022-03-04 21:59:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:59:31 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.601 | nll_loss 8.953 | ppl 495.46 | wps 38383.7 | wpb 510.9 | bsz 1 | num_updates 13822 | best_loss 7.44
2022-03-04 21:59:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13822 updates
2022-03-04 21:59:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:59:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:59:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 71 @ 13822 updates, score 9.601) (writing took 2.974871550919488 seconds)
2022-03-04 21:59:34 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 21:59:34 | INFO | train | epoch 071 | loss 3.754 | nll_loss 2.859 | ppl 7.26 | wps 20016.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13822 | lr 0.000268977 | gnorm 1.014 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 45459
2022-03-04 21:59:34 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 21:59:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:03:45 | INFO | train_inner | epoch 072:     78 / 196 loss=3.716, nll_loss=2.818, ppl=7.05, wps=19855.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=1.035, loss_scale=8, train_wall=297, gb_free=19.9, wall=45710
2022-03-04 22:07:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:09:09 | INFO | train_inner | epoch 072:    179 / 196 loss=3.765, nll_loss=2.871, ppl=7.32, wps=20209, ups=0.31, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=1.032, loss_scale=8, train_wall=300, gb_free=19.9, wall=46034
2022-03-04 22:10:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:10:09 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.617 | nll_loss 8.969 | ppl 501.1 | wps 38690.5 | wpb 510.9 | bsz 1 | num_updates 14017 | best_loss 7.44
2022-03-04 22:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14017 updates
2022-03-04 22:10:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:10:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:10:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 72 @ 14017 updates, score 9.617) (writing took 2.9618306409101933 seconds)
2022-03-04 22:10:12 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 22:10:12 | INFO | train | epoch 072 | loss 3.737 | nll_loss 2.84 | ppl 7.16 | wps 20020.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14017 | lr 0.000267099 | gnorm 1.033 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 46097
2022-03-04 22:10:12 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 22:10:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:14:39 | INFO | train_inner | epoch 073:     83 / 196 loss=3.695, nll_loss=2.795, ppl=6.94, wps=19846, ups=0.3, wpb=65367, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=1.029, loss_scale=16, train_wall=297, gb_free=19.9, wall=46364
2022-03-04 22:18:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:20:03 | INFO | train_inner | epoch 073:    184 / 196 loss=3.753, nll_loss=2.857, ppl=7.25, wps=20187.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=14200, lr=0.000265372, gnorm=1.048, loss_scale=8, train_wall=301, gb_free=19.9, wall=46688
2022-03-04 22:20:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:20:47 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.659 | nll_loss 9.011 | ppl 515.91 | wps 38404.9 | wpb 510.9 | bsz 1 | num_updates 14212 | best_loss 7.44
2022-03-04 22:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14212 updates
2022-03-04 22:20:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:20:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:20:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 73 @ 14212 updates, score 9.659) (writing took 3.010962132131681 seconds)
2022-03-04 22:20:50 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 22:20:50 | INFO | train | epoch 073 | loss 3.721 | nll_loss 2.823 | ppl 7.07 | wps 20003.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14212 | lr 0.00026526 | gnorm 1.039 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 46735
2022-03-04 22:20:50 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 22:20:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:25:32 | INFO | train_inner | epoch 074:     88 / 196 loss=3.67, nll_loss=2.768, ppl=6.81, wps=19854.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=1.042, loss_scale=16, train_wall=297, gb_free=19.9, wall=47018
2022-03-04 22:26:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:30:57 | INFO | train_inner | epoch 074:    189 / 196 loss=3.743, nll_loss=2.847, ppl=7.2, wps=20205.1, ups=0.31, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=1.037, loss_scale=8, train_wall=300, gb_free=19.9, wall=47342
2022-03-04 22:31:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:31:24 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.678 | nll_loss 9.029 | ppl 522.22 | wps 38223.1 | wpb 510.9 | bsz 1 | num_updates 14407 | best_loss 7.44
2022-03-04 22:31:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14407 updates
2022-03-04 22:31:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:31:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 74 @ 14407 updates, score 9.678) (writing took 2.996905684005469 seconds)
2022-03-04 22:31:27 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 22:31:27 | INFO | train | epoch 074 | loss 3.704 | nll_loss 2.804 | ppl 6.98 | wps 20018.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14407 | lr 0.000263459 | gnorm 1.041 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 47372
2022-03-04 22:31:27 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 22:31:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:36:26 | INFO | train_inner | epoch 075:     93 / 196 loss=3.641, nll_loss=2.736, ppl=6.66, wps=19847.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=1.038, loss_scale=16, train_wall=297, gb_free=19.9, wall=47671
2022-03-04 22:40:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:41:48 | INFO | train_inner | epoch 075:    194 / 196 loss=3.738, nll_loss=2.841, ppl=7.16, wps=20336, ups=0.31, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=1.037, loss_scale=16, train_wall=299, gb_free=19.9, wall=47994
2022-03-04 22:41:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:42:00 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.698 | nll_loss 9.05 | ppl 530.07 | wps 37858.2 | wpb 510.9 | bsz 1 | num_updates 14602 | best_loss 7.44
2022-03-04 22:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14602 updates
2022-03-04 22:42:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:42:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:42:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 75 @ 14602 updates, score 9.698) (writing took 2.956180970184505 seconds)
2022-03-04 22:42:03 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 22:42:03 | INFO | train | epoch 075 | loss 3.687 | nll_loss 2.786 | ppl 6.9 | wps 20082.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14602 | lr 0.000261694 | gnorm 1.037 | loss_scale 16 | train_wall 581 | gb_free 19.9 | wall 48008
2022-03-04 22:42:03 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 22:42:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:43:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:47:16 | INFO | train_inner | epoch 076:     99 / 196 loss=3.634, nll_loss=2.729, ppl=6.63, wps=19949.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=1.047, loss_scale=8, train_wall=296, gb_free=19.9, wall=48321
2022-03-04 22:52:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:52:28 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.762 | nll_loss 9.118 | ppl 555.76 | wps 40282.7 | wpb 510.9 | bsz 1 | num_updates 14797 | best_loss 7.44
2022-03-04 22:52:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14797 updates
2022-03-04 22:52:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:52:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 76 @ 14797 updates, score 9.762) (writing took 2.9218840540852398 seconds)
2022-03-04 22:52:31 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 22:52:31 | INFO | train | epoch 076 | loss 3.672 | nll_loss 2.769 | ppl 6.82 | wps 20317.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14797 | lr 0.000259964 | gnorm 1.055 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 48636
2022-03-04 22:52:31 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 22:52:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:52:40 | INFO | train_inner | epoch 077:      3 / 196 loss=3.709, nll_loss=2.81, ppl=7.01, wps=20152.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.066, loss_scale=16, train_wall=293, gb_free=19.9, wall=48646
2022-03-04 22:53:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:58:00 | INFO | train_inner | epoch 077:    104 / 196 loss=3.614, nll_loss=2.707, ppl=6.53, wps=20479.5, ups=0.31, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=1.046, loss_scale=8, train_wall=297, gb_free=19.9, wall=48966
2022-03-04 23:01:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:02:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:02:57 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.77 | nll_loss 9.123 | ppl 557.61 | wps 40513 | wpb 510.9 | bsz 1 | num_updates 14991 | best_loss 7.44
2022-03-04 23:02:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14991 updates
2022-03-04 23:02:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:02:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 77 @ 14991 updates, score 9.77) (writing took 2.8940296431537718 seconds)
2022-03-04 23:02:59 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 23:02:59 | INFO | train | epoch 077 | loss 3.656 | nll_loss 2.752 | ppl 6.74 | wps 20195.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14991 | lr 0.000258276 | gnorm 1.052 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 49265
2022-03-04 23:03:00 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 23:03:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:03:28 | INFO | train_inner | epoch 078:      9 / 196 loss=3.69, nll_loss=2.789, ppl=6.91, wps=19949, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.062, loss_scale=8, train_wall=296, gb_free=19.9, wall=49293
2022-03-04 23:07:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:08:48 | INFO | train_inner | epoch 078:    110 / 196 loss=3.602, nll_loss=2.694, ppl=6.47, wps=20484.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=1.061, loss_scale=8, train_wall=297, gb_free=19.9, wall=49613
2022-03-04 23:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:13:25 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.812 | nll_loss 9.166 | ppl 574.53 | wps 40102.9 | wpb 510.9 | bsz 1 | num_updates 15186 | best_loss 7.44
2022-03-04 23:13:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15186 updates
2022-03-04 23:13:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:13:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 78 @ 15186 updates, score 9.812) (writing took 2.8985926201567054 seconds)
2022-03-04 23:13:28 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 23:13:28 | INFO | train | epoch 078 | loss 3.641 | nll_loss 2.737 | ppl 6.67 | wps 20308.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15186 | lr 0.000256613 | gnorm 1.06 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 49893
2022-03-04 23:13:28 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 23:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:14:12 | INFO | train_inner | epoch 079:     14 / 196 loss=3.675, nll_loss=2.773, ppl=6.84, wps=20153.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.051, loss_scale=8, train_wall=293, gb_free=19.9, wall=49937
2022-03-04 23:19:29 | INFO | train_inner | epoch 079:    114 / 196 loss=3.598, nll_loss=2.69, ppl=6.45, wps=20698.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.052, loss_scale=16, train_wall=294, gb_free=19.9, wall=50254
2022-03-04 23:21:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:22:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:23:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:23:54 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.843 | nll_loss 9.198 | ppl 587.45 | wps 38550.1 | wpb 510.9 | bsz 1 | num_updates 15380 | best_loss 7.44
2022-03-04 23:23:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15380 updates
2022-03-04 23:23:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:23:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 79 @ 15380 updates, score 9.843) (writing took 2.955714596901089 seconds)
2022-03-04 23:23:57 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 23:23:57 | INFO | train | epoch 079 | loss 3.626 | nll_loss 2.721 | ppl 6.59 | wps 20179.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15380 | lr 0.000254989 | gnorm 1.055 | loss_scale 8 | train_wall 576 | gb_free 19.9 | wall 50522
2022-03-04 23:23:57 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 23:23:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:25:02 | INFO | train_inner | epoch 080:     20 / 196 loss=3.652, nll_loss=2.748, ppl=6.72, wps=19645.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.062, loss_scale=8, train_wall=300, gb_free=19.9, wall=50587
2022-03-04 23:30:24 | INFO | train_inner | epoch 080:    120 / 196 loss=3.589, nll_loss=2.68, ppl=6.41, wps=20341.9, ups=0.31, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=1.047, loss_scale=16, train_wall=298, gb_free=19.9, wall=50909
2022-03-04 23:32:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:34:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:34:34 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.877 | nll_loss 9.235 | ppl 602.55 | wps 37995.1 | wpb 510.9 | bsz 1 | num_updates 15575 | best_loss 7.44
2022-03-04 23:34:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15575 updates
2022-03-04 23:34:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:34:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:34:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 80 @ 15575 updates, score 9.877) (writing took 2.963215902913362 seconds)
2022-03-04 23:34:37 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 23:34:37 | INFO | train | epoch 080 | loss 3.612 | nll_loss 2.705 | ppl 6.52 | wps 19953.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 15575 | lr 0.000253388 | gnorm 1.055 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 51162
2022-03-04 23:34:37 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 23:34:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:35:57 | INFO | train_inner | epoch 081:     25 / 196 loss=3.625, nll_loss=2.718, ppl=6.58, wps=19603.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.065, loss_scale=8, train_wall=300, gb_free=19.9, wall=51242
2022-03-04 23:41:19 | INFO | train_inner | epoch 081:    125 / 196 loss=3.579, nll_loss=2.669, ppl=6.36, wps=20342.8, ups=0.31, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=1.059, loss_scale=16, train_wall=298, gb_free=19.9, wall=51565
2022-03-04 23:43:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:45:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:45:13 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.931 | nll_loss 9.293 | ppl 627.29 | wps 37936 | wpb 510.9 | bsz 1 | num_updates 15770 | best_loss 7.44
2022-03-04 23:45:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15770 updates
2022-03-04 23:45:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:45:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 81 @ 15770 updates, score 9.931) (writing took 3.0289825999643654 seconds)
2022-03-04 23:45:16 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 23:45:16 | INFO | train | epoch 081 | loss 3.6 | nll_loss 2.691 | ppl 6.46 | wps 19958 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 15770 | lr 0.000251816 | gnorm 1.064 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 51801
2022-03-04 23:45:16 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 23:45:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:46:53 | INFO | train_inner | epoch 082:     30 / 196 loss=3.612, nll_loss=2.705, ppl=6.52, wps=19600.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.078, loss_scale=8, train_wall=300, gb_free=19.9, wall=51898
2022-03-04 23:52:15 | INFO | train_inner | epoch 082:    130 / 196 loss=3.572, nll_loss=2.662, ppl=6.33, wps=20341.8, ups=0.31, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.072, loss_scale=16, train_wall=298, gb_free=19.9, wall=52220
2022-03-04 23:55:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:55:53 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.964 | nll_loss 9.33 | ppl 643.67 | wps 38318.3 | wpb 510.9 | bsz 1 | num_updates 15966 | best_loss 7.44
2022-03-04 23:55:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15966 updates
2022-03-04 23:55:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:55:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:55:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 82 @ 15966 updates, score 9.964) (writing took 3.1434173739980906 seconds)
2022-03-04 23:55:56 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 23:55:56 | INFO | train | epoch 082 | loss 3.586 | nll_loss 2.676 | ppl 6.39 | wps 20057 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 15966 | lr 0.000250266 | gnorm 1.085 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 52441
2022-03-04 23:55:56 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 23:55:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:57:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:57:48 | INFO | train_inner | epoch 083:     35 / 196 loss=3.594, nll_loss=2.686, ppl=6.43, wps=19603.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.096, loss_scale=16, train_wall=300, gb_free=19.9, wall=52554
2022-03-04 23:59:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:03:14 | INFO | train_inner | epoch 083:    136 / 196 loss=3.559, nll_loss=2.647, ppl=6.27, wps=20157.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.049, loss_scale=8, train_wall=301, gb_free=19.9, wall=52879
2022-03-05 00:06:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:06:32 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.915 | nll_loss 9.272 | ppl 618.18 | wps 38197.1 | wpb 510.9 | bsz 1 | num_updates 16160 | best_loss 7.44
2022-03-05 00:06:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16160 updates
2022-03-05 00:06:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:06:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:06:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 83 @ 16160 updates, score 9.915) (writing took 2.972193693043664 seconds)
2022-03-05 00:06:35 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-05 00:06:35 | INFO | train | epoch 083 | loss 3.571 | nll_loss 2.661 | ppl 6.32 | wps 19876.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 16160 | lr 0.000248759 | gnorm 1.067 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 53080
2022-03-05 00:06:35 | INFO | fairseq.trainer | begin training epoch 84
2022-03-05 00:06:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:07:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:08:47 | INFO | train_inner | epoch 084:     41 / 196 loss=3.575, nll_loss=2.664, ppl=6.34, wps=19628.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.088, loss_scale=8, train_wall=300, gb_free=19.9, wall=53212
2022-03-05 00:14:08 | INFO | train_inner | epoch 084:    141 / 196 loss=3.555, nll_loss=2.643, ppl=6.24, wps=20360, ups=0.31, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.07, loss_scale=16, train_wall=298, gb_free=19.9, wall=53534
2022-03-05 00:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:17:11 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.984 | nll_loss 9.337 | ppl 646.8 | wps 38383.5 | wpb 510.9 | bsz 1 | num_updates 16355 | best_loss 7.44
2022-03-05 00:17:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16355 updates
2022-03-05 00:17:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:17:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:17:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 84 @ 16355 updates, score 9.984) (writing took 2.9673956260085106 seconds)
2022-03-05 00:17:14 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-05 00:17:14 | INFO | train | epoch 084 | loss 3.559 | nll_loss 2.647 | ppl 6.27 | wps 19969.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16355 | lr 0.000247272 | gnorm 1.089 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 53719
2022-03-05 00:17:14 | INFO | fairseq.trainer | begin training epoch 85
2022-03-05 00:17:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:18:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:19:42 | INFO | train_inner | epoch 085:     46 / 196 loss=3.557, nll_loss=2.645, ppl=6.26, wps=19612.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.086, loss_scale=8, train_wall=300, gb_free=19.9, wall=53867
2022-03-05 00:25:04 | INFO | train_inner | epoch 085:    146 / 196 loss=3.544, nll_loss=2.631, ppl=6.2, wps=20369.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.079, loss_scale=8, train_wall=298, gb_free=19.9, wall=54189
2022-03-05 00:27:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:27:49 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.991 | nll_loss 9.347 | ppl 651.04 | wps 38044.7 | wpb 510.9 | bsz 1 | num_updates 16550 | best_loss 7.44
2022-03-05 00:27:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16550 updates
2022-03-05 00:27:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:27:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:27:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 85 @ 16550 updates, score 9.991) (writing took 3.0357649689540267 seconds)
2022-03-05 00:27:52 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-05 00:27:52 | INFO | train | epoch 085 | loss 3.546 | nll_loss 2.633 | ppl 6.2 | wps 19981 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16550 | lr 0.000245811 | gnorm 1.071 | loss_scale 16 | train_wall 583 | gb_free 19.9 | wall 54357
2022-03-05 00:27:52 | INFO | fairseq.trainer | begin training epoch 86
2022-03-05 00:27:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:30:34 | INFO | train_inner | epoch 086:     50 / 196 loss=3.536, nll_loss=2.622, ppl=6.16, wps=19801.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.075, loss_scale=16, train_wall=297, gb_free=19.9, wall=54519
2022-03-05 00:31:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:35:59 | INFO | train_inner | epoch 086:    151 / 196 loss=3.54, nll_loss=2.626, ppl=6.17, wps=20146.4, ups=0.31, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.093, loss_scale=8, train_wall=301, gb_free=19.9, wall=54844
2022-03-05 00:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:38:29 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.022 | nll_loss 9.383 | ppl 667.48 | wps 38021.7 | wpb 510.9 | bsz 1 | num_updates 16745 | best_loss 7.44
2022-03-05 00:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16745 updates
2022-03-05 00:38:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:38:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:38:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 86 @ 16745 updates, score 10.022) (writing took 3.0058005140163004 seconds)
2022-03-05 00:38:32 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-05 00:38:32 | INFO | train | epoch 086 | loss 3.534 | nll_loss 2.62 | ppl 6.15 | wps 19957.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 16745 | lr 0.000244375 | gnorm 1.092 | loss_scale 16 | train_wall 584 | gb_free 19.9 | wall 54997
2022-03-05 00:38:32 | INFO | fairseq.trainer | begin training epoch 87
2022-03-05 00:38:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:38:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:41:32 | INFO | train_inner | epoch 087:     56 / 196 loss=3.526, nll_loss=2.611, ppl=6.11, wps=19612.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.089, loss_scale=8, train_wall=300, gb_free=19.9, wall=55177
2022-03-05 00:46:54 | INFO | train_inner | epoch 087:    156 / 196 loss=3.526, nll_loss=2.611, ppl=6.11, wps=20356.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.099, loss_scale=16, train_wall=298, gb_free=19.9, wall=55499
2022-03-05 00:48:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:49:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:49:08 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.045 | nll_loss 9.403 | ppl 676.92 | wps 38959.8 | wpb 510.9 | bsz 1 | num_updates 16939 | best_loss 7.44
2022-03-05 00:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16939 updates
2022-03-05 00:49:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:49:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:49:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 87 @ 16939 updates, score 10.045) (writing took 2.995846669888124 seconds)
2022-03-05 00:49:11 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-05 00:49:11 | INFO | train | epoch 087 | loss 3.521 | nll_loss 2.607 | ppl 6.09 | wps 19867.9 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 16939 | lr 0.000242972 | gnorm 1.09 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 55636
2022-03-05 00:49:11 | INFO | fairseq.trainer | begin training epoch 88
2022-03-05 00:49:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:52:28 | INFO | train_inner | epoch 088:     61 / 196 loss=3.5, nll_loss=2.584, ppl=5.99, wps=19606.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.08, loss_scale=8, train_wall=300, gb_free=19.9, wall=55833
2022-03-05 00:57:50 | INFO | train_inner | epoch 088:    161 / 196 loss=3.526, nll_loss=2.611, ppl=6.11, wps=20339.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.091, loss_scale=16, train_wall=298, gb_free=19.9, wall=56155
2022-03-05 00:58:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:59:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:59:47 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.032 | nll_loss 9.393 | ppl 672.14 | wps 38071.8 | wpb 510.9 | bsz 1 | num_updates 17134 | best_loss 7.44
2022-03-05 00:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17134 updates
2022-03-05 00:59:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:59:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:59:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 88 @ 17134 updates, score 10.032) (writing took 3.020610440056771 seconds)
2022-03-05 00:59:50 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-05 00:59:50 | INFO | train | epoch 088 | loss 3.511 | nll_loss 2.595 | ppl 6.04 | wps 19955.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 17134 | lr 0.000241585 | gnorm 1.091 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 56276
2022-03-05 00:59:50 | INFO | fairseq.trainer | begin training epoch 89
2022-03-05 00:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:03:23 | INFO | train_inner | epoch 089:     66 / 196 loss=3.488, nll_loss=2.57, ppl=5.94, wps=19616.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.088, loss_scale=8, train_wall=300, gb_free=19.9, wall=56488
2022-03-05 01:05:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:08:48 | INFO | train_inner | epoch 089:    167 / 196 loss=3.518, nll_loss=2.603, ppl=6.08, wps=20154.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.097, loss_scale=8, train_wall=301, gb_free=19.9, wall=56813
2022-03-05 01:10:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:10:26 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.087 | nll_loss 9.449 | ppl 698.98 | wps 38246.1 | wpb 510.9 | bsz 1 | num_updates 17329 | best_loss 7.44
2022-03-05 01:10:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17329 updates
2022-03-05 01:10:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 89 @ 17329 updates, score 10.087) (writing took 3.03227108810097 seconds)
2022-03-05 01:10:29 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-05 01:10:29 | INFO | train | epoch 089 | loss 3.498 | nll_loss 2.582 | ppl 5.99 | wps 19968.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17329 | lr 0.000240222 | gnorm 1.083 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 56915
2022-03-05 01:10:30 | INFO | fairseq.trainer | begin training epoch 90
2022-03-05 01:10:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:14:18 | INFO | train_inner | epoch 090:     71 / 196 loss=3.467, nll_loss=2.548, ppl=5.85, wps=19796.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.087, loss_scale=16, train_wall=297, gb_free=19.9, wall=57143
2022-03-05 01:17:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:19:44 | INFO | train_inner | epoch 090:    172 / 196 loss=3.509, nll_loss=2.593, ppl=6.03, wps=20130.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.089, loss_scale=8, train_wall=301, gb_free=19.9, wall=57469
2022-03-05 01:21:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:21:06 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.092 | nll_loss 9.458 | ppl 703.32 | wps 38330.1 | wpb 510.9 | bsz 1 | num_updates 17524 | best_loss 7.44
2022-03-05 01:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17524 updates
2022-03-05 01:21:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:21:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 90 @ 17524 updates, score 10.092) (writing took 3.009182445006445 seconds)
2022-03-05 01:21:09 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-05 01:21:09 | INFO | train | epoch 090 | loss 3.487 | nll_loss 2.569 | ppl 5.93 | wps 19962.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17524 | lr 0.000238882 | gnorm 1.097 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 57554
2022-03-05 01:21:09 | INFO | fairseq.trainer | begin training epoch 91
2022-03-05 01:21:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:25:13 | INFO | train_inner | epoch 091:     76 / 196 loss=3.462, nll_loss=2.543, ppl=5.83, wps=19867.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.093, loss_scale=16, train_wall=296, gb_free=19.9, wall=57798
2022-03-05 01:26:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:30:37 | INFO | train_inner | epoch 091:    177 / 196 loss=3.501, nll_loss=2.584, ppl=6, wps=20226.6, ups=0.31, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.111, loss_scale=8, train_wall=300, gb_free=19.9, wall=58122
2022-03-05 01:31:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:31:43 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.115 | nll_loss 9.479 | ppl 713.72 | wps 38732.5 | wpb 510.9 | bsz 1 | num_updates 17719 | best_loss 7.44
2022-03-05 01:31:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17719 updates
2022-03-05 01:31:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:31:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:31:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 91 @ 17719 updates, score 10.115) (writing took 2.995886142132804 seconds)
2022-03-05 01:31:46 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-05 01:31:46 | INFO | train | epoch 091 | loss 3.475 | nll_loss 2.557 | ppl 5.88 | wps 20039.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17719 | lr 0.000237564 | gnorm 1.095 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 58191
2022-03-05 01:31:46 | INFO | fairseq.trainer | begin training epoch 92
2022-03-05 01:31:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:33:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:36:09 | INFO | train_inner | epoch 092:     82 / 196 loss=3.438, nll_loss=2.517, ppl=5.72, wps=19695.7, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.11, loss_scale=8, train_wall=299, gb_free=19.9, wall=58454
2022-03-05 01:41:30 | INFO | train_inner | epoch 092:    182 / 196 loss=3.492, nll_loss=2.575, ppl=5.96, wps=20404.7, ups=0.31, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.079, loss_scale=16, train_wall=297, gb_free=19.9, wall=58775
2022-03-05 01:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:42:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:42:20 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.163 | nll_loss 9.531 | ppl 740.06 | wps 38367.4 | wpb 510.9 | bsz 1 | num_updates 17913 | best_loss 7.44
2022-03-05 01:42:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17913 updates
2022-03-05 01:42:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:42:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:42:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 92 @ 17913 updates, score 10.163) (writing took 2.978731725132093 seconds)
2022-03-05 01:42:23 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-05 01:42:23 | INFO | train | epoch 092 | loss 3.464 | nll_loss 2.544 | ppl 5.83 | wps 19933.6 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 17913 | lr 0.000236274 | gnorm 1.096 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 58828
2022-03-05 01:42:23 | INFO | fairseq.trainer | begin training epoch 93
2022-03-05 01:42:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:47:02 | INFO | train_inner | epoch 093:     87 / 196 loss=3.426, nll_loss=2.503, ppl=5.67, wps=19690.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.088, loss_scale=8, train_wall=299, gb_free=19.9, wall=59107
2022-03-05 01:51:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:52:26 | INFO | train_inner | epoch 093:    188 / 196 loss=3.49, nll_loss=2.572, ppl=5.95, wps=20197.1, ups=0.31, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.104, loss_scale=8, train_wall=301, gb_free=19.9, wall=59431
2022-03-05 01:52:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:52:57 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.208 | nll_loss 9.575 | ppl 762.93 | wps 38261.7 | wpb 510.9 | bsz 1 | num_updates 18108 | best_loss 7.44
2022-03-05 01:52:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18108 updates
2022-03-05 01:52:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 93 @ 18108 updates, score 10.208) (writing took 3.024294533068314 seconds)
2022-03-05 01:53:00 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-05 01:53:00 | INFO | train | epoch 093 | loss 3.455 | nll_loss 2.535 | ppl 5.8 | wps 20023.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18108 | lr 0.000234998 | gnorm 1.096 | loss_scale 8 | train_wall 582 | gb_free 19.9 | wall 59465
2022-03-05 01:53:00 | INFO | fairseq.trainer | begin training epoch 94
2022-03-05 01:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:57:56 | INFO | train_inner | epoch 094:     92 / 196 loss=3.409, nll_loss=2.485, ppl=5.6, wps=19822.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.097, loss_scale=8, train_wall=297, gb_free=19.9, wall=59761
2022-03-05 01:59:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:03:21 | INFO | train_inner | epoch 094:    193 / 196 loss=3.485, nll_loss=2.567, ppl=5.93, wps=20160.3, ups=0.31, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.126, loss_scale=8, train_wall=301, gb_free=19.9, wall=60086
2022-03-05 02:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:03:36 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.167 | nll_loss 9.53 | ppl 739.14 | wps 37962.2 | wpb 510.9 | bsz 1 | num_updates 18303 | best_loss 7.44
2022-03-05 02:03:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18303 updates
2022-03-05 02:03:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:03:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:03:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 94 @ 18303 updates, score 10.167) (writing took 3.0361163900233805 seconds)
2022-03-05 02:03:39 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-05 02:03:39 | INFO | train | epoch 094 | loss 3.444 | nll_loss 2.523 | ppl 5.75 | wps 19977.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18303 | lr 0.000233743 | gnorm 1.111 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 60104
2022-03-05 02:03:39 | INFO | fairseq.trainer | begin training epoch 95
2022-03-05 02:03:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:08:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:08:54 | INFO | train_inner | epoch 095:     98 / 196 loss=3.389, nll_loss=2.463, ppl=5.51, wps=19616, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=1.11, loss_scale=8, train_wall=300, gb_free=19.9, wall=60419
2022-03-05 02:14:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:14:15 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.231 | nll_loss 9.598 | ppl 775.14 | wps 38218.2 | wpb 510.9 | bsz 1 | num_updates 18498 | best_loss 7.44
2022-03-05 02:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18498 updates
2022-03-05 02:14:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 95 @ 18498 updates, score 10.231) (writing took 3.03173434198834 seconds)
2022-03-05 02:14:18 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-05 02:14:18 | INFO | train | epoch 095 | loss 3.434 | nll_loss 2.512 | ppl 5.7 | wps 19968.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18498 | lr 0.000232508 | gnorm 1.12 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 60743
2022-03-05 02:14:18 | INFO | fairseq.trainer | begin training epoch 96
2022-03-05 02:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:14:24 | INFO | train_inner | epoch 096:      2 / 196 loss=3.479, nll_loss=2.561, ppl=5.9, wps=19803.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.13, loss_scale=8, train_wall=297, gb_free=19.9, wall=60750
2022-03-05 02:16:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:19:50 | INFO | train_inner | epoch 096:    103 / 196 loss=3.381, nll_loss=2.454, ppl=5.48, wps=20158, ups=0.31, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.109, loss_scale=8, train_wall=301, gb_free=19.9, wall=61075
2022-03-05 02:24:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:24:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:24:54 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.258 | nll_loss 9.624 | ppl 789.32 | wps 37976.5 | wpb 510.9 | bsz 1 | num_updates 18692 | best_loss 7.44
2022-03-05 02:24:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18692 updates
2022-03-05 02:24:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:24:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:24:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 96 @ 18692 updates, score 10.258) (writing took 3.0991530909668654 seconds)
2022-03-05 02:24:57 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-05 02:24:57 | INFO | train | epoch 096 | loss 3.422 | nll_loss 2.499 | ppl 5.65 | wps 19867.5 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 18692 | lr 0.000231298 | gnorm 1.116 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 61382
2022-03-05 02:24:57 | INFO | fairseq.trainer | begin training epoch 97
2022-03-05 02:24:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:25:23 | INFO | train_inner | epoch 097:      8 / 196 loss=3.457, nll_loss=2.537, ppl=5.8, wps=19613.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.118, loss_scale=8, train_wall=300, gb_free=19.9, wall=61408
2022-03-05 02:30:45 | INFO | train_inner | epoch 097:    108 / 196 loss=3.377, nll_loss=2.45, ppl=5.46, wps=20363.5, ups=0.31, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.117, loss_scale=8, train_wall=298, gb_free=19.9, wall=61730
2022-03-05 02:31:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:35:33 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.264 | nll_loss 9.63 | ppl 792.58 | wps 38671 | wpb 510.9 | bsz 1 | num_updates 18887 | best_loss 7.44
2022-03-05 02:35:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18887 updates
2022-03-05 02:35:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:35:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:35:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 97 @ 18887 updates, score 10.264) (writing took 3.039297731826082 seconds)
2022-03-05 02:35:36 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-05 02:35:36 | INFO | train | epoch 097 | loss 3.413 | nll_loss 2.49 | ppl 5.62 | wps 19974.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18887 | lr 0.000230101 | gnorm 1.119 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 62021
2022-03-05 02:35:36 | INFO | fairseq.trainer | begin training epoch 98
2022-03-05 02:35:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:36:18 | INFO | train_inner | epoch 098:     13 / 196 loss=3.445, nll_loss=2.524, ppl=5.75, wps=19617.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.124, loss_scale=8, train_wall=300, gb_free=19.9, wall=62063
2022-03-05 02:39:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:41:43 | INFO | train_inner | epoch 098:    114 / 196 loss=3.371, nll_loss=2.443, ppl=5.44, wps=20155, ups=0.31, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.094, loss_scale=8, train_wall=301, gb_free=19.9, wall=62388
2022-03-05 02:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:46:12 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.276 | nll_loss 9.639 | ppl 797.32 | wps 38025.2 | wpb 510.9 | bsz 1 | num_updates 19082 | best_loss 7.44
2022-03-05 02:46:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19082 updates
2022-03-05 02:46:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 98 @ 19082 updates, score 10.276) (writing took 3.07985213608481 seconds)
2022-03-05 02:46:15 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-05 02:46:15 | INFO | train | epoch 098 | loss 3.403 | nll_loss 2.479 | ppl 5.57 | wps 19966.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19082 | lr 0.000228922 | gnorm 1.108 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 62660
2022-03-05 02:46:15 | INFO | fairseq.trainer | begin training epoch 99
2022-03-05 02:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:46:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:47:16 | INFO | train_inner | epoch 099:     19 / 196 loss=3.431, nll_loss=2.509, ppl=5.69, wps=19612.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.127, loss_scale=8, train_wall=300, gb_free=19.9, wall=62721
2022-03-05 02:52:38 | INFO | train_inner | epoch 099:    119 / 196 loss=3.375, nll_loss=2.449, ppl=5.46, wps=20351.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.114, loss_scale=8, train_wall=298, gb_free=19.9, wall=63043
2022-03-05 02:55:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:56:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:56:50 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.326 | nll_loss 9.693 | ppl 827.62 | wps 40306.4 | wpb 510.9 | bsz 1 | num_updates 19276 | best_loss 7.44
2022-03-05 02:56:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19276 updates
2022-03-05 02:56:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:56:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:56:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 99 @ 19276 updates, score 10.326) (writing took 2.9517481380607933 seconds)
2022-03-05 02:56:53 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-05 02:56:53 | INFO | train | epoch 099 | loss 3.394 | nll_loss 2.469 | ppl 5.54 | wps 19904.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 19276 | lr 0.000227767 | gnorm 1.124 | loss_scale 8 | train_wall 583 | gb_free 19.9 | wall 63298
2022-03-05 02:56:53 | INFO | fairseq.trainer | begin training epoch 100
2022-03-05 02:56:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:58:09 | INFO | train_inner | epoch 100:     24 / 196 loss=3.405, nll_loss=2.481, ppl=5.58, wps=19758.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.122, loss_scale=8, train_wall=299, gb_free=19.9, wall=63374
2022-03-05 03:02:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:03:29 | INFO | train_inner | epoch 100:    125 / 196 loss=3.369, nll_loss=2.441, ppl=5.43, wps=20453.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.138, loss_scale=8, train_wall=297, gb_free=19.9, wall=63695
2022-03-05 03:07:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:07:19 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.326 | nll_loss 9.695 | ppl 828.89 | wps 40496.1 | wpb 510.9 | bsz 1 | num_updates 19471 | best_loss 7.44
2022-03-05 03:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19471 updates
2022-03-05 03:07:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 100 @ 19471 updates, score 10.326) (writing took 2.9619451130274683 seconds)
2022-03-05 03:07:22 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-05 03:07:22 | INFO | train | epoch 100 | loss 3.385 | nll_loss 2.459 | ppl 5.5 | wps 20292 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19471 | lr 0.000226624 | gnorm 1.134 | loss_scale 8 | train_wall 576 | gb_free 19.9 | wall 63927
2022-03-05 03:07:22 | INFO | fairseq.trainer | begin training epoch 101
2022-03-05 03:07:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:08:54 | INFO | train_inner | epoch 101:     29 / 196 loss=3.395, nll_loss=2.47, ppl=5.54, wps=20134.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.133, loss_scale=8, train_wall=293, gb_free=19.9, wall=64019
2022-03-05 03:11:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:14:14 | INFO | train_inner | epoch 101:    130 / 196 loss=3.366, nll_loss=2.438, ppl=5.42, wps=20497.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.103, loss_scale=8, train_wall=297, gb_free=19.9, wall=64339
2022-03-05 03:17:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:17:48 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.328 | nll_loss 9.698 | ppl 830.31 | wps 40642.8 | wpb 510.9 | bsz 1 | num_updates 19666 | best_loss 7.44
2022-03-05 03:17:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19666 updates
2022-03-05 03:17:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:17:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 101 @ 19666 updates, score 10.328) (writing took 2.9068967460189015 seconds)
2022-03-05 03:17:51 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-05 03:17:51 | INFO | train | epoch 101 | loss 3.376 | nll_loss 2.449 | ppl 5.46 | wps 20302.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19666 | lr 0.000225498 | gnorm 1.109 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 64556
2022-03-05 03:17:51 | INFO | fairseq.trainer | begin training epoch 102
2022-03-05 03:17:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:19:39 | INFO | train_inner | epoch 102:     34 / 196 loss=3.385, nll_loss=2.46, ppl=5.5, wps=20129.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.123, loss_scale=16, train_wall=293, gb_free=19.9, wall=64664
2022-03-05 03:22:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:24:58 | INFO | train_inner | epoch 102:    135 / 196 loss=3.352, nll_loss=2.423, ppl=5.36, wps=20510.4, ups=0.31, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.124, loss_scale=8, train_wall=297, gb_free=19.9, wall=64983
2022-03-05 03:28:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:28:16 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.34 | nll_loss 9.708 | ppl 836.33 | wps 40096.8 | wpb 510.9 | bsz 1 | num_updates 19861 | best_loss 7.44
2022-03-05 03:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19861 updates
2022-03-05 03:28:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:28:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:28:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 102 @ 19861 updates, score 10.34) (writing took 3.000205595046282 seconds)
2022-03-05 03:28:19 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-05 03:28:19 | INFO | train | epoch 102 | loss 3.367 | nll_loss 2.44 | ppl 5.43 | wps 20304.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19861 | lr 0.000224388 | gnorm 1.133 | loss_scale 8 | train_wall 575 | gb_free 19.9 | wall 65184
2022-03-05 03:28:19 | INFO | fairseq.trainer | begin training epoch 103
2022-03-05 03:28:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:30:23 | INFO | train_inner | epoch 103:     39 / 196 loss=3.378, nll_loss=2.451, ppl=5.47, wps=20137.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.145, loss_scale=16, train_wall=293, gb_free=19.9, wall=65308
2022-03-05 03:30:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:35:42 | INFO | train_inner | epoch 103:    140 / 196 loss=3.352, nll_loss=2.424, ppl=5.36, wps=20501.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.113, loss_scale=8, train_wall=297, gb_free=19.9, wall=65627
2022-03-05 03:38:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:38:44 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.375 | nll_loss 9.743 | ppl 856.9 | wps 39942.3 | wpb 510.9 | bsz 1 | num_updates 20056 | best_loss 7.44
2022-03-05 03:38:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20056 updates
2022-03-05 03:38:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:38:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:38:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 103 @ 20056 updates, score 10.375) (writing took 2.9808147691655904 seconds)
2022-03-05 03:38:47 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-05 03:38:47 | INFO | train | epoch 103 | loss 3.359 | nll_loss 2.431 | ppl 5.39 | wps 20308.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20056 | lr 0.000223294 | gnorm 1.125 | loss_scale 16 | train_wall 575 | gb_free 19.9 | wall 65813
2022-03-05 03:38:47 | INFO | fairseq.trainer | begin training epoch 104
2022-03-05 03:38:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:41:07 | INFO | train_inner | epoch 104:     44 / 196 loss=3.357, nll_loss=2.429, ppl=5.38, wps=20121.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.13, loss_scale=16, train_wall=293, gb_free=19.9, wall=65952
2022-03-05 03:44:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:45:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:46:37 | INFO | train_inner | epoch 104:    146 / 196 loss=3.35, nll_loss=2.421, ppl=5.35, wps=19891.7, ups=0.3, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.144, loss_scale=8, train_wall=304, gb_free=19.9, wall=66282
2022-03-05 03:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:49:22 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.377 | nll_loss 9.743 | ppl 856.93 | wps 38949.3 | wpb 510.9 | bsz 1 | num_updates 20250 | best_loss 7.44
2022-03-05 03:49:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20250 updates
2022-03-05 03:49:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:49:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 104 @ 20250 updates, score 10.377) (writing took 3.1322455981280655 seconds)
2022-03-05 03:49:25 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-05 03:49:25 | INFO | train | epoch 104 | loss 3.35 | nll_loss 2.421 | ppl 5.36 | wps 19925.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 20250 | lr 0.000222222 | gnorm 1.139 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 66450
2022-03-05 03:49:25 | INFO | fairseq.trainer | begin training epoch 105
2022-03-05 03:49:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:52:05 | INFO | train_inner | epoch 105:     50 / 196 loss=3.341, nll_loss=2.411, ppl=5.32, wps=19917.4, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.131, loss_scale=8, train_wall=296, gb_free=19.9, wall=66610
2022-03-05 03:53:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:57:28 | INFO | train_inner | epoch 105:    151 / 196 loss=3.346, nll_loss=2.417, ppl=5.34, wps=20283.3, ups=0.31, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.157, loss_scale=8, train_wall=299, gb_free=19.9, wall=66933
2022-03-05 03:59:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:59:57 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.444 | nll_loss 9.813 | ppl 899.52 | wps 38919.5 | wpb 510.9 | bsz 1 | num_updates 20445 | best_loss 7.44
2022-03-05 03:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20445 updates
2022-03-05 03:59:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:00:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:00:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 105 @ 20445 updates, score 10.444) (writing took 3.1292151871602982 seconds)
2022-03-05 04:00:00 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-05 04:00:00 | INFO | train | epoch 105 | loss 3.342 | nll_loss 2.412 | ppl 5.32 | wps 20088.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20445 | lr 0.00022116 | gnorm 1.144 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 67085
2022-03-05 04:00:00 | INFO | fairseq.trainer | begin training epoch 106
2022-03-05 04:00:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:02:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:02:59 | INFO | train_inner | epoch 106:     56 / 196 loss=3.334, nll_loss=2.404, ppl=5.29, wps=19722.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.128, loss_scale=8, train_wall=299, gb_free=19.9, wall=67264
2022-03-05 04:08:19 | INFO | train_inner | epoch 106:    156 / 196 loss=3.338, nll_loss=2.408, ppl=5.31, wps=20483.8, ups=0.31, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.133, loss_scale=8, train_wall=296, gb_free=19.9, wall=67584
2022-03-05 04:10:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:10:32 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.41 | nll_loss 9.783 | ppl 881.23 | wps 38494.7 | wpb 510.9 | bsz 1 | num_updates 20640 | best_loss 7.44
2022-03-05 04:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20640 updates
2022-03-05 04:10:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:10:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:10:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 106 @ 20640 updates, score 10.41) (writing took 3.0927825239486992 seconds)
2022-03-05 04:10:35 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-05 04:10:35 | INFO | train | epoch 106 | loss 3.333 | nll_loss 2.403 | ppl 5.29 | wps 20086.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20640 | lr 0.000220113 | gnorm 1.136 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 67720
2022-03-05 04:10:35 | INFO | fairseq.trainer | begin training epoch 107
2022-03-05 04:10:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:13:51 | INFO | train_inner | epoch 107:     61 / 196 loss=3.319, nll_loss=2.387, ppl=5.23, wps=19719.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.139, loss_scale=8, train_wall=299, gb_free=19.9, wall=67916
2022-03-05 04:19:11 | INFO | train_inner | epoch 107:    161 / 196 loss=3.337, nll_loss=2.407, ppl=5.3, wps=20464.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.146, loss_scale=8, train_wall=297, gb_free=19.9, wall=68236
2022-03-05 04:20:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:21:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:21:08 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.445 | nll_loss 9.814 | ppl 900.04 | wps 38736 | wpb 510.9 | bsz 1 | num_updates 20834 | best_loss 7.44
2022-03-05 04:21:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20834 updates
2022-03-05 04:21:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:21:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:21:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 107 @ 20834 updates, score 10.445) (writing took 3.09564146399498 seconds)
2022-03-05 04:21:11 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-05 04:21:11 | INFO | train | epoch 107 | loss 3.324 | nll_loss 2.393 | ppl 5.25 | wps 19979.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20834 | lr 0.000219086 | gnorm 1.134 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 68356
2022-03-05 04:21:11 | INFO | fairseq.trainer | begin training epoch 108
2022-03-05 04:21:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:24:42 | INFO | train_inner | epoch 108:     66 / 196 loss=3.305, nll_loss=2.373, ppl=5.18, wps=19744.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.123, loss_scale=8, train_wall=298, gb_free=19.9, wall=68567
2022-03-05 04:28:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:30:05 | INFO | train_inner | epoch 108:    167 / 196 loss=3.335, nll_loss=2.405, ppl=5.3, wps=20301.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.142, loss_scale=8, train_wall=299, gb_free=19.9, wall=68890
2022-03-05 04:31:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:31:42 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.485 | nll_loss 9.862 | ppl 930.42 | wps 38989 | wpb 510.9 | bsz 1 | num_updates 21029 | best_loss 7.44
2022-03-05 04:31:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21029 updates
2022-03-05 04:31:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:31:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:31:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 108 @ 21029 updates, score 10.485) (writing took 3.084716945886612 seconds)
2022-03-05 04:31:45 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-05 04:31:45 | INFO | train | epoch 108 | loss 3.316 | nll_loss 2.385 | ppl 5.22 | wps 20112 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21029 | lr 0.000218067 | gnorm 1.14 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 68991
2022-03-05 04:31:45 | INFO | fairseq.trainer | begin training epoch 109
2022-03-05 04:31:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:35:33 | INFO | train_inner | epoch 109:     71 / 196 loss=3.292, nll_loss=2.358, ppl=5.13, wps=19941.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.14, loss_scale=16, train_wall=296, gb_free=19.9, wall=69218
2022-03-05 04:37:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:40:56 | INFO | train_inner | epoch 109:    172 / 196 loss=3.332, nll_loss=2.402, ppl=5.28, wps=20278.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.133, loss_scale=8, train_wall=300, gb_free=19.9, wall=69541
2022-03-05 04:42:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:42:17 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.451 | nll_loss 9.823 | ppl 905.5 | wps 38700.6 | wpb 510.9 | bsz 1 | num_updates 21224 | best_loss 7.44
2022-03-05 04:42:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21224 updates
2022-03-05 04:42:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:42:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 109 @ 21224 updates, score 10.451) (writing took 3.054864611942321 seconds)
2022-03-05 04:42:20 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-05 04:42:20 | INFO | train | epoch 109 | loss 3.309 | nll_loss 2.377 | ppl 5.19 | wps 20095.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21224 | lr 0.000217063 | gnorm 1.136 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 69626
2022-03-05 04:42:20 | INFO | fairseq.trainer | begin training epoch 110
2022-03-05 04:42:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:44:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:46:27 | INFO | train_inner | epoch 110:     77 / 196 loss=3.28, nll_loss=2.346, ppl=5.08, wps=19752.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.149, loss_scale=8, train_wall=299, gb_free=19.9, wall=69872
2022-03-05 04:51:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:51:50 | INFO | train_inner | epoch 110:    178 / 196 loss=3.329, nll_loss=2.398, ppl=5.27, wps=20294, ups=0.31, wpb=65532.4, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.172, loss_scale=8, train_wall=299, gb_free=19.9, wall=70195
2022-03-05 04:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:52:52 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.512 | nll_loss 9.887 | ppl 947.05 | wps 38719.8 | wpb 510.9 | bsz 1 | num_updates 21418 | best_loss 7.44
2022-03-05 04:52:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21418 updates
2022-03-05 04:52:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:52:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:52:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 110 @ 21418 updates, score 10.512) (writing took 3.002690954133868 seconds)
2022-03-05 04:52:55 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-05 04:52:55 | INFO | train | epoch 110 | loss 3.301 | nll_loss 2.369 | ppl 5.16 | wps 20007.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21418 | lr 0.000216078 | gnorm 1.159 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 70260
2022-03-05 04:52:55 | INFO | fairseq.trainer | begin training epoch 111
2022-03-05 04:52:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:57:18 | INFO | train_inner | epoch 111:     82 / 196 loss=3.271, nll_loss=2.336, ppl=5.05, wps=19932.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.16, loss_scale=8, train_wall=296, gb_free=19.9, wall=70523
2022-03-05 05:00:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:02:41 | INFO | train_inner | epoch 111:    183 / 196 loss=3.319, nll_loss=2.387, ppl=5.23, wps=20273.7, ups=0.31, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.147, loss_scale=8, train_wall=300, gb_free=19.9, wall=70846
2022-03-05 05:03:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:03:27 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.491 | nll_loss 9.86 | ppl 929.58 | wps 38887 | wpb 510.9 | bsz 1 | num_updates 21613 | best_loss 7.44
2022-03-05 05:03:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21613 updates
2022-03-05 05:03:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:03:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:03:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 111 @ 21613 updates, score 10.491) (writing took 2.9784654858522117 seconds)
2022-03-05 05:03:30 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-05 05:03:30 | INFO | train | epoch 111 | loss 3.294 | nll_loss 2.361 | ppl 5.14 | wps 20093.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21613 | lr 0.000215101 | gnorm 1.155 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 70895
2022-03-05 05:03:30 | INFO | fairseq.trainer | begin training epoch 112
2022-03-05 05:03:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:08:09 | INFO | train_inner | epoch 112:     87 / 196 loss=3.26, nll_loss=2.324, ppl=5.01, wps=19941.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.146, loss_scale=16, train_wall=296, gb_free=19.9, wall=71174
2022-03-05 05:08:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:13:32 | INFO | train_inner | epoch 112:    188 / 196 loss=3.316, nll_loss=2.384, ppl=5.22, wps=20280, ups=0.31, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.148, loss_scale=8, train_wall=299, gb_free=19.9, wall=71497
2022-03-05 05:13:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:14:02 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.509 | nll_loss 9.887 | ppl 946.9 | wps 38653.2 | wpb 510.9 | bsz 1 | num_updates 21808 | best_loss 7.44
2022-03-05 05:14:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21808 updates
2022-03-05 05:14:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 112 @ 21808 updates, score 10.509) (writing took 2.9851971559692174 seconds)
2022-03-05 05:14:05 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-05 05:14:05 | INFO | train | epoch 112 | loss 3.286 | nll_loss 2.352 | ppl 5.11 | wps 20099.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21808 | lr 0.000214137 | gnorm 1.145 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 71530
2022-03-05 05:14:05 | INFO | fairseq.trainer | begin training epoch 113
2022-03-05 05:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:17:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:19:03 | INFO | train_inner | epoch 113:     93 / 196 loss=3.25, nll_loss=2.313, ppl=4.97, wps=19750.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.146, loss_scale=8, train_wall=299, gb_free=19.9, wall=71828
2022-03-05 05:24:23 | INFO | train_inner | epoch 113:    193 / 196 loss=3.314, nll_loss=2.382, ppl=5.21, wps=20486.2, ups=0.31, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.177, loss_scale=16, train_wall=296, gb_free=19.9, wall=72148
2022-03-05 05:24:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:37 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.539 | nll_loss 9.913 | ppl 964.4 | wps 38859.8 | wpb 510.9 | bsz 1 | num_updates 22003 | best_loss 7.44
2022-03-05 05:24:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22003 updates
2022-03-05 05:24:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:24:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:24:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 113 @ 22003 updates, score 10.539) (writing took 3.0001181419938803 seconds)
2022-03-05 05:24:40 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-05 05:24:40 | INFO | train | epoch 113 | loss 3.279 | nll_loss 2.344 | ppl 5.08 | wps 20102.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22003 | lr 0.000213186 | gnorm 1.161 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 72165
2022-03-05 05:24:40 | INFO | fairseq.trainer | begin training epoch 114
2022-03-05 05:24:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:24:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:29:53 | INFO | train_inner | epoch 114:     98 / 196 loss=3.234, nll_loss=2.295, ppl=4.91, wps=19759, ups=0.3, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.138, loss_scale=8, train_wall=299, gb_free=19.9, wall=72479
2022-03-05 05:34:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:35:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:35:12 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.555 | nll_loss 9.932 | ppl 976.57 | wps 38786.4 | wpb 510.9 | bsz 1 | num_updates 22197 | best_loss 7.44
2022-03-05 05:35:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22197 updates
2022-03-05 05:35:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:35:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:35:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 114 @ 22197 updates, score 10.555) (writing took 3.0213230189401656 seconds)
2022-03-05 05:35:15 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-05 05:35:15 | INFO | train | epoch 114 | loss 3.27 | nll_loss 2.334 | ppl 5.04 | wps 20003.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22197 | lr 0.000212253 | gnorm 1.141 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 72800
2022-03-05 05:35:15 | INFO | fairseq.trainer | begin training epoch 115
2022-03-05 05:35:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:35:24 | INFO | train_inner | epoch 115:      3 / 196 loss=3.304, nll_loss=2.372, ppl=5.18, wps=19746.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.147, loss_scale=8, train_wall=299, gb_free=19.9, wall=72810
2022-03-05 05:40:44 | INFO | train_inner | epoch 115:    103 / 196 loss=3.226, nll_loss=2.286, ppl=4.88, wps=20503, ups=0.31, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.163, loss_scale=8, train_wall=296, gb_free=19.9, wall=73129
2022-03-05 05:43:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:45:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:45:46 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.564 | nll_loss 9.938 | ppl 981.2 | wps 38936.7 | wpb 510.9 | bsz 1 | num_updates 22392 | best_loss 7.44
2022-03-05 05:45:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22392 updates
2022-03-05 05:45:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:45:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:45:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 115 @ 22392 updates, score 10.564) (writing took 3.0230101491324604 seconds)
2022-03-05 05:45:49 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-05 05:45:49 | INFO | train | epoch 115 | loss 3.264 | nll_loss 2.328 | ppl 5.02 | wps 20107.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22392 | lr 0.000211326 | gnorm 1.156 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 73435
2022-03-05 05:45:49 | INFO | fairseq.trainer | begin training epoch 116
2022-03-05 05:45:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:46:15 | INFO | train_inner | epoch 116:      8 / 196 loss=3.302, nll_loss=2.37, ppl=5.17, wps=19743, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.148, loss_scale=8, train_wall=299, gb_free=19.9, wall=73460
2022-03-05 05:51:35 | INFO | train_inner | epoch 116:    108 / 196 loss=3.227, nll_loss=2.288, ppl=4.88, wps=20484.3, ups=0.31, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.138, loss_scale=16, train_wall=296, gb_free=19.9, wall=73780
2022-03-05 05:51:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:56:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:56:21 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.607 | nll_loss 9.986 | ppl 1013.79 | wps 38745.4 | wpb 510.9 | bsz 1 | num_updates 22587 | best_loss 7.44
2022-03-05 05:56:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22587 updates
2022-03-05 05:56:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:56:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 116 @ 22587 updates, score 10.607) (writing took 2.985024811932817 seconds)
2022-03-05 05:56:24 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-05 05:56:24 | INFO | train | epoch 116 | loss 3.258 | nll_loss 2.321 | ppl 5 | wps 20101.2 | ups 0.31 | wpb 65449.4 | bsz 127.8 | num_updates 22587 | lr 0.000210412 | gnorm 1.155 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 74070
2022-03-05 05:56:24 | INFO | fairseq.trainer | begin training epoch 117
2022-03-05 05:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:57:06 | INFO | train_inner | epoch 117:     13 / 196 loss=3.277, nll_loss=2.343, ppl=5.07, wps=19749.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.172, loss_scale=8, train_wall=299, gb_free=19.9, wall=74111
2022-03-05 05:59:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:02:29 | INFO | train_inner | epoch 117:    114 / 196 loss=3.224, nll_loss=2.285, ppl=4.87, wps=20279.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.153, loss_scale=8, train_wall=299, gb_free=19.9, wall=74434
2022-03-05 06:06:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:06:56 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.609 | nll_loss 9.982 | ppl 1011.04 | wps 39047 | wpb 510.9 | bsz 1 | num_updates 22781 | best_loss 7.44
2022-03-05 06:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22781 updates
2022-03-05 06:06:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:06:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 117 @ 22781 updates, score 10.609) (writing took 3.003004295984283 seconds)
2022-03-05 06:06:59 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-05 06:06:59 | INFO | train | epoch 117 | loss 3.25 | nll_loss 2.312 | ppl 4.97 | wps 19993.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22781 | lr 0.000209514 | gnorm 1.154 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 74705
2022-03-05 06:06:59 | INFO | fairseq.trainer | begin training epoch 118
2022-03-05 06:06:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:08:00 | INFO | train_inner | epoch 118:     19 / 196 loss=3.272, nll_loss=2.337, ppl=5.05, wps=19738.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.145, loss_scale=8, train_wall=299, gb_free=19.9, wall=74766
2022-03-05 06:13:20 | INFO | train_inner | epoch 118:    119 / 196 loss=3.224, nll_loss=2.285, ppl=4.87, wps=20477.5, ups=0.31, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.152, loss_scale=8, train_wall=297, gb_free=19.9, wall=75086
2022-03-05 06:13:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:17:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:17:32 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.57 | nll_loss 9.951 | ppl 989.68 | wps 39050.6 | wpb 510.9 | bsz 1 | num_updates 22976 | best_loss 7.44
2022-03-05 06:17:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22976 updates
2022-03-05 06:17:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:17:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:17:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 118 @ 22976 updates, score 10.57) (writing took 2.9696012609638274 seconds)
2022-03-05 06:17:35 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-05 06:17:35 | INFO | train | epoch 118 | loss 3.244 | nll_loss 2.306 | ppl 4.95 | wps 20091.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22976 | lr 0.000208623 | gnorm 1.16 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 75340
2022-03-05 06:17:35 | INFO | fairseq.trainer | begin training epoch 119
2022-03-05 06:17:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:18:51 | INFO | train_inner | epoch 119:     24 / 196 loss=3.258, nll_loss=2.322, ppl=5, wps=19744.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.173, loss_scale=8, train_wall=299, gb_free=19.9, wall=75417
2022-03-05 06:21:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:24:15 | INFO | train_inner | epoch 119:    125 / 196 loss=3.22, nll_loss=2.28, ppl=4.86, wps=20271, ups=0.31, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.139, loss_scale=8, train_wall=299, gb_free=19.9, wall=75740
2022-03-05 06:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:28:07 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.607 | nll_loss 9.986 | ppl 1014.38 | wps 38716.2 | wpb 510.9 | bsz 1 | num_updates 23171 | best_loss 7.44
2022-03-05 06:28:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23171 updates
2022-03-05 06:28:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:28:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:28:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 119 @ 23171 updates, score 10.607) (writing took 2.973375293891877 seconds)
2022-03-05 06:28:10 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-05 06:28:10 | INFO | train | epoch 119 | loss 3.236 | nll_loss 2.298 | ppl 4.92 | wps 20090.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23171 | lr 0.000207744 | gnorm 1.15 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 75975
2022-03-05 06:28:10 | INFO | fairseq.trainer | begin training epoch 120
2022-03-05 06:28:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:29:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:29:46 | INFO | train_inner | epoch 120:     30 / 196 loss=3.252, nll_loss=2.315, ppl=4.98, wps=19743.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.161, loss_scale=8, train_wall=299, gb_free=19.9, wall=76071
2022-03-05 06:35:05 | INFO | train_inner | epoch 120:    130 / 196 loss=3.217, nll_loss=2.277, ppl=4.85, wps=20533.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.159, loss_scale=8, train_wall=296, gb_free=19.9, wall=76390
2022-03-05 06:37:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:38:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:38:40 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.617 | nll_loss 9.996 | ppl 1021.25 | wps 38954.7 | wpb 510.9 | bsz 1 | num_updates 23365 | best_loss 7.44
2022-03-05 06:38:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23365 updates
2022-03-05 06:38:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:38:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:38:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 120 @ 23365 updates, score 10.617) (writing took 2.9501033269334584 seconds)
2022-03-05 06:38:43 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-05 06:38:43 | INFO | train | epoch 120 | loss 3.229 | nll_loss 2.291 | ppl 4.89 | wps 20044.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23365 | lr 0.000206879 | gnorm 1.15 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 76608
2022-03-05 06:38:43 | INFO | fairseq.trainer | begin training epoch 121
2022-03-05 06:38:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:40:35 | INFO | train_inner | epoch 121:     35 / 196 loss=3.237, nll_loss=2.3, ppl=4.92, wps=19806.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.146, loss_scale=8, train_wall=298, gb_free=19.9, wall=76720
2022-03-05 06:43:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:45:57 | INFO | train_inner | epoch 121:    136 / 196 loss=3.209, nll_loss=2.269, ppl=4.82, wps=20337.3, ups=0.31, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.171, loss_scale=8, train_wall=299, gb_free=19.9, wall=77042
2022-03-05 06:49:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:49:14 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.593 | nll_loss 9.968 | ppl 1001.41 | wps 38849.3 | wpb 510.9 | bsz 1 | num_updates 23560 | best_loss 7.44
2022-03-05 06:49:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23560 updates
2022-03-05 06:49:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:49:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 121 @ 23560 updates, score 10.593) (writing took 2.985319894971326 seconds)
2022-03-05 06:49:16 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-05 06:49:16 | INFO | train | epoch 121 | loss 3.224 | nll_loss 2.285 | ppl 4.87 | wps 20154.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23560 | lr 0.000206021 | gnorm 1.173 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 77242
2022-03-05 06:49:17 | INFO | fairseq.trainer | begin training epoch 122
2022-03-05 06:49:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:51:24 | INFO | train_inner | epoch 122:     40 / 196 loss=3.227, nll_loss=2.288, ppl=4.88, wps=19989.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.162, loss_scale=16, train_wall=295, gb_free=19.9, wall=77369
2022-03-05 06:53:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:56:46 | INFO | train_inner | epoch 122:    141 / 196 loss=3.218, nll_loss=2.278, ppl=4.85, wps=20343.9, ups=0.31, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.158, loss_scale=8, train_wall=299, gb_free=19.9, wall=77692
2022-03-05 06:59:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:59:47 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.684 | nll_loss 10.068 | ppl 1073.58 | wps 38910.4 | wpb 510.9 | bsz 1 | num_updates 23755 | best_loss 7.44
2022-03-05 06:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23755 updates
2022-03-05 06:59:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:59:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:59:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 122 @ 23755 updates, score 10.684) (writing took 2.9687849688343704 seconds)
2022-03-05 06:59:50 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-05 06:59:50 | INFO | train | epoch 122 | loss 3.217 | nll_loss 2.278 | ppl 4.85 | wps 20153.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23755 | lr 0.000205174 | gnorm 1.158 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 77875
2022-03-05 06:59:50 | INFO | fairseq.trainer | begin training epoch 123
2022-03-05 06:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:01:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:02:16 | INFO | train_inner | epoch 123:     46 / 196 loss=3.217, nll_loss=2.277, ppl=4.85, wps=19801.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.178, loss_scale=8, train_wall=298, gb_free=19.9, wall=78022
2022-03-05 07:07:35 | INFO | train_inner | epoch 123:    146 / 196 loss=3.211, nll_loss=2.271, ppl=4.83, wps=20547.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.168, loss_scale=8, train_wall=296, gb_free=19.9, wall=78341
2022-03-05 07:08:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:10:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:10:20 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.651 | nll_loss 10.032 | ppl 1047.24 | wps 39126.6 | wpb 510.9 | bsz 1 | num_updates 23949 | best_loss 7.44
2022-03-05 07:10:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23949 updates
2022-03-05 07:10:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 123 @ 23949 updates, score 10.651) (writing took 2.966976910131052 seconds)
2022-03-05 07:10:23 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-05 07:10:23 | INFO | train | epoch 123 | loss 3.21 | nll_loss 2.27 | ppl 4.82 | wps 20057.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23949 | lr 0.000204341 | gnorm 1.173 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 78508
2022-03-05 07:10:23 | INFO | fairseq.trainer | begin training epoch 124
2022-03-05 07:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:13:05 | INFO | train_inner | epoch 124:     51 / 196 loss=3.2, nll_loss=2.259, ppl=4.79, wps=19805.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.178, loss_scale=8, train_wall=298, gb_free=19.9, wall=78671
2022-03-05 07:16:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:18:23 | INFO | train_inner | epoch 124:    152 / 196 loss=3.212, nll_loss=2.272, ppl=4.83, wps=20626.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.176, loss_scale=8, train_wall=295, gb_free=19.9, wall=78988
2022-03-05 07:20:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:20:46 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.688 | nll_loss 10.067 | ppl 1072.73 | wps 40412.6 | wpb 510.9 | bsz 1 | num_updates 24144 | best_loss 7.44
2022-03-05 07:20:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24144 updates
2022-03-05 07:20:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:20:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:20:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 124 @ 24144 updates, score 10.688) (writing took 2.9203770561143756 seconds)
2022-03-05 07:20:49 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-05 07:20:49 | INFO | train | epoch 124 | loss 3.205 | nll_loss 2.264 | ppl 4.8 | wps 20372.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24144 | lr 0.000203515 | gnorm 1.181 | loss_scale 8 | train_wall 574 | gb_free 19.9 | wall 79134
2022-03-05 07:20:49 | INFO | fairseq.trainer | begin training epoch 125
2022-03-05 07:20:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:23:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:23:48 | INFO | train_inner | epoch 125:     57 / 196 loss=3.194, nll_loss=2.252, ppl=4.76, wps=20095.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.179, loss_scale=8, train_wall=294, gb_free=19.9, wall=79314
2022-03-05 07:29:03 | INFO | train_inner | epoch 125:    157 / 196 loss=3.206, nll_loss=2.266, ppl=4.81, wps=20836.3, ups=0.32, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.164, loss_scale=8, train_wall=292, gb_free=19.9, wall=79628
2022-03-05 07:31:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:31:10 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.705 | nll_loss 10.085 | ppl 1086.45 | wps 40875.9 | wpb 510.9 | bsz 1 | num_updates 24339 | best_loss 7.44
2022-03-05 07:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24339 updates
2022-03-05 07:31:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:31:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:31:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 125 @ 24339 updates, score 10.705) (writing took 2.904616107000038 seconds)
2022-03-05 07:31:13 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-05 07:31:13 | INFO | train | epoch 125 | loss 3.198 | nll_loss 2.257 | ppl 4.78 | wps 20452.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24339 | lr 0.000202698 | gnorm 1.163 | loss_scale 16 | train_wall 572 | gb_free 19.9 | wall 79758
2022-03-05 07:31:13 | INFO | fairseq.trainer | begin training epoch 126
2022-03-05 07:31:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:34:28 | INFO | train_inner | epoch 126:     62 / 196 loss=3.183, nll_loss=2.241, ppl=4.73, wps=20090.9, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.158, loss_scale=8, train_wall=294, gb_free=19.9, wall=79953
2022-03-05 07:39:43 | INFO | train_inner | epoch 126:    162 / 196 loss=3.207, nll_loss=2.266, ppl=4.81, wps=20830.2, ups=0.32, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.177, loss_scale=8, train_wall=292, gb_free=19.9, wall=80268
2022-03-05 07:40:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:41:34 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.723 | nll_loss 10.108 | ppl 1103.63 | wps 40787.1 | wpb 510.9 | bsz 1 | num_updates 24533 | best_loss 7.44
2022-03-05 07:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24533 updates
2022-03-05 07:41:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:41:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 126 @ 24533 updates, score 10.723) (writing took 2.9217512749601156 seconds)
2022-03-05 07:41:37 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-05 07:41:37 | INFO | train | epoch 126 | loss 3.193 | nll_loss 2.252 | ppl 4.76 | wps 20343.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24533 | lr 0.000201895 | gnorm 1.173 | loss_scale 8 | train_wall 572 | gb_free 19.9 | wall 80382
2022-03-05 07:41:37 | INFO | fairseq.trainer | begin training epoch 127
2022-03-05 07:41:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:45:08 | INFO | train_inner | epoch 127:     67 / 196 loss=3.178, nll_loss=2.235, ppl=4.71, wps=20095.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.156, loss_scale=8, train_wall=294, gb_free=19.9, wall=80593
2022-03-05 07:47:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:50:26 | INFO | train_inner | epoch 127:    168 / 196 loss=3.204, nll_loss=2.263, ppl=4.8, wps=20635.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.189, loss_scale=8, train_wall=295, gb_free=19.9, wall=80911
2022-03-05 07:51:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:51:58 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.685 | nll_loss 10.067 | ppl 1072.38 | wps 40705.7 | wpb 510.9 | bsz 1 | num_updates 24728 | best_loss 7.44
2022-03-05 07:51:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24728 updates
2022-03-05 07:51:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 127 @ 24728 updates, score 10.685) (writing took 2.900148896034807 seconds)
2022-03-05 07:52:01 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-05 07:52:01 | INFO | train | epoch 127 | loss 3.187 | nll_loss 2.244 | ppl 4.74 | wps 20450.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24728 | lr 0.000201097 | gnorm 1.17 | loss_scale 8 | train_wall 572 | gb_free 19.9 | wall 81007
2022-03-05 07:52:01 | INFO | fairseq.trainer | begin training epoch 128
2022-03-05 07:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:54:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:55:52 | INFO | train_inner | epoch 128:     73 / 196 loss=3.161, nll_loss=2.217, ppl=4.65, wps=20048.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.173, loss_scale=8, train_wall=295, gb_free=19.9, wall=81237
2022-03-05 08:01:11 | INFO | train_inner | epoch 128:    173 / 196 loss=3.202, nll_loss=2.262, ppl=4.8, wps=20539.1, ups=0.31, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.178, loss_scale=8, train_wall=296, gb_free=19.9, wall=81556
2022-03-05 08:01:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:02:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:02:29 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.669 | nll_loss 10.051 | ppl 1060.61 | wps 39053.6 | wpb 510.9 | bsz 1 | num_updates 24922 | best_loss 7.44
2022-03-05 08:02:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24922 updates
2022-03-05 08:02:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:02:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:02:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 128 @ 24922 updates, score 10.669) (writing took 2.930496162036434 seconds)
2022-03-05 08:02:32 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-05 08:02:32 | INFO | train | epoch 128 | loss 3.18 | nll_loss 2.237 | ppl 4.72 | wps 20135.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24922 | lr 0.000200313 | gnorm 1.179 | loss_scale 8 | train_wall 577 | gb_free 19.9 | wall 81637
2022-03-05 08:02:32 | INFO | fairseq.trainer | begin training epoch 129
2022-03-05 08:02:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:06:41 | INFO | train_inner | epoch 129:     78 / 196 loss=3.152, nll_loss=2.207, ppl=4.62, wps=19806.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.182, loss_scale=8, train_wall=298, gb_free=19.9, wall=81886
2022-03-05 08:10:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:12:03 | INFO | train_inner | epoch 129:    179 / 196 loss=3.203, nll_loss=2.262, ppl=4.8, wps=20326.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.161, loss_scale=8, train_wall=299, gb_free=19.9, wall=82208
2022-03-05 08:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:13:02 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.757 | nll_loss 10.144 | ppl 1131.49 | wps 38839 | wpb 510.9 | bsz 1 | num_updates 25117 | best_loss 7.44
2022-03-05 08:13:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25117 updates
2022-03-05 08:13:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:13:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:13:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 129 @ 25117 updates, score 10.757) (writing took 2.9457122690510005 seconds)
2022-03-05 08:13:05 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-05 08:13:05 | INFO | train | epoch 129 | loss 3.176 | nll_loss 2.233 | ppl 4.7 | wps 20148.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25117 | lr 0.000199534 | gnorm 1.167 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 82271
2022-03-05 08:13:05 | INFO | fairseq.trainer | begin training epoch 130
2022-03-05 08:13:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:17:30 | INFO | train_inner | epoch 130:     83 / 196 loss=3.144, nll_loss=2.198, ppl=4.59, wps=19987.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.173, loss_scale=16, train_wall=295, gb_free=19.9, wall=82535
2022-03-05 08:20:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:22:53 | INFO | train_inner | epoch 130:    184 / 196 loss=3.2, nll_loss=2.259, ppl=4.79, wps=20315.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.168, loss_scale=8, train_wall=299, gb_free=19.9, wall=82858
2022-03-05 08:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:23:36 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.774 | nll_loss 10.162 | ppl 1145.47 | wps 38741.6 | wpb 510.9 | bsz 1 | num_updates 25312 | best_loss 7.44
2022-03-05 08:23:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25312 updates
2022-03-05 08:23:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:23:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:23:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 130 @ 25312 updates, score 10.774) (writing took 2.9910355671308935 seconds)
2022-03-05 08:23:39 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-05 08:23:39 | INFO | train | epoch 130 | loss 3.17 | nll_loss 2.226 | ppl 4.68 | wps 20140.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25312 | lr 0.000198764 | gnorm 1.173 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 82904
2022-03-05 08:23:39 | INFO | fairseq.trainer | begin training epoch 131
2022-03-05 08:23:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:27:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:28:23 | INFO | train_inner | epoch 131:     89 / 196 loss=3.134, nll_loss=2.187, ppl=4.55, wps=19783.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.182, loss_scale=8, train_wall=298, gb_free=19.9, wall=83188
2022-03-05 08:33:43 | INFO | train_inner | epoch 131:    189 / 196 loss=3.198, nll_loss=2.258, ppl=4.78, wps=20520.3, ups=0.31, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.196, loss_scale=8, train_wall=296, gb_free=19.9, wall=83508
2022-03-05 08:34:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:34:10 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.725 | nll_loss 10.109 | ppl 1104.6 | wps 39086.8 | wpb 510.9 | bsz 1 | num_updates 25507 | best_loss 7.44
2022-03-05 08:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25507 updates
2022-03-05 08:34:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:34:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:34:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 131 @ 25507 updates, score 10.725) (writing took 2.888304895022884 seconds)
2022-03-05 08:34:13 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-05 08:34:13 | INFO | train | epoch 131 | loss 3.163 | nll_loss 2.219 | ppl 4.66 | wps 20139 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25507 | lr 0.000198002 | gnorm 1.187 | loss_scale 8 | train_wall 579 | gb_free 19.9 | wall 83538
2022-03-05 08:34:13 | INFO | fairseq.trainer | begin training epoch 132
2022-03-05 08:34:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:39:13 | INFO | train_inner | epoch 132:     94 / 196 loss=3.126, nll_loss=2.179, ppl=4.53, wps=19799.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.173, loss_scale=8, train_wall=298, gb_free=19.9, wall=83838
2022-03-05 08:42:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:44:36 | INFO | train_inner | epoch 132:    195 / 196 loss=3.193, nll_loss=2.252, ppl=4.76, wps=20305.2, ups=0.31, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=1.18, loss_scale=8, train_wall=299, gb_free=19.9, wall=84161
2022-03-05 08:44:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:44:44 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.776 | nll_loss 10.158 | ppl 1142.44 | wps 38356.4 | wpb 510.9 | bsz 1 | num_updates 25701 | best_loss 7.44
2022-03-05 08:44:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25701 updates
2022-03-05 08:44:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:44:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:44:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 132 @ 25701 updates, score 10.776) (writing took 2.9633747490588576 seconds)
2022-03-05 08:44:47 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-05 08:44:47 | INFO | train | epoch 132 | loss 3.158 | nll_loss 2.214 | ppl 4.64 | wps 20029.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25701 | lr 0.000197254 | gnorm 1.176 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 84172
2022-03-05 08:44:47 | INFO | fairseq.trainer | begin training epoch 133
2022-03-05 08:44:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:50:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:50:07 | INFO | train_inner | epoch 133:    100 / 196 loss=3.116, nll_loss=2.168, ppl=4.49, wps=19713.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=25800, lr=0.000196875, gnorm=1.191, loss_scale=8, train_wall=299, gb_free=19.9, wall=84492
2022-03-05 08:55:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:55:20 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.779 | nll_loss 10.162 | ppl 1145.79 | wps 38664 | wpb 510.9 | bsz 1 | num_updates 25896 | best_loss 7.44
2022-03-05 08:55:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25896 updates
2022-03-05 08:55:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:55:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 133 @ 25896 updates, score 10.779) (writing took 3.001025668112561 seconds)
2022-03-05 08:55:23 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-05 08:55:23 | INFO | train | epoch 133 | loss 3.152 | nll_loss 2.207 | ppl 4.62 | wps 20067.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25896 | lr 0.00019651 | gnorm 1.194 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 84808
2022-03-05 08:55:23 | INFO | fairseq.trainer | begin training epoch 134
2022-03-05 08:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:55:36 | INFO | train_inner | epoch 134:      4 / 196 loss=3.185, nll_loss=2.243, ppl=4.73, wps=19907.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.196, loss_scale=8, train_wall=296, gb_free=19.9, wall=84821
2022-03-05 08:57:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:00:59 | INFO | train_inner | epoch 134:    105 / 196 loss=3.114, nll_loss=2.166, ppl=4.49, wps=20268.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.173, loss_scale=8, train_wall=300, gb_free=19.9, wall=85144
2022-03-05 09:04:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:05:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:05:55 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.761 | nll_loss 10.152 | ppl 1137.75 | wps 38686.9 | wpb 510.9 | bsz 1 | num_updates 26090 | best_loss 7.44
2022-03-05 09:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26090 updates
2022-03-05 09:05:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:05:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 134 @ 26090 updates, score 10.761) (writing took 2.9858279759064317 seconds)
2022-03-05 09:05:58 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-05 09:05:58 | INFO | train | epoch 134 | loss 3.146 | nll_loss 2.201 | ppl 4.6 | wps 19982.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26090 | lr 0.000195778 | gnorm 1.19 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 85443
2022-03-05 09:05:58 | INFO | fairseq.trainer | begin training epoch 135
2022-03-05 09:05:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:06:30 | INFO | train_inner | epoch 135:     10 / 196 loss=3.173, nll_loss=2.23, ppl=4.69, wps=19734.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.209, loss_scale=8, train_wall=299, gb_free=19.9, wall=85475
2022-03-05 09:11:50 | INFO | train_inner | epoch 135:    110 / 196 loss=3.112, nll_loss=2.164, ppl=4.48, wps=20476.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.182, loss_scale=8, train_wall=297, gb_free=19.9, wall=85795
2022-03-05 09:12:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:16:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:16:30 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.761 | nll_loss 10.145 | ppl 1132.19 | wps 38492.3 | wpb 510.9 | bsz 1 | num_updates 26285 | best_loss 7.44
2022-03-05 09:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26285 updates
2022-03-05 09:16:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:16:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:16:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 135 @ 26285 updates, score 10.761) (writing took 2.9702816689386964 seconds)
2022-03-05 09:16:33 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-05 09:16:33 | INFO | train | epoch 135 | loss 3.142 | nll_loss 2.196 | ppl 4.58 | wps 20088.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26285 | lr 0.00019505 | gnorm 1.195 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 86078
2022-03-05 09:16:33 | INFO | fairseq.trainer | begin training epoch 136
2022-03-05 09:16:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:17:21 | INFO | train_inner | epoch 136:     15 / 196 loss=3.168, nll_loss=2.224, ppl=4.67, wps=19727.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.197, loss_scale=8, train_wall=299, gb_free=19.9, wall=86127
2022-03-05 09:21:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:22:45 | INFO | train_inner | epoch 136:    116 / 196 loss=3.113, nll_loss=2.164, ppl=4.48, wps=20263.4, ups=0.31, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.198, loss_scale=8, train_wall=300, gb_free=19.9, wall=86450
2022-03-05 09:27:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:09 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.825 | nll_loss 10.209 | ppl 1183.7 | wps 38771.1 | wpb 510.9 | bsz 1 | num_updates 26480 | best_loss 7.44
2022-03-05 09:27:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26480 updates
2022-03-05 09:27:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:27:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:27:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 136 @ 26480 updates, score 10.825) (writing took 4.2292619871441275 seconds)
2022-03-05 09:27:14 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-05 09:27:14 | INFO | train | epoch 136 | loss 3.136 | nll_loss 2.19 | ppl 4.56 | wps 19935 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 26480 | lr 0.000194331 | gnorm 1.197 | loss_scale 8 | train_wall 581 | gb_free 19.9 | wall 86719
2022-03-05 09:27:14 | INFO | fairseq.trainer | begin training epoch 137
2022-03-05 09:27:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:28:18 | INFO | train_inner | epoch 137:     20 / 196 loss=3.157, nll_loss=2.213, ppl=4.64, wps=19635, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.196, loss_scale=16, train_wall=296, gb_free=19.9, wall=86783
2022-03-05 09:29:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:33:41 | INFO | train_inner | epoch 137:    121 / 196 loss=3.114, nll_loss=2.166, ppl=4.49, wps=20286.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.191, loss_scale=8, train_wall=299, gb_free=19.9, wall=87106
2022-03-05 09:36:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:37:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:37:46 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.791 | nll_loss 10.177 | ppl 1157.54 | wps 38808.6 | wpb 510.9 | bsz 1 | num_updates 26674 | best_loss 7.44
2022-03-05 09:37:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26674 updates
2022-03-05 09:37:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:37:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:37:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 137 @ 26674 updates, score 10.791) (writing took 3.782967567909509 seconds)
2022-03-05 09:37:49 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-05 09:37:49 | INFO | train | epoch 137 | loss 3.131 | nll_loss 2.184 | ppl 4.55 | wps 19967.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26674 | lr 0.000193623 | gnorm 1.19 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 87355
2022-03-05 09:37:49 | INFO | fairseq.trainer | begin training epoch 138
2022-03-05 09:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:39:13 | INFO | train_inner | epoch 138:     26 / 196 loss=3.141, nll_loss=2.196, ppl=4.58, wps=19705.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.198, loss_scale=8, train_wall=299, gb_free=19.9, wall=87438
2022-03-05 09:44:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:44:36 | INFO | train_inner | epoch 138:    127 / 196 loss=3.102, nll_loss=2.153, ppl=4.45, wps=20290, ups=0.31, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.164, loss_scale=8, train_wall=299, gb_free=19.9, wall=87761
2022-03-05 09:48:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:48:21 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.873 | nll_loss 10.26 | ppl 1225.95 | wps 38989 | wpb 510.9 | bsz 1 | num_updates 26869 | best_loss 7.44
2022-03-05 09:48:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26869 updates
2022-03-05 09:48:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:48:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 138 @ 26869 updates, score 10.873) (writing took 3.570951190078631 seconds)
2022-03-05 09:48:25 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-05 09:48:25 | INFO | train | epoch 138 | loss 3.126 | nll_loss 2.179 | ppl 4.53 | wps 20086 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26869 | lr 0.000192919 | gnorm 1.177 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 87990
2022-03-05 09:48:25 | INFO | fairseq.trainer | begin training epoch 139
2022-03-05 09:48:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:50:04 | INFO | train_inner | epoch 139:     31 / 196 loss=3.149, nll_loss=2.204, ppl=4.61, wps=19900, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.181, loss_scale=8, train_wall=296, gb_free=19.9, wall=88089
2022-03-05 09:51:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:55:27 | INFO | train_inner | epoch 139:    132 / 196 loss=3.109, nll_loss=2.16, ppl=4.47, wps=20289, ups=0.31, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.192, loss_scale=8, train_wall=299, gb_free=19.9, wall=88412
2022-03-05 09:58:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:58:57 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.817 | nll_loss 10.207 | ppl 1181.77 | wps 38817.4 | wpb 510.9 | bsz 1 | num_updates 27064 | best_loss 7.44
2022-03-05 09:58:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27064 updates
2022-03-05 09:58:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:59:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:59:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 139 @ 27064 updates, score 10.817) (writing took 3.4332048550713807 seconds)
2022-03-05 09:59:00 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-05 09:59:00 | INFO | train | epoch 139 | loss 3.121 | nll_loss 2.174 | ppl 4.51 | wps 20087.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27064 | lr 0.000192222 | gnorm 1.188 | loss_scale 16 | train_wall 580 | gb_free 19.9 | wall 88625
2022-03-05 09:59:00 | INFO | fairseq.trainer | begin training epoch 140
2022-03-05 09:59:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:59:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:00:59 | INFO | train_inner | epoch 140:     37 / 196 loss=3.129, nll_loss=2.182, ppl=4.54, wps=19714.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.191, loss_scale=8, train_wall=299, gb_free=19.9, wall=88744
2022-03-05 10:06:19 | INFO | train_inner | epoch 140:    137 / 196 loss=3.114, nll_loss=2.166, ppl=4.49, wps=20481.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.186, loss_scale=16, train_wall=296, gb_free=19.9, wall=89064
2022-03-05 10:06:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:09:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:09:32 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.864 | nll_loss 10.253 | ppl 1220.48 | wps 38737.1 | wpb 510.9 | bsz 1 | num_updates 27258 | best_loss 7.44
2022-03-05 10:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27258 updates
2022-03-05 10:09:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:09:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 140 @ 27258 updates, score 10.864) (writing took 3.2119752541184425 seconds)
2022-03-05 10:09:35 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-05 10:09:35 | INFO | train | epoch 140 | loss 3.117 | nll_loss 2.169 | ppl 4.5 | wps 19987.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27258 | lr 0.000191537 | gnorm 1.193 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 89260
2022-03-05 10:09:35 | INFO | fairseq.trainer | begin training epoch 141
2022-03-05 10:09:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:11:50 | INFO | train_inner | epoch 141:     42 / 196 loss=3.116, nll_loss=2.169, ppl=4.5, wps=19730.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.211, loss_scale=8, train_wall=299, gb_free=19.9, wall=89395
2022-03-05 10:14:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:17:13 | INFO | train_inner | epoch 141:    143 / 196 loss=3.108, nll_loss=2.16, ppl=4.47, wps=20290, ups=0.31, wpb=65532.4, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.198, loss_scale=8, train_wall=299, gb_free=19.9, wall=89718
2022-03-05 10:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:20:07 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.864 | nll_loss 10.251 | ppl 1218.52 | wps 38869.1 | wpb 510.9 | bsz 1 | num_updates 27453 | best_loss 7.44
2022-03-05 10:20:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27453 updates
2022-03-05 10:20:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:20:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:20:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 141 @ 27453 updates, score 10.864) (writing took 4.013265251880512 seconds)
2022-03-05 10:20:11 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-05 10:20:11 | INFO | train | epoch 141 | loss 3.112 | nll_loss 2.164 | ppl 4.48 | wps 20071.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27453 | lr 0.000190856 | gnorm 1.197 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 89896
2022-03-05 10:20:11 | INFO | fairseq.trainer | begin training epoch 142
2022-03-05 10:20:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:21:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:22:45 | INFO | train_inner | epoch 142:     48 / 196 loss=3.103, nll_loss=2.154, ppl=4.45, wps=19692.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.208, loss_scale=8, train_wall=299, gb_free=19.9, wall=90050
2022-03-05 10:28:05 | INFO | train_inner | epoch 142:    148 / 196 loss=3.115, nll_loss=2.167, ppl=4.49, wps=20494.9, ups=0.31, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.208, loss_scale=8, train_wall=296, gb_free=19.9, wall=90370
2022-03-05 10:28:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:30:43 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.867 | nll_loss 10.257 | ppl 1223.26 | wps 38805.2 | wpb 510.9 | bsz 1 | num_updates 27647 | best_loss 7.44
2022-03-05 10:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27647 updates
2022-03-05 10:30:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 142 @ 27647 updates, score 10.867) (writing took 3.886032081907615 seconds)
2022-03-05 10:30:47 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-05 10:30:47 | INFO | train | epoch 142 | loss 3.105 | nll_loss 2.157 | ppl 4.46 | wps 19973.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27647 | lr 0.000190185 | gnorm 1.217 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 90532
2022-03-05 10:30:47 | INFO | fairseq.trainer | begin training epoch 143
2022-03-05 10:30:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:33:36 | INFO | train_inner | epoch 143:     53 / 196 loss=3.099, nll_loss=2.151, ppl=4.44, wps=19702.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.193, loss_scale=8, train_wall=299, gb_free=19.9, wall=90701
2022-03-05 10:35:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:38:59 | INFO | train_inner | epoch 143:    154 / 196 loss=3.104, nll_loss=2.155, ppl=4.46, wps=20289.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.189, loss_scale=8, train_wall=299, gb_free=19.9, wall=91024
2022-03-05 10:41:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:41:18 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.903 | nll_loss 10.299 | ppl 1259.57 | wps 38768.2 | wpb 510.9 | bsz 1 | num_updates 27842 | best_loss 7.44
2022-03-05 10:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27842 updates
2022-03-05 10:41:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 143 @ 27842 updates, score 10.903) (writing took 4.208371573826298 seconds)
2022-03-05 10:41:23 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-05 10:41:23 | INFO | train | epoch 143 | loss 3.102 | nll_loss 2.154 | ppl 4.45 | wps 20070.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27842 | lr 0.000189518 | gnorm 1.194 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 91168
2022-03-05 10:41:23 | INFO | fairseq.trainer | begin training epoch 144
2022-03-05 10:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:42:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:44:32 | INFO | train_inner | epoch 144:     59 / 196 loss=3.092, nll_loss=2.142, ppl=4.41, wps=19670.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.201, loss_scale=8, train_wall=299, gb_free=19.9, wall=91357
2022-03-05 10:49:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:49:55 | INFO | train_inner | epoch 144:    160 / 196 loss=3.111, nll_loss=2.163, ppl=4.48, wps=20290.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.204, loss_scale=8, train_wall=299, gb_free=19.9, wall=91680
2022-03-05 10:51:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:51:55 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.881 | nll_loss 10.269 | ppl 1233.78 | wps 38827.4 | wpb 510.9 | bsz 1 | num_updates 28036 | best_loss 7.44
2022-03-05 10:51:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28036 updates
2022-03-05 10:51:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:51:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 144 @ 28036 updates, score 10.881) (writing took 4.065429899143055 seconds)
2022-03-05 10:51:59 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-05 10:51:59 | INFO | train | epoch 144 | loss 3.097 | nll_loss 2.148 | ppl 4.43 | wps 19966.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28036 | lr 0.000188861 | gnorm 1.194 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 91804
2022-03-05 10:51:59 | INFO | fairseq.trainer | begin training epoch 145
2022-03-05 10:51:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:55:23 | INFO | train_inner | epoch 145:     64 / 196 loss=3.081, nll_loss=2.131, ppl=4.38, wps=19876.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.206, loss_scale=8, train_wall=296, gb_free=19.9, wall=92009
2022-03-05 10:56:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:00:46 | INFO | train_inner | epoch 145:    165 / 196 loss=3.107, nll_loss=2.159, ppl=4.46, wps=20287.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.218, loss_scale=8, train_wall=299, gb_free=19.9, wall=92332
2022-03-05 11:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:02:30 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.927 | nll_loss 10.316 | ppl 1274.66 | wps 38975.4 | wpb 510.9 | bsz 1 | num_updates 28231 | best_loss 7.44
2022-03-05 11:02:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28231 updates
2022-03-05 11:02:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 145 @ 28231 updates, score 10.927) (writing took 3.937665026867762 seconds)
2022-03-05 11:02:34 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-05 11:02:34 | INFO | train | epoch 145 | loss 3.093 | nll_loss 2.143 | ppl 4.42 | wps 20075.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28231 | lr 0.000188207 | gnorm 1.212 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 92439
2022-03-05 11:02:34 | INFO | fairseq.trainer | begin training epoch 146
2022-03-05 11:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:04:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:06:18 | INFO | train_inner | epoch 146:     70 / 196 loss=3.071, nll_loss=2.12, ppl=4.35, wps=19698.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.185, loss_scale=8, train_wall=299, gb_free=19.9, wall=92663
2022-03-05 11:11:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:11:41 | INFO | train_inner | epoch 146:    171 / 196 loss=3.109, nll_loss=2.161, ppl=4.47, wps=20297.7, ups=0.31, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.207, loss_scale=8, train_wall=299, gb_free=19.9, wall=92986
2022-03-05 11:13:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:13:06 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.942 | nll_loss 10.331 | ppl 1287.72 | wps 38686.1 | wpb 510.9 | bsz 1 | num_updates 28425 | best_loss 7.44
2022-03-05 11:13:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28425 updates
2022-03-05 11:13:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:13:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:13:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 146 @ 28425 updates, score 10.942) (writing took 3.8863127098884434 seconds)
2022-03-05 11:13:10 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-05 11:13:10 | INFO | train | epoch 146 | loss 3.088 | nll_loss 2.138 | ppl 4.4 | wps 19980 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28425 | lr 0.000187564 | gnorm 1.197 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 93075
2022-03-05 11:13:10 | INFO | fairseq.trainer | begin training epoch 147
2022-03-05 11:13:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:17:10 | INFO | train_inner | epoch 147:     75 / 196 loss=3.067, nll_loss=2.115, ppl=4.33, wps=19893.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.19, loss_scale=8, train_wall=296, gb_free=19.9, wall=93315
2022-03-05 11:18:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:22:33 | INFO | train_inner | epoch 147:    176 / 196 loss=3.103, nll_loss=2.154, ppl=4.45, wps=20268.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.222, loss_scale=8, train_wall=300, gb_free=19.9, wall=93638
2022-03-05 11:23:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:23:42 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.851 | nll_loss 10.238 | ppl 1207.76 | wps 38677.4 | wpb 510.9 | bsz 1 | num_updates 28620 | best_loss 7.44
2022-03-05 11:23:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28620 updates
2022-03-05 11:23:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:23:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:23:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 147 @ 28620 updates, score 10.851) (writing took 3.818591275019571 seconds)
2022-03-05 11:23:46 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-05 11:23:46 | INFO | train | epoch 147 | loss 3.084 | nll_loss 2.134 | ppl 4.39 | wps 20068.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28620 | lr 0.000186924 | gnorm 1.21 | loss_scale 8 | train_wall 580 | gb_free 19.9 | wall 93711
2022-03-05 11:23:46 | INFO | fairseq.trainer | begin training epoch 148
2022-03-05 11:23:46 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4222, in multi_head_attention_forward
    q = q * scaling
KeyboardInterrupt
