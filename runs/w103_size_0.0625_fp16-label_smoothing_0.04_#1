Sender: LSF System <lsfadmin@eu-g3-067>
Subject: Job 207133291: <w103_size_0.0625_fp16_label_smoothing_0.04_#1> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_label_smoothing_0.04_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:38:31 2022
Job was executed on host(s) <eu-g3-067>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 11:47:47 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 11:47:47 2022
Terminated at Sat Mar  5 11:25:21 2022
Results reported at Sat Mar  5 11:25:21 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.04 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   84984.94 sec.
    Max Memory :                                 8124 MB
    Average Memory :                             4116.19 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11876.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   85054 sec.
    Turnaround time :                            92810 sec.

The output (if any) follows:

2022-03-04 11:47:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.04, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 11:47:54 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 11:47:56 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 11:47:56 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 11:47:56 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 11:47:56 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 11:47:56 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 11:47:56 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 11:47:56 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 11:47:58 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 11:47:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 11:47:58 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-04 11:47:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 11:47:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 11:47:58 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 11:47:58 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 11:47:58 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 11:47:58 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 11:47:58 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 11:47:58 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 11:47:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:48:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 11:48:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:48:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:48:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:48:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:52:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:52:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.611 | nll_loss 14.476 | ppl 22787.3 | wps 42897.6 | wpb 510.9 | bsz 1 | num_updates 92
2022-03-04 11:52:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 92 updates
2022-03-04 11:52:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:52:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 1 @ 92 updates, score 14.611) (writing took 4.794106984511018 seconds)
2022-03-04 11:52:38 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 11:52:38 | INFO | train | epoch 001 | loss 16.011 | nll_loss 15.934 | ppl 62616.7 | wps 23849.6 | ups 0.36 | wpb 65489.2 | bsz 127.9 | num_updates 92 | lr 1.15977e-05 | gnorm 3.392 | loss_scale 4 | train_wall 246 | gb_free 21 | wall 279
2022-03-04 11:52:38 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 11:52:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:52:59 | INFO | train_inner | epoch 002:      8 / 97 loss=15.9, nll_loss=15.819, ppl=57807.8, wps=23932.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.255, loss_scale=4, train_wall=265, gb_free=21, wall=300
2022-03-04 11:56:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:56:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.021 | nll_loss 12.818 | ppl 7222.91 | wps 42638.2 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 13.021
2022-03-04 11:56:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 11:56:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:56:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 11:57:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 2 @ 189 updates, score 13.021) (writing took 5.441876367665827 seconds)
2022-03-04 11:57:01 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 11:57:01 | INFO | train | epoch 002 | loss 13.951 | nll_loss 13.79 | ppl 14166.1 | wps 24068.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.541 | loss_scale 8 | train_wall 230 | gb_free 21 | wall 543
2022-03-04 11:57:01 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 11:57:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:57:30 | INFO | train_inner | epoch 003:     11 / 97 loss=13.79, nll_loss=13.622, ppl=12610.4, wps=24103.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.502, loss_scale=8, train_wall=237, gb_free=21, wall=572
2022-03-04 12:01:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:01:20 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.4 | nll_loss 11.11 | ppl 2210.44 | wps 42697.9 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.4
2022-03-04 12:01:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 12:01:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:01:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.4) (writing took 5.241045312024653 seconds)
2022-03-04 12:01:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 12:01:25 | INFO | train | epoch 003 | loss 12.196 | nll_loss 11.953 | ppl 3964.11 | wps 24079.6 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 1.083 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 807
2022-03-04 12:01:25 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 12:01:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:02:02 | INFO | train_inner | epoch 004:     14 / 97 loss=11.986, nll_loss=11.731, ppl=3400.1, wps=24105.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=1.012, loss_scale=16, train_wall=237, gb_free=21, wall=844
2022-03-04 12:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:05:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.636 | nll_loss 10.276 | ppl 1239.73 | wps 42240.9 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 10.636
2022-03-04 12:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 12:05:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:05:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:05:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 4 @ 383 updates, score 10.636) (writing took 5.081195924431086 seconds)
2022-03-04 12:05:49 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 12:05:49 | INFO | train | epoch 004 | loss 10.939 | nll_loss 10.612 | ppl 1565.56 | wps 24075.4 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.574 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 1071
2022-03-04 12:05:49 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 12:05:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:06:34 | INFO | train_inner | epoch 005:     17 / 97 loss=10.823, nll_loss=10.486, ppl=1434.6, wps=24106.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.533, loss_scale=32, train_wall=237, gb_free=21, wall=1115
2022-03-04 12:10:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:10:08 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.28 | nll_loss 9.881 | ppl 943.21 | wps 42493.1 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 10.28
2022-03-04 12:10:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 12:10:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:10:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 5 @ 480 updates, score 10.28) (writing took 5.332800664007664 seconds)
2022-03-04 12:10:13 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 12:10:13 | INFO | train | epoch 005 | loss 10.424 | nll_loss 10.043 | ppl 1055.17 | wps 24076.6 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.479 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 1335
2022-03-04 12:10:13 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 12:10:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:11:05 | INFO | train_inner | epoch 006:     20 / 97 loss=10.358, nll_loss=9.971, ppl=1003.66, wps=24104, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.49, loss_scale=32, train_wall=237, gb_free=21, wall=1387
2022-03-04 12:12:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:14:32 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.984 | nll_loss 9.564 | ppl 756.79 | wps 42646.6 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 9.984
2022-03-04 12:14:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 12:14:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:14:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 6 @ 576 updates, score 9.984) (writing took 5.1087650610134006 seconds)
2022-03-04 12:14:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 12:14:37 | INFO | train | epoch 006 | loss 10.104 | nll_loss 9.694 | ppl 828.53 | wps 23834.7 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.545 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 1599
2022-03-04 12:14:37 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 12:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:15:40 | INFO | train_inner | epoch 007:     24 / 97 loss=10.033, nll_loss=9.619, ppl=786.28, wps=23883.3, ups=0.36, wpb=65495, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.555, loss_scale=32, train_wall=240, gb_free=21, wall=1661
2022-03-04 12:18:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:18:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:18:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.738 | nll_loss 9.298 | ppl 629.35 | wps 42787 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 9.738
2022-03-04 12:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 12:18:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:19:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 7 @ 672 updates, score 9.738) (writing took 5.211248335428536 seconds)
2022-03-04 12:19:01 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 12:19:01 | INFO | train | epoch 007 | loss 9.825 | nll_loss 9.397 | ppl 674.03 | wps 23820.6 | ups 0.36 | wpb 65493.3 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.621 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 1863
2022-03-04 12:19:01 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 12:19:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:20:14 | INFO | train_inner | epoch 008:     28 / 97 loss=9.757, nll_loss=9.324, ppl=640.86, wps=23870.6, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.655, loss_scale=32, train_wall=240, gb_free=21, wall=1936
2022-03-04 12:21:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:23:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:23:19 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.523 | nll_loss 9.069 | ppl 537.09 | wps 42765.6 | wpb 510.9 | bsz 1 | num_updates 768 | best_loss 9.523
2022-03-04 12:23:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 768 updates
2022-03-04 12:23:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:23:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 8 @ 768 updates, score 9.523) (writing took 5.270239422097802 seconds)
2022-03-04 12:23:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 12:23:25 | INFO | train | epoch 008 | loss 9.576 | nll_loss 9.131 | ppl 560.78 | wps 23826.4 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 768 | lr 9.60808e-05 | gnorm 0.739 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 2126
2022-03-04 12:23:25 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 12:23:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:24:48 | INFO | train_inner | epoch 009:     32 / 97 loss=9.503, nll_loss=9.053, ppl=531.27, wps=23875.9, ups=0.36, wpb=65495, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.756, loss_scale=16, train_wall=240, gb_free=21, wall=2210
2022-03-04 12:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:27:43 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.316 | nll_loss 8.848 | ppl 460.71 | wps 42477.2 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 9.316
2022-03-04 12:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 12:27:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 9 @ 865 updates, score 9.316) (writing took 5.077990853227675 seconds)
2022-03-04 12:27:49 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 12:27:49 | INFO | train | epoch 009 | loss 9.343 | nll_loss 8.883 | ppl 472.11 | wps 24072.3 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.797 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 2390
2022-03-04 12:27:49 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 12:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:29:20 | INFO | train_inner | epoch 010:     35 / 97 loss=9.263, nll_loss=8.797, ppl=444.94, wps=24090.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.833, loss_scale=32, train_wall=238, gb_free=21, wall=2482
2022-03-04 12:32:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:32:07 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.135 | nll_loss 8.656 | ppl 403.36 | wps 42871.8 | wpb 510.9 | bsz 1 | num_updates 962 | best_loss 9.135
2022-03-04 12:32:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 962 updates
2022-03-04 12:32:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:32:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:32:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 10 @ 962 updates, score 9.135) (writing took 5.195258755236864 seconds)
2022-03-04 12:32:12 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 12:32:12 | INFO | train | epoch 010 | loss 9.124 | nll_loss 8.649 | ppl 401.47 | wps 24065.4 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 962 | lr 0.000120326 | gnorm 0.875 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 2654
2022-03-04 12:32:13 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 12:32:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:33:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:33:55 | INFO | train_inner | epoch 011:     39 / 97 loss=9.042, nll_loss=8.562, ppl=377.87, wps=23870.2, ups=0.36, wpb=65490.8, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.868, loss_scale=32, train_wall=240, gb_free=21, wall=2756
2022-03-04 12:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:36:31 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.974 | nll_loss 8.479 | ppl 356.78 | wps 42445.9 | wpb 510.9 | bsz 1 | num_updates 1058 | best_loss 8.974
2022-03-04 12:36:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1058 updates
2022-03-04 12:36:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:36:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:36:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 11 @ 1058 updates, score 8.974) (writing took 5.192044992931187 seconds)
2022-03-04 12:36:36 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 12:36:36 | INFO | train | epoch 011 | loss 8.926 | nll_loss 8.438 | ppl 346.73 | wps 23817.1 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1058 | lr 0.000132324 | gnorm 0.864 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 2918
2022-03-04 12:36:36 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 12:36:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:38:26 | INFO | train_inner | epoch 012:     42 / 97 loss=8.85, nll_loss=8.357, ppl=327.84, wps=24091.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.891, loss_scale=32, train_wall=237, gb_free=21, wall=3028
2022-03-04 12:40:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:40:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:40:55 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.838 | nll_loss 8.336 | ppl 323.05 | wps 42365.2 | wpb 510.9 | bsz 1 | num_updates 1154 | best_loss 8.838
2022-03-04 12:40:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1154 updates
2022-03-04 12:40:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:40:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 12 @ 1154 updates, score 8.838) (writing took 5.15376818459481 seconds)
2022-03-04 12:41:00 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 12:41:00 | INFO | train | epoch 012 | loss 8.747 | nll_loss 8.247 | ppl 303.73 | wps 23826.5 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1154 | lr 0.000144321 | gnorm 0.915 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 3182
2022-03-04 12:41:00 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 12:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:43:01 | INFO | train_inner | epoch 013:     46 / 97 loss=8.666, nll_loss=8.161, ppl=286.15, wps=23862.2, ups=0.36, wpb=65490.8, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.899, loss_scale=32, train_wall=240, gb_free=21, wall=3303
2022-03-04 12:45:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:45:19 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.714 | nll_loss 8.205 | ppl 294.99 | wps 42772.5 | wpb 510.9 | bsz 1 | num_updates 1251 | best_loss 8.714
2022-03-04 12:45:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1251 updates
2022-03-04 12:45:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:45:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 13 @ 1251 updates, score 8.714) (writing took 5.1691535376012325 seconds)
2022-03-04 12:45:24 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 12:45:24 | INFO | train | epoch 013 | loss 8.577 | nll_loss 8.065 | ppl 267.88 | wps 24059.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1251 | lr 0.000156444 | gnorm 0.894 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 3446
2022-03-04 12:45:24 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 12:45:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:46:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:47:35 | INFO | train_inner | epoch 014:     50 / 97 loss=8.495, nll_loss=7.979, ppl=252.27, wps=23878.5, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.935, loss_scale=32, train_wall=240, gb_free=21, wall=3577
2022-03-04 12:49:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:49:43 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.606 | nll_loss 8.088 | ppl 272.12 | wps 42293 | wpb 510.9 | bsz 1 | num_updates 1347 | best_loss 8.606
2022-03-04 12:49:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1347 updates
2022-03-04 12:49:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:49:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:49:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 14 @ 1347 updates, score 8.606) (writing took 5.203971070237458 seconds)
2022-03-04 12:49:48 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 12:49:48 | INFO | train | epoch 014 | loss 8.418 | nll_loss 7.896 | ppl 238.22 | wps 23814.9 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1347 | lr 0.000168441 | gnorm 0.946 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 3710
2022-03-04 12:49:48 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 12:49:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:52:07 | INFO | train_inner | epoch 015:     53 / 97 loss=8.34, nll_loss=7.813, ppl=224.83, wps=24073.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=0.956, loss_scale=32, train_wall=238, gb_free=21, wall=3849
2022-03-04 12:52:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:54:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:54:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.502 | nll_loss 7.972 | ppl 251.14 | wps 41960.9 | wpb 510.9 | bsz 1 | num_updates 1443 | best_loss 8.502
2022-03-04 12:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1443 updates
2022-03-04 12:54:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:54:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:54:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 15 @ 1443 updates, score 8.502) (writing took 5.1971250073984265 seconds)
2022-03-04 12:54:12 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 12:54:12 | INFO | train | epoch 015 | loss 8.262 | nll_loss 7.731 | ppl 212.38 | wps 23818 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1443 | lr 0.000180439 | gnorm 0.942 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 3974
2022-03-04 12:54:12 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 12:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:56:42 | INFO | train_inner | epoch 016:     57 / 97 loss=8.169, nll_loss=7.631, ppl=198.18, wps=23861.7, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=0.951, loss_scale=32, train_wall=240, gb_free=21, wall=4123
2022-03-04 12:58:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:58:31 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.401 | nll_loss 7.858 | ppl 232.08 | wps 41624.6 | wpb 510.9 | bsz 1 | num_updates 1540 | best_loss 8.401
2022-03-04 12:58:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1540 updates
2022-03-04 12:58:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:58:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 12:58:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 16 @ 1540 updates, score 8.401) (writing took 5.522179774940014 seconds)
2022-03-04 12:58:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 12:58:37 | INFO | train | epoch 016 | loss 8.112 | nll_loss 7.57 | ppl 190.07 | wps 24015.7 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1540 | lr 0.000192562 | gnorm 0.973 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 4239
2022-03-04 12:58:37 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 12:58:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:59:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:01:16 | INFO | train_inner | epoch 017:     61 / 97 loss=8.03, nll_loss=7.483, ppl=178.87, wps=23829.7, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=0.966, loss_scale=32, train_wall=240, gb_free=21, wall=4398
2022-03-04 13:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:02:56 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.309 | nll_loss 7.764 | ppl 217.42 | wps 41779.1 | wpb 510.9 | bsz 1 | num_updates 1636 | best_loss 8.309
2022-03-04 13:02:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1636 updates
2022-03-04 13:02:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:02:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:03:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 17 @ 1636 updates, score 8.309) (writing took 5.280003545805812 seconds)
2022-03-04 13:03:01 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 13:03:01 | INFO | train | epoch 017 | loss 7.961 | nll_loss 7.41 | ppl 170.05 | wps 23800.7 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1636 | lr 0.000204559 | gnorm 0.967 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 4503
2022-03-04 13:03:01 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 13:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:05:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:05:51 | INFO | train_inner | epoch 018:     65 / 97 loss=7.86, nll_loss=7.302, ppl=157.82, wps=23843, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=0.943, loss_scale=32, train_wall=240, gb_free=21, wall=4673
2022-03-04 13:07:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:07:20 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.239 | nll_loss 7.678 | ppl 204.78 | wps 41209.1 | wpb 510.9 | bsz 1 | num_updates 1732 | best_loss 8.239
2022-03-04 13:07:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1732 updates
2022-03-04 13:07:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:07:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 18 @ 1732 updates, score 8.239) (writing took 5.21348318643868 seconds)
2022-03-04 13:07:25 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 13:07:25 | INFO | train | epoch 018 | loss 7.813 | nll_loss 7.252 | ppl 152.4 | wps 23802.1 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1732 | lr 0.000216557 | gnorm 0.959 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 4767
2022-03-04 13:07:25 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 13:07:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:10:23 | INFO | train_inner | epoch 019:     68 / 97 loss=7.717, nll_loss=7.149, ppl=141.95, wps=24098.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.985, loss_scale=32, train_wall=237, gb_free=21, wall=4945
2022-03-04 13:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:11:44 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.16 | nll_loss 7.599 | ppl 193.9 | wps 41503 | wpb 510.9 | bsz 1 | num_updates 1829 | best_loss 8.16
2022-03-04 13:11:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1829 updates
2022-03-04 13:11:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:11:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 19 @ 1829 updates, score 8.16) (writing took 5.344739749096334 seconds)
2022-03-04 13:11:49 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 13:11:49 | INFO | train | epoch 019 | loss 7.67 | nll_loss 7.099 | ppl 137.1 | wps 24058.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1829 | lr 0.000228679 | gnorm 0.966 | loss_scale 64 | train_wall 230 | gb_free 21 | wall 5031
2022-03-04 13:11:49 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 13:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:12:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:14:58 | INFO | train_inner | epoch 020:     72 / 97 loss=7.566, nll_loss=6.988, ppl=126.91, wps=23841.9, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=0.957, loss_scale=32, train_wall=240, gb_free=21, wall=5219
2022-03-04 13:16:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:16:08 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.112 | nll_loss 7.539 | ppl 186.03 | wps 41798.7 | wpb 510.9 | bsz 1 | num_updates 1925 | best_loss 8.112
2022-03-04 13:16:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1925 updates
2022-03-04 13:16:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:16:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:16:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 20 @ 1925 updates, score 8.112) (writing took 5.2931457394734025 seconds)
2022-03-04 13:16:13 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 13:16:13 | INFO | train | epoch 020 | loss 7.527 | nll_loss 6.946 | ppl 123.32 | wps 23795.8 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1925 | lr 0.000240677 | gnorm 0.958 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 5295
2022-03-04 13:16:13 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 13:16:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:18:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:19:32 | INFO | train_inner | epoch 021:     76 / 97 loss=7.423, nll_loss=6.835, ppl=114.2, wps=23847.2, ups=0.36, wpb=65495, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.966, loss_scale=32, train_wall=240, gb_free=21, wall=5494
2022-03-04 13:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:20:32 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.053 | nll_loss 7.48 | ppl 178.58 | wps 41626.4 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 8.053
2022-03-04 13:20:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 13:20:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:20:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:20:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 21 @ 2021 updates, score 8.053) (writing took 5.276910934597254 seconds)
2022-03-04 13:20:38 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 13:20:38 | INFO | train | epoch 021 | loss 7.394 | nll_loss 6.804 | ppl 111.71 | wps 23798.4 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 0.984 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 5559
2022-03-04 13:20:38 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 13:20:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:24:04 | INFO | train_inner | epoch 022:     79 / 97 loss=7.289, nll_loss=6.691, ppl=103.35, wps=24086.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=0.973, loss_scale=32, train_wall=237, gb_free=21, wall=5766
2022-03-04 13:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:24:56 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.022 | nll_loss 7.449 | ppl 174.79 | wps 41547.3 | wpb 510.9 | bsz 1 | num_updates 2118 | best_loss 8.022
2022-03-04 13:24:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2118 updates
2022-03-04 13:24:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:24:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:25:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 22 @ 2118 updates, score 8.022) (writing took 5.340215532109141 seconds)
2022-03-04 13:25:02 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 13:25:02 | INFO | train | epoch 022 | loss 7.262 | nll_loss 6.662 | ppl 101.29 | wps 24058 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2118 | lr 0.000264797 | gnorm 0.958 | loss_scale 64 | train_wall 230 | gb_free 21 | wall 5823
2022-03-04 13:25:02 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 13:25:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:25:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:28:39 | INFO | train_inner | epoch 023:     83 / 97 loss=7.159, nll_loss=6.552, ppl=93.84, wps=23849.5, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=0.948, loss_scale=32, train_wall=240, gb_free=21, wall=6041
2022-03-04 13:29:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:29:21 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.992 | nll_loss 7.407 | ppl 169.75 | wps 42218.8 | wpb 510.9 | bsz 1 | num_updates 2214 | best_loss 7.992
2022-03-04 13:29:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2214 updates
2022-03-04 13:29:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:29:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:29:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 23 @ 2214 updates, score 7.992) (writing took 5.415985937230289 seconds)
2022-03-04 13:29:26 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 13:29:26 | INFO | train | epoch 023 | loss 7.135 | nll_loss 6.526 | ppl 92.17 | wps 23791.6 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 2214 | lr 0.000276795 | gnorm 0.97 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 6088
2022-03-04 13:29:26 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 13:29:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:31:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:33:14 | INFO | train_inner | epoch 024:     87 / 97 loss=7.026, nll_loss=6.41, ppl=85.03, wps=23837.3, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.967, loss_scale=32, train_wall=240, gb_free=21, wall=6315
2022-03-04 13:33:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:33:45 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.966 | nll_loss 7.375 | ppl 165.94 | wps 42852.8 | wpb 510.9 | bsz 1 | num_updates 2310 | best_loss 7.966
2022-03-04 13:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2310 updates
2022-03-04 13:33:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:33:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:33:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 24 @ 2310 updates, score 7.966) (writing took 5.3388467617332935 seconds)
2022-03-04 13:33:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 13:33:50 | INFO | train | epoch 024 | loss 7.011 | nll_loss 6.394 | ppl 84.07 | wps 23808.4 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 2310 | lr 0.000288792 | gnorm 0.952 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 6352
2022-03-04 13:33:50 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 13:33:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:37:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:37:48 | INFO | train_inner | epoch 025:     91 / 97 loss=6.902, nll_loss=6.277, ppl=77.55, wps=23859.9, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=0.984, loss_scale=32, train_wall=240, gb_free=21, wall=6590
2022-03-04 13:38:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:38:09 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.962 | nll_loss 7.377 | ppl 166.25 | wps 42921.9 | wpb 510.9 | bsz 1 | num_updates 2406 | best_loss 7.962
2022-03-04 13:38:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2406 updates
2022-03-04 13:38:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:38:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt
2022-03-04 13:38:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_best.pt (epoch 25 @ 2406 updates, score 7.962) (writing took 5.239854119718075 seconds)
2022-03-04 13:38:14 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 13:38:14 | INFO | train | epoch 025 | loss 6.891 | nll_loss 6.265 | ppl 76.9 | wps 23817.2 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 2406 | lr 0.00030079 | gnorm 0.977 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 6616
2022-03-04 13:38:14 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 13:38:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:42:20 | INFO | train_inner | epoch 026:     94 / 97 loss=6.78, nll_loss=6.146, ppl=70.82, wps=24108.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=0.946, loss_scale=32, train_wall=237, gb_free=21, wall=6861
2022-03-04 13:42:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:42:33 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.999 | nll_loss 7.421 | ppl 171.36 | wps 42420.8 | wpb 510.9 | bsz 1 | num_updates 2503 | best_loss 7.962
2022-03-04 13:42:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2503 updates
2022-03-04 13:42:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:42:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:42:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 26 @ 2503 updates, score 7.999) (writing took 2.356286230497062 seconds)
2022-03-04 13:42:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 13:42:35 | INFO | train | epoch 026 | loss 6.772 | nll_loss 6.137 | ppl 70.39 | wps 24347 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2503 | lr 0.000312912 | gnorm 0.95 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 6877
2022-03-04 13:42:35 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 13:42:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:44:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:46:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:46:54 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.999 | nll_loss 7.417 | ppl 170.9 | wps 42760.7 | wpb 510.9 | bsz 1 | num_updates 2599 | best_loss 7.962
2022-03-04 13:46:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2599 updates
2022-03-04 13:46:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:46:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:46:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 27 @ 2599 updates, score 7.999) (writing took 2.2143262177705765 seconds)
2022-03-04 13:46:56 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 13:46:56 | INFO | train | epoch 027 | loss 6.662 | nll_loss 6.019 | ppl 64.84 | wps 24087.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2599 | lr 0.00032491 | gnorm 1.022 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 7138
2022-03-04 13:46:56 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 13:46:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:46:59 | INFO | train_inner | epoch 028:      1 / 97 loss=6.664, nll_loss=6.022, ppl=64.98, wps=23461.6, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=1.02, loss_scale=32, train_wall=240, gb_free=21, wall=7140
2022-03-04 13:49:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:51:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:51:15 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.985 | nll_loss 7.39 | ppl 167.72 | wps 42683.9 | wpb 510.9 | bsz 1 | num_updates 2695 | best_loss 7.962
2022-03-04 13:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2695 updates
2022-03-04 13:51:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 28 @ 2695 updates, score 7.985) (writing took 2.242885360494256 seconds)
2022-03-04 13:51:17 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 13:51:17 | INFO | train | epoch 028 | loss 6.543 | nll_loss 5.892 | ppl 59.39 | wps 24088.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2695 | lr 0.000336908 | gnorm 0.944 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 7399
2022-03-04 13:51:17 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 13:51:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:51:30 | INFO | train_inner | epoch 029:      5 / 97 loss=6.536, nll_loss=5.885, ppl=59.08, wps=24128.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=0.947, loss_scale=32, train_wall=240, gb_free=21, wall=7412
2022-03-04 13:55:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:55:36 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.02 | nll_loss 7.421 | ppl 171.4 | wps 42365.4 | wpb 510.9 | bsz 1 | num_updates 2792 | best_loss 7.962
2022-03-04 13:55:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2792 updates
2022-03-04 13:55:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:55:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:55:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 29 @ 2792 updates, score 8.02) (writing took 2.331438133493066 seconds)
2022-03-04 13:55:38 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 13:55:38 | INFO | train | epoch 029 | loss 6.439 | nll_loss 5.779 | ppl 54.92 | wps 24315 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2792 | lr 0.00034903 | gnorm 0.981 | loss_scale 64 | train_wall 230 | gb_free 21 | wall 7660
2022-03-04 13:55:38 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 13:55:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:55:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:56:02 | INFO | train_inner | epoch 030:      9 / 97 loss=6.429, nll_loss=5.769, ppl=54.52, wps=24101, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=0.982, loss_scale=32, train_wall=240, gb_free=21, wall=7684
2022-03-04 13:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:59:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:59:57 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.048 | nll_loss 7.451 | ppl 174.95 | wps 42689.6 | wpb 510.9 | bsz 1 | num_updates 2887 | best_loss 7.962
2022-03-04 13:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2887 updates
2022-03-04 13:59:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:59:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 13:59:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 30 @ 2887 updates, score 8.048) (writing took 2.2346482034772635 seconds)
2022-03-04 13:59:59 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 13:59:59 | INFO | train | epoch 030 | loss 6.329 | nll_loss 5.661 | ppl 50.61 | wps 23831.3 | ups 0.36 | wpb 65490.6 | bsz 127.9 | num_updates 2887 | lr 0.000360903 | gnorm 1.024 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 7921
2022-03-04 13:59:59 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 13:59:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:00:33 | INFO | train_inner | epoch 031:     13 / 97 loss=6.312, nll_loss=5.643, ppl=49.97, wps=24115.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=1.013, loss_scale=16, train_wall=240, gb_free=21, wall=7955
2022-03-04 14:04:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:04:18 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.091 | nll_loss 7.494 | ppl 180.26 | wps 42327.6 | wpb 510.9 | bsz 1 | num_updates 2984 | best_loss 7.962
2022-03-04 14:04:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2984 updates
2022-03-04 14:04:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:04:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 31 @ 2984 updates, score 8.091) (writing took 2.2595485793426633 seconds)
2022-03-04 14:04:20 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 14:04:20 | INFO | train | epoch 031 | loss 6.228 | nll_loss 5.552 | ppl 46.92 | wps 24345.1 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2984 | lr 0.000373025 | gnorm 0.998 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 8182
2022-03-04 14:04:20 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 14:04:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:05:02 | INFO | train_inner | epoch 032:     16 / 97 loss=6.209, nll_loss=5.532, ppl=46.26, wps=24379.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=0.99, loss_scale=32, train_wall=237, gb_free=21, wall=8224
2022-03-04 14:08:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:08:39 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.129 | nll_loss 7.532 | ppl 185.09 | wps 42579.7 | wpb 510.9 | bsz 1 | num_updates 3081 | best_loss 7.962
2022-03-04 14:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3081 updates
2022-03-04 14:08:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:08:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 32 @ 3081 updates, score 8.129) (writing took 2.359165659174323 seconds)
2022-03-04 14:08:41 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 14:08:41 | INFO | train | epoch 032 | loss 6.117 | nll_loss 5.433 | ppl 43.21 | wps 24334.6 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3081 | lr 0.000385148 | gnorm 0.989 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 8443
2022-03-04 14:08:41 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 14:08:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:08:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:09:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:09:36 | INFO | train_inner | epoch 033:     21 / 97 loss=6.101, nll_loss=5.416, ppl=42.7, wps=23885.8, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=1.043, loss_scale=16, train_wall=242, gb_free=21, wall=8498
2022-03-04 14:12:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:13:00 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.171 | nll_loss 7.581 | ppl 191.47 | wps 43041.6 | wpb 510.9 | bsz 1 | num_updates 3176 | best_loss 7.962
2022-03-04 14:13:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3176 updates
2022-03-04 14:13:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:13:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:13:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 33 @ 3176 updates, score 8.171) (writing took 2.231491181999445 seconds)
2022-03-04 14:13:02 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 14:13:02 | INFO | train | epoch 033 | loss 6.016 | nll_loss 5.324 | ppl 40.06 | wps 23830.6 | ups 0.36 | wpb 65490.6 | bsz 127.9 | num_updates 3176 | lr 0.000397021 | gnorm 1.009 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 8704
2022-03-04 14:13:02 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 14:13:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:14:05 | INFO | train_inner | epoch 034:     24 / 97 loss=5.989, nll_loss=5.295, ppl=39.27, wps=24354.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=1.032, loss_scale=16, train_wall=238, gb_free=21, wall=8767
2022-03-04 14:17:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:17:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.204 | nll_loss 7.599 | ppl 193.89 | wps 42790.7 | wpb 510.9 | bsz 1 | num_updates 3273 | best_loss 7.962
2022-03-04 14:17:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3273 updates
2022-03-04 14:17:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:17:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:17:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 34 @ 3273 updates, score 8.204) (writing took 2.2976026963442564 seconds)
2022-03-04 14:17:24 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 14:17:24 | INFO | train | epoch 034 | loss 5.918 | nll_loss 5.218 | ppl 37.23 | wps 24324.3 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3273 | lr 0.000409143 | gnorm 1.047 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 8965
2022-03-04 14:17:24 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 14:17:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:18:34 | INFO | train_inner | epoch 035:     27 / 97 loss=5.895, nll_loss=5.193, ppl=36.58, wps=24343.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=1.035, loss_scale=32, train_wall=238, gb_free=21, wall=9036
2022-03-04 14:20:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:21:42 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.239 | nll_loss 7.638 | ppl 199.16 | wps 42700.7 | wpb 510.9 | bsz 1 | num_updates 3369 | best_loss 7.962
2022-03-04 14:21:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3369 updates
2022-03-04 14:21:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:21:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:21:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 35 @ 3369 updates, score 8.239) (writing took 2.3052324317395687 seconds)
2022-03-04 14:21:45 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 14:21:45 | INFO | train | epoch 035 | loss 5.82 | nll_loss 5.112 | ppl 34.59 | wps 24083.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3369 | lr 0.000421141 | gnorm 1.053 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 9226
2022-03-04 14:21:45 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 14:21:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:23:06 | INFO | train_inner | epoch 036:     31 / 97 loss=5.78, nll_loss=5.069, ppl=33.57, wps=24124, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=1.042, loss_scale=32, train_wall=240, gb_free=21, wall=9307
2022-03-04 14:25:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:26:03 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.303 | nll_loss 7.709 | ppl 209.17 | wps 42518 | wpb 510.9 | bsz 1 | num_updates 3466 | best_loss 7.962
2022-03-04 14:26:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3466 updates
2022-03-04 14:26:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 36 @ 3466 updates, score 8.303) (writing took 2.2454035682603717 seconds)
2022-03-04 14:26:06 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 14:26:06 | INFO | train | epoch 036 | loss 5.719 | nll_loss 5.003 | ppl 32.07 | wps 24334.1 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3466 | lr 0.000433263 | gnorm 1.071 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 9487
2022-03-04 14:26:06 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 14:26:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:27:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:27:37 | INFO | train_inner | epoch 037:     35 / 97 loss=5.684, nll_loss=4.965, ppl=31.24, wps=24117, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=1.042, loss_scale=32, train_wall=240, gb_free=21, wall=9579
2022-03-04 14:28:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:30:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:30:24 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.37 | nll_loss 7.776 | ppl 219.24 | wps 42067.1 | wpb 510.9 | bsz 1 | num_updates 3561 | best_loss 7.962
2022-03-04 14:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3561 updates
2022-03-04 14:30:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:30:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 37 @ 3561 updates, score 8.37) (writing took 2.2556193564087152 seconds)
2022-03-04 14:30:27 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 14:30:27 | INFO | train | epoch 037 | loss 5.618 | nll_loss 4.894 | ppl 29.73 | wps 23833 | ups 0.36 | wpb 65490.6 | bsz 127.9 | num_updates 3561 | lr 0.000445136 | gnorm 1.071 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 9748
2022-03-04 14:30:27 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 14:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:32:09 | INFO | train_inner | epoch 038:     39 / 97 loss=5.583, nll_loss=4.856, ppl=28.96, wps=24117, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=1.087, loss_scale=16, train_wall=240, gb_free=21, wall=9851
2022-03-04 14:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:34:46 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.376 | nll_loss 7.761 | ppl 216.96 | wps 42064.1 | wpb 510.9 | bsz 1 | num_updates 3658 | best_loss 7.962
2022-03-04 14:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3658 updates
2022-03-04 14:34:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:34:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 38 @ 3658 updates, score 8.376) (writing took 2.3322139801457524 seconds)
2022-03-04 14:34:48 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 14:34:48 | INFO | train | epoch 038 | loss 5.531 | nll_loss 4.799 | ppl 27.84 | wps 24320.8 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3658 | lr 0.000457259 | gnorm 1.093 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 10010
2022-03-04 14:34:48 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 14:34:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:35:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:36:40 | INFO | train_inner | epoch 039:     43 / 97 loss=5.49, nll_loss=4.755, ppl=27, wps=24121.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.098, loss_scale=16, train_wall=240, gb_free=21, wall=10122
2022-03-04 14:39:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:39:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.415 | nll_loss 7.805 | ppl 223.63 | wps 41510.3 | wpb 510.9 | bsz 1 | num_updates 3754 | best_loss 7.962
2022-03-04 14:39:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3754 updates
2022-03-04 14:39:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:39:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:39:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 39 @ 3754 updates, score 8.415) (writing took 2.245291876606643 seconds)
2022-03-04 14:39:09 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 14:39:09 | INFO | train | epoch 039 | loss 5.44 | nll_loss 4.701 | ppl 26 | wps 24069 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3754 | lr 0.000469256 | gnorm 1.129 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 10271
2022-03-04 14:39:09 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 14:39:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:41:10 | INFO | train_inner | epoch 040:     46 / 97 loss=5.393, nll_loss=4.649, ppl=25.09, wps=24322.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.116, loss_scale=16, train_wall=238, gb_free=21, wall=10391
2022-03-04 14:43:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:43:28 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.51 | nll_loss 7.898 | ppl 238.5 | wps 41986.2 | wpb 510.9 | bsz 1 | num_updates 3851 | best_loss 7.962
2022-03-04 14:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3851 updates
2022-03-04 14:43:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:43:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:43:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 40 @ 3851 updates, score 8.51) (writing took 2.291589624248445 seconds)
2022-03-04 14:43:30 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 14:43:30 | INFO | train | epoch 040 | loss 5.343 | nll_loss 4.595 | ppl 24.17 | wps 24307.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3851 | lr 0.000481379 | gnorm 1.1 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 10532
2022-03-04 14:43:30 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 14:43:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:44:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:45:41 | INFO | train_inner | epoch 041:     50 / 97 loss=5.297, nll_loss=4.545, ppl=23.34, wps=24111.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.108, loss_scale=16, train_wall=240, gb_free=21, wall=10663
2022-03-04 14:47:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:47:49 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.534 | nll_loss 7.919 | ppl 241.97 | wps 41670 | wpb 510.9 | bsz 1 | num_updates 3947 | best_loss 7.962
2022-03-04 14:47:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3947 updates
2022-03-04 14:47:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:47:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:47:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 41 @ 3947 updates, score 8.534) (writing took 2.3972436105832458 seconds)
2022-03-04 14:47:52 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 14:47:52 | INFO | train | epoch 041 | loss 5.252 | nll_loss 4.496 | ppl 22.56 | wps 24073.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3947 | lr 0.000493376 | gnorm 1.097 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 10793
2022-03-04 14:47:52 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 14:47:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:50:10 | INFO | train_inner | epoch 042:     53 / 97 loss=5.205, nll_loss=4.445, ppl=21.78, wps=24343.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.122, loss_scale=32, train_wall=238, gb_free=21, wall=10932
2022-03-04 14:51:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:52:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:52:10 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.627 | nll_loss 8.014 | ppl 258.46 | wps 41080.8 | wpb 510.9 | bsz 1 | num_updates 4043 | best_loss 7.962
2022-03-04 14:52:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4043 updates
2022-03-04 14:52:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:52:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 42 @ 4043 updates, score 8.627) (writing took 2.4515370968729258 seconds)
2022-03-04 14:52:13 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 14:52:13 | INFO | train | epoch 042 | loss 5.163 | nll_loss 4.399 | ppl 21.1 | wps 24060.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4043 | lr 0.000497334 | gnorm 1.14 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 11055
2022-03-04 14:52:13 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 14:52:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:54:42 | INFO | train_inner | epoch 043:     57 / 97 loss=5.106, nll_loss=4.338, ppl=20.22, wps=24086.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.103, loss_scale=16, train_wall=240, gb_free=21, wall=11204
2022-03-04 14:56:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:56:32 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.656 | nll_loss 8.027 | ppl 260.77 | wps 41740.7 | wpb 510.9 | bsz 1 | num_updates 4140 | best_loss 7.962
2022-03-04 14:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4140 updates
2022-03-04 14:56:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:56:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 14:56:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 43 @ 4140 updates, score 8.656) (writing took 2.253245046362281 seconds)
2022-03-04 14:56:34 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 14:56:34 | INFO | train | epoch 043 | loss 5.064 | nll_loss 4.292 | ppl 19.59 | wps 24320.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4140 | lr 0.000491473 | gnorm 1.075 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 11316
2022-03-04 14:56:34 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 14:56:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:57:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:59:14 | INFO | train_inner | epoch 044:     61 / 97 loss=5.006, nll_loss=4.228, ppl=18.74, wps=24126.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.103, loss_scale=16, train_wall=240, gb_free=21, wall=11475
2022-03-04 15:00:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:00:53 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.759 | nll_loss 8.134 | ppl 280.98 | wps 41973.8 | wpb 510.9 | bsz 1 | num_updates 4236 | best_loss 7.962
2022-03-04 15:00:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4236 updates
2022-03-04 15:00:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:00:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:00:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 44 @ 4236 updates, score 8.759) (writing took 2.444092385470867 seconds)
2022-03-04 15:00:55 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 15:00:55 | INFO | train | epoch 044 | loss 4.967 | nll_loss 4.187 | ppl 18.21 | wps 24069.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4236 | lr 0.000485872 | gnorm 1.108 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 11577
2022-03-04 15:00:55 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 15:00:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:03:43 | INFO | train_inner | epoch 045:     64 / 97 loss=4.914, nll_loss=4.129, ppl=17.49, wps=24329.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=1.092, loss_scale=32, train_wall=238, gb_free=21, wall=11745
2022-03-04 15:05:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:05:14 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.837 | nll_loss 8.214 | ppl 296.9 | wps 41917.8 | wpb 510.9 | bsz 1 | num_updates 4333 | best_loss 7.962
2022-03-04 15:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4333 updates
2022-03-04 15:05:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:05:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:05:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 45 @ 4333 updates, score 8.837) (writing took 2.3086782600730658 seconds)
2022-03-04 15:05:16 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 15:05:16 | INFO | train | epoch 045 | loss 4.878 | nll_loss 4.089 | ppl 17.02 | wps 24331.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4333 | lr 0.000480403 | gnorm 1.092 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 11838
2022-03-04 15:05:16 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 15:05:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:05:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:08:14 | INFO | train_inner | epoch 046:     68 / 97 loss=4.819, nll_loss=4.025, ppl=16.28, wps=24128.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.11, loss_scale=16, train_wall=240, gb_free=21, wall=12016
2022-03-04 15:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:09:35 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.915 | nll_loss 8.3 | ppl 315.12 | wps 41584 | wpb 510.9 | bsz 1 | num_updates 4429 | best_loss 7.962
2022-03-04 15:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4429 updates
2022-03-04 15:09:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 46 @ 4429 updates, score 8.915) (writing took 2.3246298544108868 seconds)
2022-03-04 15:09:38 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 15:09:38 | INFO | train | epoch 046 | loss 4.788 | nll_loss 3.992 | ppl 15.91 | wps 24079.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4429 | lr 0.000475168 | gnorm 1.108 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 12099
2022-03-04 15:09:38 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 15:09:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:12:43 | INFO | train_inner | epoch 047:     71 / 97 loss=4.725, nll_loss=3.923, ppl=15.17, wps=24337.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=1.079, loss_scale=32, train_wall=238, gb_free=21, wall=12285
2022-03-04 15:13:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:13:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:13:56 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.963 | nll_loss 8.346 | ppl 325.36 | wps 41775.7 | wpb 510.9 | bsz 1 | num_updates 4525 | best_loss 7.962
2022-03-04 15:13:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4525 updates
2022-03-04 15:13:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 47 @ 4525 updates, score 8.963) (writing took 2.4560847831889987 seconds)
2022-03-04 15:13:59 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 15:13:59 | INFO | train | epoch 047 | loss 4.702 | nll_loss 3.898 | ppl 14.9 | wps 24051.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4525 | lr 0.0004701 | gnorm 1.097 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 12361
2022-03-04 15:13:59 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 15:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:17:15 | INFO | train_inner | epoch 048:     75 / 97 loss=4.64, nll_loss=3.83, ppl=14.22, wps=24109.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.096, loss_scale=16, train_wall=240, gb_free=21, wall=12557
2022-03-04 15:18:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:18:18 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.071 | nll_loss 8.454 | ppl 350.62 | wps 41489.4 | wpb 510.9 | bsz 1 | num_updates 4622 | best_loss 7.962
2022-03-04 15:18:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4622 updates
2022-03-04 15:18:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:18:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:18:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 48 @ 4622 updates, score 9.071) (writing took 2.3447333201766014 seconds)
2022-03-04 15:18:20 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 15:18:20 | INFO | train | epoch 048 | loss 4.619 | nll_loss 3.807 | ppl 13.99 | wps 24327.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4622 | lr 0.000465141 | gnorm 1.081 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 12622
2022-03-04 15:18:20 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 15:18:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:18:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:21:46 | INFO | train_inner | epoch 049:     79 / 97 loss=4.563, nll_loss=3.746, ppl=13.41, wps=24127.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=1.077, loss_scale=16, train_wall=240, gb_free=21, wall=12828
2022-03-04 15:22:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:22:38 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.122 | nll_loss 8.498 | ppl 361.43 | wps 42087.7 | wpb 510.9 | bsz 1 | num_updates 4718 | best_loss 7.962
2022-03-04 15:22:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4718 updates
2022-03-04 15:22:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:22:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:22:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 49 @ 4718 updates, score 9.122) (writing took 2.3194628162309527 seconds)
2022-03-04 15:22:41 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 15:22:41 | INFO | train | epoch 049 | loss 4.537 | nll_loss 3.718 | ppl 13.16 | wps 24111.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4718 | lr 0.000460385 | gnorm 1.069 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 12883
2022-03-04 15:22:41 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 15:22:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:24:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:26:18 | INFO | train_inner | epoch 050:     83 / 97 loss=4.475, nll_loss=3.649, ppl=12.55, wps=24131.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=1.076, loss_scale=16, train_wall=240, gb_free=21, wall=13100
2022-03-04 15:26:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:26:59 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.196 | nll_loss 8.565 | ppl 378.66 | wps 42068.9 | wpb 510.9 | bsz 1 | num_updates 4814 | best_loss 7.962
2022-03-04 15:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4814 updates
2022-03-04 15:26:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:27:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:27:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 50 @ 4814 updates, score 9.196) (writing took 2.496662818826735 seconds)
2022-03-04 15:27:02 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 15:27:02 | INFO | train | epoch 050 | loss 4.461 | nll_loss 3.634 | ppl 12.42 | wps 24074.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4814 | lr 0.000455771 | gnorm 1.089 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 13144
2022-03-04 15:27:02 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 15:27:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:30:47 | INFO | train_inner | epoch 051:     86 / 97 loss=4.4, nll_loss=3.567, ppl=11.85, wps=24347.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.103, loss_scale=32, train_wall=237, gb_free=21, wall=13369
2022-03-04 15:30:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:31:21 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.288 | nll_loss 8.669 | ppl 406.96 | wps 42197.3 | wpb 510.9 | bsz 1 | num_updates 4910 | best_loss 7.962
2022-03-04 15:31:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4910 updates
2022-03-04 15:31:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:31:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:31:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 51 @ 4910 updates, score 9.288) (writing took 2.439162557013333 seconds)
2022-03-04 15:31:23 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 15:31:23 | INFO | train | epoch 051 | loss 4.388 | nll_loss 3.555 | ppl 11.75 | wps 24075.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4910 | lr 0.000451294 | gnorm 1.104 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 13405
2022-03-04 15:31:23 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 15:31:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:35:19 | INFO | train_inner | epoch 052:     90 / 97 loss=4.324, nll_loss=3.484, ppl=11.19, wps=24094.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1.074, loss_scale=16, train_wall=240, gb_free=21, wall=13640
2022-03-04 15:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:35:42 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.401 | nll_loss 8.787 | ppl 441.61 | wps 42118.3 | wpb 510.9 | bsz 1 | num_updates 5007 | best_loss 7.962
2022-03-04 15:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5007 updates
2022-03-04 15:35:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:35:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 52 @ 5007 updates, score 9.401) (writing took 2.414090291596949 seconds)
2022-03-04 15:35:44 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 15:35:44 | INFO | train | epoch 052 | loss 4.313 | nll_loss 3.473 | ppl 11.1 | wps 24306 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 5007 | lr 0.000446901 | gnorm 1.064 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 13666
2022-03-04 15:35:44 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 15:35:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:36:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:39:50 | INFO | train_inner | epoch 053:     94 / 97 loss=4.252, nll_loss=3.406, ppl=10.6, wps=24109.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=1.094, loss_scale=16, train_wall=240, gb_free=21, wall=13912
2022-03-04 15:39:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:40:03 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.428 | nll_loss 8.803 | ppl 446.79 | wps 42841.3 | wpb 510.9 | bsz 1 | num_updates 5103 | best_loss 7.962
2022-03-04 15:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5103 updates
2022-03-04 15:40:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:40:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:40:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 53 @ 5103 updates, score 9.428) (writing took 2.418431267142296 seconds)
2022-03-04 15:40:06 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 15:40:06 | INFO | train | epoch 053 | loss 4.244 | nll_loss 3.397 | ppl 10.53 | wps 24079.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5103 | lr 0.000442677 | gnorm 1.093 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 13927
2022-03-04 15:40:06 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 15:40:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:42:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:44:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:44:24 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.579 | nll_loss 8.953 | ppl 495.68 | wps 42525.7 | wpb 510.9 | bsz 1 | num_updates 5199 | best_loss 7.962
2022-03-04 15:44:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5199 updates
2022-03-04 15:44:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:44:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:44:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 54 @ 5199 updates, score 9.579) (writing took 2.318917967379093 seconds)
2022-03-04 15:44:26 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 15:44:26 | INFO | train | epoch 054 | loss 4.18 | nll_loss 3.327 | ppl 10.03 | wps 24104 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5199 | lr 0.000438571 | gnorm 1.112 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 14188
2022-03-04 15:44:26 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 15:44:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:44:29 | INFO | train_inner | epoch 055:      1 / 97 loss=4.183, nll_loss=3.33, ppl=10.06, wps=23473.4, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=5200, lr=0.000438529, gnorm=1.107, loss_scale=16, train_wall=240, gb_free=21, wall=14191
2022-03-04 15:48:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:48:45 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.6 | nll_loss 8.979 | ppl 504.48 | wps 42600.9 | wpb 510.9 | bsz 1 | num_updates 5296 | best_loss 7.962
2022-03-04 15:48:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5296 updates
2022-03-04 15:48:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:48:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 55 @ 5296 updates, score 9.6) (writing took 2.401872554793954 seconds)
2022-03-04 15:48:48 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 15:48:48 | INFO | train | epoch 055 | loss 4.115 | nll_loss 3.256 | ppl 9.55 | wps 24325.3 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 5296 | lr 0.000434536 | gnorm 1.067 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 14449
2022-03-04 15:48:48 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 15:48:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:48:58 | INFO | train_inner | epoch 056:      4 / 97 loss=4.109, nll_loss=3.249, ppl=9.51, wps=24347.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5300, lr=0.000434372, gnorm=1.067, loss_scale=32, train_wall=237, gb_free=21, wall=14460
2022-03-04 15:49:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:52:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:53:03 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.758 | nll_loss 9.148 | ppl 567.39 | wps 44018.5 | wpb 510.9 | bsz 1 | num_updates 5392 | best_loss 7.962
2022-03-04 15:53:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5392 updates
2022-03-04 15:53:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:53:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 56 @ 5392 updates, score 9.758) (writing took 2.1715995958074927 seconds)
2022-03-04 15:53:05 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 15:53:05 | INFO | train | epoch 056 | loss 4.054 | nll_loss 3.189 | ppl 9.12 | wps 24409.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5392 | lr 0.000430651 | gnorm 1.097 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 14707
2022-03-04 15:53:05 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 15:53:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:53:26 | INFO | train_inner | epoch 057:      8 / 97 loss=4.047, nll_loss=3.181, ppl=9.07, wps=24464.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=1.104, loss_scale=16, train_wall=237, gb_free=21, wall=14728
2022-03-04 15:56:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:57:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:57:20 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.775 | nll_loss 9.154 | ppl 569.56 | wps 43933.4 | wpb 510.9 | bsz 1 | num_updates 5488 | best_loss 7.962
2022-03-04 15:57:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5488 updates
2022-03-04 15:57:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:57:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 15:57:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 57 @ 5488 updates, score 9.775) (writing took 2.2927058646455407 seconds)
2022-03-04 15:57:22 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 15:57:22 | INFO | train | epoch 057 | loss 3.993 | nll_loss 3.122 | ppl 8.7 | wps 24436.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5488 | lr 0.000426867 | gnorm 1.091 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 14964
2022-03-04 15:57:22 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 15:57:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:57:53 | INFO | train_inner | epoch 058:     12 / 97 loss=3.982, nll_loss=3.11, ppl=8.64, wps=24469.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.081, loss_scale=16, train_wall=237, gb_free=21, wall=14995
2022-03-04 16:01:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:01:38 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.833 | nll_loss 9.226 | ppl 598.65 | wps 44023.1 | wpb 510.9 | bsz 1 | num_updates 5585 | best_loss 7.962
2022-03-04 16:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5585 updates
2022-03-04 16:01:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:01:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 58 @ 5585 updates, score 9.833) (writing took 2.362470328807831 seconds)
2022-03-04 16:01:40 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 16:01:40 | INFO | train | epoch 058 | loss 3.935 | nll_loss 3.058 | ppl 8.33 | wps 24664.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5585 | lr 0.000423144 | gnorm 1.086 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 15222
2022-03-04 16:01:40 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 16:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:02:19 | INFO | train_inner | epoch 059:     15 / 97 loss=3.926, nll_loss=3.049, ppl=8.27, wps=24684.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.079, loss_scale=32, train_wall=235, gb_free=21, wall=15261
2022-03-04 16:03:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:05:55 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.94 | nll_loss 9.331 | ppl 644.06 | wps 44043.6 | wpb 510.9 | bsz 1 | num_updates 5681 | best_loss 7.962
2022-03-04 16:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5681 updates
2022-03-04 16:05:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:05:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:05:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 59 @ 5681 updates, score 9.94) (writing took 2.2082464145496488 seconds)
2022-03-04 16:05:57 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 16:05:57 | INFO | train | epoch 059 | loss 3.878 | nll_loss 2.996 | ppl 7.98 | wps 24431.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5681 | lr 0.000419554 | gnorm 1.07 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 15479
2022-03-04 16:05:57 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 16:05:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:06:46 | INFO | train_inner | epoch 060:     19 / 97 loss=3.866, nll_loss=2.982, ppl=7.9, wps=24472, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.077, loss_scale=16, train_wall=237, gb_free=21, wall=15528
2022-03-04 16:09:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:10:12 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 10.056 | nll_loss 9.447 | ppl 697.95 | wps 44221.8 | wpb 510.9 | bsz 1 | num_updates 5777 | best_loss 7.962
2022-03-04 16:10:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5777 updates
2022-03-04 16:10:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 60 @ 5777 updates, score 10.056) (writing took 2.454182834364474 seconds)
2022-03-04 16:10:15 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 16:10:15 | INFO | train | epoch 060 | loss 3.824 | nll_loss 2.936 | ppl 7.65 | wps 24418.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5777 | lr 0.000416053 | gnorm 1.085 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 15737
2022-03-04 16:10:15 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 16:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:11:14 | INFO | train_inner | epoch 061:     23 / 97 loss=3.809, nll_loss=2.92, ppl=7.57, wps=24463.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.08, loss_scale=16, train_wall=237, gb_free=21, wall=15796
2022-03-04 16:14:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:14:30 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 10.044 | nll_loss 9.426 | ppl 687.94 | wps 43460.9 | wpb 510.9 | bsz 1 | num_updates 5874 | best_loss 7.962
2022-03-04 16:14:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5874 updates
2022-03-04 16:14:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:14:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:14:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 61 @ 5874 updates, score 10.044) (writing took 2.201163643039763 seconds)
2022-03-04 16:14:32 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 16:14:32 | INFO | train | epoch 061 | loss 3.773 | nll_loss 2.881 | ppl 7.37 | wps 24706.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5874 | lr 0.000412604 | gnorm 1.084 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 15994
2022-03-04 16:14:32 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 16:14:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:15:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:15:42 | INFO | train_inner | epoch 062:     27 / 97 loss=3.759, nll_loss=2.866, ppl=7.29, wps=24488.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.096, loss_scale=16, train_wall=237, gb_free=21, wall=16063
2022-03-04 16:18:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:18:47 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 10.183 | nll_loss 9.575 | ppl 762.6 | wps 44232.6 | wpb 510.9 | bsz 1 | num_updates 5970 | best_loss 7.962
2022-03-04 16:18:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5970 updates
2022-03-04 16:18:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:18:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:18:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 62 @ 5970 updates, score 10.183) (writing took 2.4054548060521483 seconds)
2022-03-04 16:18:50 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 16:18:50 | INFO | train | epoch 062 | loss 3.723 | nll_loss 2.825 | ppl 7.09 | wps 24406 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5970 | lr 0.000409273 | gnorm 1.105 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 16251
2022-03-04 16:18:50 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 16:18:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:20:07 | INFO | train_inner | epoch 063:     30 / 97 loss=3.704, nll_loss=2.805, ppl=6.99, wps=24693.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.105, loss_scale=16, train_wall=235, gb_free=21, wall=16329
2022-03-04 16:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:23:04 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 10.286 | nll_loss 9.679 | ppl 819.93 | wps 43630.5 | wpb 510.9 | bsz 1 | num_updates 6067 | best_loss 7.962
2022-03-04 16:23:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6067 updates
2022-03-04 16:23:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:23:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 63 @ 6067 updates, score 10.286) (writing took 2.1938360380008817 seconds)
2022-03-04 16:23:07 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 16:23:07 | INFO | train | epoch 063 | loss 3.674 | nll_loss 2.771 | ppl 6.83 | wps 24711.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6067 | lr 0.000405988 | gnorm 1.092 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 16508
2022-03-04 16:23:07 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 16:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:23:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:24:34 | INFO | train_inner | epoch 064:     34 / 97 loss=3.658, nll_loss=2.755, ppl=6.75, wps=24492.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.09, loss_scale=16, train_wall=237, gb_free=21, wall=16596
2022-03-04 16:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:27:21 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10.339 | nll_loss 9.729 | ppl 848.87 | wps 44335.5 | wpb 510.9 | bsz 1 | num_updates 6163 | best_loss 7.962
2022-03-04 16:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6163 updates
2022-03-04 16:27:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:27:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:27:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 64 @ 6163 updates, score 10.339) (writing took 2.219736604951322 seconds)
2022-03-04 16:27:23 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 16:27:23 | INFO | train | epoch 064 | loss 3.627 | nll_loss 2.72 | ppl 6.59 | wps 24472.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6163 | lr 0.000402813 | gnorm 1.104 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 16765
2022-03-04 16:27:24 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 16:27:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:28:59 | INFO | train_inner | epoch 065:     37 / 97 loss=3.606, nll_loss=2.698, ppl=6.49, wps=24731.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.093, loss_scale=16, train_wall=235, gb_free=21, wall=16861
2022-03-04 16:30:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:31:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:31:38 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 10.376 | nll_loss 9.771 | ppl 873.94 | wps 44185.7 | wpb 510.9 | bsz 1 | num_updates 6259 | best_loss 7.962
2022-03-04 16:31:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6259 updates
2022-03-04 16:31:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:31:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 65 @ 6259 updates, score 10.376) (writing took 2.370059880428016 seconds)
2022-03-04 16:31:41 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 16:31:41 | INFO | train | epoch 065 | loss 3.581 | nll_loss 2.669 | ppl 6.36 | wps 24431.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6259 | lr 0.000399712 | gnorm 1.106 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 17023
2022-03-04 16:31:41 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 16:31:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:33:27 | INFO | train_inner | epoch 066:     41 / 97 loss=3.562, nll_loss=2.649, ppl=6.27, wps=24474.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.111, loss_scale=16, train_wall=237, gb_free=21, wall=17128
2022-03-04 16:35:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:35:56 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 10.451 | nll_loss 9.839 | ppl 916.1 | wps 44352.1 | wpb 510.9 | bsz 1 | num_updates 6356 | best_loss 7.962
2022-03-04 16:35:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6356 updates
2022-03-04 16:35:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:35:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:35:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 66 @ 6356 updates, score 10.451) (writing took 2.181828297674656 seconds)
2022-03-04 16:35:58 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 16:35:58 | INFO | train | epoch 066 | loss 3.539 | nll_loss 2.624 | ppl 6.16 | wps 24706.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6356 | lr 0.000396651 | gnorm 1.101 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 17280
2022-03-04 16:35:58 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 16:35:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:36:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:37:54 | INFO | train_inner | epoch 067:     45 / 97 loss=3.519, nll_loss=2.602, ppl=6.07, wps=24489.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.093, loss_scale=16, train_wall=237, gb_free=21, wall=17396
2022-03-04 16:40:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:40:13 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.528 | nll_loss 9.924 | ppl 971.46 | wps 43694.1 | wpb 510.9 | bsz 1 | num_updates 6452 | best_loss 7.962
2022-03-04 16:40:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6452 updates
2022-03-04 16:40:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:40:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 67 @ 6452 updates, score 10.528) (writing took 2.239283362403512 seconds)
2022-03-04 16:40:15 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 16:40:15 | INFO | train | epoch 067 | loss 3.493 | nll_loss 2.573 | ppl 5.95 | wps 24441.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6452 | lr 0.000393689 | gnorm 1.079 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 17537
2022-03-04 16:40:15 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 16:40:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:42:22 | INFO | train_inner | epoch 068:     49 / 97 loss=3.477, nll_loss=2.556, ppl=5.88, wps=24477.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1.082, loss_scale=16, train_wall=237, gb_free=21, wall=17663
2022-03-04 16:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:44:30 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.612 | nll_loss 10.01 | ppl 1031.28 | wps 44085.9 | wpb 510.9 | bsz 1 | num_updates 6548 | best_loss 7.962
2022-03-04 16:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6548 updates
2022-03-04 16:44:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:44:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:44:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 68 @ 6548 updates, score 10.612) (writing took 2.3690404184162617 seconds)
2022-03-04 16:44:32 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 16:44:32 | INFO | train | epoch 068 | loss 3.454 | nll_loss 2.531 | ppl 5.78 | wps 24438.2 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 6548 | lr 0.000390792 | gnorm 1.093 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 17794
2022-03-04 16:44:32 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 16:44:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:46:47 | INFO | train_inner | epoch 069:     52 / 97 loss=3.438, nll_loss=2.512, ppl=5.7, wps=24652.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.085, loss_scale=16, train_wall=235, gb_free=21, wall=17929
2022-03-04 16:47:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:48:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:48:50 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.689 | nll_loss 10.089 | ppl 1089.31 | wps 42679.9 | wpb 510.9 | bsz 1 | num_updates 6644 | best_loss 7.962
2022-03-04 16:48:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6644 updates
2022-03-04 16:48:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:48:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:48:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 69 @ 6644 updates, score 10.689) (writing took 2.3216566368937492 seconds)
2022-03-04 16:48:52 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 16:48:52 | INFO | train | epoch 069 | loss 3.416 | nll_loss 2.489 | ppl 5.61 | wps 24206.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6644 | lr 0.000387958 | gnorm 1.091 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 18054
2022-03-04 16:48:52 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 16:48:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:51:19 | INFO | train_inner | epoch 070:     56 / 97 loss=3.392, nll_loss=2.462, ppl=5.51, wps=24148, ups=0.37, wpb=65495, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.108, loss_scale=16, train_wall=240, gb_free=21, wall=18200
2022-03-04 16:53:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:53:11 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.728 | nll_loss 10.124 | ppl 1115.57 | wps 42849.1 | wpb 510.9 | bsz 1 | num_updates 6741 | best_loss 7.962
2022-03-04 16:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6741 updates
2022-03-04 16:53:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:53:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:53:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 70 @ 6741 updates, score 10.728) (writing took 2.3011209750548005 seconds)
2022-03-04 16:53:13 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 16:53:13 | INFO | train | epoch 070 | loss 3.38 | nll_loss 2.448 | ppl 5.46 | wps 24372 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 6741 | lr 0.000385157 | gnorm 1.101 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 18315
2022-03-04 16:53:13 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 16:53:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:53:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:55:50 | INFO | train_inner | epoch 071:     60 / 97 loss=3.36, nll_loss=2.426, ppl=5.38, wps=24154, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.083, loss_scale=16, train_wall=240, gb_free=21, wall=18471
2022-03-04 16:57:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:57:31 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.804 | nll_loss 10.21 | ppl 1184.76 | wps 43107 | wpb 510.9 | bsz 1 | num_updates 6837 | best_loss 7.962
2022-03-04 16:57:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6837 updates
2022-03-04 16:57:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:57:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 16:57:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 71 @ 6837 updates, score 10.804) (writing took 2.37900850828737 seconds)
2022-03-04 16:57:34 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 16:57:34 | INFO | train | epoch 071 | loss 3.338 | nll_loss 2.403 | ppl 5.29 | wps 24108.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6837 | lr 0.000382443 | gnorm 1.077 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 18575
2022-03-04 16:57:34 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 16:57:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:00:21 | INFO | train_inner | epoch 072:     64 / 97 loss=3.312, nll_loss=2.375, ppl=5.19, wps=24148, ups=0.37, wpb=65495, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.095, loss_scale=16, train_wall=240, gb_free=21, wall=18743
2022-03-04 17:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:01:52 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.796 | nll_loss 10.192 | ppl 1169.51 | wps 42506.7 | wpb 510.9 | bsz 1 | num_updates 6933 | best_loss 7.962
2022-03-04 17:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6933 updates
2022-03-04 17:01:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:01:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:01:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 72 @ 6933 updates, score 10.796) (writing took 2.319728637114167 seconds)
2022-03-04 17:01:54 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 17:01:54 | INFO | train | epoch 072 | loss 3.303 | nll_loss 2.364 | ppl 5.15 | wps 24104.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6933 | lr 0.000379786 | gnorm 1.106 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 18836
2022-03-04 17:01:54 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 17:01:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:04:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:04:52 | INFO | train_inner | epoch 073:     68 / 97 loss=3.279, nll_loss=2.338, ppl=5.06, wps=24126.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.088, loss_scale=16, train_wall=240, gb_free=21, wall=19014
2022-03-04 17:06:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:06:13 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.923 | nll_loss 10.328 | ppl 1285.68 | wps 43114.8 | wpb 510.9 | bsz 1 | num_updates 7029 | best_loss 7.962
2022-03-04 17:06:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7029 updates
2022-03-04 17:06:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:06:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:06:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 73 @ 7029 updates, score 10.923) (writing took 2.3038710886612535 seconds)
2022-03-04 17:06:15 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 17:06:15 | INFO | train | epoch 073 | loss 3.266 | nll_loss 2.324 | ppl 5.01 | wps 24095.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7029 | lr 0.000377184 | gnorm 1.068 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 19097
2022-03-04 17:06:15 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 17:06:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:09:21 | INFO | train_inner | epoch 074:     71 / 97 loss=3.252, nll_loss=2.309, ppl=4.95, wps=24366.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.091, loss_scale=16, train_wall=237, gb_free=21, wall=19283
2022-03-04 17:10:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:10:34 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.95 | nll_loss 10.352 | ppl 1306.79 | wps 42859.6 | wpb 510.9 | bsz 1 | num_updates 7126 | best_loss 7.962
2022-03-04 17:10:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7126 updates
2022-03-04 17:10:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 74 @ 7126 updates, score 10.95) (writing took 2.3629019716754556 seconds)
2022-03-04 17:10:36 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 17:10:36 | INFO | train | epoch 074 | loss 3.237 | nll_loss 2.293 | ppl 4.9 | wps 24336 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7126 | lr 0.000374608 | gnorm 1.101 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 19358
2022-03-04 17:10:36 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 17:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:11:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:13:52 | INFO | train_inner | epoch 075:     75 / 97 loss=3.213, nll_loss=2.266, ppl=4.81, wps=24154.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.106, loss_scale=16, train_wall=240, gb_free=21, wall=19554
2022-03-04 17:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:14:55 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 11.063 | nll_loss 10.471 | ppl 1418.93 | wps 42421 | wpb 510.9 | bsz 1 | num_updates 7222 | best_loss 7.962
2022-03-04 17:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7222 updates
2022-03-04 17:14:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:14:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 75 @ 7222 updates, score 11.063) (writing took 2.3188778031617403 seconds)
2022-03-04 17:14:57 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 17:14:57 | INFO | train | epoch 075 | loss 3.204 | nll_loss 2.256 | ppl 4.78 | wps 24121.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7222 | lr 0.00037211 | gnorm 1.104 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 19619
2022-03-04 17:14:57 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 17:14:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:18:24 | INFO | train_inner | epoch 076:     79 / 97 loss=3.179, nll_loss=2.229, ppl=4.69, wps=24136.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.083, loss_scale=16, train_wall=240, gb_free=21, wall=19825
2022-03-04 17:19:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:19:16 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 11.123 | nll_loss 10.539 | ppl 1487.92 | wps 42839.5 | wpb 510.9 | bsz 1 | num_updates 7318 | best_loss 7.962
2022-03-04 17:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7318 updates
2022-03-04 17:19:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:19:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:19:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 76 @ 7318 updates, score 11.123) (writing took 2.3248422713950276 seconds)
2022-03-04 17:19:18 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 17:19:18 | INFO | train | epoch 076 | loss 3.171 | nll_loss 2.22 | ppl 4.66 | wps 24103.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7318 | lr 0.000369661 | gnorm 1.084 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 19880
2022-03-04 17:19:18 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 17:19:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:22:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:55 | INFO | train_inner | epoch 077:     83 / 97 loss=3.145, nll_loss=2.192, ppl=4.57, wps=24126.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.081, loss_scale=16, train_wall=240, gb_free=21, wall=20097
2022-03-04 17:23:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:23:37 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 11.155 | nll_loss 10.564 | ppl 1513.41 | wps 42760.8 | wpb 510.9 | bsz 1 | num_updates 7414 | best_loss 7.962
2022-03-04 17:23:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7414 updates
2022-03-04 17:23:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:23:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:23:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 77 @ 7414 updates, score 11.155) (writing took 2.306103855371475 seconds)
2022-03-04 17:23:39 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 17:23:39 | INFO | train | epoch 077 | loss 3.139 | nll_loss 2.185 | ppl 4.55 | wps 24095.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7414 | lr 0.00036726 | gnorm 1.078 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 20141
2022-03-04 17:23:39 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 17:23:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:27:24 | INFO | train_inner | epoch 078:     86 / 97 loss=3.119, nll_loss=2.163, ppl=4.48, wps=24384.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.084, loss_scale=16, train_wall=237, gb_free=21, wall=20365
2022-03-04 17:27:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:27:57 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 11.233 | nll_loss 10.651 | ppl 1608.18 | wps 42808.7 | wpb 510.9 | bsz 1 | num_updates 7511 | best_loss 7.962
2022-03-04 17:27:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7511 updates
2022-03-04 17:27:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:28:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:28:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 78 @ 7511 updates, score 11.233) (writing took 2.31822388805449 seconds)
2022-03-04 17:28:00 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 17:28:00 | INFO | train | epoch 078 | loss 3.113 | nll_loss 2.156 | ppl 4.46 | wps 24355.8 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7511 | lr 0.000364881 | gnorm 1.085 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 20401
2022-03-04 17:28:00 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 17:28:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:28:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:31:55 | INFO | train_inner | epoch 079:     90 / 97 loss=3.088, nll_loss=2.129, ppl=4.37, wps=24148, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.099, loss_scale=8, train_wall=240, gb_free=21, wall=20637
2022-03-04 17:32:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:32:18 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 11.267 | nll_loss 10.683 | ppl 1643.45 | wps 42511.7 | wpb 510.9 | bsz 1 | num_updates 7607 | best_loss 7.962
2022-03-04 17:32:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7607 updates
2022-03-04 17:32:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:32:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 79 @ 7607 updates, score 11.267) (writing took 2.3040733076632023 seconds)
2022-03-04 17:32:21 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 17:32:21 | INFO | train | epoch 079 | loss 3.081 | nll_loss 2.122 | ppl 4.35 | wps 24105.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7607 | lr 0.000362571 | gnorm 1.096 | loss_scale 8 | train_wall 230 | gb_free 21 | wall 20662
2022-03-04 17:32:21 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 17:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:36:24 | INFO | train_inner | epoch 080:     93 / 97 loss=3.058, nll_loss=2.096, ppl=4.28, wps=24377.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.089, loss_scale=16, train_wall=237, gb_free=21, wall=20905
2022-03-04 17:36:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:36:39 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 11.236 | nll_loss 10.648 | ppl 1604.87 | wps 42853.7 | wpb 510.9 | bsz 1 | num_updates 7704 | best_loss 7.962
2022-03-04 17:36:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7704 updates
2022-03-04 17:36:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 80 @ 7704 updates, score 11.236) (writing took 2.3723366241902113 seconds)
2022-03-04 17:36:41 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 17:36:41 | INFO | train | epoch 080 | loss 3.054 | nll_loss 2.092 | ppl 4.26 | wps 24358.3 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7704 | lr 0.000360281 | gnorm 1.091 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 20923
2022-03-04 17:36:41 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 17:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:39:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:40:55 | INFO | train_inner | epoch 081:     97 / 97 loss=3.03, nll_loss=2.066, ppl=4.19, wps=24106.8, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=7800, lr=0.000358057, gnorm=1.085, loss_scale=16, train_wall=240, gb_free=21, wall=21177
2022-03-04 17:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:41:00 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 11.325 | nll_loss 10.737 | ppl 1707.13 | wps 43031.8 | wpb 510.9 | bsz 1 | num_updates 7800 | best_loss 7.962
2022-03-04 17:41:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7800 updates
2022-03-04 17:41:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:41:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:41:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 81 @ 7800 updates, score 11.325) (writing took 2.2769588800147176 seconds)
2022-03-04 17:41:02 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 17:41:02 | INFO | train | epoch 081 | loss 3.025 | nll_loss 2.06 | ppl 4.17 | wps 24079.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7800 | lr 0.000358057 | gnorm 1.082 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 21184
2022-03-04 17:41:02 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 17:41:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:45:21 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 11.368 | nll_loss 10.783 | ppl 1761.86 | wps 42751.2 | wpb 510.9 | bsz 1 | num_updates 7897 | best_loss 7.962
2022-03-04 17:45:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7897 updates
2022-03-04 17:45:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:45:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:45:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 82 @ 7897 updates, score 11.368) (writing took 2.395236097276211 seconds)
2022-03-04 17:45:23 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 17:45:23 | INFO | train | epoch 082 | loss 2.999 | nll_loss 2.032 | ppl 4.09 | wps 24334.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7897 | lr 0.000355852 | gnorm 1.089 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 21445
2022-03-04 17:45:23 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 17:45:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:31 | INFO | train_inner | epoch 083:      3 / 97 loss=2.996, nll_loss=2.029, ppl=4.08, wps=23697.6, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=7900, lr=0.000355784, gnorm=1.088, loss_scale=16, train_wall=237, gb_free=21, wall=21453
2022-03-04 17:45:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:49:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:49:42 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 11.4 | nll_loss 10.811 | ppl 1796.69 | wps 42102 | wpb 510.9 | bsz 1 | num_updates 7993 | best_loss 7.962
2022-03-04 17:49:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7993 updates
2022-03-04 17:49:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:49:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 83 @ 7993 updates, score 11.4) (writing took 2.3684418089687824 seconds)
2022-03-04 17:49:44 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 17:49:44 | INFO | train | epoch 083 | loss 2.973 | nll_loss 2.003 | ppl 4.01 | wps 24094.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7993 | lr 0.000353708 | gnorm 1.077 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 21706
2022-03-04 17:49:44 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 17:49:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:50:03 | INFO | train_inner | epoch 084:      7 / 97 loss=2.967, nll_loss=1.997, ppl=3.99, wps=24136.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8000, lr=0.000353553, gnorm=1.074, loss_scale=16, train_wall=240, gb_free=21, wall=21724
2022-03-04 17:51:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:54:03 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 11.495 | nll_loss 10.916 | ppl 1931.77 | wps 42444.6 | wpb 510.9 | bsz 1 | num_updates 8089 | best_loss 7.962
2022-03-04 17:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8089 updates
2022-03-04 17:54:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:54:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:54:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 84 @ 8089 updates, score 11.495) (writing took 2.4275366701185703 seconds)
2022-03-04 17:54:05 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 17:54:05 | INFO | train | epoch 084 | loss 2.949 | nll_loss 1.977 | ppl 3.94 | wps 24088.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8089 | lr 0.000351603 | gnorm 1.08 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 21967
2022-03-04 17:54:05 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 17:54:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:54:34 | INFO | train_inner | epoch 085:     11 / 97 loss=2.945, nll_loss=1.974, ppl=3.93, wps=24116.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.081, loss_scale=16, train_wall=240, gb_free=21, wall=21996
2022-03-04 17:58:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:58:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:58:24 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 11.534 | nll_loss 10.954 | ppl 1984.07 | wps 42695.1 | wpb 510.9 | bsz 1 | num_updates 8185 | best_loss 7.962
2022-03-04 17:58:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8185 updates
2022-03-04 17:58:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:58:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 17:58:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 85 @ 8185 updates, score 11.534) (writing took 2.2159621799364686 seconds)
2022-03-04 17:58:26 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 17:58:26 | INFO | train | epoch 085 | loss 2.924 | nll_loss 1.95 | ppl 3.86 | wps 24103.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8185 | lr 0.000349535 | gnorm 1.081 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 22228
2022-03-04 17:58:26 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 17:58:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:59:06 | INFO | train_inner | epoch 086:     15 / 97 loss=2.917, nll_loss=1.942, ppl=3.84, wps=24144.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.082, loss_scale=16, train_wall=240, gb_free=21, wall=22267
2022-03-04 18:02:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:02:45 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 11.578 | nll_loss 11.002 | ppl 2050.14 | wps 42685.1 | wpb 510.9 | bsz 1 | num_updates 8282 | best_loss 7.962
2022-03-04 18:02:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8282 updates
2022-03-04 18:02:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:02:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:02:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 86 @ 8282 updates, score 11.578) (writing took 2.4044765308499336 seconds)
2022-03-04 18:02:47 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 18:02:47 | INFO | train | epoch 086 | loss 2.901 | nll_loss 1.926 | ppl 3.8 | wps 24337.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8282 | lr 0.000347482 | gnorm 1.075 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 22489
2022-03-04 18:02:47 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 18:02:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:03:34 | INFO | train_inner | epoch 087:     18 / 97 loss=2.896, nll_loss=1.92, ppl=3.78, wps=24355.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.073, loss_scale=16, train_wall=238, gb_free=21, wall=22536
2022-03-04 18:04:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:07:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:07:06 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.586 | nll_loss 11.009 | ppl 2060.12 | wps 42841.6 | wpb 510.9 | bsz 1 | num_updates 8378 | best_loss 7.962
2022-03-04 18:07:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8378 updates
2022-03-04 18:07:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 87 @ 8378 updates, score 11.586) (writing took 2.3664647238329053 seconds)
2022-03-04 18:07:08 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 18:07:08 | INFO | train | epoch 087 | loss 2.875 | nll_loss 1.897 | ppl 3.73 | wps 24073.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8378 | lr 0.000345485 | gnorm 1.064 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 22750
2022-03-04 18:07:08 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 18:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:08:06 | INFO | train_inner | epoch 088:     22 / 97 loss=2.868, nll_loss=1.889, ppl=3.7, wps=24113.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.068, loss_scale=16, train_wall=240, gb_free=21, wall=22808
2022-03-04 18:09:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:11:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:11:27 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.629 | nll_loss 11.05 | ppl 2120.72 | wps 42642.3 | wpb 510.9 | bsz 1 | num_updates 8474 | best_loss 7.962
2022-03-04 18:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8474 updates
2022-03-04 18:11:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:11:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:11:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 88 @ 8474 updates, score 11.629) (writing took 2.2618650291115046 seconds)
2022-03-04 18:11:29 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 18:11:29 | INFO | train | epoch 088 | loss 2.854 | nll_loss 1.874 | ppl 3.67 | wps 24083.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8474 | lr 0.000343523 | gnorm 1.087 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 23011
2022-03-04 18:11:30 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 18:11:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:12:38 | INFO | train_inner | epoch 089:     26 / 97 loss=2.847, nll_loss=1.867, ppl=3.65, wps=24117.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.08, loss_scale=16, train_wall=240, gb_free=21, wall=23079
2022-03-04 18:15:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:15:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:15:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:15:48 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.672 | nll_loss 11.101 | ppl 2196.87 | wps 42548.9 | wpb 510.9 | bsz 1 | num_updates 8569 | best_loss 7.962
2022-03-04 18:15:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8569 updates
2022-03-04 18:15:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:15:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:15:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 89 @ 8569 updates, score 11.672) (writing took 2.365154320374131 seconds)
2022-03-04 18:15:51 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 18:15:51 | INFO | train | epoch 089 | loss 2.832 | nll_loss 1.85 | ppl 3.61 | wps 23834.8 | ups 0.36 | wpb 65490.6 | bsz 127.9 | num_updates 8569 | lr 0.000341613 | gnorm 1.082 | loss_scale 8 | train_wall 230 | gb_free 21 | wall 23272
2022-03-04 18:15:51 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 18:15:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:17:12 | INFO | train_inner | epoch 090:     31 / 97 loss=2.825, nll_loss=1.842, ppl=3.59, wps=23888.4, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.085, loss_scale=8, train_wall=242, gb_free=21, wall=23354
2022-03-04 18:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:20:09 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.741 | nll_loss 11.161 | ppl 2289.92 | wps 42565 | wpb 510.9 | bsz 1 | num_updates 8666 | best_loss 7.962
2022-03-04 18:20:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8666 updates
2022-03-04 18:20:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:20:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 90 @ 8666 updates, score 11.741) (writing took 2.3399155410006642 seconds)
2022-03-04 18:20:12 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 18:20:12 | INFO | train | epoch 090 | loss 2.812 | nll_loss 1.829 | ppl 3.55 | wps 24311.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8666 | lr 0.000339696 | gnorm 1.077 | loss_scale 8 | train_wall 230 | gb_free 21 | wall 23534
2022-03-04 18:20:12 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 18:20:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:21:41 | INFO | train_inner | epoch 091:     34 / 97 loss=2.805, nll_loss=1.821, ppl=3.53, wps=24350.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8700, lr=0.000339032, gnorm=1.075, loss_scale=16, train_wall=237, gb_free=21, wall=23623
2022-03-04 18:24:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:24:31 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.74 | nll_loss 11.164 | ppl 2294.66 | wps 42336.8 | wpb 510.9 | bsz 1 | num_updates 8763 | best_loss 7.962
2022-03-04 18:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8763 updates
2022-03-04 18:24:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:24:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:24:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 91 @ 8763 updates, score 11.74) (writing took 2.255823075771332 seconds)
2022-03-04 18:24:33 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 18:24:33 | INFO | train | epoch 091 | loss 2.791 | nll_loss 1.806 | ppl 3.5 | wps 24335.8 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8763 | lr 0.000337811 | gnorm 1.069 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 23795
2022-03-04 18:24:33 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 18:24:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:26:10 | INFO | train_inner | epoch 092:     37 / 97 loss=2.783, nll_loss=1.797, ppl=3.47, wps=24342.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.076, loss_scale=16, train_wall=238, gb_free=21, wall=23892
2022-03-04 18:27:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:28:52 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.789 | nll_loss 11.215 | ppl 2377.33 | wps 42695.1 | wpb 510.9 | bsz 1 | num_updates 8859 | best_loss 7.962
2022-03-04 18:28:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8859 updates
2022-03-04 18:28:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:28:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:28:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 92 @ 8859 updates, score 11.789) (writing took 2.348562619648874 seconds)
2022-03-04 18:28:54 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 18:28:54 | INFO | train | epoch 092 | loss 2.772 | nll_loss 1.785 | ppl 3.45 | wps 24076 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8859 | lr 0.000335976 | gnorm 1.086 | loss_scale 8 | train_wall 230 | gb_free 21 | wall 24056
2022-03-04 18:28:54 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 18:28:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:30:41 | INFO | train_inner | epoch 093:     41 / 97 loss=2.763, nll_loss=1.775, ppl=3.42, wps=24118.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.077, loss_scale=8, train_wall=240, gb_free=21, wall=24163
2022-03-04 18:33:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:33:11 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.746 | nll_loss 11.172 | ppl 2307.18 | wps 44010.7 | wpb 510.9 | bsz 1 | num_updates 8956 | best_loss 7.962
2022-03-04 18:33:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8956 updates
2022-03-04 18:33:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:33:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 93 @ 8956 updates, score 11.746) (writing took 2.4025887846946716 seconds)
2022-03-04 18:33:13 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 18:33:13 | INFO | train | epoch 093 | loss 2.752 | nll_loss 1.763 | ppl 3.39 | wps 24506.8 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8956 | lr 0.000334151 | gnorm 1.079 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 24315
2022-03-04 18:33:13 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 18:33:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:35:07 | INFO | train_inner | epoch 094:     44 / 97 loss=2.743, nll_loss=1.754, ppl=3.37, wps=24677.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.072, loss_scale=16, train_wall=235, gb_free=21, wall=24428
2022-03-04 18:37:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:37:28 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.887 | nll_loss 11.326 | ppl 2566.59 | wps 43939.7 | wpb 510.9 | bsz 1 | num_updates 9053 | best_loss 7.962
2022-03-04 18:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9053 updates
2022-03-04 18:37:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:37:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:37:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 94 @ 9053 updates, score 11.887) (writing took 2.5897950949147344 seconds)
2022-03-04 18:37:31 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 18:37:31 | INFO | train | epoch 094 | loss 2.733 | nll_loss 1.742 | ppl 3.35 | wps 24672.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9053 | lr 0.000332356 | gnorm 1.049 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24572
2022-03-04 18:37:31 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 18:37:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:38:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:39:35 | INFO | train_inner | epoch 095:     48 / 97 loss=2.725, nll_loss=1.734, ppl=3.33, wps=24441.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=9100, lr=0.000331497, gnorm=1.049, loss_scale=16, train_wall=237, gb_free=21, wall=24696
2022-03-04 18:41:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:41:46 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.883 | nll_loss 11.311 | ppl 2540.32 | wps 44034.6 | wpb 510.9 | bsz 1 | num_updates 9149 | best_loss 7.962
2022-03-04 18:41:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9149 updates
2022-03-04 18:41:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:41:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:41:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 95 @ 9149 updates, score 11.883) (writing took 2.4831730956211686 seconds)
2022-03-04 18:41:48 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 18:41:48 | INFO | train | epoch 095 | loss 2.715 | nll_loss 1.723 | ppl 3.3 | wps 24395.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9149 | lr 0.000330608 | gnorm 1.067 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24830
2022-03-04 18:41:48 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 18:41:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:42:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:44:03 | INFO | train_inner | epoch 096:     52 / 97 loss=2.705, nll_loss=1.712, ppl=3.28, wps=24443.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.069, loss_scale=8, train_wall=237, gb_free=21, wall=24964
2022-03-04 18:45:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:46:04 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.956 | nll_loss 11.394 | ppl 2690.62 | wps 43849.1 | wpb 510.9 | bsz 1 | num_updates 9245 | best_loss 7.962
2022-03-04 18:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9245 updates
2022-03-04 18:46:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:46:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:46:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 96 @ 9245 updates, score 11.956) (writing took 2.4941489789634943 seconds)
2022-03-04 18:46:06 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 18:46:06 | INFO | train | epoch 096 | loss 2.695 | nll_loss 1.701 | ppl 3.25 | wps 24408.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9245 | lr 0.000328887 | gnorm 1.06 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 25088
2022-03-04 18:46:06 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 18:46:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:48:28 | INFO | train_inner | epoch 097:     55 / 97 loss=2.686, nll_loss=1.692, ppl=3.23, wps=24678.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.047, loss_scale=16, train_wall=235, gb_free=21, wall=25230
2022-03-04 18:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:50:21 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.971 | nll_loss 11.409 | ppl 2718.65 | wps 44123.1 | wpb 510.9 | bsz 1 | num_updates 9342 | best_loss 7.962
2022-03-04 18:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9342 updates
2022-03-04 18:50:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:50:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:50:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 97 @ 9342 updates, score 11.971) (writing took 2.481977297924459 seconds)
2022-03-04 18:50:24 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 18:50:24 | INFO | train | epoch 097 | loss 2.679 | nll_loss 1.684 | ppl 3.21 | wps 24658.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9342 | lr 0.000327175 | gnorm 1.051 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25345
2022-03-04 18:50:24 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 18:50:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:52:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:52:56 | INFO | train_inner | epoch 098:     59 / 97 loss=2.669, nll_loss=1.673, ppl=3.19, wps=24440.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.074, loss_scale=8, train_wall=237, gb_free=21, wall=25498
2022-03-04 18:54:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:54:39 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.98 | nll_loss 11.419 | ppl 2737.24 | wps 44032.4 | wpb 510.9 | bsz 1 | num_updates 9438 | best_loss 7.962
2022-03-04 18:54:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9438 updates
2022-03-04 18:54:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:54:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:54:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 98 @ 9438 updates, score 11.98) (writing took 2.486068763770163 seconds)
2022-03-04 18:54:41 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 18:54:41 | INFO | train | epoch 098 | loss 2.661 | nll_loss 1.665 | ppl 3.17 | wps 24408 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9438 | lr 0.000325507 | gnorm 1.07 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 25603
2022-03-04 18:54:41 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 18:54:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:57:21 | INFO | train_inner | epoch 099:     62 / 97 loss=2.653, nll_loss=1.656, ppl=3.15, wps=24696.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.076, loss_scale=8, train_wall=235, gb_free=21, wall=25763
2022-03-04 18:58:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:58:56 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 11.982 | nll_loss 11.417 | ppl 2733.51 | wps 43946.4 | wpb 510.9 | bsz 1 | num_updates 9535 | best_loss 7.962
2022-03-04 18:58:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9535 updates
2022-03-04 18:58:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:58:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 18:58:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 99 @ 9535 updates, score 11.982) (writing took 2.4789470434188843 seconds)
2022-03-04 18:58:59 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 18:58:59 | INFO | train | epoch 099 | loss 2.645 | nll_loss 1.647 | ppl 3.13 | wps 24667.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9535 | lr 0.000323847 | gnorm 1.079 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25861
2022-03-04 18:58:59 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 18:58:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:01:46 | INFO | train_inner | epoch 100:     65 / 97 loss=2.634, nll_loss=1.635, ppl=3.11, wps=24687, ups=0.38, wpb=65495, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.069, loss_scale=16, train_wall=235, gb_free=21, wall=26028
2022-03-04 19:03:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:03:14 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 12.022 | nll_loss 11.462 | ppl 2820.81 | wps 44309.8 | wpb 510.9 | bsz 1 | num_updates 9632 | best_loss 7.962
2022-03-04 19:03:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9632 updates
2022-03-04 19:03:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:03:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:03:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 100 @ 9632 updates, score 12.022) (writing took 2.466118620708585 seconds)
2022-03-04 19:03:16 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 19:03:16 | INFO | train | epoch 100 | loss 2.629 | nll_loss 1.63 | ppl 3.09 | wps 24670.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9632 | lr 0.000322212 | gnorm 1.062 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26118
2022-03-04 19:03:16 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 19:03:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:04:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:06:14 | INFO | train_inner | epoch 101:     69 / 97 loss=2.618, nll_loss=1.618, ppl=3.07, wps=24444.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.061, loss_scale=16, train_wall=237, gb_free=21, wall=26296
2022-03-04 19:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:07:31 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 12.105 | nll_loss 11.543 | ppl 2983.71 | wps 43858.3 | wpb 510.9 | bsz 1 | num_updates 9728 | best_loss 7.962
2022-03-04 19:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9728 updates
2022-03-04 19:07:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:07:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:07:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 101 @ 9728 updates, score 12.105) (writing took 2.5112914834171534 seconds)
2022-03-04 19:07:34 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 19:07:34 | INFO | train | epoch 101 | loss 2.611 | nll_loss 1.611 | ppl 3.05 | wps 24400.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9728 | lr 0.000320618 | gnorm 1.059 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26376
2022-03-04 19:07:34 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 19:07:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:10:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:10:42 | INFO | train_inner | epoch 102:     73 / 97 loss=2.603, nll_loss=1.601, ppl=3.03, wps=24439.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.055, loss_scale=16, train_wall=237, gb_free=21, wall=26564
2022-03-04 19:11:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:11:49 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 12.062 | nll_loss 11.501 | ppl 2897.82 | wps 44128.9 | wpb 510.9 | bsz 1 | num_updates 9824 | best_loss 7.962
2022-03-04 19:11:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9824 updates
2022-03-04 19:11:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:11:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 102 @ 9824 updates, score 12.062) (writing took 2.488007008098066 seconds)
2022-03-04 19:11:52 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 19:11:52 | INFO | train | epoch 102 | loss 2.596 | nll_loss 1.594 | ppl 3.02 | wps 24402.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9824 | lr 0.000319048 | gnorm 1.065 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26633
2022-03-04 19:11:52 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 19:11:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:11:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:15:10 | INFO | train_inner | epoch 103:     77 / 97 loss=2.582, nll_loss=1.579, ppl=2.99, wps=24458.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.05, loss_scale=8, train_wall=237, gb_free=21, wall=26832
2022-03-04 19:16:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:16:07 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 12.181 | nll_loss 11.626 | ppl 3160.91 | wps 44010.2 | wpb 510.9 | bsz 1 | num_updates 9920 | best_loss 7.962
2022-03-04 19:16:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9920 updates
2022-03-04 19:16:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:16:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 103 @ 9920 updates, score 12.181) (writing took 2.481135002337396 seconds)
2022-03-04 19:16:09 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 19:16:09 | INFO | train | epoch 103 | loss 2.579 | nll_loss 1.576 | ppl 2.98 | wps 24418.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9920 | lr 0.0003175 | gnorm 1.04 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 26891
2022-03-04 19:16:09 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 19:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:19:35 | INFO | train_inner | epoch 104:     80 / 97 loss=2.574, nll_loss=1.57, ppl=2.97, wps=24683.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.058, loss_scale=16, train_wall=235, gb_free=21, wall=27097
2022-03-04 19:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:20:24 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 12.162 | nll_loss 11.607 | ppl 3118.55 | wps 43901.9 | wpb 510.9 | bsz 1 | num_updates 10017 | best_loss 7.962
2022-03-04 19:20:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10017 updates
2022-03-04 19:20:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:20:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:20:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 104 @ 10017 updates, score 12.162) (writing took 2.488026757724583 seconds)
2022-03-04 19:20:27 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 19:20:27 | INFO | train | epoch 104 | loss 2.568 | nll_loss 1.564 | ppl 2.96 | wps 24653.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10017 | lr 0.000315959 | gnorm 1.061 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 27148
2022-03-04 19:20:27 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 19:20:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:22:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:24:04 | INFO | train_inner | epoch 105:     84 / 97 loss=2.556, nll_loss=1.551, ppl=2.93, wps=24428.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.063, loss_scale=8, train_wall=238, gb_free=21, wall=27365
2022-03-04 19:24:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:24:42 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 12.249 | nll_loss 11.696 | ppl 3317.32 | wps 44025 | wpb 510.9 | bsz 1 | num_updates 10113 | best_loss 7.962
2022-03-04 19:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10113 updates
2022-03-04 19:24:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:24:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:24:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 105 @ 10113 updates, score 12.249) (writing took 2.4745571399107575 seconds)
2022-03-04 19:24:44 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 19:24:44 | INFO | train | epoch 105 | loss 2.551 | nll_loss 1.546 | ppl 2.92 | wps 24394.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10113 | lr 0.000314456 | gnorm 1.059 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 27406
2022-03-04 19:24:44 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 19:24:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:28:29 | INFO | train_inner | epoch 106:     87 / 97 loss=2.54, nll_loss=1.533, ppl=2.89, wps=24681.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10200, lr=0.000313112, gnorm=1.049, loss_scale=16, train_wall=235, gb_free=21, wall=27631
2022-03-04 19:28:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:29:00 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 12.173 | nll_loss 11.616 | ppl 3139.03 | wps 43650 | wpb 510.9 | bsz 1 | num_updates 10210 | best_loss 7.962
2022-03-04 19:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10210 updates
2022-03-04 19:29:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:29:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 106 @ 10210 updates, score 12.173) (writing took 2.4672485971823335 seconds)
2022-03-04 19:29:02 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 19:29:02 | INFO | train | epoch 106 | loss 2.538 | nll_loss 1.531 | ppl 2.89 | wps 24658.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10210 | lr 0.000312959 | gnorm 1.052 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 27664
2022-03-04 19:29:02 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 19:29:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:32:54 | INFO | train_inner | epoch 107:     90 / 97 loss=2.528, nll_loss=1.521, ppl=2.87, wps=24679.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.049, loss_scale=16, train_wall=235, gb_free=21, wall=27896
2022-03-04 19:33:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:33:17 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 12.227 | nll_loss 11.674 | ppl 3266.59 | wps 44128 | wpb 510.9 | bsz 1 | num_updates 10307 | best_loss 7.962
2022-03-04 19:33:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10307 updates
2022-03-04 19:33:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:33:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:33:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 107 @ 10307 updates, score 12.227) (writing took 2.4835589975118637 seconds)
2022-03-04 19:33:20 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 19:33:20 | INFO | train | epoch 107 | loss 2.524 | nll_loss 1.516 | ppl 2.86 | wps 24660 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10307 | lr 0.000311483 | gnorm 1.047 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 27921
2022-03-04 19:33:20 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 19:33:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:34:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:37:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:37:25 | INFO | train_inner | epoch 108:     95 / 97 loss=2.515, nll_loss=1.506, ppl=2.84, wps=24214.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.054, loss_scale=8, train_wall=240, gb_free=21, wall=28167
2022-03-04 19:37:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:37:35 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 12.208 | nll_loss 11.647 | ppl 3207.55 | wps 43674.5 | wpb 510.9 | bsz 1 | num_updates 10402 | best_loss 7.962
2022-03-04 19:37:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10402 updates
2022-03-04 19:37:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:37:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:37:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 108 @ 10402 updates, score 12.208) (writing took 2.4866514736786485 seconds)
2022-03-04 19:37:37 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 19:37:37 | INFO | train | epoch 108 | loss 2.51 | nll_loss 1.502 | ppl 2.83 | wps 24146.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 10402 | lr 0.000310057 | gnorm 1.051 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 28179
2022-03-04 19:37:37 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 19:37:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:41:52 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 12.297 | nll_loss 11.748 | ppl 3439.89 | wps 44094.3 | wpb 510.9 | bsz 1 | num_updates 10499 | best_loss 7.962
2022-03-04 19:41:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10499 updates
2022-03-04 19:41:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:41:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:41:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 109 @ 10499 updates, score 12.297) (writing took 2.506874844431877 seconds)
2022-03-04 19:41:55 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 19:41:55 | INFO | train | epoch 109 | loss 2.498 | nll_loss 1.488 | ppl 2.81 | wps 24655.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10499 | lr 0.000308621 | gnorm 1.05 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 28437
2022-03-04 19:41:55 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 19:41:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:41:58 | INFO | train_inner | epoch 110:      1 / 97 loss=2.498, nll_loss=1.489, ppl=2.81, wps=23983.7, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=10500, lr=0.000308607, gnorm=1.05, loss_scale=8, train_wall=235, gb_free=21, wall=28439
2022-03-04 19:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:46:10 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 12.342 | nll_loss 11.79 | ppl 3540.77 | wps 44104.4 | wpb 510.9 | bsz 1 | num_updates 10596 | best_loss 7.962
2022-03-04 19:46:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10596 updates
2022-03-04 19:46:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:46:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:46:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 110 @ 10596 updates, score 12.342) (writing took 2.4833634085953236 seconds)
2022-03-04 19:46:13 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 19:46:13 | INFO | train | epoch 110 | loss 2.484 | nll_loss 1.474 | ppl 2.78 | wps 24654.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10596 | lr 0.000307206 | gnorm 1.05 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28694
2022-03-04 19:46:13 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 19:46:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:46:23 | INFO | train_inner | epoch 111:      4 / 97 loss=2.483, nll_loss=1.472, ppl=2.77, wps=24673.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10600, lr=0.000307148, gnorm=1.05, loss_scale=16, train_wall=235, gb_free=21, wall=28705
2022-03-04 19:48:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:50:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:50:28 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 12.318 | nll_loss 11.763 | ppl 3476.61 | wps 44171.2 | wpb 510.9 | bsz 1 | num_updates 10692 | best_loss 7.962
2022-03-04 19:50:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10692 updates
2022-03-04 19:50:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:50:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:50:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 111 @ 10692 updates, score 12.318) (writing took 2.509857158176601 seconds)
2022-03-04 19:50:30 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 19:50:30 | INFO | train | epoch 111 | loss 2.471 | nll_loss 1.459 | ppl 2.75 | wps 24398.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10692 | lr 0.000305823 | gnorm 1.038 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28952
2022-03-04 19:50:30 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 19:50:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:50:51 | INFO | train_inner | epoch 112:      8 / 97 loss=2.467, nll_loss=1.455, ppl=2.74, wps=24441.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.039, loss_scale=16, train_wall=237, gb_free=21, wall=28973
2022-03-04 19:54:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:54:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:54:46 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 12.425 | nll_loss 11.881 | ppl 3772.81 | wps 44111.7 | wpb 510.9 | bsz 1 | num_updates 10788 | best_loss 7.962
2022-03-04 19:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10788 updates
2022-03-04 19:54:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 112 @ 10788 updates, score 12.425) (writing took 2.513847322203219 seconds)
2022-03-04 19:54:48 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 19:54:48 | INFO | train | epoch 112 | loss 2.46 | nll_loss 1.448 | ppl 2.73 | wps 24399.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10788 | lr 0.00030446 | gnorm 1.057 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29210
2022-03-04 19:54:48 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 19:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:55:19 | INFO | train_inner | epoch 113:     12 / 97 loss=2.456, nll_loss=1.443, ppl=2.72, wps=24438.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.048, loss_scale=16, train_wall=237, gb_free=21, wall=29241
2022-03-04 19:57:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:58:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:59:03 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 12.406 | nll_loss 11.86 | ppl 3718.31 | wps 44024.6 | wpb 510.9 | bsz 1 | num_updates 10884 | best_loss 7.962
2022-03-04 19:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10884 updates
2022-03-04 19:59:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:59:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 19:59:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 113 @ 10884 updates, score 12.406) (writing took 2.506705575622618 seconds)
2022-03-04 19:59:06 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 19:59:06 | INFO | train | epoch 113 | loss 2.447 | nll_loss 1.433 | ppl 2.7 | wps 24408.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10884 | lr 0.000303114 | gnorm 1.055 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 29467
2022-03-04 19:59:06 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 19:59:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:59:47 | INFO | train_inner | epoch 114:     16 / 97 loss=2.445, nll_loss=1.431, ppl=2.7, wps=24445, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.063, loss_scale=8, train_wall=237, gb_free=21, wall=29509
2022-03-04 20:03:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:03:21 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 12.422 | nll_loss 11.877 | ppl 3760.32 | wps 44033.7 | wpb 510.9 | bsz 1 | num_updates 10981 | best_loss 7.962
2022-03-04 20:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10981 updates
2022-03-04 20:03:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:03:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 114 @ 10981 updates, score 12.422) (writing took 2.483789629302919 seconds)
2022-03-04 20:03:23 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 20:03:23 | INFO | train | epoch 114 | loss 2.435 | nll_loss 1.42 | ppl 2.68 | wps 24657.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10981 | lr 0.000301772 | gnorm 1.045 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29725
2022-03-04 20:03:23 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 20:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:04:12 | INFO | train_inner | epoch 115:     19 / 97 loss=2.432, nll_loss=1.417, ppl=2.67, wps=24676.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.041, loss_scale=16, train_wall=235, gb_free=21, wall=29774
2022-03-04 20:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:07:38 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 12.361 | nll_loss 11.814 | ppl 3600.87 | wps 43950.7 | wpb 510.9 | bsz 1 | num_updates 11078 | best_loss 7.962
2022-03-04 20:07:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11078 updates
2022-03-04 20:07:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:07:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:07:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 115 @ 11078 updates, score 12.361) (writing took 2.465469545684755 seconds)
2022-03-04 20:07:41 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 20:07:41 | INFO | train | epoch 115 | loss 2.424 | nll_loss 1.409 | ppl 2.66 | wps 24659.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11078 | lr 0.000300448 | gnorm 1.055 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29983
2022-03-04 20:07:41 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 20:07:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:08:38 | INFO | train_inner | epoch 116:     22 / 97 loss=2.42, nll_loss=1.404, ppl=2.65, wps=24685.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.052, loss_scale=16, train_wall=235, gb_free=21, wall=30039
2022-03-04 20:08:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:11:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:11:56 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 12.406 | nll_loss 11.86 | ppl 3717.71 | wps 44089.5 | wpb 510.9 | bsz 1 | num_updates 11174 | best_loss 7.962
2022-03-04 20:11:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11174 updates
2022-03-04 20:11:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:11:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 116 @ 11174 updates, score 12.406) (writing took 2.5020257867872715 seconds)
2022-03-04 20:11:58 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 20:11:58 | INFO | train | epoch 116 | loss 2.412 | nll_loss 1.396 | ppl 2.63 | wps 24409.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11174 | lr 0.000299155 | gnorm 1.043 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30240
2022-03-04 20:11:58 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 20:11:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:13:06 | INFO | train_inner | epoch 117:     26 / 97 loss=2.409, nll_loss=1.393, ppl=2.63, wps=24440.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.044, loss_scale=16, train_wall=237, gb_free=21, wall=30307
2022-03-04 20:14:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:16:14 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 12.505 | nll_loss 11.96 | ppl 3983.8 | wps 43986.9 | wpb 510.9 | bsz 1 | num_updates 11270 | best_loss 7.962
2022-03-04 20:16:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11270 updates
2022-03-04 20:16:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:16:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 117 @ 11270 updates, score 12.505) (writing took 2.5232961792498827 seconds)
2022-03-04 20:16:16 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 20:16:16 | INFO | train | epoch 117 | loss 2.401 | nll_loss 1.385 | ppl 2.61 | wps 24398.2 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 11270 | lr 0.000297878 | gnorm 1.045 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30498
2022-03-04 20:16:16 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 20:16:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:17:34 | INFO | train_inner | epoch 118:     30 / 97 loss=2.395, nll_loss=1.378, ppl=2.6, wps=24445.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.055, loss_scale=16, train_wall=237, gb_free=21, wall=30575
2022-03-04 20:20:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:20:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:20:31 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 12.455 | nll_loss 11.909 | ppl 3846.88 | wps 44058.7 | wpb 510.9 | bsz 1 | num_updates 11366 | best_loss 7.962
2022-03-04 20:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11366 updates
2022-03-04 20:20:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:20:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:20:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 118 @ 11366 updates, score 12.455) (writing took 2.4717790950089693 seconds)
2022-03-04 20:20:34 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 20:20:34 | INFO | train | epoch 118 | loss 2.389 | nll_loss 1.372 | ppl 2.59 | wps 24410.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11366 | lr 0.000296617 | gnorm 1.052 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30755
2022-03-04 20:20:34 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 20:20:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:22:02 | INFO | train_inner | epoch 119:     34 / 97 loss=2.385, nll_loss=1.367, ppl=2.58, wps=24440.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.041, loss_scale=16, train_wall=237, gb_free=21, wall=30843
2022-03-04 20:24:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:24:49 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 12.46 | nll_loss 11.917 | ppl 3866.69 | wps 44032.2 | wpb 510.9 | bsz 1 | num_updates 11463 | best_loss 7.962
2022-03-04 20:24:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11463 updates
2022-03-04 20:24:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:24:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:24:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 119 @ 11463 updates, score 12.46) (writing took 2.4669040823355317 seconds)
2022-03-04 20:24:51 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 20:24:51 | INFO | train | epoch 119 | loss 2.379 | nll_loss 1.361 | ppl 2.57 | wps 24658.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11463 | lr 0.000295359 | gnorm 1.042 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31013
2022-03-04 20:24:51 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 20:24:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:26:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:26:29 | INFO | train_inner | epoch 120:     38 / 97 loss=2.375, nll_loss=1.356, ppl=2.56, wps=24453.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.034, loss_scale=16, train_wall=237, gb_free=21, wall=31111
2022-03-04 20:29:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:29:06 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 12.548 | nll_loss 12.008 | ppl 4119.18 | wps 44046 | wpb 510.9 | bsz 1 | num_updates 11559 | best_loss 7.962
2022-03-04 20:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11559 updates
2022-03-04 20:29:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:29:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 120 @ 11559 updates, score 12.548) (writing took 2.48985215369612 seconds)
2022-03-04 20:29:09 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 20:29:09 | INFO | train | epoch 120 | loss 2.368 | nll_loss 1.349 | ppl 2.55 | wps 24409.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11559 | lr 0.00029413 | gnorm 1.029 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31271
2022-03-04 20:29:09 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 20:29:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:30:55 | INFO | train_inner | epoch 121:     41 / 97 loss=2.366, nll_loss=1.347, ppl=2.54, wps=24674.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.033, loss_scale=16, train_wall=235, gb_free=21, wall=31377
2022-03-04 20:31:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:33:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:33:24 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 12.493 | nll_loss 11.948 | ppl 3951.09 | wps 43740.8 | wpb 510.9 | bsz 1 | num_updates 11655 | best_loss 7.962
2022-03-04 20:33:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11655 updates
2022-03-04 20:33:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:33:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 121 @ 11655 updates, score 12.493) (writing took 2.422512854449451 seconds)
2022-03-04 20:33:27 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 20:33:27 | INFO | train | epoch 121 | loss 2.358 | nll_loss 1.339 | ppl 2.53 | wps 24398.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11655 | lr 0.000292917 | gnorm 1.036 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31528
2022-03-04 20:33:27 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 20:33:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:35:23 | INFO | train_inner | epoch 122:     45 / 97 loss=2.354, nll_loss=1.334, ppl=2.52, wps=24429.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=1.036, loss_scale=16, train_wall=238, gb_free=21, wall=31645
2022-03-04 20:37:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:37:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:37:42 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 12.571 | nll_loss 12.033 | ppl 4191.51 | wps 44215.6 | wpb 510.9 | bsz 1 | num_updates 11751 | best_loss 7.962
2022-03-04 20:37:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11751 updates
2022-03-04 20:37:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:37:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 122 @ 11751 updates, score 12.571) (writing took 2.430809679441154 seconds)
2022-03-04 20:37:44 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 20:37:44 | INFO | train | epoch 122 | loss 2.349 | nll_loss 1.328 | ppl 2.51 | wps 24405.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11751 | lr 0.000291718 | gnorm 1.029 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31786
2022-03-04 20:37:44 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 20:37:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:39:51 | INFO | train_inner | epoch 123:     49 / 97 loss=2.344, nll_loss=1.323, ppl=2.5, wps=24469.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.023, loss_scale=16, train_wall=237, gb_free=21, wall=31912
2022-03-04 20:41:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:41:59 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 12.613 | nll_loss 12.077 | ppl 4321.43 | wps 44189.7 | wpb 510.9 | bsz 1 | num_updates 11848 | best_loss 7.962
2022-03-04 20:41:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11848 updates
2022-03-04 20:41:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 123 @ 11848 updates, score 12.613) (writing took 2.4289007540792227 seconds)
2022-03-04 20:42:02 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 20:42:02 | INFO | train | epoch 123 | loss 2.34 | nll_loss 1.319 | ppl 2.49 | wps 24685.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11848 | lr 0.000290521 | gnorm 1.024 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 32043
2022-03-04 20:42:02 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 20:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:43:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:44:18 | INFO | train_inner | epoch 124:     53 / 97 loss=2.332, nll_loss=1.31, ppl=2.48, wps=24469.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.031, loss_scale=16, train_wall=237, gb_free=21, wall=32180
2022-03-04 20:46:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:46:17 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 12.589 | nll_loss 12.049 | ppl 4236.58 | wps 43942.6 | wpb 510.9 | bsz 1 | num_updates 11944 | best_loss 7.962
2022-03-04 20:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11944 updates
2022-03-04 20:46:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:46:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:46:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 124 @ 11944 updates, score 12.589) (writing took 2.3631411530077457 seconds)
2022-03-04 20:46:19 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 20:46:19 | INFO | train | epoch 124 | loss 2.329 | nll_loss 1.307 | ppl 2.47 | wps 24428.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11944 | lr 0.000289351 | gnorm 1.041 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 32301
2022-03-04 20:46:19 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 20:46:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:48:43 | INFO | train_inner | epoch 125:     56 / 97 loss=2.327, nll_loss=1.305, ppl=2.47, wps=24706, ups=0.38, wpb=65495, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.039, loss_scale=16, train_wall=235, gb_free=21, wall=32445
2022-03-04 20:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:50:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:50:34 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 12.598 | nll_loss 12.066 | ppl 4286.56 | wps 44092.7 | wpb 510.9 | bsz 1 | num_updates 12040 | best_loss 7.962
2022-03-04 20:50:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12040 updates
2022-03-04 20:50:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:50:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:50:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 125 @ 12040 updates, score 12.598) (writing took 2.4082098100334406 seconds)
2022-03-04 20:50:36 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 20:50:36 | INFO | train | epoch 125 | loss 2.319 | nll_loss 1.296 | ppl 2.46 | wps 24433.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12040 | lr 0.000288195 | gnorm 1.024 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 32558
2022-03-04 20:50:36 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 20:50:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:53:11 | INFO | train_inner | epoch 126:     60 / 97 loss=2.312, nll_loss=1.289, ppl=2.44, wps=24463.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=1.019, loss_scale=16, train_wall=237, gb_free=21, wall=32713
2022-03-04 20:54:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:54:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:54:51 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 12.613 | nll_loss 12.08 | ppl 4328.21 | wps 44755.5 | wpb 510.9 | bsz 1 | num_updates 12136 | best_loss 7.962
2022-03-04 20:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12136 updates
2022-03-04 20:54:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:54:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:54:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 126 @ 12136 updates, score 12.613) (writing took 2.4069481566548347 seconds)
2022-03-04 20:54:53 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 20:54:53 | INFO | train | epoch 126 | loss 2.309 | nll_loss 1.286 | ppl 2.44 | wps 24461.4 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 12136 | lr 0.000287053 | gnorm 1.025 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 32815
2022-03-04 20:54:53 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 20:54:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:57:37 | INFO | train_inner | epoch 127:     64 / 97 loss=2.305, nll_loss=1.282, ppl=2.43, wps=24596.6, ups=0.38, wpb=65533.9, bsz=128, num_updates=12200, lr=0.000286299, gnorm=1.035, loss_scale=16, train_wall=236, gb_free=21, wall=32979
2022-03-04 20:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:59:07 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 12.585 | nll_loss 12.047 | ppl 4231.55 | wps 44769.8 | wpb 510.9 | bsz 1 | num_updates 12233 | best_loss 7.962
2022-03-04 20:59:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12233 updates
2022-03-04 20:59:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:59:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 20:59:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 127 @ 12233 updates, score 12.585) (writing took 2.397393811494112 seconds)
2022-03-04 20:59:09 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 20:59:09 | INFO | train | epoch 127 | loss 2.303 | nll_loss 1.279 | ppl 2.43 | wps 24839.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12233 | lr 0.000285913 | gnorm 1.031 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33071
2022-03-04 20:59:09 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 20:59:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:00:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:02:04 | INFO | train_inner | epoch 128:     68 / 97 loss=2.297, nll_loss=1.273, ppl=2.42, wps=24611.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=1.015, loss_scale=16, train_wall=236, gb_free=21, wall=33245
2022-03-04 21:03:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:03:23 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 12.603 | nll_loss 12.067 | ppl 4290.9 | wps 44882.1 | wpb 510.9 | bsz 1 | num_updates 12329 | best_loss 7.962
2022-03-04 21:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12329 updates
2022-03-04 21:03:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:03:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:03:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 128 @ 12329 updates, score 12.603) (writing took 2.4037182899191976 seconds)
2022-03-04 21:03:25 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 21:03:25 | INFO | train | epoch 128 | loss 2.292 | nll_loss 1.268 | ppl 2.41 | wps 24568.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 12329 | lr 0.000284797 | gnorm 1.025 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33327
2022-03-04 21:03:25 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 21:03:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:04:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:06:30 | INFO | train_inner | epoch 129:     72 / 97 loss=2.29, nll_loss=1.266, ppl=2.41, wps=24601.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=1.027, loss_scale=8, train_wall=236, gb_free=21, wall=33512
2022-03-04 21:07:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:07:39 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 12.612 | nll_loss 12.076 | ppl 4317.34 | wps 44720.7 | wpb 510.9 | bsz 1 | num_updates 12425 | best_loss 7.962
2022-03-04 21:07:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12425 updates
2022-03-04 21:07:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:07:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:07:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 129 @ 12425 updates, score 12.612) (writing took 2.4597306046634912 seconds)
2022-03-04 21:07:41 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 21:07:41 | INFO | train | epoch 129 | loss 2.285 | nll_loss 1.26 | ppl 2.4 | wps 24557.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12425 | lr 0.000283695 | gnorm 1.012 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 33583
2022-03-04 21:07:41 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 21:07:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:10:54 | INFO | train_inner | epoch 130:     75 / 97 loss=2.278, nll_loss=1.253, ppl=2.38, wps=24832.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=1.014, loss_scale=16, train_wall=234, gb_free=21, wall=33775
2022-03-04 21:11:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:11:55 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 12.7 | nll_loss 12.167 | ppl 4598.96 | wps 44583.9 | wpb 510.9 | bsz 1 | num_updates 12522 | best_loss 7.962
2022-03-04 21:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12522 updates
2022-03-04 21:11:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:11:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:11:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 130 @ 12522 updates, score 12.7) (writing took 2.4425243558362126 seconds)
2022-03-04 21:11:57 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 21:11:57 | INFO | train | epoch 130 | loss 2.276 | nll_loss 1.251 | ppl 2.38 | wps 24806.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12522 | lr 0.000282594 | gnorm 1.018 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33839
2022-03-04 21:11:57 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 21:11:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:15:17 | INFO | train_inner | epoch 131:     78 / 97 loss=2.268, nll_loss=1.243, ppl=2.37, wps=24833, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=1.023, loss_scale=16, train_wall=234, gb_free=21, wall=34039
2022-03-04 21:15:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:16:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:16:11 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 12.684 | nll_loss 12.154 | ppl 4556.32 | wps 45179.7 | wpb 510.9 | bsz 1 | num_updates 12618 | best_loss 7.962
2022-03-04 21:16:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12618 updates
2022-03-04 21:16:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:16:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:16:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 131 @ 12618 updates, score 12.684) (writing took 2.4377993531525135 seconds)
2022-03-04 21:16:13 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 21:16:13 | INFO | train | epoch 131 | loss 2.266 | nll_loss 1.241 | ppl 2.36 | wps 24566.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 12618 | lr 0.000281517 | gnorm 1.028 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34095
2022-03-04 21:16:13 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 21:16:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:19:43 | INFO | train_inner | epoch 132:     82 / 97 loss=2.26, nll_loss=1.235, ppl=2.35, wps=24616.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=1.029, loss_scale=16, train_wall=236, gb_free=21, wall=34305
2022-03-04 21:20:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:20:27 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 12.676 | nll_loss 12.148 | ppl 4537.42 | wps 44617.1 | wpb 510.9 | bsz 1 | num_updates 12715 | best_loss 7.962
2022-03-04 21:20:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12715 updates
2022-03-04 21:20:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:20:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 132 @ 12715 updates, score 12.676) (writing took 2.4305518809705973 seconds)
2022-03-04 21:20:29 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 21:20:29 | INFO | train | epoch 132 | loss 2.258 | nll_loss 1.232 | ppl 2.35 | wps 24823.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12715 | lr 0.000280441 | gnorm 1.023 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34351
2022-03-04 21:20:29 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 21:20:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:21:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:24:10 | INFO | train_inner | epoch 133:     86 / 97 loss=2.253, nll_loss=1.226, ppl=2.34, wps=24587.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=1.02, loss_scale=16, train_wall=236, gb_free=21, wall=34571
2022-03-04 21:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:24:43 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 12.697 | nll_loss 12.167 | ppl 4599.06 | wps 44584.7 | wpb 510.9 | bsz 1 | num_updates 12811 | best_loss 7.962
2022-03-04 21:24:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12811 updates
2022-03-04 21:24:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:24:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:24:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 133 @ 12811 updates, score 12.697) (writing took 2.4952428666874766 seconds)
2022-03-04 21:24:45 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 21:24:45 | INFO | train | epoch 133 | loss 2.249 | nll_loss 1.223 | ppl 2.33 | wps 24542.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12811 | lr 0.000279388 | gnorm 1.019 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34607
2022-03-04 21:24:45 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 21:24:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:27:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:28:36 | INFO | train_inner | epoch 134:     90 / 97 loss=2.243, nll_loss=1.216, ppl=2.32, wps=24590.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=1.014, loss_scale=16, train_wall=236, gb_free=21, wall=34838
2022-03-04 21:28:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:28:59 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 12.741 | nll_loss 12.216 | ppl 4758.17 | wps 44802.9 | wpb 510.9 | bsz 1 | num_updates 12907 | best_loss 7.962
2022-03-04 21:28:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12907 updates
2022-03-04 21:28:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:29:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 134 @ 12907 updates, score 12.741) (writing took 2.4437821181491017 seconds)
2022-03-04 21:29:01 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 21:29:01 | INFO | train | epoch 134 | loss 2.242 | nll_loss 1.215 | ppl 2.32 | wps 24558.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12907 | lr 0.000278348 | gnorm 1.016 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34863
2022-03-04 21:29:01 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 21:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:33:00 | INFO | train_inner | epoch 135:     93 / 97 loss=2.239, nll_loss=1.212, ppl=2.32, wps=24836.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=1.01, loss_scale=16, train_wall=234, gb_free=21, wall=35101
2022-03-04 21:33:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:33:15 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 12.715 | nll_loss 12.188 | ppl 4666.98 | wps 44728.9 | wpb 510.9 | bsz 1 | num_updates 13004 | best_loss 7.962
2022-03-04 21:33:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13004 updates
2022-03-04 21:33:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:33:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:33:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 135 @ 13004 updates, score 12.715) (writing took 2.4511669827625155 seconds)
2022-03-04 21:33:17 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 21:33:17 | INFO | train | epoch 135 | loss 2.235 | nll_loss 1.207 | ppl 2.31 | wps 24814.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13004 | lr 0.000277307 | gnorm 1.008 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 35119
2022-03-04 21:33:17 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 21:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:33:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:37:26 | INFO | train_inner | epoch 136:     97 / 97 loss=2.229, nll_loss=1.201, ppl=2.3, wps=24600.8, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=13100, lr=0.000276289, gnorm=1.016, loss_scale=16, train_wall=236, gb_free=21, wall=35368
2022-03-04 21:37:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:37:31 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 12.722 | nll_loss 12.197 | ppl 4693.82 | wps 44736 | wpb 510.9 | bsz 1 | num_updates 13100 | best_loss 7.962
2022-03-04 21:37:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13100 updates
2022-03-04 21:37:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:37:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:37:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 136 @ 13100 updates, score 12.722) (writing took 2.424645060673356 seconds)
2022-03-04 21:37:33 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 21:37:33 | INFO | train | epoch 136 | loss 2.227 | nll_loss 1.199 | ppl 2.3 | wps 24565.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13100 | lr 0.000276289 | gnorm 1.014 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35375
2022-03-04 21:37:33 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 21:37:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:39:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:41:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:41:47 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 12.739 | nll_loss 12.214 | ppl 4751.84 | wps 44762.4 | wpb 510.9 | bsz 1 | num_updates 13196 | best_loss 7.962
2022-03-04 21:41:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13196 updates
2022-03-04 21:41:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:41:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:41:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 137 @ 13196 updates, score 12.739) (writing took 2.4344385471194983 seconds)
2022-03-04 21:41:49 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 21:41:49 | INFO | train | epoch 137 | loss 2.219 | nll_loss 1.191 | ppl 2.28 | wps 24552.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13196 | lr 0.000275283 | gnorm 1.003 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35631
2022-03-04 21:41:49 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 21:41:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:42:00 | INFO | train_inner | epoch 138:      4 / 97 loss=2.217, nll_loss=1.189, ppl=2.28, wps=23919.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13200, lr=0.000275241, gnorm=1.002, loss_scale=16, train_wall=236, gb_free=21, wall=35641
2022-03-04 21:44:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:46:03 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 12.761 | nll_loss 12.236 | ppl 4824.06 | wps 44846.5 | wpb 510.9 | bsz 1 | num_updates 13292 | best_loss 7.962
2022-03-04 21:46:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13292 updates
2022-03-04 21:46:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:46:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:46:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 138 @ 13292 updates, score 12.761) (writing took 2.4508555252104998 seconds)
2022-03-04 21:46:05 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 21:46:05 | INFO | train | epoch 138 | loss 2.21 | nll_loss 1.182 | ppl 2.27 | wps 24557.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13292 | lr 0.000274287 | gnorm 1.002 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35887
2022-03-04 21:46:05 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 21:46:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:46:26 | INFO | train_inner | epoch 139:      8 / 97 loss=2.209, nll_loss=1.18, ppl=2.27, wps=24602.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13300, lr=0.000274204, gnorm=1.001, loss_scale=16, train_wall=236, gb_free=21, wall=35908
2022-03-04 21:50:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:50:19 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 12.755 | nll_loss 12.235 | ppl 4820.84 | wps 44621.2 | wpb 510.9 | bsz 1 | num_updates 13389 | best_loss 7.962
2022-03-04 21:50:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13389 updates
2022-03-04 21:50:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:50:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 139 @ 13389 updates, score 12.755) (writing took 2.4409280102699995 seconds)
2022-03-04 21:50:21 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 21:50:21 | INFO | train | epoch 139 | loss 2.204 | nll_loss 1.175 | ppl 2.26 | wps 24827 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13389 | lr 0.000273291 | gnorm 1.006 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36143
2022-03-04 21:50:21 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 21:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:50:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:50:52 | INFO | train_inner | epoch 140:     12 / 97 loss=2.203, nll_loss=1.173, ppl=2.26, wps=24611.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13400, lr=0.000273179, gnorm=1.005, loss_scale=16, train_wall=236, gb_free=21, wall=36174
2022-03-04 21:54:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:54:35 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 12.74 | nll_loss 12.212 | ppl 4743.26 | wps 44365.1 | wpb 510.9 | bsz 1 | num_updates 13485 | best_loss 7.962
2022-03-04 21:54:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13485 updates
2022-03-04 21:54:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:54:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:54:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 140 @ 13485 updates, score 12.74) (writing took 2.4390518674626946 seconds)
2022-03-04 21:54:37 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 21:54:37 | INFO | train | epoch 140 | loss 2.196 | nll_loss 1.167 | ppl 2.25 | wps 24549.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13485 | lr 0.000272317 | gnorm 1.005 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36399
2022-03-04 21:54:37 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 21:54:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:55:16 | INFO | train_inner | epoch 141:     15 / 97 loss=2.191, nll_loss=1.161, ppl=2.24, wps=24820.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1.004, loss_scale=16, train_wall=234, gb_free=21, wall=36437
2022-03-04 21:56:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:58:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:58:51 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 12.803 | nll_loss 12.275 | ppl 4955.91 | wps 44687.5 | wpb 510.9 | bsz 1 | num_updates 13581 | best_loss 7.962
2022-03-04 21:58:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13581 updates
2022-03-04 21:58:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:58:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 21:58:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 141 @ 13581 updates, score 12.803) (writing took 2.455724044702947 seconds)
2022-03-04 21:58:53 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 21:58:53 | INFO | train | epoch 141 | loss 2.189 | nll_loss 1.158 | ppl 2.23 | wps 24553.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13581 | lr 0.000271353 | gnorm 1.008 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36655
2022-03-04 21:58:53 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 21:58:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:59:42 | INFO | train_inner | epoch 142:     19 / 97 loss=2.187, nll_loss=1.157, ppl=2.23, wps=24592.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=1.009, loss_scale=16, train_wall=236, gb_free=21, wall=36704
2022-03-04 22:01:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:03:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:03:07 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 12.813 | nll_loss 12.283 | ppl 4983.3 | wps 44784.4 | wpb 510.9 | bsz 1 | num_updates 13677 | best_loss 7.962
2022-03-04 22:03:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13677 updates
2022-03-04 22:03:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:03:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:03:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 142 @ 13677 updates, score 12.813) (writing took 2.3927640644833446 seconds)
2022-03-04 22:03:09 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 22:03:09 | INFO | train | epoch 142 | loss 2.184 | nll_loss 1.154 | ppl 2.23 | wps 24559.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13677 | lr 0.000270399 | gnorm 1.006 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36911
2022-03-04 22:03:09 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 22:03:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:04:08 | INFO | train_inner | epoch 143:     23 / 97 loss=2.181, nll_loss=1.15, ppl=2.22, wps=24599.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1.006, loss_scale=16, train_wall=236, gb_free=21, wall=36970
2022-03-04 22:07:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:07:23 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 12.8 | nll_loss 12.273 | ppl 4949.4 | wps 44612.6 | wpb 510.9 | bsz 1 | num_updates 13774 | best_loss 7.962
2022-03-04 22:07:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13774 updates
2022-03-04 22:07:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:07:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:07:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 143 @ 13774 updates, score 12.8) (writing took 2.4288871781900525 seconds)
2022-03-04 22:07:25 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 22:07:25 | INFO | train | epoch 143 | loss 2.176 | nll_loss 1.145 | ppl 2.21 | wps 24821.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13774 | lr 0.000269445 | gnorm 1 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37167
2022-03-04 22:07:25 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 22:07:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:07:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:08:35 | INFO | train_inner | epoch 144:     27 / 97 loss=2.174, nll_loss=1.143, ppl=2.21, wps=24592.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=0.992, loss_scale=16, train_wall=236, gb_free=21, wall=37236
2022-03-04 22:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:11:39 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 12.848 | nll_loss 12.325 | ppl 5132.39 | wps 44654.8 | wpb 510.9 | bsz 1 | num_updates 13870 | best_loss 7.962
2022-03-04 22:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13870 updates
2022-03-04 22:11:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:11:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:11:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 144 @ 13870 updates, score 12.848) (writing took 2.4341296795755625 seconds)
2022-03-04 22:11:41 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 22:11:41 | INFO | train | epoch 144 | loss 2.169 | nll_loss 1.137 | ppl 2.2 | wps 24546.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13870 | lr 0.000268511 | gnorm 0.99 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37423
2022-03-04 22:11:41 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 22:11:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:12:58 | INFO | train_inner | epoch 145:     30 / 97 loss=2.167, nll_loss=1.136, ppl=2.2, wps=24835.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=0.99, loss_scale=16, train_wall=234, gb_free=21, wall=37500
2022-03-04 22:13:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:15:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:15:55 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 12.803 | nll_loss 12.275 | ppl 4957.6 | wps 44632.8 | wpb 510.9 | bsz 1 | num_updates 13966 | best_loss 7.962
2022-03-04 22:15:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13966 updates
2022-03-04 22:15:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:15:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:15:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 145 @ 13966 updates, score 12.803) (writing took 2.4164689630270004 seconds)
2022-03-04 22:15:57 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 22:15:57 | INFO | train | epoch 145 | loss 2.162 | nll_loss 1.131 | ppl 2.19 | wps 24578 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 13966 | lr 0.000267586 | gnorm 0.998 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37679
2022-03-04 22:15:57 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 22:15:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:17:24 | INFO | train_inner | epoch 146:     34 / 97 loss=2.16, nll_loss=1.129, ppl=2.19, wps=24615.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=1.004, loss_scale=16, train_wall=236, gb_free=21, wall=37766
2022-03-04 22:19:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:20:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:20:11 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 12.854 | nll_loss 12.329 | ppl 5145.6 | wps 44743.9 | wpb 510.9 | bsz 1 | num_updates 14062 | best_loss 7.962
2022-03-04 22:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14062 updates
2022-03-04 22:20:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:20:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:20:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 146 @ 14062 updates, score 12.854) (writing took 2.47416128590703 seconds)
2022-03-04 22:20:13 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 22:20:13 | INFO | train | epoch 146 | loss 2.155 | nll_loss 1.124 | ppl 2.18 | wps 24563.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 14062 | lr 0.000266671 | gnorm 0.997 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37935
2022-03-04 22:20:13 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 22:20:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:21:51 | INFO | train_inner | epoch 147:     38 / 97 loss=2.151, nll_loss=1.119, ppl=2.17, wps=24598.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=1.001, loss_scale=16, train_wall=236, gb_free=21, wall=38032
2022-03-04 22:24:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:24:27 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 12.829 | nll_loss 12.308 | ppl 5071.06 | wps 44734.2 | wpb 510.9 | bsz 1 | num_updates 14159 | best_loss 7.962
2022-03-04 22:24:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14159 updates
2022-03-04 22:24:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:24:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:24:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 147 @ 14159 updates, score 12.829) (writing took 2.431772676296532 seconds)
2022-03-04 22:24:29 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 22:24:29 | INFO | train | epoch 147 | loss 2.15 | nll_loss 1.118 | ppl 2.17 | wps 24821 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14159 | lr 0.000265756 | gnorm 0.998 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38191
2022-03-04 22:24:29 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 22:24:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:24:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:26:17 | INFO | train_inner | epoch 148:     42 / 97 loss=2.146, nll_loss=1.114, ppl=2.16, wps=24604.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=0.988, loss_scale=16, train_wall=236, gb_free=21, wall=38299
2022-03-04 22:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:28:43 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 12.864 | nll_loss 12.346 | ppl 5206.78 | wps 44580.6 | wpb 510.9 | bsz 1 | num_updates 14255 | best_loss 7.962
2022-03-04 22:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14255 updates
2022-03-04 22:28:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:28:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:28:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 148 @ 14255 updates, score 12.864) (writing took 2.4391432562842965 seconds)
2022-03-04 22:28:45 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 22:28:45 | INFO | train | epoch 148 | loss 2.142 | nll_loss 1.11 | ppl 2.16 | wps 24556.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14255 | lr 0.00026486 | gnorm 0.993 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38447
2022-03-04 22:28:45 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 22:28:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:30:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:30:43 | INFO | train_inner | epoch 149:     46 / 97 loss=2.141, nll_loss=1.109, ppl=2.16, wps=24596.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=0.993, loss_scale=16, train_wall=236, gb_free=21, wall=38565
2022-03-04 22:32:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:32:59 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 12.877 | nll_loss 12.355 | ppl 5238.32 | wps 44345 | wpb 510.9 | bsz 1 | num_updates 14351 | best_loss 7.962
2022-03-04 22:32:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14351 updates
2022-03-04 22:32:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:33:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:33:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 149 @ 14351 updates, score 12.877) (writing took 2.4736973363906145 seconds)
2022-03-04 22:33:01 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-04 22:33:01 | INFO | train | epoch 149 | loss 2.136 | nll_loss 1.104 | ppl 2.15 | wps 24539.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14351 | lr 0.000263973 | gnorm 0.993 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38703
2022-03-04 22:33:01 | INFO | fairseq.trainer | begin training epoch 150
2022-03-04 22:33:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:35:07 | INFO | train_inner | epoch 150:     49 / 97 loss=2.134, nll_loss=1.101, ppl=2.14, wps=24804.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=0.989, loss_scale=16, train_wall=234, gb_free=21, wall=38829
2022-03-04 22:36:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:37:15 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 12.869 | nll_loss 12.345 | ppl 5204.15 | wps 44788.1 | wpb 510.9 | bsz 1 | num_updates 14447 | best_loss 7.962
2022-03-04 22:37:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14447 updates
2022-03-04 22:37:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:37:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:37:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 150 @ 14447 updates, score 12.869) (writing took 2.4311966616660357 seconds)
2022-03-04 22:37:17 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-04 22:37:17 | INFO | train | epoch 150 | loss 2.131 | nll_loss 1.098 | ppl 2.14 | wps 24553.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14447 | lr 0.000263094 | gnorm 0.994 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38959
2022-03-04 22:37:17 | INFO | fairseq.trainer | begin training epoch 151
2022-03-04 22:37:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:39:33 | INFO | train_inner | epoch 151:     53 / 97 loss=2.128, nll_loss=1.095, ppl=2.14, wps=24611.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=1, loss_scale=16, train_wall=236, gb_free=21, wall=39095
2022-03-04 22:41:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:41:31 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 12.895 | nll_loss 12.377 | ppl 5318.5 | wps 44694.8 | wpb 510.9 | bsz 1 | num_updates 14544 | best_loss 7.962
2022-03-04 22:41:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14544 updates
2022-03-04 22:41:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:41:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 151 @ 14544 updates, score 12.895) (writing took 2.4248426584526896 seconds)
2022-03-04 22:41:33 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-04 22:41:33 | INFO | train | epoch 151 | loss 2.125 | nll_loss 1.092 | ppl 2.13 | wps 24826.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14544 | lr 0.000262215 | gnorm 0.995 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39215
2022-03-04 22:41:33 | INFO | fairseq.trainer | begin training epoch 152
2022-03-04 22:41:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:42:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:43:59 | INFO | train_inner | epoch 152:     57 / 97 loss=2.12, nll_loss=1.087, ppl=2.12, wps=24599.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14600, lr=0.000261712, gnorm=0.988, loss_scale=16, train_wall=236, gb_free=21, wall=39361
2022-03-04 22:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:45:47 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 12.866 | nll_loss 12.348 | ppl 5214.97 | wps 44596.3 | wpb 510.9 | bsz 1 | num_updates 14640 | best_loss 7.962
2022-03-04 22:45:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14640 updates
2022-03-04 22:45:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:45:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:45:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 152 @ 14640 updates, score 12.866) (writing took 2.443446546792984 seconds)
2022-03-04 22:45:49 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-04 22:45:49 | INFO | train | epoch 152 | loss 2.119 | nll_loss 1.085 | ppl 2.12 | wps 24560 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 14640 | lr 0.000261354 | gnorm 0.988 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39471
2022-03-04 22:45:49 | INFO | fairseq.trainer | begin training epoch 153
2022-03-04 22:45:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:47:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:48:26 | INFO | train_inner | epoch 153:     61 / 97 loss=2.116, nll_loss=1.082, ppl=2.12, wps=24591.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=0.99, loss_scale=16, train_wall=236, gb_free=21, wall=39628
2022-03-04 22:49:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:50:03 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 12.907 | nll_loss 12.389 | ppl 5365.25 | wps 44774.4 | wpb 510.9 | bsz 1 | num_updates 14736 | best_loss 7.962
2022-03-04 22:50:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14736 updates
2022-03-04 22:50:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:50:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:50:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 153 @ 14736 updates, score 12.907) (writing took 2.431935858912766 seconds)
2022-03-04 22:50:05 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-04 22:50:05 | INFO | train | epoch 153 | loss 2.113 | nll_loss 1.08 | ppl 2.11 | wps 24549.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14736 | lr 0.000260501 | gnorm 0.994 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39727
2022-03-04 22:50:05 | INFO | fairseq.trainer | begin training epoch 154
2022-03-04 22:50:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:52:49 | INFO | train_inner | epoch 154:     64 / 97 loss=2.11, nll_loss=1.076, ppl=2.11, wps=24835.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=0.985, loss_scale=16, train_wall=234, gb_free=21, wall=39891
2022-03-04 22:53:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:54:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:54:19 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 12.902 | nll_loss 12.381 | ppl 5335.36 | wps 44711 | wpb 510.9 | bsz 1 | num_updates 14832 | best_loss 7.962
2022-03-04 22:54:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14832 updates
2022-03-04 22:54:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:54:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 154 @ 14832 updates, score 12.902) (writing took 2.454016433097422 seconds)
2022-03-04 22:54:21 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-04 22:54:21 | INFO | train | epoch 154 | loss 2.106 | nll_loss 1.072 | ppl 2.1 | wps 24559.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 14832 | lr 0.000259657 | gnorm 0.972 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39983
2022-03-04 22:54:21 | INFO | fairseq.trainer | begin training epoch 155
2022-03-04 22:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:57:16 | INFO | train_inner | epoch 155:     68 / 97 loss=2.105, nll_loss=1.071, ppl=2.1, wps=24589.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=0.972, loss_scale=16, train_wall=236, gb_free=21, wall=40158
2022-03-04 22:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:58:35 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 12.953 | nll_loss 12.442 | ppl 5563.23 | wps 44690.1 | wpb 510.9 | bsz 1 | num_updates 14929 | best_loss 7.962
2022-03-04 22:58:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14929 updates
2022-03-04 22:58:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:58:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 22:58:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 155 @ 14929 updates, score 12.953) (writing took 2.420588605105877 seconds)
2022-03-04 22:58:37 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-04 22:58:37 | INFO | train | epoch 155 | loss 2.102 | nll_loss 1.068 | ppl 2.1 | wps 24806.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14929 | lr 0.000258812 | gnorm 0.976 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40239
2022-03-04 22:58:37 | INFO | fairseq.trainer | begin training epoch 156
2022-03-04 22:58:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:59:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:01:42 | INFO | train_inner | epoch 156:     72 / 97 loss=2.099, nll_loss=1.064, ppl=2.09, wps=24590.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=0.978, loss_scale=16, train_wall=236, gb_free=21, wall=40424
2022-03-04 23:02:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:02:51 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 12.95 | nll_loss 12.438 | ppl 5547.2 | wps 44906.8 | wpb 510.9 | bsz 1 | num_updates 15025 | best_loss 7.962
2022-03-04 23:02:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15025 updates
2022-03-04 23:02:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:02:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:02:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 156 @ 15025 updates, score 12.95) (writing took 2.445828417316079 seconds)
2022-03-04 23:02:53 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-04 23:02:53 | INFO | train | epoch 156 | loss 2.095 | nll_loss 1.061 | ppl 2.09 | wps 24552.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15025 | lr 0.000257984 | gnorm 0.971 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40495
2022-03-04 23:02:53 | INFO | fairseq.trainer | begin training epoch 157
2022-03-04 23:02:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:04:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:06:08 | INFO | train_inner | epoch 157:     76 / 97 loss=2.092, nll_loss=1.057, ppl=2.08, wps=24598.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=0.985, loss_scale=16, train_wall=236, gb_free=21, wall=40690
2022-03-04 23:07:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:07:07 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 12.969 | nll_loss 12.459 | ppl 5629.15 | wps 44610.4 | wpb 510.9 | bsz 1 | num_updates 15121 | best_loss 7.962
2022-03-04 23:07:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15121 updates
2022-03-04 23:07:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:07:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:07:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 157 @ 15121 updates, score 12.969) (writing took 2.4484875705093145 seconds)
2022-03-04 23:07:09 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-04 23:07:09 | INFO | train | epoch 157 | loss 2.09 | nll_loss 1.055 | ppl 2.08 | wps 24558.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15121 | lr 0.000257164 | gnorm 0.993 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40751
2022-03-04 23:07:09 | INFO | fairseq.trainer | begin training epoch 158
2022-03-04 23:07:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:10:32 | INFO | train_inner | epoch 158:     79 / 97 loss=2.087, nll_loss=1.052, ppl=2.07, wps=24831.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=0.977, loss_scale=32, train_wall=234, gb_free=21, wall=40954
2022-03-04 23:10:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:11:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:11:23 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 12.967 | nll_loss 12.454 | ppl 5609.9 | wps 44272.8 | wpb 510.9 | bsz 1 | num_updates 15217 | best_loss 7.962
2022-03-04 23:11:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15217 updates
2022-03-04 23:11:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:11:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:11:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 158 @ 15217 updates, score 12.967) (writing took 2.4360231421887875 seconds)
2022-03-04 23:11:26 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-04 23:11:26 | INFO | train | epoch 158 | loss 2.084 | nll_loss 1.049 | ppl 2.07 | wps 24552.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15217 | lr 0.000256351 | gnorm 0.972 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41007
2022-03-04 23:11:26 | INFO | fairseq.trainer | begin training epoch 159
2022-03-04 23:11:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:14:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:15:02 | INFO | train_inner | epoch 159:     84 / 97 loss=2.082, nll_loss=1.047, ppl=2.07, wps=24235.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=0.98, loss_scale=8, train_wall=240, gb_free=21, wall=41224
2022-03-04 23:15:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:15:41 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 12.931 | nll_loss 12.418 | ppl 5474.31 | wps 44080.8 | wpb 510.9 | bsz 1 | num_updates 15313 | best_loss 7.962
2022-03-04 23:15:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15313 updates
2022-03-04 23:15:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 159 @ 15313 updates, score 12.931) (writing took 2.247016290202737 seconds)
2022-03-04 23:15:43 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-04 23:15:43 | INFO | train | epoch 159 | loss 2.079 | nll_loss 1.044 | ppl 2.06 | wps 24421.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15313 | lr 0.000255546 | gnorm 0.978 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 41265
2022-03-04 23:15:43 | INFO | fairseq.trainer | begin training epoch 160
2022-03-04 23:15:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:19:27 | INFO | train_inner | epoch 160:     87 / 97 loss=2.077, nll_loss=1.041, ppl=2.06, wps=24734.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=0.98, loss_scale=8, train_wall=235, gb_free=21, wall=41489
2022-03-04 23:19:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:19:58 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 12.946 | nll_loss 12.429 | ppl 5512.57 | wps 43987.3 | wpb 510.9 | bsz 1 | num_updates 15410 | best_loss 7.962
2022-03-04 23:19:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15410 updates
2022-03-04 23:19:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:20:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:20:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 160 @ 15410 updates, score 12.946) (writing took 2.3523165630176663 seconds)
2022-03-04 23:20:00 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-04 23:20:00 | INFO | train | epoch 160 | loss 2.076 | nll_loss 1.041 | ppl 2.06 | wps 24704.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15410 | lr 0.000254741 | gnorm 0.983 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 41522
2022-03-04 23:20:00 | INFO | fairseq.trainer | begin training epoch 161
2022-03-04 23:20:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:23:52 | INFO | train_inner | epoch 161:     90 / 97 loss=2.071, nll_loss=1.035, ppl=2.05, wps=24735.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=0.986, loss_scale=16, train_wall=235, gb_free=21, wall=41754
2022-03-04 23:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:24:15 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 12.942 | nll_loss 12.43 | ppl 5517.08 | wps 44324.8 | wpb 510.9 | bsz 1 | num_updates 15507 | best_loss 7.962
2022-03-04 23:24:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15507 updates
2022-03-04 23:24:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:24:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:24:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 161 @ 15507 updates, score 12.942) (writing took 2.4133415622636676 seconds)
2022-03-04 23:24:17 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-04 23:24:17 | INFO | train | epoch 161 | loss 2.069 | nll_loss 1.034 | ppl 2.05 | wps 24712 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15507 | lr 0.000253943 | gnorm 0.986 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41779
2022-03-04 23:24:17 | INFO | fairseq.trainer | begin training epoch 162
2022-03-04 23:24:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:25:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:28:19 | INFO | train_inner | epoch 162:     94 / 97 loss=2.065, nll_loss=1.029, ppl=2.04, wps=24489.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=0.985, loss_scale=16, train_wall=237, gb_free=21, wall=42021
2022-03-04 23:28:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:28:32 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 12.952 | nll_loss 12.439 | ppl 5554.64 | wps 44158.9 | wpb 510.9 | bsz 1 | num_updates 15603 | best_loss 7.962
2022-03-04 23:28:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15603 updates
2022-03-04 23:28:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:28:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:28:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 162 @ 15603 updates, score 12.952) (writing took 2.426874223165214 seconds)
2022-03-04 23:28:34 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-04 23:28:34 | INFO | train | epoch 162 | loss 2.063 | nll_loss 1.027 | ppl 2.04 | wps 24445 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15603 | lr 0.000253161 | gnorm 0.983 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 42036
2022-03-04 23:28:34 | INFO | fairseq.trainer | begin training epoch 163
2022-03-04 23:28:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:28:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:32:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:32:49 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 12.974 | nll_loss 12.466 | ppl 5656.85 | wps 44224.1 | wpb 510.9 | bsz 1 | num_updates 15699 | best_loss 7.962
2022-03-04 23:32:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15699 updates
2022-03-04 23:32:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:32:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:32:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 163 @ 15699 updates, score 12.974) (writing took 2.3496773717924953 seconds)
2022-03-04 23:32:51 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-04 23:32:51 | INFO | train | epoch 163 | loss 2.058 | nll_loss 1.023 | ppl 2.03 | wps 24451 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15699 | lr 0.000252385 | gnorm 0.98 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 42293
2022-03-04 23:32:52 | INFO | fairseq.trainer | begin training epoch 164
2022-03-04 23:32:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:32:54 | INFO | train_inner | epoch 164:      1 / 97 loss=2.06, nll_loss=1.025, ppl=2.03, wps=23819.9, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=15700, lr=0.000252377, gnorm=0.981, loss_scale=8, train_wall=237, gb_free=21, wall=42296
2022-03-04 23:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:37:06 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 13.018 | nll_loss 12.509 | ppl 5830.3 | wps 44084.2 | wpb 510.9 | bsz 1 | num_updates 15796 | best_loss 7.962
2022-03-04 23:37:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15796 updates
2022-03-04 23:37:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 164 @ 15796 updates, score 13.018) (writing took 2.457317564636469 seconds)
2022-03-04 23:37:08 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-04 23:37:08 | INFO | train | epoch 164 | loss 2.054 | nll_loss 1.018 | ppl 2.03 | wps 24723.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15796 | lr 0.000251609 | gnorm 0.968 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42550
2022-03-04 23:37:08 | INFO | fairseq.trainer | begin training epoch 165
2022-03-04 23:37:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:37:19 | INFO | train_inner | epoch 165:      4 / 97 loss=2.052, nll_loss=1.016, ppl=2.02, wps=24743.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15800, lr=0.000251577, gnorm=0.968, loss_scale=16, train_wall=234, gb_free=21, wall=42561
2022-03-04 23:40:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:41:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:41:23 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 13.021 | nll_loss 12.512 | ppl 5840.15 | wps 44362.4 | wpb 510.9 | bsz 1 | num_updates 15892 | best_loss 7.962
2022-03-04 23:41:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15892 updates
2022-03-04 23:41:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:41:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:41:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 165 @ 15892 updates, score 13.021) (writing took 2.418768304400146 seconds)
2022-03-04 23:41:25 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-04 23:41:25 | INFO | train | epoch 165 | loss 2.049 | nll_loss 1.013 | ppl 2.02 | wps 24459.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15892 | lr 0.000250848 | gnorm 0.971 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 42807
2022-03-04 23:41:26 | INFO | fairseq.trainer | begin training epoch 166
2022-03-04 23:41:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:41:46 | INFO | train_inner | epoch 166:      8 / 97 loss=2.048, nll_loss=1.012, ppl=2.02, wps=24502.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15900, lr=0.000250785, gnorm=0.971, loss_scale=16, train_wall=237, gb_free=21, wall=42828
2022-03-04 23:45:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:45:40 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 13.015 | nll_loss 12.512 | ppl 5841.48 | wps 44177.4 | wpb 510.9 | bsz 1 | num_updates 15989 | best_loss 7.962
2022-03-04 23:45:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15989 updates
2022-03-04 23:45:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:45:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 166 @ 15989 updates, score 13.015) (writing took 2.3886444084346294 seconds)
2022-03-04 23:45:42 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-04 23:45:42 | INFO | train | epoch 166 | loss 2.044 | nll_loss 1.008 | ppl 2.01 | wps 24740.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15989 | lr 0.000250086 | gnorm 0.966 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43064
2022-03-04 23:45:42 | INFO | fairseq.trainer | begin training epoch 167
2022-03-04 23:45:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:46:11 | INFO | train_inner | epoch 167:     11 / 97 loss=2.043, nll_loss=1.006, ppl=2.01, wps=24750.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16000, lr=0.00025, gnorm=0.963, loss_scale=32, train_wall=234, gb_free=21, wall=43093
2022-03-04 23:46:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:49:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:49:57 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 12.993 | nll_loss 12.486 | ppl 5736.96 | wps 43982.3 | wpb 510.9 | bsz 1 | num_updates 16085 | best_loss 7.962
2022-03-04 23:49:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16085 updates
2022-03-04 23:49:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:49:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 167 @ 16085 updates, score 12.993) (writing took 2.352402507327497 seconds)
2022-03-04 23:49:59 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-04 23:49:59 | INFO | train | epoch 167 | loss 2.04 | nll_loss 1.004 | ppl 2 | wps 24453 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16085 | lr 0.000249339 | gnorm 0.975 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43321
2022-03-04 23:49:59 | INFO | fairseq.trainer | begin training epoch 168
2022-03-04 23:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:50:38 | INFO | train_inner | epoch 168:     15 / 97 loss=2.036, nll_loss=0.999, ppl=2, wps=24502.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=0.972, loss_scale=16, train_wall=237, gb_free=21, wall=43360
2022-03-04 23:52:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:54:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:54:14 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 13.051 | nll_loss 12.547 | ppl 5986.43 | wps 44127 | wpb 510.9 | bsz 1 | num_updates 16181 | best_loss 7.962
2022-03-04 23:54:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16181 updates
2022-03-04 23:54:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:54:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 168 @ 16181 updates, score 13.051) (writing took 2.4181939465925097 seconds)
2022-03-04 23:54:16 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-04 23:54:16 | INFO | train | epoch 168 | loss 2.034 | nll_loss 0.998 | ppl 2 | wps 24479.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16181 | lr 0.000248598 | gnorm 0.978 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43578
2022-03-04 23:54:16 | INFO | fairseq.trainer | begin training epoch 169
2022-03-04 23:54:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:55:05 | INFO | train_inner | epoch 169:     19 / 97 loss=2.034, nll_loss=0.997, ppl=2, wps=24515.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=0.982, loss_scale=16, train_wall=237, gb_free=21, wall=43627
2022-03-04 23:58:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:58:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:58:31 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 12.997 | nll_loss 12.49 | ppl 5754.33 | wps 44112 | wpb 510.9 | bsz 1 | num_updates 16277 | best_loss 7.962
2022-03-04 23:58:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16277 updates
2022-03-04 23:58:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-04 23:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 169 @ 16277 updates, score 12.997) (writing took 2.318008375354111 seconds)
2022-03-04 23:58:33 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-04 23:58:33 | INFO | train | epoch 169 | loss 2.028 | nll_loss 0.991 | ppl 1.99 | wps 24472.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16277 | lr 0.000247864 | gnorm 0.961 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 43835
2022-03-04 23:58:33 | INFO | fairseq.trainer | begin training epoch 170
2022-03-04 23:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:59:32 | INFO | train_inner | epoch 170:     23 / 97 loss=2.026, nll_loss=0.989, ppl=1.98, wps=24505.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=0.958, loss_scale=16, train_wall=237, gb_free=21, wall=43894
2022-03-05 00:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:02:48 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 13.011 | nll_loss 12.508 | ppl 5825.67 | wps 44079.8 | wpb 510.9 | bsz 1 | num_updates 16374 | best_loss 7.962
2022-03-05 00:02:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16374 updates
2022-03-05 00:02:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:02:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 170 @ 16374 updates, score 13.011) (writing took 2.359307849779725 seconds)
2022-03-05 00:02:50 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-05 00:02:50 | INFO | train | epoch 170 | loss 2.026 | nll_loss 0.989 | ppl 1.98 | wps 24698.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16374 | lr 0.000247128 | gnorm 0.951 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 44092
2022-03-05 00:02:50 | INFO | fairseq.trainer | begin training epoch 171
2022-03-05 00:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:03:57 | INFO | train_inner | epoch 171:     26 / 97 loss=2.023, nll_loss=0.985, ppl=1.98, wps=24724.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=0.947, loss_scale=32, train_wall=235, gb_free=21, wall=44159
2022-03-05 00:04:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:07:05 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 13.066 | nll_loss 12.565 | ppl 6059.9 | wps 44157.8 | wpb 510.9 | bsz 1 | num_updates 16470 | best_loss 7.962
2022-03-05 00:07:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16470 updates
2022-03-05 00:07:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:07:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:07:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 171 @ 16470 updates, score 13.066) (writing took 2.3999430295079947 seconds)
2022-03-05 00:07:07 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-05 00:07:07 | INFO | train | epoch 171 | loss 2.02 | nll_loss 0.983 | ppl 1.98 | wps 24462.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16470 | lr 0.000246407 | gnorm 0.96 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44349
2022-03-05 00:07:07 | INFO | fairseq.trainer | begin training epoch 172
2022-03-05 00:07:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:08:25 | INFO | train_inner | epoch 172:     30 / 97 loss=2.019, nll_loss=0.982, ppl=1.98, wps=24499.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=0.957, loss_scale=16, train_wall=237, gb_free=21, wall=44426
2022-03-05 00:09:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:11:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:11:22 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 13.079 | nll_loss 12.573 | ppl 6095.14 | wps 44097 | wpb 510.9 | bsz 1 | num_updates 16566 | best_loss 7.962
2022-03-05 00:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16566 updates
2022-03-05 00:11:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 172 @ 16566 updates, score 13.079) (writing took 2.4043314959853888 seconds)
2022-03-05 00:11:24 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-05 00:11:24 | INFO | train | epoch 172 | loss 2.016 | nll_loss 0.979 | ppl 1.97 | wps 24469.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16566 | lr 0.000245692 | gnorm 0.956 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44606
2022-03-05 00:11:24 | INFO | fairseq.trainer | begin training epoch 173
2022-03-05 00:11:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:12:52 | INFO | train_inner | epoch 173:     34 / 97 loss=2.016, nll_loss=0.979, ppl=1.97, wps=24501.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=0.97, loss_scale=16, train_wall=237, gb_free=21, wall=44694
2022-03-05 00:15:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:15:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:15:39 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 13.05 | nll_loss 12.546 | ppl 5980.44 | wps 44089 | wpb 510.9 | bsz 1 | num_updates 16662 | best_loss 7.962
2022-03-05 00:15:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16662 updates
2022-03-05 00:15:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:15:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:15:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 173 @ 16662 updates, score 13.05) (writing took 2.4341118801385164 seconds)
2022-03-05 00:15:41 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-05 00:15:41 | INFO | train | epoch 173 | loss 2.012 | nll_loss 0.975 | ppl 1.97 | wps 24454.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16662 | lr 0.000244983 | gnorm 0.971 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 44863
2022-03-05 00:15:41 | INFO | fairseq.trainer | begin training epoch 174
2022-03-05 00:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:17:19 | INFO | train_inner | epoch 174:     38 / 97 loss=2.009, nll_loss=0.971, ppl=1.96, wps=24497.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=0.962, loss_scale=16, train_wall=237, gb_free=21, wall=44961
2022-03-05 00:19:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:19:56 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 12.968 | nll_loss 12.462 | ppl 5640.96 | wps 44123.6 | wpb 510.9 | bsz 1 | num_updates 16759 | best_loss 7.962
2022-03-05 00:19:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16759 updates
2022-03-05 00:19:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:19:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 174 @ 16759 updates, score 12.968) (writing took 2.3138808254152536 seconds)
2022-03-05 00:19:58 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-05 00:19:58 | INFO | train | epoch 174 | loss 2.008 | nll_loss 0.97 | ppl 1.96 | wps 24722.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16759 | lr 0.000244273 | gnorm 0.963 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 45120
2022-03-05 00:19:58 | INFO | fairseq.trainer | begin training epoch 175
2022-03-05 00:19:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:21:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:21:47 | INFO | train_inner | epoch 175:     42 / 97 loss=2.006, nll_loss=0.968, ppl=1.96, wps=24506.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=0.962, loss_scale=16, train_wall=237, gb_free=21, wall=45228
2022-03-05 00:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:24:13 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 13.021 | nll_loss 12.513 | ppl 5846.07 | wps 44222.6 | wpb 510.9 | bsz 1 | num_updates 16855 | best_loss 7.962
2022-03-05 00:24:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16855 updates
2022-03-05 00:24:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:24:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:24:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 175 @ 16855 updates, score 13.021) (writing took 2.597569356672466 seconds)
2022-03-05 00:24:16 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-05 00:24:16 | INFO | train | epoch 175 | loss 2.002 | nll_loss 0.964 | ppl 1.95 | wps 24444.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16855 | lr 0.000243577 | gnorm 0.96 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 45377
2022-03-05 00:24:16 | INFO | fairseq.trainer | begin training epoch 176
2022-03-05 00:24:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:26:11 | INFO | train_inner | epoch 176:     45 / 97 loss=1.999, nll_loss=0.961, ppl=1.95, wps=24718.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=16900, lr=0.000243252, gnorm=0.957, loss_scale=16, train_wall=235, gb_free=21, wall=45493
2022-03-05 00:27:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:28:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:28:30 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 13.071 | nll_loss 12.57 | ppl 6078.79 | wps 44162.4 | wpb 510.9 | bsz 1 | num_updates 16950 | best_loss 7.962
2022-03-05 00:28:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16950 updates
2022-03-05 00:28:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:28:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:28:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 176 @ 16950 updates, score 13.071) (writing took 2.4204824352636933 seconds)
2022-03-05 00:28:33 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-05 00:28:33 | INFO | train | epoch 176 | loss 1.998 | nll_loss 0.96 | ppl 1.95 | wps 24204.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 16950 | lr 0.000242893 | gnorm 0.963 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 45634
2022-03-05 00:28:33 | INFO | fairseq.trainer | begin training epoch 177
2022-03-05 00:28:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:30:41 | INFO | train_inner | epoch 177:     50 / 97 loss=1.997, nll_loss=0.959, ppl=1.94, wps=24271, ups=0.37, wpb=65495, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=0.956, loss_scale=8, train_wall=239, gb_free=21, wall=45763
2022-03-05 00:32:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:32:47 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 13.031 | nll_loss 12.522 | ppl 5882.24 | wps 44220.4 | wpb 510.9 | bsz 1 | num_updates 17047 | best_loss 7.962
2022-03-05 00:32:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17047 updates
2022-03-05 00:32:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:32:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:32:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 177 @ 17047 updates, score 13.031) (writing took 2.4185096425935626 seconds)
2022-03-05 00:32:50 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-05 00:32:50 | INFO | train | epoch 177 | loss 1.995 | nll_loss 0.957 | ppl 1.94 | wps 24723.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17047 | lr 0.000242201 | gnorm 0.945 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 45891
2022-03-05 00:32:50 | INFO | fairseq.trainer | begin training epoch 178
2022-03-05 00:32:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:35:06 | INFO | train_inner | epoch 178:     53 / 97 loss=1.991, nll_loss=0.953, ppl=1.94, wps=24739.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=0.953, loss_scale=16, train_wall=235, gb_free=21, wall=46028
2022-03-05 00:36:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:37:04 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 13.061 | nll_loss 12.561 | ppl 6044.31 | wps 43764 | wpb 510.9 | bsz 1 | num_updates 17144 | best_loss 7.962
2022-03-05 00:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17144 updates
2022-03-05 00:37:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:37:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 178 @ 17144 updates, score 13.061) (writing took 2.4127640454098582 seconds)
2022-03-05 00:37:07 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-05 00:37:07 | INFO | train | epoch 178 | loss 1.991 | nll_loss 0.953 | ppl 1.94 | wps 24713.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17144 | lr 0.000241515 | gnorm 0.965 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46148
2022-03-05 00:37:07 | INFO | fairseq.trainer | begin training epoch 179
2022-03-05 00:37:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:39:31 | INFO | train_inner | epoch 179:     56 / 97 loss=1.99, nll_loss=0.952, ppl=1.93, wps=24741.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=0.958, loss_scale=32, train_wall=234, gb_free=21, wall=46293
2022-03-05 00:39:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:41:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:41:21 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 13.102 | nll_loss 12.602 | ppl 6215.24 | wps 44276 | wpb 510.9 | bsz 1 | num_updates 17240 | best_loss 7.962
2022-03-05 00:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17240 updates
2022-03-05 00:41:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 179 @ 17240 updates, score 13.102) (writing took 2.411268848925829 seconds)
2022-03-05 00:41:23 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-05 00:41:23 | INFO | train | epoch 179 | loss 1.984 | nll_loss 0.946 | ppl 1.93 | wps 24472.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17240 | lr 0.000240842 | gnorm 0.941 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46405
2022-03-05 00:41:23 | INFO | fairseq.trainer | begin training epoch 180
2022-03-05 00:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:43:58 | INFO | train_inner | epoch 180:     60 / 97 loss=1.985, nll_loss=0.947, ppl=1.93, wps=24499.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=0.938, loss_scale=16, train_wall=237, gb_free=21, wall=46560
2022-03-05 00:45:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:45:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:45:38 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 13.002 | nll_loss 12.496 | ppl 5775.07 | wps 44194.8 | wpb 510.9 | bsz 1 | num_updates 17336 | best_loss 7.962
2022-03-05 00:45:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17336 updates
2022-03-05 00:45:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:45:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 180 @ 17336 updates, score 13.002) (writing took 2.4037579549476504 seconds)
2022-03-05 00:45:41 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-05 00:45:41 | INFO | train | epoch 180 | loss 1.983 | nll_loss 0.945 | ppl 1.92 | wps 24456.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17336 | lr 0.000240174 | gnorm 0.941 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 46662
2022-03-05 00:45:41 | INFO | fairseq.trainer | begin training epoch 181
2022-03-05 00:45:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:48:26 | INFO | train_inner | epoch 181:     64 / 97 loss=1.979, nll_loss=0.94, ppl=1.92, wps=24489.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=0.945, loss_scale=16, train_wall=237, gb_free=21, wall=46827
2022-03-05 00:49:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:49:55 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 13.037 | nll_loss 12.535 | ppl 5933.46 | wps 44489.1 | wpb 510.9 | bsz 1 | num_updates 17433 | best_loss 7.962
2022-03-05 00:49:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17433 updates
2022-03-05 00:49:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:49:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:49:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 181 @ 17433 updates, score 13.037) (writing took 2.451580476015806 seconds)
2022-03-05 00:49:58 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-05 00:49:58 | INFO | train | epoch 181 | loss 1.979 | nll_loss 0.94 | ppl 1.92 | wps 24697.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17433 | lr 0.000239505 | gnorm 0.945 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 46920
2022-03-05 00:49:58 | INFO | fairseq.trainer | begin training epoch 182
2022-03-05 00:49:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:51:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:52:53 | INFO | train_inner | epoch 182:     68 / 97 loss=1.977, nll_loss=0.938, ppl=1.92, wps=24498.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=0.949, loss_scale=16, train_wall=237, gb_free=21, wall=47095
2022-03-05 00:54:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:54:12 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 13.157 | nll_loss 12.663 | ppl 6483.91 | wps 44264.5 | wpb 510.9 | bsz 1 | num_updates 17529 | best_loss 7.962
2022-03-05 00:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17529 updates
2022-03-05 00:54:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:54:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:54:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 182 @ 17529 updates, score 13.157) (writing took 2.4184671379625797 seconds)
2022-03-05 00:54:15 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-05 00:54:15 | INFO | train | epoch 182 | loss 1.975 | nll_loss 0.936 | ppl 1.91 | wps 24469.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17529 | lr 0.000238848 | gnorm 0.949 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47176
2022-03-05 00:54:15 | INFO | fairseq.trainer | begin training epoch 183
2022-03-05 00:54:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:57:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:57:20 | INFO | train_inner | epoch 183:     72 / 97 loss=1.972, nll_loss=0.934, ppl=1.91, wps=24501.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=0.953, loss_scale=16, train_wall=237, gb_free=21, wall=47362
2022-03-05 00:58:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:58:29 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 13.119 | nll_loss 12.618 | ppl 6288.34 | wps 44019.5 | wpb 510.9 | bsz 1 | num_updates 17625 | best_loss 7.962
2022-03-05 00:58:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17625 updates
2022-03-05 00:58:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:58:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 00:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 183 @ 17625 updates, score 13.119) (writing took 2.402294502593577 seconds)
2022-03-05 00:58:32 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-05 00:58:32 | INFO | train | epoch 183 | loss 1.969 | nll_loss 0.93 | ppl 1.91 | wps 24457.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17625 | lr 0.000238197 | gnorm 0.943 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47434
2022-03-05 00:58:32 | INFO | fairseq.trainer | begin training epoch 184
2022-03-05 00:58:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:00:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:01:47 | INFO | train_inner | epoch 184:     76 / 97 loss=1.969, nll_loss=0.93, ppl=1.91, wps=24507.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.95, loss_scale=8, train_wall=237, gb_free=21, wall=47629
2022-03-05 01:02:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:02:46 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 13.127 | nll_loss 12.626 | ppl 6320.32 | wps 44132.6 | wpb 510.9 | bsz 1 | num_updates 17721 | best_loss 7.962
2022-03-05 01:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17721 updates
2022-03-05 01:02:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 184 @ 17721 updates, score 13.127) (writing took 2.4012249941006303 seconds)
2022-03-05 01:02:49 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-05 01:02:49 | INFO | train | epoch 184 | loss 1.967 | nll_loss 0.929 | ppl 1.9 | wps 24469.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17721 | lr 0.00023755 | gnorm 0.958 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 47690
2022-03-05 01:02:49 | INFO | fairseq.trainer | begin training epoch 185
2022-03-05 01:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:06:12 | INFO | train_inner | epoch 185:     79 / 97 loss=1.965, nll_loss=0.926, ppl=1.9, wps=24733.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=0.945, loss_scale=8, train_wall=234, gb_free=21, wall=47894
2022-03-05 01:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:07:03 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 13.096 | nll_loss 12.601 | ppl 6211.83 | wps 44154.8 | wpb 510.9 | bsz 1 | num_updates 17818 | best_loss 7.962
2022-03-05 01:07:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17818 updates
2022-03-05 01:07:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:07:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:07:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 185 @ 17818 updates, score 13.096) (writing took 2.4343676511198282 seconds)
2022-03-05 01:07:06 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-05 01:07:06 | INFO | train | epoch 185 | loss 1.963 | nll_loss 0.924 | ppl 1.9 | wps 24708.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17818 | lr 0.000236903 | gnorm 0.942 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 47948
2022-03-05 01:07:06 | INFO | fairseq.trainer | begin training epoch 186
2022-03-05 01:07:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:10:37 | INFO | train_inner | epoch 186:     82 / 97 loss=1.958, nll_loss=0.919, ppl=1.89, wps=24727.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17900, lr=0.00023636, gnorm=0.935, loss_scale=16, train_wall=235, gb_free=21, wall=48159
2022-03-05 01:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:11:21 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 13.054 | nll_loss 12.556 | ppl 6020.69 | wps 44124.6 | wpb 510.9 | bsz 1 | num_updates 17915 | best_loss 7.962
2022-03-05 01:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17915 updates
2022-03-05 01:11:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:11:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 186 @ 17915 updates, score 13.054) (writing took 2.43587545119226 seconds)
2022-03-05 01:11:23 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-05 01:11:23 | INFO | train | epoch 186 | loss 1.957 | nll_loss 0.918 | ppl 1.89 | wps 24702.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17915 | lr 0.000236261 | gnorm 0.934 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 48205
2022-03-05 01:11:23 | INFO | fairseq.trainer | begin training epoch 187
2022-03-05 01:11:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:12:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:15:04 | INFO | train_inner | epoch 187:     86 / 97 loss=1.957, nll_loss=0.919, ppl=1.89, wps=24490.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=0.944, loss_scale=16, train_wall=237, gb_free=21, wall=48426
2022-03-05 01:15:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:15:38 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 13.131 | nll_loss 12.637 | ppl 6368.21 | wps 44021.1 | wpb 510.9 | bsz 1 | num_updates 18011 | best_loss 7.962
2022-03-05 01:15:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18011 updates
2022-03-05 01:15:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:15:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:15:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 187 @ 18011 updates, score 13.131) (writing took 2.4149597100913525 seconds)
2022-03-05 01:15:40 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-05 01:15:40 | INFO | train | epoch 187 | loss 1.956 | nll_loss 0.917 | ppl 1.89 | wps 24459.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18011 | lr 0.00023563 | gnorm 0.949 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 48462
2022-03-05 01:15:40 | INFO | fairseq.trainer | begin training epoch 188
2022-03-05 01:15:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:17:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:19:32 | INFO | train_inner | epoch 188:     90 / 97 loss=1.953, nll_loss=0.914, ppl=1.88, wps=24491.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.954, loss_scale=16, train_wall=237, gb_free=21, wall=48694
2022-03-05 01:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:19:55 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 13.127 | nll_loss 12.629 | ppl 6333.9 | wps 44215.8 | wpb 510.9 | bsz 1 | num_updates 18107 | best_loss 7.962
2022-03-05 01:19:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18107 updates
2022-03-05 01:19:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:19:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 188 @ 18107 updates, score 13.127) (writing took 2.4550288431346416 seconds)
2022-03-05 01:19:57 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-05 01:19:57 | INFO | train | epoch 188 | loss 1.951 | nll_loss 0.912 | ppl 1.88 | wps 24450.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18107 | lr 0.000235005 | gnorm 0.95 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 48719
2022-03-05 01:19:57 | INFO | fairseq.trainer | begin training epoch 189
2022-03-05 01:19:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:23:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:23:59 | INFO | train_inner | epoch 189:     94 / 97 loss=1.95, nll_loss=0.911, ppl=1.88, wps=24474.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.938, loss_scale=16, train_wall=237, gb_free=21, wall=48961
2022-03-05 01:24:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:24:12 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 13.193 | nll_loss 12.701 | ppl 6660.4 | wps 44251.3 | wpb 510.9 | bsz 1 | num_updates 18203 | best_loss 7.962
2022-03-05 01:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18203 updates
2022-03-05 01:24:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:24:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:24:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 189 @ 18203 updates, score 13.193) (writing took 2.351865421049297 seconds)
2022-03-05 01:24:14 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 01:24:14 | INFO | train | epoch 189 | loss 1.948 | nll_loss 0.909 | ppl 1.88 | wps 24444.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18203 | lr 0.000234384 | gnorm 0.939 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 48976
2022-03-05 01:24:14 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 01:24:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:28:24 | INFO | train_inner | epoch 190:     97 / 97 loss=1.946, nll_loss=0.907, ppl=1.88, wps=24738, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=18300, lr=0.000233762, gnorm=0.948, loss_scale=16, train_wall=234, gb_free=21, wall=49226
2022-03-05 01:28:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:28:29 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 13.135 | nll_loss 12.641 | ppl 6389.05 | wps 43661.4 | wpb 510.9 | bsz 1 | num_updates 18300 | best_loss 7.962
2022-03-05 01:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18300 updates
2022-03-05 01:28:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:28:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 190 @ 18300 updates, score 13.135) (writing took 2.4979782728478312 seconds)
2022-03-05 01:28:32 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 01:28:32 | INFO | train | epoch 190 | loss 1.945 | nll_loss 0.906 | ppl 1.87 | wps 24697.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18300 | lr 0.000233762 | gnorm 0.946 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 49233
2022-03-05 01:28:32 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 01:28:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:29:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:32:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:32:46 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 13.115 | nll_loss 12.621 | ppl 6298.77 | wps 44318.3 | wpb 510.9 | bsz 1 | num_updates 18396 | best_loss 7.962
2022-03-05 01:32:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18396 updates
2022-03-05 01:32:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:32:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:32:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 191 @ 18396 updates, score 13.115) (writing took 2.389362094923854 seconds)
2022-03-05 01:32:49 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 01:32:49 | INFO | train | epoch 191 | loss 1.941 | nll_loss 0.903 | ppl 1.87 | wps 24462.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18396 | lr 0.000233152 | gnorm 0.935 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 49490
2022-03-05 01:32:49 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 01:32:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:32:59 | INFO | train_inner | epoch 192:      4 / 97 loss=1.94, nll_loss=0.901, ppl=1.87, wps=23821.6, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=18400, lr=0.000233126, gnorm=0.935, loss_scale=16, train_wall=237, gb_free=21, wall=49501
2022-03-05 01:35:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:36:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:37:03 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 13.145 | nll_loss 12.652 | ppl 6437.3 | wps 44219.4 | wpb 510.9 | bsz 1 | num_updates 18492 | best_loss 7.962
2022-03-05 01:37:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18492 updates
2022-03-05 01:37:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:37:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 192 @ 18492 updates, score 13.145) (writing took 2.4137591011822224 seconds)
2022-03-05 01:37:06 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 01:37:06 | INFO | train | epoch 192 | loss 1.937 | nll_loss 0.898 | ppl 1.86 | wps 24469.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18492 | lr 0.000232546 | gnorm 0.936 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 49747
2022-03-05 01:37:06 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 01:37:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:37:26 | INFO | train_inner | epoch 193:      8 / 97 loss=1.935, nll_loss=0.896, ppl=1.86, wps=24503.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18500, lr=0.000232495, gnorm=0.937, loss_scale=16, train_wall=237, gb_free=21, wall=49768
2022-03-05 01:41:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:41:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:41:20 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 13.124 | nll_loss 12.626 | ppl 6321.11 | wps 44032.2 | wpb 510.9 | bsz 1 | num_updates 18588 | best_loss 7.962
2022-03-05 01:41:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18588 updates
2022-03-05 01:41:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:41:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 193 @ 18588 updates, score 13.124) (writing took 2.4231367390602827 seconds)
2022-03-05 01:41:23 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 01:41:23 | INFO | train | epoch 193 | loss 1.933 | nll_loss 0.894 | ppl 1.86 | wps 24462.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18588 | lr 0.000231944 | gnorm 0.927 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50004
2022-03-05 01:41:23 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 01:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:41:54 | INFO | train_inner | epoch 194:     12 / 97 loss=1.931, nll_loss=0.892, ppl=1.86, wps=24505.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18600, lr=0.000231869, gnorm=0.923, loss_scale=16, train_wall=237, gb_free=21, wall=50035
2022-03-05 01:44:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:45:37 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 13.148 | nll_loss 12.657 | ppl 6458.53 | wps 44174.2 | wpb 510.9 | bsz 1 | num_updates 18684 | best_loss 7.962
2022-03-05 01:45:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18684 updates
2022-03-05 01:45:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:45:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 194 @ 18684 updates, score 13.148) (writing took 2.329913316294551 seconds)
2022-03-05 01:45:40 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 01:45:40 | INFO | train | epoch 194 | loss 1.931 | nll_loss 0.892 | ppl 1.86 | wps 24468.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18684 | lr 0.000231348 | gnorm 0.937 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 50261
2022-03-05 01:45:40 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 01:45:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:46:21 | INFO | train_inner | epoch 195:     16 / 97 loss=1.93, nll_loss=0.891, ppl=1.85, wps=24503.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18700, lr=0.000231249, gnorm=0.942, loss_scale=8, train_wall=237, gb_free=21, wall=50303
2022-03-05 01:49:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:49:54 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 13.124 | nll_loss 12.63 | ppl 6339.81 | wps 43956.4 | wpb 510.9 | bsz 1 | num_updates 18781 | best_loss 7.962
2022-03-05 01:49:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18781 updates
2022-03-05 01:49:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:49:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:49:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 195 @ 18781 updates, score 13.124) (writing took 2.432988458313048 seconds)
2022-03-05 01:49:56 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 01:49:56 | INFO | train | epoch 195 | loss 1.926 | nll_loss 0.887 | ppl 1.85 | wps 24722.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18781 | lr 0.000230749 | gnorm 0.932 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 50518
2022-03-05 01:49:56 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 01:49:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:50:45 | INFO | train_inner | epoch 196:     19 / 97 loss=1.925, nll_loss=0.885, ppl=1.85, wps=24747.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.929, loss_scale=16, train_wall=234, gb_free=21, wall=50567
2022-03-05 01:54:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:54:11 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 13.092 | nll_loss 12.595 | ppl 6185.22 | wps 44055.4 | wpb 510.9 | bsz 1 | num_updates 18878 | best_loss 7.962
2022-03-05 01:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18878 updates
2022-03-05 01:54:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:54:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:54:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 196 @ 18878 updates, score 13.092) (writing took 2.4058931823819876 seconds)
2022-03-05 01:54:14 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 01:54:14 | INFO | train | epoch 196 | loss 1.923 | nll_loss 0.884 | ppl 1.85 | wps 24713.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18878 | lr 0.000230156 | gnorm 0.924 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 50775
2022-03-05 01:54:14 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 01:54:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:55:10 | INFO | train_inner | epoch 197:     22 / 97 loss=1.922, nll_loss=0.882, ppl=1.84, wps=24728.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.921, loss_scale=16, train_wall=234, gb_free=21, wall=50832
2022-03-05 01:55:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:58:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:58:28 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 13.159 | nll_loss 12.668 | ppl 6507.46 | wps 44098.6 | wpb 510.9 | bsz 1 | num_updates 18974 | best_loss 7.962
2022-03-05 01:58:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18974 updates
2022-03-05 01:58:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:58:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 01:58:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 197 @ 18974 updates, score 13.159) (writing took 2.4128018468618393 seconds)
2022-03-05 01:58:31 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 01:58:31 | INFO | train | epoch 197 | loss 1.92 | nll_loss 0.881 | ppl 1.84 | wps 24451.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18974 | lr 0.000229573 | gnorm 0.925 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51032
2022-03-05 01:58:31 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 01:58:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:59:38 | INFO | train_inner | epoch 198:     26 / 97 loss=1.916, nll_loss=0.876, ppl=1.84, wps=24500.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.921, loss_scale=16, train_wall=237, gb_free=21, wall=51099
2022-03-05 02:01:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:02:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:02:45 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 13.166 | nll_loss 12.676 | ppl 6543.99 | wps 44273.7 | wpb 510.9 | bsz 1 | num_updates 19070 | best_loss 7.962
2022-03-05 02:02:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19070 updates
2022-03-05 02:02:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:02:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:02:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 198 @ 19070 updates, score 13.166) (writing took 2.423367972485721 seconds)
2022-03-05 02:02:48 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 02:02:48 | INFO | train | epoch 198 | loss 1.917 | nll_loss 0.877 | ppl 1.84 | wps 24458.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19070 | lr 0.000228994 | gnorm 0.925 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 51289
2022-03-05 02:02:48 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 02:02:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:05 | INFO | train_inner | epoch 199:     30 / 97 loss=1.916, nll_loss=0.877, ppl=1.84, wps=24487.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.927, loss_scale=16, train_wall=237, gb_free=21, wall=51367
2022-03-05 02:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:07:02 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 13.144 | nll_loss 12.651 | ppl 6433.41 | wps 44173.8 | wpb 510.9 | bsz 1 | num_updates 19167 | best_loss 7.962
2022-03-05 02:07:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19167 updates
2022-03-05 02:07:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 199 @ 19167 updates, score 13.144) (writing took 2.4443720122799277 seconds)
2022-03-05 02:07:05 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 02:07:05 | INFO | train | epoch 199 | loss 1.914 | nll_loss 0.874 | ppl 1.83 | wps 24700.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19167 | lr 0.000228414 | gnorm 0.929 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51547
2022-03-05 02:07:05 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 02:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:08:32 | INFO | train_inner | epoch 200:     34 / 97 loss=1.911, nll_loss=0.871, ppl=1.83, wps=24491, ups=0.37, wpb=65495, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.923, loss_scale=16, train_wall=237, gb_free=21, wall=51634
2022-03-05 02:11:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:11:20 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 13.19 | nll_loss 12.7 | ppl 6656.02 | wps 44090.3 | wpb 510.9 | bsz 1 | num_updates 19263 | best_loss 7.962
2022-03-05 02:11:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19263 updates
2022-03-05 02:11:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:11:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:11:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 200 @ 19263 updates, score 13.19) (writing took 2.316743651404977 seconds)
2022-03-05 02:11:22 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 02:11:22 | INFO | train | epoch 200 | loss 1.909 | nll_loss 0.87 | ppl 1.83 | wps 24460.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19263 | lr 0.000227844 | gnorm 0.915 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51804
2022-03-05 02:11:22 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 02:11:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:12:57 | INFO | train_inner | epoch 201:     37 / 97 loss=1.909, nll_loss=0.87, ppl=1.83, wps=24734.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.917, loss_scale=16, train_wall=235, gb_free=21, wall=51899
2022-03-05 02:13:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:15:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:15:37 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 13.207 | nll_loss 12.72 | ppl 6744.75 | wps 44154.8 | wpb 510.9 | bsz 1 | num_updates 19359 | best_loss 7.962
2022-03-05 02:15:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19359 updates
2022-03-05 02:15:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:15:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:15:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 201 @ 19359 updates, score 13.207) (writing took 2.358620520681143 seconds)
2022-03-05 02:15:39 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 02:15:39 | INFO | train | epoch 201 | loss 1.906 | nll_loss 0.866 | ppl 1.82 | wps 24468.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19359 | lr 0.000227279 | gnorm 0.918 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52061
2022-03-05 02:15:39 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 02:15:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:17:25 | INFO | train_inner | epoch 202:     41 / 97 loss=1.904, nll_loss=0.864, ppl=1.82, wps=24508.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.922, loss_scale=16, train_wall=237, gb_free=21, wall=52166
2022-03-05 02:18:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:19:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:19:54 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 13.178 | nll_loss 12.687 | ppl 6592.71 | wps 44252.8 | wpb 510.9 | bsz 1 | num_updates 19455 | best_loss 7.962
2022-03-05 02:19:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19455 updates
2022-03-05 02:19:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:19:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:19:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 202 @ 19455 updates, score 13.178) (writing took 2.4387881737202406 seconds)
2022-03-05 02:19:56 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 02:19:56 | INFO | train | epoch 202 | loss 1.904 | nll_loss 0.864 | ppl 1.82 | wps 24457.2 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 19455 | lr 0.000226717 | gnorm 0.921 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52318
2022-03-05 02:19:56 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 02:19:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:21:52 | INFO | train_inner | epoch 203:     45 / 97 loss=1.904, nll_loss=0.865, ppl=1.82, wps=24498.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.92, loss_scale=16, train_wall=237, gb_free=21, wall=52434
2022-03-05 02:24:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:24:11 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 13.172 | nll_loss 12.685 | ppl 6587.31 | wps 43866.5 | wpb 510.9 | bsz 1 | num_updates 19552 | best_loss 7.962
2022-03-05 02:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19552 updates
2022-03-05 02:24:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:24:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:24:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 203 @ 19552 updates, score 13.172) (writing took 2.4854067442938685 seconds)
2022-03-05 02:24:13 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 02:24:13 | INFO | train | epoch 203 | loss 1.902 | nll_loss 0.862 | ppl 1.82 | wps 24707.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19552 | lr 0.000226154 | gnorm 0.923 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52575
2022-03-05 02:24:13 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 02:24:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:24:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:26:19 | INFO | train_inner | epoch 204:     49 / 97 loss=1.898, nll_loss=0.858, ppl=1.81, wps=24491.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.923, loss_scale=16, train_wall=237, gb_free=21, wall=52701
2022-03-05 02:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:28:28 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 13.165 | nll_loss 12.675 | ppl 6539 | wps 44099.8 | wpb 510.9 | bsz 1 | num_updates 19648 | best_loss 7.962
2022-03-05 02:28:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19648 updates
2022-03-05 02:28:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:28:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 204 @ 19648 updates, score 13.165) (writing took 2.4629996372386813 seconds)
2022-03-05 02:28:30 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 02:28:30 | INFO | train | epoch 204 | loss 1.898 | nll_loss 0.859 | ppl 1.81 | wps 24469.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19648 | lr 0.000225601 | gnorm 0.919 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 52832
2022-03-05 02:28:30 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 02:28:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:30:46 | INFO | train_inner | epoch 205:     53 / 97 loss=1.898, nll_loss=0.859, ppl=1.81, wps=24511.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.922, loss_scale=16, train_wall=237, gb_free=21, wall=52968
2022-03-05 02:32:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:32:45 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 13.199 | nll_loss 12.713 | ppl 6713.92 | wps 44243.6 | wpb 510.9 | bsz 1 | num_updates 19744 | best_loss 7.962
2022-03-05 02:32:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19744 updates
2022-03-05 02:32:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:32:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 205 @ 19744 updates, score 13.199) (writing took 2.4617267549037933 seconds)
2022-03-05 02:32:47 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 02:32:47 | INFO | train | epoch 205 | loss 1.895 | nll_loss 0.855 | ppl 1.81 | wps 24467.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19744 | lr 0.000225052 | gnorm 0.915 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53089
2022-03-05 02:32:47 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 02:32:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:35:11 | INFO | train_inner | epoch 206:     56 / 97 loss=1.893, nll_loss=0.853, ppl=1.81, wps=24729.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.913, loss_scale=16, train_wall=235, gb_free=21, wall=53233
2022-03-05 02:36:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:36:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:37:02 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 13.181 | nll_loss 12.694 | ppl 6624.71 | wps 44139.3 | wpb 510.9 | bsz 1 | num_updates 19840 | best_loss 7.962
2022-03-05 02:37:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19840 updates
2022-03-05 02:37:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:37:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:37:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 206 @ 19840 updates, score 13.181) (writing took 2.3592087188735604 seconds)
2022-03-05 02:37:04 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 02:37:04 | INFO | train | epoch 206 | loss 1.892 | nll_loss 0.852 | ppl 1.81 | wps 24466.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19840 | lr 0.000224507 | gnorm 0.915 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53346
2022-03-05 02:37:04 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 02:37:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:39:39 | INFO | train_inner | epoch 207:     60 / 97 loss=1.891, nll_loss=0.852, ppl=1.8, wps=24506.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.919, loss_scale=16, train_wall=237, gb_free=21, wall=53500
2022-03-05 02:41:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:41:19 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 13.213 | nll_loss 12.727 | ppl 6780.5 | wps 44212.2 | wpb 510.9 | bsz 1 | num_updates 19937 | best_loss 7.962
2022-03-05 02:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19937 updates
2022-03-05 02:41:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:41:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:41:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 207 @ 19937 updates, score 13.213) (writing took 2.5802924055606127 seconds)
2022-03-05 02:41:21 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 02:41:21 | INFO | train | epoch 207 | loss 1.889 | nll_loss 0.849 | ppl 1.8 | wps 24692.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19937 | lr 0.00022396 | gnorm 0.925 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 53603
2022-03-05 02:41:21 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 02:41:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:42:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:44:06 | INFO | train_inner | epoch 208:     64 / 97 loss=1.886, nll_loss=0.846, ppl=1.8, wps=24475.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.918, loss_scale=16, train_wall=237, gb_free=21, wall=53768
2022-03-05 02:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:45:36 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 13.202 | nll_loss 12.717 | ppl 6733.74 | wps 44180.2 | wpb 510.9 | bsz 1 | num_updates 20033 | best_loss 7.962
2022-03-05 02:45:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20033 updates
2022-03-05 02:45:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:45:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:45:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 208 @ 20033 updates, score 13.202) (writing took 2.4181169038638473 seconds)
2022-03-05 02:45:38 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 02:45:38 | INFO | train | epoch 208 | loss 1.885 | nll_loss 0.845 | ppl 1.8 | wps 24452.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20033 | lr 0.000223423 | gnorm 0.914 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 53860
2022-03-05 02:45:38 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 02:45:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:47:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:48:33 | INFO | train_inner | epoch 209:     68 / 97 loss=1.883, nll_loss=0.843, ppl=1.79, wps=24499.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.917, loss_scale=16, train_wall=237, gb_free=21, wall=54035
2022-03-05 02:49:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:49:53 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 13.241 | nll_loss 12.753 | ppl 6902.31 | wps 44403.9 | wpb 510.9 | bsz 1 | num_updates 20129 | best_loss 7.962
2022-03-05 02:49:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20129 updates
2022-03-05 02:49:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:49:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:49:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 209 @ 20129 updates, score 13.241) (writing took 2.416042761877179 seconds)
2022-03-05 02:49:55 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 02:49:55 | INFO | train | epoch 209 | loss 1.882 | nll_loss 0.842 | ppl 1.79 | wps 24469.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20129 | lr 0.000222889 | gnorm 0.917 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54117
2022-03-05 02:49:55 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 02:49:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:52:58 | INFO | train_inner | epoch 210:     71 / 97 loss=1.882, nll_loss=0.842, ppl=1.79, wps=24742.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.914, loss_scale=16, train_wall=234, gb_free=21, wall=54300
2022-03-05 02:53:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:54:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:54:10 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 13.214 | nll_loss 12.732 | ppl 6802.63 | wps 44190.3 | wpb 510.9 | bsz 1 | num_updates 20225 | best_loss 7.962
2022-03-05 02:54:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20225 updates
2022-03-05 02:54:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:54:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:54:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 210 @ 20225 updates, score 13.214) (writing took 2.4257823433727026 seconds)
2022-03-05 02:54:12 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 02:54:12 | INFO | train | epoch 210 | loss 1.88 | nll_loss 0.84 | ppl 1.79 | wps 24462.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20225 | lr 0.00022236 | gnorm 0.922 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54374
2022-03-05 02:54:12 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 02:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:57:26 | INFO | train_inner | epoch 211:     75 / 97 loss=1.878, nll_loss=0.838, ppl=1.79, wps=24489.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.926, loss_scale=16, train_wall=237, gb_free=21, wall=54567
2022-03-05 02:58:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:58:27 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 13.18 | nll_loss 12.695 | ppl 6631.48 | wps 44362.2 | wpb 510.9 | bsz 1 | num_updates 20322 | best_loss 7.962
2022-03-05 02:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20322 updates
2022-03-05 02:58:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:58:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 02:58:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 211 @ 20322 updates, score 13.18) (writing took 2.5300320386886597 seconds)
2022-03-05 02:58:29 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 02:58:29 | INFO | train | epoch 211 | loss 1.877 | nll_loss 0.837 | ppl 1.79 | wps 24700.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20322 | lr 0.000221828 | gnorm 0.914 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54631
2022-03-05 02:58:29 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 02:58:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:59:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:01:53 | INFO | train_inner | epoch 212:     79 / 97 loss=1.876, nll_loss=0.836, ppl=1.79, wps=24524.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.913, loss_scale=16, train_wall=237, gb_free=21, wall=54834
2022-03-05 03:02:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:02:44 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 13.273 | nll_loss 12.791 | ppl 7087.08 | wps 44171.5 | wpb 510.9 | bsz 1 | num_updates 20418 | best_loss 7.962
2022-03-05 03:02:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20418 updates
2022-03-05 03:02:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:02:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 212 @ 20418 updates, score 13.273) (writing took 2.495451289229095 seconds)
2022-03-05 03:02:46 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 03:02:46 | INFO | train | epoch 212 | loss 1.875 | nll_loss 0.835 | ppl 1.78 | wps 24484.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20418 | lr 0.000221306 | gnorm 0.918 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 54888
2022-03-05 03:02:46 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 03:02:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:04:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:06:20 | INFO | train_inner | epoch 213:     83 / 97 loss=1.874, nll_loss=0.834, ppl=1.78, wps=24528.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.912, loss_scale=16, train_wall=236, gb_free=21, wall=55101
2022-03-05 03:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:07:00 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 13.237 | nll_loss 12.751 | ppl 6893.65 | wps 44241.1 | wpb 510.9 | bsz 1 | num_updates 20514 | best_loss 7.962
2022-03-05 03:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20514 updates
2022-03-05 03:07:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 213 @ 20514 updates, score 13.237) (writing took 2.474303630180657 seconds)
2022-03-05 03:07:03 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 03:07:03 | INFO | train | epoch 213 | loss 1.871 | nll_loss 0.831 | ppl 1.78 | wps 24497.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20514 | lr 0.000220788 | gnorm 0.906 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55145
2022-03-05 03:07:03 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 03:07:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:10:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:10:47 | INFO | train_inner | epoch 214:     87 / 97 loss=1.869, nll_loss=0.829, ppl=1.78, wps=24523, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.906, loss_scale=16, train_wall=236, gb_free=21, wall=55368
2022-03-05 03:11:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:11:17 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 13.227 | nll_loss 12.743 | ppl 6853.64 | wps 44057.2 | wpb 510.9 | bsz 1 | num_updates 20610 | best_loss 7.962
2022-03-05 03:11:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20610 updates
2022-03-05 03:11:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 214 @ 20610 updates, score 13.227) (writing took 2.492451469413936 seconds)
2022-03-05 03:11:20 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 03:11:20 | INFO | train | epoch 214 | loss 1.868 | nll_loss 0.828 | ppl 1.78 | wps 24477.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20610 | lr 0.000220273 | gnorm 0.907 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55401
2022-03-05 03:11:20 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 03:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:15:11 | INFO | train_inner | epoch 215:     90 / 97 loss=1.867, nll_loss=0.828, ppl=1.77, wps=24763.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.912, loss_scale=16, train_wall=234, gb_free=21, wall=55633
2022-03-05 03:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:15:34 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 13.187 | nll_loss 12.702 | ppl 6664.77 | wps 44283.2 | wpb 510.9 | bsz 1 | num_updates 20707 | best_loss 7.962
2022-03-05 03:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20707 updates
2022-03-05 03:15:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 215 @ 20707 updates, score 13.187) (writing took 2.4989431388676167 seconds)
2022-03-05 03:15:36 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 03:15:36 | INFO | train | epoch 215 | loss 1.866 | nll_loss 0.826 | ppl 1.77 | wps 24745 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20707 | lr 0.000219756 | gnorm 0.912 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55658
2022-03-05 03:15:36 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 03:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:19:38 | INFO | train_inner | epoch 216:     94 / 97 loss=1.864, nll_loss=0.824, ppl=1.77, wps=24530.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.908, loss_scale=16, train_wall=236, gb_free=21, wall=55900
2022-03-05 03:19:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:19:51 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 13.262 | nll_loss 12.78 | ppl 7033.12 | wps 44293.6 | wpb 510.9 | bsz 1 | num_updates 20803 | best_loss 7.962
2022-03-05 03:19:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20803 updates
2022-03-05 03:19:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:19:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:19:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 216 @ 20803 updates, score 13.262) (writing took 2.5167630752548575 seconds)
2022-03-05 03:19:53 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 03:19:53 | INFO | train | epoch 216 | loss 1.863 | nll_loss 0.823 | ppl 1.77 | wps 24488.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20803 | lr 0.000219249 | gnorm 0.907 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 55915
2022-03-05 03:19:53 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 03:19:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:24:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:24:07 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 13.27 | nll_loss 12.788 | ppl 7073.4 | wps 44312.5 | wpb 510.9 | bsz 1 | num_updates 20899 | best_loss 7.962
2022-03-05 03:24:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20899 updates
2022-03-05 03:24:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:24:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:24:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 217 @ 20899 updates, score 13.27) (writing took 2.512033828534186 seconds)
2022-03-05 03:24:10 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 03:24:10 | INFO | train | epoch 217 | loss 1.861 | nll_loss 0.821 | ppl 1.77 | wps 24490.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20899 | lr 0.000218745 | gnorm 0.911 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56172
2022-03-05 03:24:10 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 03:24:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:24:13 | INFO | train_inner | epoch 218:      1 / 97 loss=1.862, nll_loss=0.822, ppl=1.77, wps=23848.9, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=20900, lr=0.000218739, gnorm=0.912, loss_scale=16, train_wall=236, gb_free=21, wall=56174
2022-03-05 03:27:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:28:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:28:24 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 13.24 | nll_loss 12.757 | ppl 6921.51 | wps 44386.3 | wpb 510.9 | bsz 1 | num_updates 20995 | best_loss 7.962
2022-03-05 03:28:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 20995 updates
2022-03-05 03:28:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:28:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:28:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 218 @ 20995 updates, score 13.24) (writing took 2.4996095011010766 seconds)
2022-03-05 03:28:27 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 03:28:27 | INFO | train | epoch 218 | loss 1.859 | nll_loss 0.819 | ppl 1.76 | wps 24503.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20995 | lr 0.000218244 | gnorm 0.915 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56428
2022-03-05 03:28:27 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 03:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:28:39 | INFO | train_inner | epoch 219:      5 / 97 loss=1.857, nll_loss=0.817, ppl=1.76, wps=24543.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21000, lr=0.000218218, gnorm=0.914, loss_scale=16, train_wall=236, gb_free=21, wall=56441
2022-03-05 03:32:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:32:41 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 13.249 | nll_loss 12.769 | ppl 6978.18 | wps 44274.7 | wpb 510.9 | bsz 1 | num_updates 21092 | best_loss 7.962
2022-03-05 03:32:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21092 updates
2022-03-05 03:32:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:32:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:32:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 219 @ 21092 updates, score 13.249) (writing took 2.4801097689196467 seconds)
2022-03-05 03:32:43 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 03:32:43 | INFO | train | epoch 219 | loss 1.855 | nll_loss 0.815 | ppl 1.76 | wps 24757.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21092 | lr 0.000217741 | gnorm 0.907 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56685
2022-03-05 03:32:43 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 03:32:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:33:04 | INFO | train_inner | epoch 220:      8 / 97 loss=1.854, nll_loss=0.814, ppl=1.76, wps=24781.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21100, lr=0.0002177, gnorm=0.908, loss_scale=16, train_wall=234, gb_free=21, wall=56705
2022-03-05 03:33:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:36:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:36:57 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 13.192 | nll_loss 12.708 | ppl 6690.14 | wps 44220.8 | wpb 510.9 | bsz 1 | num_updates 21188 | best_loss 7.962
2022-03-05 03:36:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21188 updates
2022-03-05 03:36:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:37:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:37:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 220 @ 21188 updates, score 13.192) (writing took 2.4721884122118354 seconds)
2022-03-05 03:37:00 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 03:37:00 | INFO | train | epoch 220 | loss 1.853 | nll_loss 0.813 | ppl 1.76 | wps 24498.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21188 | lr 0.000217248 | gnorm 0.903 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 56941
2022-03-05 03:37:00 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 03:37:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:37:31 | INFO | train_inner | epoch 221:     12 / 97 loss=1.851, nll_loss=0.811, ppl=1.75, wps=24537.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21200, lr=0.000217186, gnorm=0.902, loss_scale=16, train_wall=237, gb_free=21, wall=56972
2022-03-05 03:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:41:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:41:14 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 13.198 | nll_loss 12.716 | ppl 6726.59 | wps 44256.8 | wpb 510.9 | bsz 1 | num_updates 21284 | best_loss 7.962
2022-03-05 03:41:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21284 updates
2022-03-05 03:41:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:41:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:41:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 221 @ 21284 updates, score 13.198) (writing took 2.512572795152664 seconds)
2022-03-05 03:41:16 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 03:41:16 | INFO | train | epoch 221 | loss 1.851 | nll_loss 0.811 | ppl 1.75 | wps 24485.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21284 | lr 0.000216757 | gnorm 0.914 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57198
2022-03-05 03:41:17 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 03:41:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:41:58 | INFO | train_inner | epoch 222:     16 / 97 loss=1.85, nll_loss=0.81, ppl=1.75, wps=24521.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21300, lr=0.000216676, gnorm=0.909, loss_scale=16, train_wall=237, gb_free=21, wall=57239
2022-03-05 03:45:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:45:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:45:31 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 13.192 | nll_loss 12.706 | ppl 6680.61 | wps 44312.8 | wpb 510.9 | bsz 1 | num_updates 21380 | best_loss 7.962
2022-03-05 03:45:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21380 updates
2022-03-05 03:45:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 222 @ 21380 updates, score 13.192) (writing took 2.5022310530766845 seconds)
2022-03-05 03:45:33 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 03:45:33 | INFO | train | epoch 222 | loss 1.847 | nll_loss 0.807 | ppl 1.75 | wps 24494.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21380 | lr 0.00021627 | gnorm 0.891 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57455
2022-03-05 03:45:33 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 03:45:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:46:25 | INFO | train_inner | epoch 223:     20 / 97 loss=1.845, nll_loss=0.805, ppl=1.75, wps=24532.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.892, loss_scale=16, train_wall=236, gb_free=21, wall=57506
2022-03-05 03:49:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:49:48 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 13.248 | nll_loss 12.768 | ppl 6975.43 | wps 44157.1 | wpb 510.9 | bsz 1 | num_updates 21477 | best_loss 7.962
2022-03-05 03:49:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21477 updates
2022-03-05 03:49:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:49:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:49:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 223 @ 21477 updates, score 13.248) (writing took 2.482658354565501 seconds)
2022-03-05 03:49:50 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 03:49:50 | INFO | train | epoch 223 | loss 1.846 | nll_loss 0.806 | ppl 1.75 | wps 24734.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21477 | lr 0.000215781 | gnorm 0.909 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57712
2022-03-05 03:49:50 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 03:49:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:50:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:50:52 | INFO | train_inner | epoch 224:     24 / 97 loss=1.844, nll_loss=0.804, ppl=1.75, wps=24524.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.912, loss_scale=16, train_wall=237, gb_free=21, wall=57774
2022-03-05 03:53:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:54:04 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 13.194 | nll_loss 12.713 | ppl 6715.48 | wps 44295.2 | wpb 510.9 | bsz 1 | num_updates 21573 | best_loss 7.962
2022-03-05 03:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21573 updates
2022-03-05 03:54:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 224 @ 21573 updates, score 13.194) (writing took 2.4740356793627143 seconds)
2022-03-05 03:54:07 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 03:54:07 | INFO | train | epoch 224 | loss 1.842 | nll_loss 0.802 | ppl 1.74 | wps 24487.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21573 | lr 0.0002153 | gnorm 0.904 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 57969
2022-03-05 03:54:07 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 03:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:55:16 | INFO | train_inner | epoch 225:     27 / 97 loss=1.84, nll_loss=0.8, ppl=1.74, wps=24763.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.903, loss_scale=16, train_wall=234, gb_free=21, wall=58038
2022-03-05 03:56:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:58:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:58:21 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 13.197 | nll_loss 12.711 | ppl 6707.19 | wps 44213.4 | wpb 510.9 | bsz 1 | num_updates 21669 | best_loss 7.962
2022-03-05 03:58:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21669 updates
2022-03-05 03:58:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 03:58:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 225 @ 21669 updates, score 13.197) (writing took 2.483700447715819 seconds)
2022-03-05 03:58:23 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 03:58:23 | INFO | train | epoch 225 | loss 1.838 | nll_loss 0.798 | ppl 1.74 | wps 24494.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21669 | lr 0.000214823 | gnorm 0.911 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58225
2022-03-05 03:58:23 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 03:58:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:59:43 | INFO | train_inner | epoch 226:     31 / 97 loss=1.838, nll_loss=0.798, ppl=1.74, wps=24527.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.911, loss_scale=16, train_wall=236, gb_free=21, wall=58305
2022-03-05 04:02:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:02:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:02:38 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 13.243 | nll_loss 12.762 | ppl 6947.38 | wps 44289.3 | wpb 510.9 | bsz 1 | num_updates 21765 | best_loss 7.962
2022-03-05 04:02:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21765 updates
2022-03-05 04:02:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:02:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:02:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 226 @ 21765 updates, score 13.243) (writing took 2.493254895322025 seconds)
2022-03-05 04:02:40 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 04:02:40 | INFO | train | epoch 226 | loss 1.836 | nll_loss 0.796 | ppl 1.74 | wps 24494.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21765 | lr 0.000214349 | gnorm 0.907 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58482
2022-03-05 04:02:40 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 04:02:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:04:10 | INFO | train_inner | epoch 227:     35 / 97 loss=1.835, nll_loss=0.795, ppl=1.74, wps=24544.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.897, loss_scale=16, train_wall=236, gb_free=21, wall=58572
2022-03-05 04:06:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:06:54 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 13.183 | nll_loss 12.699 | ppl 6647.48 | wps 44135 | wpb 510.9 | bsz 1 | num_updates 21862 | best_loss 7.962
2022-03-05 04:06:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21862 updates
2022-03-05 04:06:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:06:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 227 @ 21862 updates, score 13.183) (writing took 2.496181615628302 seconds)
2022-03-05 04:06:57 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 04:06:57 | INFO | train | epoch 227 | loss 1.834 | nll_loss 0.793 | ppl 1.73 | wps 24754.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21862 | lr 0.000213873 | gnorm 0.883 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58738
2022-03-05 04:06:57 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 04:06:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:07:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:08:37 | INFO | train_inner | epoch 228:     39 / 97 loss=1.833, nll_loss=0.793, ppl=1.73, wps=24531.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.889, loss_scale=16, train_wall=236, gb_free=21, wall=58839
2022-03-05 04:11:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:11:11 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 13.283 | nll_loss 12.802 | ppl 7142.75 | wps 44257.1 | wpb 510.9 | bsz 1 | num_updates 21958 | best_loss 7.962
2022-03-05 04:11:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21958 updates
2022-03-05 04:11:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:11:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:11:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 228 @ 21958 updates, score 13.283) (writing took 2.4295907635241747 seconds)
2022-03-05 04:11:13 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 04:11:13 | INFO | train | epoch 228 | loss 1.832 | nll_loss 0.792 | ppl 1.73 | wps 24491.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21958 | lr 0.000213405 | gnorm 0.892 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 58995
2022-03-05 04:11:13 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 04:11:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:13:01 | INFO | train_inner | epoch 229:     42 / 97 loss=1.83, nll_loss=0.79, ppl=1.73, wps=24770, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.894, loss_scale=16, train_wall=234, gb_free=21, wall=59103
2022-03-05 04:13:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:15:28 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 13.254 | nll_loss 12.769 | ppl 6981.53 | wps 44225.5 | wpb 510.9 | bsz 1 | num_updates 22054 | best_loss 7.962
2022-03-05 04:15:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22054 updates
2022-03-05 04:15:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:15:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:15:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 229 @ 22054 updates, score 13.254) (writing took 2.4901098255068064 seconds)
2022-03-05 04:15:30 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 04:15:30 | INFO | train | epoch 229 | loss 1.83 | nll_loss 0.79 | ppl 1.73 | wps 24494.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22054 | lr 0.00021294 | gnorm 0.9 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 59252
2022-03-05 04:15:30 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 04:15:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:17:28 | INFO | train_inner | epoch 230:     46 / 97 loss=1.828, nll_loss=0.788, ppl=1.73, wps=24525.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.901, loss_scale=16, train_wall=236, gb_free=21, wall=59370
2022-03-05 04:19:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:19:44 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 13.308 | nll_loss 12.834 | ppl 7300.93 | wps 44252.6 | wpb 510.9 | bsz 1 | num_updates 22150 | best_loss 7.962
2022-03-05 04:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22150 updates
2022-03-05 04:19:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:19:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:19:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 230 @ 22150 updates, score 13.308) (writing took 2.54663672298193 seconds)
2022-03-05 04:19:47 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 04:19:47 | INFO | train | epoch 230 | loss 1.828 | nll_loss 0.788 | ppl 1.73 | wps 24469 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22150 | lr 0.000212478 | gnorm 0.9 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 59509
2022-03-05 04:19:47 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 04:19:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:21:56 | INFO | train_inner | epoch 231:     50 / 97 loss=1.826, nll_loss=0.786, ppl=1.72, wps=24518.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=22200, lr=0.000212238, gnorm=0.891, loss_scale=16, train_wall=237, gb_free=21, wall=59637
2022-03-05 04:23:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:24:01 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 13.276 | nll_loss 12.797 | ppl 7117.19 | wps 44226.2 | wpb 510.9 | bsz 1 | num_updates 22247 | best_loss 7.962
2022-03-05 04:24:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22247 updates
2022-03-05 04:24:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:24:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:24:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 231 @ 22247 updates, score 13.276) (writing took 2.484330117702484 seconds)
2022-03-05 04:24:04 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 04:24:04 | INFO | train | epoch 231 | loss 1.824 | nll_loss 0.784 | ppl 1.72 | wps 24750.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22247 | lr 0.000212014 | gnorm 0.892 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 59765
2022-03-05 04:24:04 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 04:24:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:25:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:26:23 | INFO | train_inner | epoch 232:     54 / 97 loss=1.824, nll_loss=0.784, ppl=1.72, wps=24530.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.887, loss_scale=16, train_wall=237, gb_free=21, wall=59904
2022-03-05 04:28:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:28:18 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 13.267 | nll_loss 12.79 | ppl 7082.16 | wps 44308.2 | wpb 510.9 | bsz 1 | num_updates 22343 | best_loss 7.962
2022-03-05 04:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22343 updates
2022-03-05 04:28:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:28:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:28:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 232 @ 22343 updates, score 13.267) (writing took 2.453779799863696 seconds)
2022-03-05 04:28:20 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 04:28:20 | INFO | train | epoch 232 | loss 1.823 | nll_loss 0.783 | ppl 1.72 | wps 24502.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22343 | lr 0.000211558 | gnorm 0.882 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 60022
2022-03-05 04:28:20 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 04:28:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:30:47 | INFO | train_inner | epoch 233:     57 / 97 loss=1.821, nll_loss=0.781, ppl=1.72, wps=24774.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.883, loss_scale=16, train_wall=234, gb_free=21, wall=60169
2022-03-05 04:30:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:32:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:32:35 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 13.192 | nll_loss 12.711 | ppl 6704.59 | wps 44230.4 | wpb 510.9 | bsz 1 | num_updates 22439 | best_loss 7.962
2022-03-05 04:32:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22439 updates
2022-03-05 04:32:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:32:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:32:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 233 @ 22439 updates, score 13.192) (writing took 2.5018789153546095 seconds)
2022-03-05 04:32:37 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 04:32:37 | INFO | train | epoch 233 | loss 1.82 | nll_loss 0.779 | ppl 1.72 | wps 24485.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22439 | lr 0.000211105 | gnorm 0.883 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 60279
2022-03-05 04:32:37 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 04:32:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:35:14 | INFO | train_inner | epoch 234:     61 / 97 loss=1.818, nll_loss=0.778, ppl=1.72, wps=24537.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.886, loss_scale=16, train_wall=236, gb_free=21, wall=60436
2022-03-05 04:36:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:36:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:36:51 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 13.211 | nll_loss 12.73 | ppl 6793.02 | wps 44277.6 | wpb 510.9 | bsz 1 | num_updates 22535 | best_loss 7.962
2022-03-05 04:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22535 updates
2022-03-05 04:36:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:36:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:36:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 234 @ 22535 updates, score 13.211) (writing took 2.478101427666843 seconds)
2022-03-05 04:36:54 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 04:36:54 | INFO | train | epoch 234 | loss 1.818 | nll_loss 0.778 | ppl 1.71 | wps 24503.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22535 | lr 0.000210655 | gnorm 0.89 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 60535
2022-03-05 04:36:54 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 04:36:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:39:41 | INFO | train_inner | epoch 235:     65 / 97 loss=1.817, nll_loss=0.777, ppl=1.71, wps=24534.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.899, loss_scale=16, train_wall=236, gb_free=21, wall=60703
2022-03-05 04:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:41:08 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 13.268 | nll_loss 12.792 | ppl 7090.01 | wps 44198.7 | wpb 510.9 | bsz 1 | num_updates 22632 | best_loss 7.962
2022-03-05 04:41:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22632 updates
2022-03-05 04:41:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:41:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 235 @ 22632 updates, score 13.268) (writing took 2.4700413504615426 seconds)
2022-03-05 04:41:10 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 04:41:10 | INFO | train | epoch 235 | loss 1.816 | nll_loss 0.776 | ppl 1.71 | wps 24748.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22632 | lr 0.000210203 | gnorm 0.892 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 60792
2022-03-05 04:41:10 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 04:41:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:42:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:44:08 | INFO | train_inner | epoch 236:     69 / 97 loss=1.815, nll_loss=0.775, ppl=1.71, wps=24527.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.88, loss_scale=16, train_wall=237, gb_free=21, wall=60970
2022-03-05 04:45:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:45:25 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 13.298 | nll_loss 12.821 | ppl 7234.89 | wps 44151.4 | wpb 510.9 | bsz 1 | num_updates 22728 | best_loss 7.962
2022-03-05 04:45:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22728 updates
2022-03-05 04:45:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:45:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 236 @ 22728 updates, score 13.298) (writing took 2.458245914429426 seconds)
2022-03-05 04:45:27 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 04:45:27 | INFO | train | epoch 236 | loss 1.814 | nll_loss 0.774 | ppl 1.71 | wps 24493.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22728 | lr 0.000209758 | gnorm 0.884 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61049
2022-03-05 04:45:27 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 04:45:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:48:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:48:35 | INFO | train_inner | epoch 237:     73 / 97 loss=1.814, nll_loss=0.774, ppl=1.71, wps=24536.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.893, loss_scale=16, train_wall=236, gb_free=21, wall=61236
2022-03-05 04:49:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:49:41 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 13.259 | nll_loss 12.782 | ppl 7044.8 | wps 44200.6 | wpb 510.9 | bsz 1 | num_updates 22824 | best_loss 7.962
2022-03-05 04:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22824 updates
2022-03-05 04:49:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:49:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 237 @ 22824 updates, score 13.259) (writing took 2.469429347664118 seconds)
2022-03-05 04:49:44 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 04:49:44 | INFO | train | epoch 237 | loss 1.813 | nll_loss 0.774 | ppl 1.71 | wps 24501.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22824 | lr 0.000209317 | gnorm 0.895 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61305
2022-03-05 04:49:44 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 04:49:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:52:59 | INFO | train_inner | epoch 238:     76 / 97 loss=1.813, nll_loss=0.773, ppl=1.71, wps=24779.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.9, loss_scale=16, train_wall=234, gb_free=21, wall=61501
2022-03-05 04:53:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:53:58 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 13.292 | nll_loss 12.819 | ppl 7226.27 | wps 44272 | wpb 510.9 | bsz 1 | num_updates 22920 | best_loss 7.962
2022-03-05 04:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22920 updates
2022-03-05 04:53:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:54:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:54:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 238 @ 22920 updates, score 13.292) (writing took 2.498176895081997 seconds)
2022-03-05 04:54:00 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 04:54:00 | INFO | train | epoch 238 | loss 1.809 | nll_loss 0.769 | ppl 1.7 | wps 24495.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22920 | lr 0.000208878 | gnorm 0.893 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61562
2022-03-05 04:54:00 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 04:54:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:57:26 | INFO | train_inner | epoch 239:     80 / 97 loss=1.806, nll_loss=0.765, ppl=1.7, wps=24528.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.883, loss_scale=16, train_wall=236, gb_free=21, wall=61768
2022-03-05 04:58:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:58:15 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 13.308 | nll_loss 12.835 | ppl 7304.92 | wps 44260.3 | wpb 510.9 | bsz 1 | num_updates 23017 | best_loss 7.962
2022-03-05 04:58:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23017 updates
2022-03-05 04:58:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:58:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 04:58:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 239 @ 23017 updates, score 13.308) (writing took 2.4569953717291355 seconds)
2022-03-05 04:58:17 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 04:58:17 | INFO | train | epoch 239 | loss 1.807 | nll_loss 0.767 | ppl 1.7 | wps 24750.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23017 | lr 0.000208437 | gnorm 0.886 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 61819
2022-03-05 04:58:17 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 04:58:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:59:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:01:53 | INFO | train_inner | epoch 240:     84 / 97 loss=1.807, nll_loss=0.767, ppl=1.7, wps=24533.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.892, loss_scale=16, train_wall=237, gb_free=21, wall=62035
2022-03-05 05:02:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:02:31 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 13.249 | nll_loss 12.766 | ppl 6966.14 | wps 44442.9 | wpb 510.9 | bsz 1 | num_updates 23113 | best_loss 7.962
2022-03-05 05:02:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23113 updates
2022-03-05 05:02:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 240 @ 23113 updates, score 13.249) (writing took 2.4454197827726603 seconds)
2022-03-05 05:02:34 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 05:02:34 | INFO | train | epoch 240 | loss 1.805 | nll_loss 0.765 | ppl 1.7 | wps 24496.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23113 | lr 0.000208004 | gnorm 0.896 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62075
2022-03-05 05:02:34 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 05:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:05:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:06:20 | INFO | train_inner | epoch 241:     88 / 97 loss=1.803, nll_loss=0.763, ppl=1.7, wps=24533.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.883, loss_scale=16, train_wall=236, gb_free=21, wall=62302
2022-03-05 05:06:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:06:48 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 13.276 | nll_loss 12.8 | ppl 7132.16 | wps 44719 | wpb 510.9 | bsz 1 | num_updates 23209 | best_loss 7.962
2022-03-05 05:06:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23209 updates
2022-03-05 05:06:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:06:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 241 @ 23209 updates, score 13.276) (writing took 2.454579175449908 seconds)
2022-03-05 05:06:50 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 05:06:50 | INFO | train | epoch 241 | loss 1.801 | nll_loss 0.761 | ppl 1.69 | wps 24494.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23209 | lr 0.000207573 | gnorm 0.88 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62332
2022-03-05 05:06:50 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 05:06:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:10:44 | INFO | train_inner | epoch 242:     91 / 97 loss=1.802, nll_loss=0.763, ppl=1.7, wps=24776.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.895, loss_scale=16, train_wall=234, gb_free=21, wall=62566
2022-03-05 05:11:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:11:04 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 13.27 | nll_loss 12.794 | ppl 7100.36 | wps 44694 | wpb 510.9 | bsz 1 | num_updates 23306 | best_loss 7.962
2022-03-05 05:11:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23306 updates
2022-03-05 05:11:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:11:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 242 @ 23306 updates, score 13.27) (writing took 2.5112343253567815 seconds)
2022-03-05 05:11:07 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 05:11:07 | INFO | train | epoch 242 | loss 1.802 | nll_loss 0.762 | ppl 1.7 | wps 24750.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23306 | lr 0.000207141 | gnorm 0.896 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 62589
2022-03-05 05:11:07 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 05:11:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:11:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:15:11 | INFO | train_inner | epoch 243:     95 / 97 loss=1.799, nll_loss=0.76, ppl=1.69, wps=24524.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.886, loss_scale=16, train_wall=237, gb_free=21, wall=62833
2022-03-05 05:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:15:21 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 13.256 | nll_loss 12.783 | ppl 7045.73 | wps 44200.9 | wpb 510.9 | bsz 1 | num_updates 23402 | best_loss 7.962
2022-03-05 05:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23402 updates
2022-03-05 05:15:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:15:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 243 @ 23402 updates, score 13.256) (writing took 2.4672753186896443 seconds)
2022-03-05 05:15:24 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 05:15:24 | INFO | train | epoch 243 | loss 1.798 | nll_loss 0.758 | ppl 1.69 | wps 24486.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23402 | lr 0.000206716 | gnorm 0.884 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 62846
2022-03-05 05:15:24 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 05:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:16:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:19:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:19:38 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 13.318 | nll_loss 12.845 | ppl 7356.05 | wps 44227.4 | wpb 510.9 | bsz 1 | num_updates 23498 | best_loss 7.962
2022-03-05 05:19:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23498 updates
2022-03-05 05:19:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:19:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:19:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 244 @ 23498 updates, score 13.318) (writing took 2.474543628282845 seconds)
2022-03-05 05:19:40 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 05:19:40 | INFO | train | epoch 244 | loss 1.797 | nll_loss 0.757 | ppl 1.69 | wps 24492.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23498 | lr 0.000206293 | gnorm 0.892 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 63102
2022-03-05 05:19:40 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 05:19:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:19:46 | INFO | train_inner | epoch 245:      2 / 97 loss=1.797, nll_loss=0.757, ppl=1.69, wps=23856.7, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=23500, lr=0.000206284, gnorm=0.893, loss_scale=16, train_wall=236, gb_free=21, wall=63107
2022-03-05 05:22:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:23:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:23:55 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 13.279 | nll_loss 12.806 | ppl 7161.06 | wps 44094.1 | wpb 510.9 | bsz 1 | num_updates 23594 | best_loss 7.962
2022-03-05 05:23:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23594 updates
2022-03-05 05:23:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:23:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 245 @ 23594 updates, score 13.279) (writing took 2.4385258769616485 seconds)
2022-03-05 05:23:57 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 05:23:57 | INFO | train | epoch 245 | loss 1.794 | nll_loss 0.754 | ppl 1.69 | wps 24506.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23594 | lr 0.000205873 | gnorm 0.881 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 63359
2022-03-05 05:23:57 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 05:23:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:24:13 | INFO | train_inner | epoch 246:      6 / 97 loss=1.793, nll_loss=0.753, ppl=1.69, wps=24543.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23600, lr=0.000205847, gnorm=0.879, loss_scale=16, train_wall=236, gb_free=21, wall=63374
2022-03-05 05:28:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:28:11 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 13.353 | nll_loss 12.884 | ppl 7560.26 | wps 44358.2 | wpb 510.9 | bsz 1 | num_updates 23691 | best_loss 7.962
2022-03-05 05:28:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23691 updates
2022-03-05 05:28:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:28:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:28:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 246 @ 23691 updates, score 13.353) (writing took 2.4510820042341948 seconds)
2022-03-05 05:28:14 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 05:28:14 | INFO | train | epoch 246 | loss 1.792 | nll_loss 0.752 | ppl 1.68 | wps 24741.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23691 | lr 0.000205451 | gnorm 0.881 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 63616
2022-03-05 05:28:14 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 05:28:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:28:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:28:40 | INFO | train_inner | epoch 247:     10 / 97 loss=1.791, nll_loss=0.751, ppl=1.68, wps=24529.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=0.881, loss_scale=16, train_wall=237, gb_free=21, wall=63641
2022-03-05 05:32:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:32:28 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 13.27 | nll_loss 12.799 | ppl 7125.67 | wps 44149.3 | wpb 510.9 | bsz 1 | num_updates 23787 | best_loss 7.962
2022-03-05 05:32:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23787 updates
2022-03-05 05:32:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:32:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:32:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 247 @ 23787 updates, score 13.27) (writing took 2.4525953782722354 seconds)
2022-03-05 05:32:30 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 05:32:30 | INFO | train | epoch 247 | loss 1.791 | nll_loss 0.751 | ppl 1.68 | wps 24497.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23787 | lr 0.000205036 | gnorm 0.878 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 63872
2022-03-05 05:32:30 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 05:32:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:33:04 | INFO | train_inner | epoch 248:     13 / 97 loss=1.79, nll_loss=0.75, ppl=1.68, wps=24772, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23800, lr=0.00020498, gnorm=0.877, loss_scale=16, train_wall=234, gb_free=21, wall=63906
2022-03-05 05:34:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:36:45 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 13.239 | nll_loss 12.765 | ppl 6961.07 | wps 44490.2 | wpb 510.9 | bsz 1 | num_updates 23883 | best_loss 7.962
2022-03-05 05:36:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23883 updates
2022-03-05 05:36:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:36:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 248 @ 23883 updates, score 13.239) (writing took 2.451013292185962 seconds)
2022-03-05 05:36:47 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 05:36:47 | INFO | train | epoch 248 | loss 1.788 | nll_loss 0.749 | ppl 1.68 | wps 24494.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23883 | lr 0.000204624 | gnorm 0.873 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64129
2022-03-05 05:36:47 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 05:36:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:37:31 | INFO | train_inner | epoch 249:     17 / 97 loss=1.786, nll_loss=0.746, ppl=1.68, wps=24532.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=23900, lr=0.000204551, gnorm=0.874, loss_scale=16, train_wall=237, gb_free=21, wall=64173
2022-03-05 05:39:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:40:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:41:01 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 13.278 | nll_loss 12.805 | ppl 7155.08 | wps 44322.8 | wpb 510.9 | bsz 1 | num_updates 23979 | best_loss 7.962
2022-03-05 05:41:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23979 updates
2022-03-05 05:41:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:41:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:41:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 249 @ 23979 updates, score 13.278) (writing took 2.450530289672315 seconds)
2022-03-05 05:41:04 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 05:41:04 | INFO | train | epoch 249 | loss 1.786 | nll_loss 0.746 | ppl 1.68 | wps 24491.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23979 | lr 0.000204214 | gnorm 0.883 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64386
2022-03-05 05:41:04 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 05:41:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:41:58 | INFO | train_inner | epoch 250:     21 / 97 loss=1.785, nll_loss=0.745, ppl=1.68, wps=24532.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.879, loss_scale=16, train_wall=236, gb_free=21, wall=64440
2022-03-05 05:45:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:45:18 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 13.245 | nll_loss 12.769 | ppl 6978.48 | wps 44064.5 | wpb 510.9 | bsz 1 | num_updates 24076 | best_loss 7.962
2022-03-05 05:45:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24076 updates
2022-03-05 05:45:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:45:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 250 @ 24076 updates, score 13.245) (writing took 2.471372378990054 seconds)
2022-03-05 05:45:20 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 05:45:20 | INFO | train | epoch 250 | loss 1.783 | nll_loss 0.743 | ppl 1.67 | wps 24751 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24076 | lr 0.000203802 | gnorm 0.873 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64642
2022-03-05 05:45:20 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 05:45:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:45:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:46:25 | INFO | train_inner | epoch 251:     25 / 97 loss=1.781, nll_loss=0.741, ppl=1.67, wps=24533.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.872, loss_scale=16, train_wall=236, gb_free=21, wall=64707
2022-03-05 05:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:49:35 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 13.332 | nll_loss 12.861 | ppl 7440.64 | wps 44268.9 | wpb 510.9 | bsz 1 | num_updates 24172 | best_loss 7.962
2022-03-05 05:49:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24172 updates
2022-03-05 05:49:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:49:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:49:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 251 @ 24172 updates, score 13.332) (writing took 2.4489826057106256 seconds)
2022-03-05 05:49:37 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 05:49:37 | INFO | train | epoch 251 | loss 1.781 | nll_loss 0.742 | ppl 1.67 | wps 24507.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24172 | lr 0.000203397 | gnorm 0.881 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 64899
2022-03-05 05:49:37 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 05:49:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:50:49 | INFO | train_inner | epoch 252:     28 / 97 loss=1.781, nll_loss=0.741, ppl=1.67, wps=24781.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.886, loss_scale=16, train_wall=234, gb_free=21, wall=64971
2022-03-05 05:51:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:53:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:53:51 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 13.295 | nll_loss 12.822 | ppl 7240.79 | wps 44238.2 | wpb 510.9 | bsz 1 | num_updates 24268 | best_loss 7.962
2022-03-05 05:53:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24268 updates
2022-03-05 05:53:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:53:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:53:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 252 @ 24268 updates, score 13.295) (writing took 2.4505281066522 seconds)
2022-03-05 05:53:54 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 05:53:54 | INFO | train | epoch 252 | loss 1.78 | nll_loss 0.74 | ppl 1.67 | wps 24498.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24268 | lr 0.000202994 | gnorm 0.883 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 65155
2022-03-05 05:53:54 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 05:53:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:16 | INFO | train_inner | epoch 253:     32 / 97 loss=1.779, nll_loss=0.739, ppl=1.67, wps=24535.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.877, loss_scale=16, train_wall=236, gb_free=21, wall=65238
2022-03-05 05:57:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:57:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:58:08 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 13.313 | nll_loss 12.841 | ppl 7335.42 | wps 44251.8 | wpb 510.9 | bsz 1 | num_updates 24363 | best_loss 7.962
2022-03-05 05:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24363 updates
2022-03-05 05:58:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 05:58:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 253 @ 24363 updates, score 13.313) (writing took 2.440332463942468 seconds)
2022-03-05 05:58:10 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 05:58:10 | INFO | train | epoch 253 | loss 1.778 | nll_loss 0.738 | ppl 1.67 | wps 24232.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 24363 | lr 0.000202598 | gnorm 0.87 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 65412
2022-03-05 05:58:10 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 05:58:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:59:45 | INFO | train_inner | epoch 254:     37 / 97 loss=1.777, nll_loss=0.738, ppl=1.67, wps=24303, ups=0.37, wpb=65495, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.876, loss_scale=8, train_wall=239, gb_free=21, wall=65507
2022-03-05 06:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:02:25 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 13.383 | nll_loss 12.916 | ppl 7729.7 | wps 44398.7 | wpb 510.9 | bsz 1 | num_updates 24460 | best_loss 7.962
2022-03-05 06:02:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24460 updates
2022-03-05 06:02:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:02:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:02:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 254 @ 24460 updates, score 13.383) (writing took 2.4471819130703807 seconds)
2022-03-05 06:02:27 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 06:02:27 | INFO | train | epoch 254 | loss 1.776 | nll_loss 0.736 | ppl 1.67 | wps 24760.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24460 | lr 0.000202196 | gnorm 0.87 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 65669
2022-03-05 06:02:27 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 06:02:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:04:10 | INFO | train_inner | epoch 255:     40 / 97 loss=1.775, nll_loss=0.735, ppl=1.66, wps=24773.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.866, loss_scale=16, train_wall=234, gb_free=21, wall=65772
2022-03-05 06:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:06:41 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 13.319 | nll_loss 12.848 | ppl 7371.33 | wps 44268.7 | wpb 510.9 | bsz 1 | num_updates 24557 | best_loss 7.962
2022-03-05 06:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24557 updates
2022-03-05 06:06:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:06:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:06:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 255 @ 24557 updates, score 13.319) (writing took 2.4531275099143386 seconds)
2022-03-05 06:06:44 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 06:06:44 | INFO | train | epoch 255 | loss 1.775 | nll_loss 0.735 | ppl 1.66 | wps 24748 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24557 | lr 0.000201796 | gnorm 0.875 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 65925
2022-03-05 06:06:44 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 06:06:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:08:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:08:37 | INFO | train_inner | epoch 256:     44 / 97 loss=1.774, nll_loss=0.735, ppl=1.66, wps=24539.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.871, loss_scale=16, train_wall=236, gb_free=21, wall=66039
2022-03-05 06:10:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:10:58 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 13.271 | nll_loss 12.799 | ppl 7128.91 | wps 44343.1 | wpb 510.9 | bsz 1 | num_updates 24653 | best_loss 7.962
2022-03-05 06:10:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24653 updates
2022-03-05 06:10:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:11:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:11:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 256 @ 24653 updates, score 13.271) (writing took 2.4312338270246983 seconds)
2022-03-05 06:11:00 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 06:11:00 | INFO | train | epoch 256 | loss 1.773 | nll_loss 0.733 | ppl 1.66 | wps 24506.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24653 | lr 0.000201403 | gnorm 0.887 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66182
2022-03-05 06:11:00 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 06:11:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:13:01 | INFO | train_inner | epoch 257:     47 / 97 loss=1.77, nll_loss=0.731, ppl=1.66, wps=24788, ups=0.38, wpb=65495, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.891, loss_scale=16, train_wall=234, gb_free=21, wall=66303
2022-03-05 06:14:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:15:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:15:14 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 13.324 | nll_loss 12.855 | ppl 7406.9 | wps 44377 | wpb 510.9 | bsz 1 | num_updates 24749 | best_loss 7.962
2022-03-05 06:15:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24749 updates
2022-03-05 06:15:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:15:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:15:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 257 @ 24749 updates, score 13.324) (writing took 2.4511591773480177 seconds)
2022-03-05 06:15:17 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 06:15:17 | INFO | train | epoch 257 | loss 1.77 | nll_loss 0.731 | ppl 1.66 | wps 24517 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24749 | lr 0.000201012 | gnorm 0.874 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66438
2022-03-05 06:15:17 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 06:15:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:17:28 | INFO | train_inner | epoch 258:     51 / 97 loss=1.768, nll_loss=0.729, ppl=1.66, wps=24550, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.863, loss_scale=16, train_wall=236, gb_free=21, wall=66569
2022-03-05 06:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:19:31 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 13.247 | nll_loss 12.769 | ppl 6980.3 | wps 44328.8 | wpb 510.9 | bsz 1 | num_updates 24846 | best_loss 7.962
2022-03-05 06:19:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24846 updates
2022-03-05 06:19:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:19:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:19:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 258 @ 24846 updates, score 13.247) (writing took 2.469697253778577 seconds)
2022-03-05 06:19:33 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 06:19:33 | INFO | train | epoch 258 | loss 1.768 | nll_loss 0.729 | ppl 1.66 | wps 24765.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24846 | lr 0.000200619 | gnorm 0.864 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66695
2022-03-05 06:19:33 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 06:19:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:19:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:21:55 | INFO | train_inner | epoch 259:     55 / 97 loss=1.769, nll_loss=0.729, ppl=1.66, wps=24548.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.879, loss_scale=16, train_wall=236, gb_free=21, wall=66836
2022-03-05 06:23:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:23:47 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 13.291 | nll_loss 12.822 | ppl 7241.41 | wps 44303.2 | wpb 510.9 | bsz 1 | num_updates 24942 | best_loss 7.962
2022-03-05 06:23:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24942 updates
2022-03-05 06:23:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:23:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 259 @ 24942 updates, score 13.291) (writing took 2.4658260168507695 seconds)
2022-03-05 06:23:50 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 06:23:50 | INFO | train | epoch 259 | loss 1.767 | nll_loss 0.727 | ppl 1.66 | wps 24510.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24942 | lr 0.000200232 | gnorm 0.875 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 66951
2022-03-05 06:23:50 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 06:23:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:25:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:26:21 | INFO | train_inner | epoch 260:     59 / 97 loss=1.766, nll_loss=0.726, ppl=1.65, wps=24571.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25000, lr=0.0002, gnorm=0.866, loss_scale=16, train_wall=236, gb_free=21, wall=67103
2022-03-05 06:27:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:28:03 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 13.315 | nll_loss 12.848 | ppl 7373.15 | wps 44349.1 | wpb 510.9 | bsz 1 | num_updates 25038 | best_loss 7.962
2022-03-05 06:28:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25038 updates
2022-03-05 06:28:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:28:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:28:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 260 @ 25038 updates, score 13.315) (writing took 2.4388465797528625 seconds)
2022-03-05 06:28:06 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 06:28:06 | INFO | train | epoch 260 | loss 1.764 | nll_loss 0.725 | ppl 1.65 | wps 24547.1 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 25038 | lr 0.000199848 | gnorm 0.878 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67208
2022-03-05 06:28:06 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 06:28:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:30:45 | INFO | train_inner | epoch 261:     62 / 97 loss=1.765, nll_loss=0.725, ppl=1.65, wps=24824.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.875, loss_scale=16, train_wall=234, gb_free=21, wall=67367
2022-03-05 06:31:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:32:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:32:20 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 13.277 | nll_loss 12.807 | ppl 7164.89 | wps 44359.2 | wpb 510.9 | bsz 1 | num_updates 25134 | best_loss 7.962
2022-03-05 06:32:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25134 updates
2022-03-05 06:32:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:32:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:32:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 261 @ 25134 updates, score 13.277) (writing took 2.530684381723404 seconds)
2022-03-05 06:32:22 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 06:32:22 | INFO | train | epoch 261 | loss 1.763 | nll_loss 0.724 | ppl 1.65 | wps 24536.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25134 | lr 0.000199466 | gnorm 0.863 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67464
2022-03-05 06:32:22 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 06:32:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:35:11 | INFO | train_inner | epoch 262:     66 / 97 loss=1.763, nll_loss=0.723, ppl=1.65, wps=24574.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.872, loss_scale=16, train_wall=236, gb_free=21, wall=67633
2022-03-05 06:36:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:36:36 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 13.331 | nll_loss 12.862 | ppl 7443.66 | wps 44488.3 | wpb 510.9 | bsz 1 | num_updates 25231 | best_loss 7.962
2022-03-05 06:36:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25231 updates
2022-03-05 06:36:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:36:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:36:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 262 @ 25231 updates, score 13.331) (writing took 2.4756067879498005 seconds)
2022-03-05 06:36:38 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 06:36:38 | INFO | train | epoch 262 | loss 1.762 | nll_loss 0.723 | ppl 1.65 | wps 24797.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25231 | lr 0.000199082 | gnorm 0.87 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67720
2022-03-05 06:36:38 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 06:36:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:37:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:39:38 | INFO | train_inner | epoch 263:     70 / 97 loss=1.759, nll_loss=0.72, ppl=1.65, wps=24581.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.854, loss_scale=16, train_wall=236, gb_free=21, wall=67900
2022-03-05 06:40:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:40:52 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 13.264 | nll_loss 12.794 | ppl 7099.9 | wps 44281.7 | wpb 510.9 | bsz 1 | num_updates 25327 | best_loss 7.962
2022-03-05 06:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25327 updates
2022-03-05 06:40:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:40:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:40:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 263 @ 25327 updates, score 13.264) (writing took 2.5256107803434134 seconds)
2022-03-05 06:40:55 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 06:40:55 | INFO | train | epoch 263 | loss 1.759 | nll_loss 0.719 | ppl 1.65 | wps 24531.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25327 | lr 0.000198705 | gnorm 0.857 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 67976
2022-03-05 06:40:55 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 06:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:43:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:44:04 | INFO | train_inner | epoch 264:     74 / 97 loss=1.759, nll_loss=0.72, ppl=1.65, wps=24580.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.873, loss_scale=16, train_wall=236, gb_free=21, wall=68166
2022-03-05 06:45:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:45:08 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 13.311 | nll_loss 12.842 | ppl 7341.46 | wps 44560 | wpb 510.9 | bsz 1 | num_updates 25423 | best_loss 7.962
2022-03-05 06:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25423 updates
2022-03-05 06:45:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:45:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:45:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 264 @ 25423 updates, score 13.311) (writing took 2.5202280366793275 seconds)
2022-03-05 06:45:11 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 06:45:11 | INFO | train | epoch 264 | loss 1.758 | nll_loss 0.719 | ppl 1.65 | wps 24550 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25423 | lr 0.000198329 | gnorm 0.875 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68232
2022-03-05 06:45:11 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 06:45:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:48:28 | INFO | train_inner | epoch 265:     77 / 97 loss=1.757, nll_loss=0.717, ppl=1.64, wps=24821.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.874, loss_scale=16, train_wall=234, gb_free=21, wall=68430
2022-03-05 06:48:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:49:24 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 13.323 | nll_loss 12.853 | ppl 7399.9 | wps 44333.7 | wpb 510.9 | bsz 1 | num_updates 25519 | best_loss 7.962
2022-03-05 06:49:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25519 updates
2022-03-05 06:49:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:49:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 265 @ 25519 updates, score 13.323) (writing took 2.4765688134357333 seconds)
2022-03-05 06:49:27 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 06:49:27 | INFO | train | epoch 265 | loss 1.756 | nll_loss 0.717 | ppl 1.64 | wps 24546.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25519 | lr 0.000197956 | gnorm 0.87 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68488
2022-03-05 06:49:27 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 06:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:52:55 | INFO | train_inner | epoch 266:     81 / 97 loss=1.755, nll_loss=0.716, ppl=1.64, wps=24579.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.86, loss_scale=16, train_wall=236, gb_free=21, wall=68696
2022-03-05 06:53:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:53:40 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 13.294 | nll_loss 12.821 | ppl 7237.65 | wps 44484.3 | wpb 510.9 | bsz 1 | num_updates 25616 | best_loss 7.962
2022-03-05 06:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25616 updates
2022-03-05 06:53:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:53:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 266 @ 25616 updates, score 13.294) (writing took 2.5042318161576986 seconds)
2022-03-05 06:53:43 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 06:53:43 | INFO | train | epoch 266 | loss 1.754 | nll_loss 0.715 | ppl 1.64 | wps 24788.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25616 | lr 0.000197581 | gnorm 0.862 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 68745
2022-03-05 06:53:43 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 06:53:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:54:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:57:21 | INFO | train_inner | epoch 267:     85 / 97 loss=1.754, nll_loss=0.715, ppl=1.64, wps=24588, ups=0.38, wpb=65495, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.87, loss_scale=16, train_wall=236, gb_free=21, wall=68963
2022-03-05 06:57:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:57 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 13.322 | nll_loss 12.856 | ppl 7412.11 | wps 44426.7 | wpb 510.9 | bsz 1 | num_updates 25712 | best_loss 7.962
2022-03-05 06:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25712 updates
2022-03-05 06:57:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:57:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 06:57:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 267 @ 25712 updates, score 13.322) (writing took 2.5183812845498323 seconds)
2022-03-05 06:57:59 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 06:57:59 | INFO | train | epoch 267 | loss 1.753 | nll_loss 0.714 | ppl 1.64 | wps 24554 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25712 | lr 0.000197211 | gnorm 0.87 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69001
2022-03-05 06:57:59 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 06:57:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:00:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:01:47 | INFO | train_inner | epoch 268:     89 / 97 loss=1.751, nll_loss=0.712, ppl=1.64, wps=24572.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.869, loss_scale=16, train_wall=236, gb_free=21, wall=69229
2022-03-05 07:02:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:02:13 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 13.307 | nll_loss 12.838 | ppl 7319.63 | wps 44629.4 | wpb 510.9 | bsz 1 | num_updates 25808 | best_loss 7.962
2022-03-05 07:02:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25808 updates
2022-03-05 07:02:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:02:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:02:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 268 @ 25808 updates, score 13.307) (writing took 2.4913718961179256 seconds)
2022-03-05 07:02:15 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 07:02:15 | INFO | train | epoch 268 | loss 1.75 | nll_loss 0.711 | ppl 1.64 | wps 24537.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25808 | lr 0.000196844 | gnorm 0.868 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69257
2022-03-05 07:02:15 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 07:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:05:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:06:14 | INFO | train_inner | epoch 269:     93 / 97 loss=1.749, nll_loss=0.71, ppl=1.64, wps=24588.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.862, loss_scale=16, train_wall=236, gb_free=21, wall=69496
2022-03-05 07:06:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:06:29 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 13.311 | nll_loss 12.844 | ppl 7350.6 | wps 44464.5 | wpb 510.9 | bsz 1 | num_updates 25904 | best_loss 7.962
2022-03-05 07:06:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25904 updates
2022-03-05 07:06:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:06:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:06:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 269 @ 25904 updates, score 13.311) (writing took 2.5339065454900265 seconds)
2022-03-05 07:06:31 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 07:06:31 | INFO | train | epoch 269 | loss 1.747 | nll_loss 0.708 | ppl 1.63 | wps 24545.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25904 | lr 0.000196479 | gnorm 0.858 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69513
2022-03-05 07:06:31 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 07:06:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:10:38 | INFO | train_inner | epoch 270:     96 / 97 loss=1.748, nll_loss=0.709, ppl=1.63, wps=24820, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.876, loss_scale=16, train_wall=234, gb_free=21, wall=69759
2022-03-05 07:10:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:10:45 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 13.392 | nll_loss 12.929 | ppl 7798.82 | wps 44385.2 | wpb 510.9 | bsz 1 | num_updates 26001 | best_loss 7.962
2022-03-05 07:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26001 updates
2022-03-05 07:10:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:10:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 270 @ 26001 updates, score 13.392) (writing took 2.5526673663407564 seconds)
2022-03-05 07:10:48 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 07:10:48 | INFO | train | epoch 270 | loss 1.748 | nll_loss 0.708 | ppl 1.63 | wps 24795.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26001 | lr 0.000196112 | gnorm 0.876 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 69769
2022-03-05 07:10:48 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 07:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:11:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:14:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:15:01 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 13.342 | nll_loss 12.877 | ppl 7522.86 | wps 44279.7 | wpb 510.9 | bsz 1 | num_updates 26097 | best_loss 7.962
2022-03-05 07:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26097 updates
2022-03-05 07:15:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:15:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 271 @ 26097 updates, score 13.342) (writing took 2.4941877517849207 seconds)
2022-03-05 07:15:04 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 07:15:04 | INFO | train | epoch 271 | loss 1.745 | nll_loss 0.706 | ppl 1.63 | wps 24546.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26097 | lr 0.000195751 | gnorm 0.865 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70026
2022-03-05 07:15:04 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 07:15:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:15:12 | INFO | train_inner | epoch 272:      3 / 97 loss=1.745, nll_loss=0.706, ppl=1.63, wps=23899.7, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=26100, lr=0.00019574, gnorm=0.866, loss_scale=16, train_wall=236, gb_free=21, wall=70033
2022-03-05 07:17:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:19:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:19:17 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 13.324 | nll_loss 12.859 | ppl 7429.13 | wps 44608.2 | wpb 510.9 | bsz 1 | num_updates 26193 | best_loss 7.962
2022-03-05 07:19:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26193 updates
2022-03-05 07:19:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:19:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:19:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 272 @ 26193 updates, score 13.324) (writing took 2.5157092912122607 seconds)
2022-03-05 07:19:20 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 07:19:20 | INFO | train | epoch 272 | loss 1.743 | nll_loss 0.704 | ppl 1.63 | wps 24541.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26193 | lr 0.000195392 | gnorm 0.869 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70282
2022-03-05 07:19:20 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 07:19:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:19:38 | INFO | train_inner | epoch 273:      7 / 97 loss=1.742, nll_loss=0.703, ppl=1.63, wps=24581.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26200, lr=0.000195366, gnorm=0.865, loss_scale=16, train_wall=236, gb_free=21, wall=70300
2022-03-05 07:23:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:23:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:23:34 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 13.373 | nll_loss 12.911 | ppl 7700.49 | wps 44293 | wpb 510.9 | bsz 1 | num_updates 26289 | best_loss 7.962
2022-03-05 07:23:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26289 updates
2022-03-05 07:23:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:23:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 273 @ 26289 updates, score 13.373) (writing took 2.420154088176787 seconds)
2022-03-05 07:23:36 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 07:23:36 | INFO | train | epoch 273 | loss 1.741 | nll_loss 0.702 | ppl 1.63 | wps 24552.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26289 | lr 0.000195035 | gnorm 0.868 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70538
2022-03-05 07:23:36 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 07:23:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:24:04 | INFO | train_inner | epoch 274:     11 / 97 loss=1.74, nll_loss=0.701, ppl=1.63, wps=24592.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26300, lr=0.000194994, gnorm=0.869, loss_scale=16, train_wall=236, gb_free=21, wall=70566
2022-03-05 07:27:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:27:50 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 13.318 | nll_loss 12.853 | ppl 7396.4 | wps 44534.7 | wpb 510.9 | bsz 1 | num_updates 26386 | best_loss 7.962
2022-03-05 07:27:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26386 updates
2022-03-05 07:27:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:27:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:27:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 274 @ 26386 updates, score 13.318) (writing took 2.5157005479559302 seconds)
2022-03-05 07:27:52 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 07:27:52 | INFO | train | epoch 274 | loss 1.739 | nll_loss 0.7 | ppl 1.62 | wps 24795.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26386 | lr 0.000194676 | gnorm 0.856 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 70794
2022-03-05 07:27:52 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 07:27:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:28:28 | INFO | train_inner | epoch 275:     14 / 97 loss=1.738, nll_loss=0.698, ppl=1.62, wps=24818.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.851, loss_scale=16, train_wall=234, gb_free=21, wall=70830
2022-03-05 07:29:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:32:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:32:06 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 13.322 | nll_loss 12.857 | ppl 7416.96 | wps 44250.7 | wpb 510.9 | bsz 1 | num_updates 26482 | best_loss 7.962
2022-03-05 07:32:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26482 updates
2022-03-05 07:32:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:32:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:32:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 275 @ 26482 updates, score 13.322) (writing took 2.541833152063191 seconds)
2022-03-05 07:32:08 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 07:32:08 | INFO | train | epoch 275 | loss 1.739 | nll_loss 0.7 | ppl 1.62 | wps 24538.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26482 | lr 0.000194323 | gnorm 0.856 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 71050
2022-03-05 07:32:08 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 07:32:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:32:55 | INFO | train_inner | epoch 276:     18 / 97 loss=1.737, nll_loss=0.698, ppl=1.62, wps=24578.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26500, lr=0.000194257, gnorm=0.857, loss_scale=16, train_wall=236, gb_free=21, wall=71096
2022-03-05 07:34:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:36:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:36:22 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 13.326 | nll_loss 12.857 | ppl 7419.28 | wps 44367.3 | wpb 510.9 | bsz 1 | num_updates 26578 | best_loss 7.962
2022-03-05 07:36:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26578 updates
2022-03-05 07:36:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:36:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 276 @ 26578 updates, score 13.326) (writing took 2.4955980563536286 seconds)
2022-03-05 07:36:25 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 07:36:25 | INFO | train | epoch 276 | loss 1.736 | nll_loss 0.697 | ppl 1.62 | wps 24547.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26578 | lr 0.000193972 | gnorm 0.865 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 71306
2022-03-05 07:36:25 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 07:36:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:37:21 | INFO | train_inner | epoch 277:     22 / 97 loss=1.736, nll_loss=0.697, ppl=1.62, wps=24584.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=26600, lr=0.000193892, gnorm=0.869, loss_scale=16, train_wall=236, gb_free=21, wall=71363
2022-03-05 07:40:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:40:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:40:38 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 13.324 | nll_loss 12.859 | ppl 7431.77 | wps 44298.6 | wpb 510.9 | bsz 1 | num_updates 26674 | best_loss 7.962
2022-03-05 07:40:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26674 updates
2022-03-05 07:40:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:40:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:40:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 277 @ 26674 updates, score 13.324) (writing took 2.740199306048453 seconds)
2022-03-05 07:40:41 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 07:40:41 | INFO | train | epoch 277 | loss 1.735 | nll_loss 0.697 | ppl 1.62 | wps 24525.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26674 | lr 0.000193623 | gnorm 0.856 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 71563
2022-03-05 07:40:41 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 07:40:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:41:48 | INFO | train_inner | epoch 278:     26 / 97 loss=1.736, nll_loss=0.697, ppl=1.62, wps=24566.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.854, loss_scale=16, train_wall=236, gb_free=21, wall=71629
2022-03-05 07:44:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:44:55 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 13.364 | nll_loss 12.899 | ppl 7638.12 | wps 44231.9 | wpb 510.9 | bsz 1 | num_updates 26771 | best_loss 7.962
2022-03-05 07:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26771 updates
2022-03-05 07:44:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:44:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:44:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 278 @ 26771 updates, score 13.364) (writing took 2.552453705109656 seconds)
2022-03-05 07:44:57 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 07:44:57 | INFO | train | epoch 278 | loss 1.734 | nll_loss 0.695 | ppl 1.62 | wps 24781.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26771 | lr 0.000193271 | gnorm 0.861 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 71819
2022-03-05 07:44:57 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 07:44:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:46:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:46:14 | INFO | train_inner | epoch 279:     30 / 97 loss=1.733, nll_loss=0.694, ppl=1.62, wps=24562.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.864, loss_scale=16, train_wall=236, gb_free=21, wall=71896
2022-03-05 07:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:49:11 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 13.31 | nll_loss 12.844 | ppl 7351.17 | wps 44128.6 | wpb 510.9 | bsz 1 | num_updates 26867 | best_loss 7.962
2022-03-05 07:49:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26867 updates
2022-03-05 07:49:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:49:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 279 @ 26867 updates, score 13.31) (writing took 2.5372052788734436 seconds)
2022-03-05 07:49:14 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 07:49:14 | INFO | train | epoch 279 | loss 1.732 | nll_loss 0.693 | ppl 1.62 | wps 24534.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26867 | lr 0.000192926 | gnorm 0.862 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 72075
2022-03-05 07:49:14 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 07:49:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:50:38 | INFO | train_inner | epoch 280:     33 / 97 loss=1.731, nll_loss=0.692, ppl=1.62, wps=24816.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.847, loss_scale=16, train_wall=234, gb_free=21, wall=72160
2022-03-05 07:51:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:53:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:53:27 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 13.294 | nll_loss 12.827 | ppl 7265.88 | wps 43312.4 | wpb 510.9 | bsz 1 | num_updates 26963 | best_loss 7.962
2022-03-05 07:53:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26963 updates
2022-03-05 07:53:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:53:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 280 @ 26963 updates, score 13.294) (writing took 2.5201611248776317 seconds)
2022-03-05 07:53:30 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 07:53:30 | INFO | train | epoch 280 | loss 1.73 | nll_loss 0.691 | ppl 1.61 | wps 24536.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26963 | lr 0.000192582 | gnorm 0.84 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 72332
2022-03-05 07:53:30 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 07:53:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:53:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:55:07 | INFO | train_inner | epoch 281:     38 / 97 loss=1.73, nll_loss=0.691, ppl=1.61, wps=24344.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.851, loss_scale=8, train_wall=238, gb_free=21, wall=72429
2022-03-05 07:57:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:57:43 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 13.367 | nll_loss 12.906 | ppl 7677.52 | wps 43221.7 | wpb 510.9 | bsz 1 | num_updates 27059 | best_loss 7.962
2022-03-05 07:57:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27059 updates
2022-03-05 07:57:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:57:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 07:57:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 281 @ 27059 updates, score 13.367) (writing took 2.5512676360085607 seconds)
2022-03-05 07:57:46 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 07:57:46 | INFO | train | epoch 281 | loss 1.729 | nll_loss 0.69 | ppl 1.61 | wps 24536.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27059 | lr 0.00019224 | gnorm 0.851 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 72588
2022-03-05 07:57:46 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 07:57:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:59:31 | INFO | train_inner | epoch 282:     41 / 97 loss=1.727, nll_loss=0.689, ppl=1.61, wps=24807, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.853, loss_scale=16, train_wall=234, gb_free=21, wall=72693
2022-03-05 08:01:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:02:00 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 13.354 | nll_loss 12.892 | ppl 7598.79 | wps 43825.5 | wpb 510.9 | bsz 1 | num_updates 27156 | best_loss 7.962
2022-03-05 08:02:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27156 updates
2022-03-05 08:02:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:02:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:02:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 282 @ 27156 updates, score 13.354) (writing took 5.459411998279393 seconds)
2022-03-05 08:02:05 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 08:02:05 | INFO | train | epoch 282 | loss 1.728 | nll_loss 0.689 | ppl 1.61 | wps 24507.7 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 27156 | lr 0.000191897 | gnorm 0.86 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 72847
2022-03-05 08:02:05 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 08:02:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:03:58 | INFO | train_inner | epoch 283:     44 / 97 loss=1.727, nll_loss=0.688, ppl=1.61, wps=24537.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.855, loss_scale=16, train_wall=234, gb_free=21, wall=72960
2022-03-05 08:05:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:06:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:06:19 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 13.386 | nll_loss 12.929 | ppl 7799.02 | wps 43261.7 | wpb 510.9 | bsz 1 | num_updates 27252 | best_loss 7.962
2022-03-05 08:06:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27252 updates
2022-03-05 08:06:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:06:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 283 @ 27252 updates, score 13.386) (writing took 2.499726071022451 seconds)
2022-03-05 08:06:22 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 08:06:22 | INFO | train | epoch 283 | loss 1.725 | nll_loss 0.686 | ppl 1.61 | wps 24527.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27252 | lr 0.000191558 | gnorm 0.85 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 73103
2022-03-05 08:06:22 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 08:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:06:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:08:27 | INFO | train_inner | epoch 284:     49 / 97 loss=1.724, nll_loss=0.685, ppl=1.61, wps=24341, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.85, loss_scale=8, train_wall=238, gb_free=21, wall=73229
2022-03-05 08:10:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:10:35 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 13.328 | nll_loss 12.862 | ppl 7446.51 | wps 44067.4 | wpb 510.9 | bsz 1 | num_updates 27348 | best_loss 7.962
2022-03-05 08:10:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27348 updates
2022-03-05 08:10:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:10:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 284 @ 27348 updates, score 13.328) (writing took 2.51997590623796 seconds)
2022-03-05 08:10:38 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 08:10:38 | INFO | train | epoch 284 | loss 1.725 | nll_loss 0.686 | ppl 1.61 | wps 24554.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27348 | lr 0.000191222 | gnorm 0.856 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 73359
2022-03-05 08:10:38 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 08:10:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:12:51 | INFO | train_inner | epoch 285:     52 / 97 loss=1.725, nll_loss=0.686, ppl=1.61, wps=24820.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.848, loss_scale=16, train_wall=234, gb_free=21, wall=73493
2022-03-05 08:14:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:14:51 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 13.324 | nll_loss 12.861 | ppl 7439.44 | wps 44258.6 | wpb 510.9 | bsz 1 | num_updates 27445 | best_loss 7.962
2022-03-05 08:14:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27445 updates
2022-03-05 08:14:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:14:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 285 @ 27445 updates, score 13.324) (writing took 2.5104612363502383 seconds)
2022-03-05 08:14:54 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 08:14:54 | INFO | train | epoch 285 | loss 1.722 | nll_loss 0.684 | ppl 1.61 | wps 24791.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27445 | lr 0.000190883 | gnorm 0.85 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 73616
2022-03-05 08:14:54 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 08:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:17:15 | INFO | train_inner | epoch 286:     55 / 97 loss=1.721, nll_loss=0.683, ppl=1.61, wps=24816.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27500, lr=0.000190693, gnorm=0.856, loss_scale=16, train_wall=234, gb_free=21, wall=73757
2022-03-05 08:17:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:19:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:19:07 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 13.402 | nll_loss 12.941 | ppl 7866.03 | wps 44322 | wpb 510.9 | bsz 1 | num_updates 27541 | best_loss 7.962
2022-03-05 08:19:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27541 updates
2022-03-05 08:19:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:19:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:19:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 286 @ 27541 updates, score 13.402) (writing took 2.5227587409317493 seconds)
2022-03-05 08:19:10 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 08:19:10 | INFO | train | epoch 286 | loss 1.721 | nll_loss 0.683 | ppl 1.61 | wps 24544.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27541 | lr 0.000190551 | gnorm 0.847 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 73872
2022-03-05 08:19:10 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 08:19:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:21:41 | INFO | train_inner | epoch 287:     59 / 97 loss=1.721, nll_loss=0.682, ppl=1.6, wps=24582.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.851, loss_scale=16, train_wall=236, gb_free=21, wall=74023
2022-03-05 08:23:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:23:24 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 13.362 | nll_loss 12.897 | ppl 7625.47 | wps 44270.4 | wpb 510.9 | bsz 1 | num_updates 27638 | best_loss 7.962
2022-03-05 08:23:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27638 updates
2022-03-05 08:23:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:23:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:23:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 287 @ 27638 updates, score 13.362) (writing took 2.539405985735357 seconds)
2022-03-05 08:23:26 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 08:23:26 | INFO | train | epoch 287 | loss 1.72 | nll_loss 0.682 | ppl 1.6 | wps 24793 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27638 | lr 0.000190216 | gnorm 0.855 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 74128
2022-03-05 08:23:26 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 08:23:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:23:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:26:08 | INFO | train_inner | epoch 288:     63 / 97 loss=1.719, nll_loss=0.681, ppl=1.6, wps=24572, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.85, loss_scale=16, train_wall=236, gb_free=21, wall=74290
2022-03-05 08:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:27:40 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 13.386 | nll_loss 12.925 | ppl 7774.76 | wps 44602.1 | wpb 510.9 | bsz 1 | num_updates 27734 | best_loss 7.962
2022-03-05 08:27:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27734 updates
2022-03-05 08:27:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:27:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:27:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 288 @ 27734 updates, score 13.386) (writing took 2.5405868673697114 seconds)
2022-03-05 08:27:42 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 08:27:42 | INFO | train | epoch 288 | loss 1.718 | nll_loss 0.679 | ppl 1.6 | wps 24534.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27734 | lr 0.000189886 | gnorm 0.845 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 74384
2022-03-05 08:27:43 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 08:27:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:29:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:30:34 | INFO | train_inner | epoch 289:     67 / 97 loss=1.717, nll_loss=0.679, ppl=1.6, wps=24582.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.845, loss_scale=16, train_wall=236, gb_free=21, wall=74556
2022-03-05 08:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:31:56 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 13.34 | nll_loss 12.88 | ppl 7538.96 | wps 44513.8 | wpb 510.9 | bsz 1 | num_updates 27830 | best_loss 7.962
2022-03-05 08:31:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27830 updates
2022-03-05 08:31:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:31:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:31:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 289 @ 27830 updates, score 13.34) (writing took 2.478527320548892 seconds)
2022-03-05 08:31:59 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 08:31:59 | INFO | train | epoch 289 | loss 1.717 | nll_loss 0.679 | ppl 1.6 | wps 24545.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27830 | lr 0.000189559 | gnorm 0.846 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 74640
2022-03-05 08:31:59 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 08:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:34:58 | INFO | train_inner | epoch 290:     70 / 97 loss=1.717, nll_loss=0.678, ppl=1.6, wps=24826.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.843, loss_scale=32, train_wall=234, gb_free=21, wall=74820
2022-03-05 08:35:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:36:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:36:12 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 13.371 | nll_loss 12.909 | ppl 7691.81 | wps 44649.3 | wpb 510.9 | bsz 1 | num_updates 27926 | best_loss 7.962
2022-03-05 08:36:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27926 updates
2022-03-05 08:36:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:36:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 290 @ 27926 updates, score 13.371) (writing took 2.5689928214997053 seconds)
2022-03-05 08:36:15 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 08:36:15 | INFO | train | epoch 290 | loss 1.715 | nll_loss 0.676 | ppl 1.6 | wps 24558.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27926 | lr 0.000189232 | gnorm 0.843 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 74896
2022-03-05 08:36:15 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 08:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:39:24 | INFO | train_inner | epoch 291:     74 / 97 loss=1.714, nll_loss=0.676, ppl=1.6, wps=24609.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.854, loss_scale=16, train_wall=236, gb_free=21, wall=75086
2022-03-05 08:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:40:28 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 13.354 | nll_loss 12.894 | ppl 7611.81 | wps 44706.9 | wpb 510.9 | bsz 1 | num_updates 28023 | best_loss 7.962
2022-03-05 08:40:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28023 updates
2022-03-05 08:40:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:40:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 291 @ 28023 updates, score 13.354) (writing took 2.6004614625126123 seconds)
2022-03-05 08:40:31 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 08:40:31 | INFO | train | epoch 291 | loss 1.714 | nll_loss 0.676 | ppl 1.6 | wps 24820.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28023 | lr 0.000188905 | gnorm 0.854 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75152
2022-03-05 08:40:31 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 08:40:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:40:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:43:51 | INFO | train_inner | epoch 292:     78 / 97 loss=1.714, nll_loss=0.676, ppl=1.6, wps=24603, ups=0.38, wpb=65495, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.853, loss_scale=16, train_wall=236, gb_free=21, wall=75352
2022-03-05 08:44:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:44:44 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 13.393 | nll_loss 12.934 | ppl 7826.94 | wps 44151.4 | wpb 510.9 | bsz 1 | num_updates 28119 | best_loss 7.962
2022-03-05 08:44:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28119 updates
2022-03-05 08:44:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:44:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:44:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 292 @ 28119 updates, score 13.393) (writing took 2.474540149793029 seconds)
2022-03-05 08:44:47 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 08:44:47 | INFO | train | epoch 292 | loss 1.712 | nll_loss 0.674 | ppl 1.6 | wps 24566.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28119 | lr 0.000188582 | gnorm 0.849 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 75408
2022-03-05 08:44:47 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 08:44:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:46:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:48:17 | INFO | train_inner | epoch 293:     82 / 97 loss=1.71, nll_loss=0.671, ppl=1.59, wps=24596.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.843, loss_scale=16, train_wall=236, gb_free=21, wall=75619
2022-03-05 08:48:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:49:00 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 13.341 | nll_loss 12.879 | ppl 7530.86 | wps 44457.6 | wpb 510.9 | bsz 1 | num_updates 28215 | best_loss 7.962
2022-03-05 08:49:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28215 updates
2022-03-05 08:49:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:49:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 293 @ 28215 updates, score 13.341) (writing took 2.5602593617513776 seconds)
2022-03-05 08:49:03 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 08:49:03 | INFO | train | epoch 293 | loss 1.71 | nll_loss 0.672 | ppl 1.59 | wps 24552.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28215 | lr 0.000188261 | gnorm 0.846 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 75664
2022-03-05 08:49:03 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 08:49:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:52:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:52:43 | INFO | train_inner | epoch 294:     86 / 97 loss=1.712, nll_loss=0.674, ppl=1.6, wps=24599, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.842, loss_scale=16, train_wall=236, gb_free=21, wall=75885
2022-03-05 08:53:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:53:16 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 13.342 | nll_loss 12.881 | ppl 7545.02 | wps 44630.3 | wpb 510.9 | bsz 1 | num_updates 28311 | best_loss 7.962
2022-03-05 08:53:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28311 updates
2022-03-05 08:53:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:53:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:53:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 294 @ 28311 updates, score 13.342) (writing took 2.6223461516201496 seconds)
2022-03-05 08:53:19 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 08:53:19 | INFO | train | epoch 294 | loss 1.71 | nll_loss 0.672 | ppl 1.59 | wps 24555.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28311 | lr 0.000187941 | gnorm 0.839 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75920
2022-03-05 08:53:19 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 08:53:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:57:07 | INFO | train_inner | epoch 295:     89 / 97 loss=1.708, nll_loss=0.669, ppl=1.59, wps=24819.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.848, loss_scale=16, train_wall=234, gb_free=21, wall=76149
2022-03-05 08:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:57:32 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 13.347 | nll_loss 12.885 | ppl 7563.19 | wps 44483.7 | wpb 510.9 | bsz 1 | num_updates 28408 | best_loss 7.962
2022-03-05 08:57:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28408 updates
2022-03-05 08:57:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:57:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 08:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 295 @ 28408 updates, score 13.347) (writing took 2.541100756265223 seconds)
2022-03-05 08:57:35 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 08:57:35 | INFO | train | epoch 295 | loss 1.707 | nll_loss 0.669 | ppl 1.59 | wps 24798.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28408 | lr 0.00018762 | gnorm 0.848 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 76177
2022-03-05 08:57:35 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 08:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:58:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:01:33 | INFO | train_inner | epoch 296:     93 / 97 loss=1.708, nll_loss=0.67, ppl=1.59, wps=24596.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.848, loss_scale=16, train_wall=236, gb_free=21, wall=76415
2022-03-05 09:01:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:01:48 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 13.366 | nll_loss 12.906 | ppl 7676.21 | wps 44363.9 | wpb 510.9 | bsz 1 | num_updates 28504 | best_loss 7.962
2022-03-05 09:01:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28504 updates
2022-03-05 09:01:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:01:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:01:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 296 @ 28504 updates, score 13.366) (writing took 2.556034430861473 seconds)
2022-03-05 09:01:51 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 09:01:51 | INFO | train | epoch 296 | loss 1.706 | nll_loss 0.668 | ppl 1.59 | wps 24558.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28504 | lr 0.000187304 | gnorm 0.847 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 76433
2022-03-05 09:01:51 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 09:01:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:03:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:05:59 | INFO | train_inner | epoch 297:     97 / 97 loss=1.705, nll_loss=0.667, ppl=1.59, wps=24592.1, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=28600, lr=0.000186989, gnorm=0.844, loss_scale=16, train_wall=236, gb_free=21, wall=76681
2022-03-05 09:05:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:06:04 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 13.411 | nll_loss 12.953 | ppl 7928.08 | wps 44335.6 | wpb 510.9 | bsz 1 | num_updates 28600 | best_loss 7.962
2022-03-05 09:06:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28600 updates
2022-03-05 09:06:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:06:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 297 @ 28600 updates, score 13.411) (writing took 2.6238176338374615 seconds)
2022-03-05 09:06:07 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 09:06:07 | INFO | train | epoch 297 | loss 1.705 | nll_loss 0.666 | ppl 1.59 | wps 24548.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28600 | lr 0.000186989 | gnorm 0.843 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 76689
2022-03-05 09:06:07 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 09:06:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:10:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:10:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:10:20 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 13.296 | nll_loss 12.838 | ppl 7319.37 | wps 44381.1 | wpb 510.9 | bsz 1 | num_updates 28695 | best_loss 7.962
2022-03-05 09:10:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28695 updates
2022-03-05 09:10:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 298 @ 28695 updates, score 13.296) (writing took 2.536731746979058 seconds)
2022-03-05 09:10:23 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 09:10:23 | INFO | train | epoch 298 | loss 1.703 | nll_loss 0.665 | ppl 1.59 | wps 24295.9 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 28695 | lr 0.00018668 | gnorm 0.848 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 76945
2022-03-05 09:10:23 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 09:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:10:36 | INFO | train_inner | epoch 299:      5 / 97 loss=1.702, nll_loss=0.664, ppl=1.58, wps=23679.9, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.848, loss_scale=8, train_wall=238, gb_free=21, wall=76958
2022-03-05 09:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:14:36 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 13.42 | nll_loss 12.96 | ppl 7966.82 | wps 44157.7 | wpb 510.9 | bsz 1 | num_updates 28792 | best_loss 7.962
2022-03-05 09:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28792 updates
2022-03-05 09:14:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:14:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:14:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 299 @ 28792 updates, score 13.42) (writing took 2.5643650721758604 seconds)
2022-03-05 09:14:39 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 09:14:39 | INFO | train | epoch 299 | loss 1.702 | nll_loss 0.664 | ppl 1.58 | wps 24817.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28792 | lr 0.000186365 | gnorm 0.843 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 77201
2022-03-05 09:14:39 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 09:14:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:15:00 | INFO | train_inner | epoch 300:      8 / 97 loss=1.701, nll_loss=0.663, ppl=1.58, wps=24839.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28800, lr=0.000186339, gnorm=0.84, loss_scale=8, train_wall=233, gb_free=21, wall=77221
2022-03-05 09:18:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:18:52 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 13.4 | nll_loss 12.943 | ppl 7873.06 | wps 44392.2 | wpb 510.9 | bsz 1 | num_updates 28889 | best_loss 7.962
2022-03-05 09:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28889 updates
2022-03-05 09:18:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:18:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:18:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 300 @ 28889 updates, score 13.4) (writing took 2.5663339402526617 seconds)
2022-03-05 09:18:55 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 09:18:55 | INFO | train | epoch 300 | loss 1.701 | nll_loss 0.663 | ppl 1.58 | wps 24814.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28889 | lr 0.000186052 | gnorm 0.835 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 77457
2022-03-05 09:18:55 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 09:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:19:23 | INFO | train_inner | epoch 301:     11 / 97 loss=1.7, nll_loss=0.662, ppl=1.58, wps=24831.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.835, loss_scale=16, train_wall=234, gb_free=21, wall=77485
2022-03-05 09:21:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:23:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:23:08 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 13.393 | nll_loss 12.935 | ppl 7832.73 | wps 44674.7 | wpb 510.9 | bsz 1 | num_updates 28985 | best_loss 7.962
2022-03-05 09:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 28985 updates
2022-03-05 09:23:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:23:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 301 @ 28985 updates, score 13.393) (writing took 2.628053999505937 seconds)
2022-03-05 09:23:11 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 09:23:11 | INFO | train | epoch 301 | loss 1.7 | nll_loss 0.662 | ppl 1.58 | wps 24552 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28985 | lr 0.000185743 | gnorm 0.848 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77713
2022-03-05 09:23:11 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 09:23:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:23:50 | INFO | train_inner | epoch 302:     15 / 97 loss=1.698, nll_loss=0.66, ppl=1.58, wps=24593.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29000, lr=0.000185695, gnorm=0.851, loss_scale=16, train_wall=236, gb_free=21, wall=77751
2022-03-05 09:27:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:27:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:24 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 13.417 | nll_loss 12.961 | ppl 7974.48 | wps 44290.4 | wpb 510.9 | bsz 1 | num_updates 29081 | best_loss 7.962
2022-03-05 09:27:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29081 updates
2022-03-05 09:27:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:27:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 302 @ 29081 updates, score 13.417) (writing took 2.693719733506441 seconds)
2022-03-05 09:27:27 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 09:27:27 | INFO | train | epoch 302 | loss 1.698 | nll_loss 0.661 | ppl 1.58 | wps 24551.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29081 | lr 0.000185437 | gnorm 0.836 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77969
2022-03-05 09:27:27 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 09:27:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:28:16 | INFO | train_inner | epoch 303:     19 / 97 loss=1.697, nll_loss=0.659, ppl=1.58, wps=24591.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29100, lr=0.000185376, gnorm=0.834, loss_scale=16, train_wall=236, gb_free=21, wall=78018
2022-03-05 09:31:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:31:41 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 13.345 | nll_loss 12.885 | ppl 7561.74 | wps 44450.2 | wpb 510.9 | bsz 1 | num_updates 29177 | best_loss 7.962
2022-03-05 09:31:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29177 updates
2022-03-05 09:31:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:31:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:31:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 303 @ 29177 updates, score 13.345) (writing took 2.5613288022577763 seconds)
2022-03-05 09:31:43 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 09:31:43 | INFO | train | epoch 303 | loss 1.696 | nll_loss 0.658 | ppl 1.58 | wps 24552.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29177 | lr 0.000185131 | gnorm 0.838 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 78225
2022-03-05 09:31:43 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 09:31:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:32:42 | INFO | train_inner | epoch 304:     23 / 97 loss=1.696, nll_loss=0.658, ppl=1.58, wps=24584, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29200, lr=0.000185058, gnorm=0.833, loss_scale=8, train_wall=236, gb_free=21, wall=78284
2022-03-05 09:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:35:57 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 13.405 | nll_loss 12.947 | ppl 7897.26 | wps 44504.2 | wpb 510.9 | bsz 1 | num_updates 29274 | best_loss 7.962
2022-03-05 09:35:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29274 updates
2022-03-05 09:35:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:35:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 304 @ 29274 updates, score 13.405) (writing took 2.6772014210000634 seconds)
2022-03-05 09:35:59 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 09:35:59 | INFO | train | epoch 304 | loss 1.696 | nll_loss 0.659 | ppl 1.58 | wps 24789.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29274 | lr 0.000184824 | gnorm 0.84 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 78481
2022-03-05 09:35:59 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 09:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:37:06 | INFO | train_inner | epoch 305:     26 / 97 loss=1.696, nll_loss=0.658, ppl=1.58, wps=24824.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29300, lr=0.000184742, gnorm=0.84, loss_scale=16, train_wall=234, gb_free=21, wall=78548
2022-03-05 09:40:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:40:13 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 13.342 | nll_loss 12.883 | ppl 7554.41 | wps 44628.9 | wpb 510.9 | bsz 1 | num_updates 29371 | best_loss 7.962
2022-03-05 09:40:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29371 updates
2022-03-05 09:40:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:40:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 305 @ 29371 updates, score 13.342) (writing took 2.559537086635828 seconds)
2022-03-05 09:40:15 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 09:40:15 | INFO | train | epoch 305 | loss 1.694 | nll_loss 0.656 | ppl 1.58 | wps 24825.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29371 | lr 0.000184519 | gnorm 0.824 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 78737
2022-03-05 09:40:15 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 09:40:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:41:30 | INFO | train_inner | epoch 306:     29 / 97 loss=1.694, nll_loss=0.656, ppl=1.58, wps=24844, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.834, loss_scale=16, train_wall=234, gb_free=21, wall=78812
2022-03-05 09:42:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:44:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:44:29 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 13.367 | nll_loss 12.906 | ppl 7676.18 | wps 44453.5 | wpb 510.9 | bsz 1 | num_updates 29467 | best_loss 7.962
2022-03-05 09:44:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29467 updates
2022-03-05 09:44:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:44:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 306 @ 29467 updates, score 13.367) (writing took 2.586238831281662 seconds)
2022-03-05 09:44:31 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 09:44:31 | INFO | train | epoch 306 | loss 1.693 | nll_loss 0.656 | ppl 1.58 | wps 24554.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29467 | lr 0.000184218 | gnorm 0.845 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 78993
2022-03-05 09:44:31 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 09:44:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:45:56 | INFO | train_inner | epoch 307:     33 / 97 loss=1.691, nll_loss=0.654, ppl=1.57, wps=24588.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.839, loss_scale=16, train_wall=236, gb_free=21, wall=79078
2022-03-05 09:48:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:48:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:48:45 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 13.383 | nll_loss 12.926 | ppl 7783.93 | wps 44375.9 | wpb 510.9 | bsz 1 | num_updates 29563 | best_loss 7.962
2022-03-05 09:48:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29563 updates
2022-03-05 09:48:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:48:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 307 @ 29563 updates, score 13.383) (writing took 2.5973746655508876 seconds)
2022-03-05 09:48:48 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 09:48:48 | INFO | train | epoch 307 | loss 1.692 | nll_loss 0.654 | ppl 1.57 | wps 24540.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29563 | lr 0.000183919 | gnorm 0.835 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 79249
2022-03-05 09:48:48 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 09:48:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:50:23 | INFO | train_inner | epoch 308:     37 / 97 loss=1.692, nll_loss=0.655, ppl=1.57, wps=24579.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.84, loss_scale=16, train_wall=236, gb_free=21, wall=79344
2022-03-05 09:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:53:01 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 13.379 | nll_loss 12.918 | ppl 7740.96 | wps 44325.8 | wpb 510.9 | bsz 1 | num_updates 29660 | best_loss 7.962
2022-03-05 09:53:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29660 updates
2022-03-05 09:53:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:53:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 308 @ 29660 updates, score 13.379) (writing took 2.6161747397854924 seconds)
2022-03-05 09:53:04 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 09:53:04 | INFO | train | epoch 308 | loss 1.691 | nll_loss 0.653 | ppl 1.57 | wps 24802.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29660 | lr 0.000183618 | gnorm 0.847 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 79505
2022-03-05 09:53:04 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 09:53:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:53:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:54:49 | INFO | train_inner | epoch 309:     41 / 97 loss=1.689, nll_loss=0.652, ppl=1.57, wps=24592.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.843, loss_scale=16, train_wall=236, gb_free=21, wall=79611
2022-03-05 09:56:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:57:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:57:17 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 13.438 | nll_loss 12.984 | ppl 8102.78 | wps 44302.7 | wpb 510.9 | bsz 1 | num_updates 29755 | best_loss 7.962
2022-03-05 09:57:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 29755 updates
2022-03-05 09:57:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:57:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 09:57:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 309 @ 29755 updates, score 13.438) (writing took 2.5565853798761964 seconds)
2022-03-05 09:57:20 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-03-05 09:57:20 | INFO | train | epoch 309 | loss 1.687 | nll_loss 0.649 | ppl 1.57 | wps 24301.8 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 29755 | lr 0.000183324 | gnorm 0.835 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 79762
2022-03-05 09:57:20 | INFO | fairseq.trainer | begin training epoch 310
2022-03-05 09:57:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:59:15 | INFO | train_inner | epoch 310:     45 / 97 loss=1.685, nll_loss=0.648, ppl=1.57, wps=24593.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29800, lr=0.000183186, gnorm=0.829, loss_scale=8, train_wall=236, gb_free=21, wall=79877
2022-03-05 10:01:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:01:33 | INFO | valid | epoch 310 | valid on 'valid' subset | loss 13.353 | nll_loss 12.898 | ppl 7632.03 | wps 44393.1 | wpb 510.9 | bsz 1 | num_updates 29852 | best_loss 7.962
2022-03-05 10:01:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 310 @ 29852 updates
2022-03-05 10:01:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:01:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:01:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 310 @ 29852 updates, score 13.353) (writing took 2.6711871894076467 seconds)
2022-03-05 10:01:36 | INFO | fairseq_cli.train | end of epoch 310 (average epoch stats below)
2022-03-05 10:01:36 | INFO | train | epoch 310 | loss 1.687 | nll_loss 0.65 | ppl 1.57 | wps 24794.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29852 | lr 0.000183026 | gnorm 0.829 | loss_scale 8 | train_wall 227 | gb_free 21 | wall 80018
2022-03-05 10:01:36 | INFO | fairseq.trainer | begin training epoch 311
2022-03-05 10:01:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:03:39 | INFO | train_inner | epoch 311:     48 / 97 loss=1.687, nll_loss=0.65, ppl=1.57, wps=24826.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=29900, lr=0.000182879, gnorm=0.833, loss_scale=16, train_wall=234, gb_free=21, wall=80141
2022-03-05 10:05:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:05:49 | INFO | valid | epoch 311 | valid on 'valid' subset | loss 13.382 | nll_loss 12.928 | ppl 7792.79 | wps 44407.5 | wpb 510.9 | bsz 1 | num_updates 29949 | best_loss 7.962
2022-03-05 10:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 311 @ 29949 updates
2022-03-05 10:05:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:05:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:05:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 311 @ 29949 updates, score 13.382) (writing took 2.5836981208994985 seconds)
2022-03-05 10:05:52 | INFO | fairseq_cli.train | end of epoch 311 (average epoch stats below)
2022-03-05 10:05:52 | INFO | train | epoch 311 | loss 1.686 | nll_loss 0.649 | ppl 1.57 | wps 24814.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29949 | lr 0.00018273 | gnorm 0.828 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 80274
2022-03-05 10:05:52 | INFO | fairseq.trainer | begin training epoch 312
2022-03-05 10:05:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:07:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:08:05 | INFO | train_inner | epoch 312:     52 / 97 loss=1.685, nll_loss=0.648, ppl=1.57, wps=24592, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=30000, lr=0.000182574, gnorm=0.822, loss_scale=16, train_wall=236, gb_free=21, wall=80407
2022-03-05 10:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:10:05 | INFO | valid | epoch 312 | valid on 'valid' subset | loss 13.356 | nll_loss 12.897 | ppl 7628.56 | wps 44500 | wpb 510.9 | bsz 1 | num_updates 30045 | best_loss 7.962
2022-03-05 10:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 312 @ 30045 updates
2022-03-05 10:10:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:10:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 312 @ 30045 updates, score 13.356) (writing took 2.6195754120126367 seconds)
2022-03-05 10:10:08 | INFO | fairseq_cli.train | end of epoch 312 (average epoch stats below)
2022-03-05 10:10:08 | INFO | train | epoch 312 | loss 1.685 | nll_loss 0.648 | ppl 1.57 | wps 24553.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30045 | lr 0.000182437 | gnorm 0.825 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 80530
2022-03-05 10:10:08 | INFO | fairseq.trainer | begin training epoch 313
2022-03-05 10:10:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:12:29 | INFO | train_inner | epoch 313:     55 / 97 loss=1.685, nll_loss=0.648, ppl=1.57, wps=24824.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=30100, lr=0.000182271, gnorm=0.828, loss_scale=16, train_wall=234, gb_free=21, wall=80671
2022-03-05 10:13:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:14:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:14:22 | INFO | valid | epoch 313 | valid on 'valid' subset | loss 13.381 | nll_loss 12.923 | ppl 7765.17 | wps 44555.9 | wpb 510.9 | bsz 1 | num_updates 30141 | best_loss 7.962
2022-03-05 10:14:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 313 @ 30141 updates
2022-03-05 10:14:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:14:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:14:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 313 @ 30141 updates, score 13.381) (writing took 2.6171799916774035 seconds)
2022-03-05 10:14:24 | INFO | fairseq_cli.train | end of epoch 313 (average epoch stats below)
2022-03-05 10:14:24 | INFO | train | epoch 313 | loss 1.684 | nll_loss 0.647 | ppl 1.57 | wps 24538.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30141 | lr 0.000182147 | gnorm 0.828 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 80786
2022-03-05 10:14:24 | INFO | fairseq.trainer | begin training epoch 314
2022-03-05 10:14:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:16:55 | INFO | train_inner | epoch 314:     59 / 97 loss=1.684, nll_loss=0.647, ppl=1.57, wps=24590.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=30200, lr=0.000181969, gnorm=0.829, loss_scale=16, train_wall=236, gb_free=21, wall=80937
2022-03-05 10:18:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:18:38 | INFO | valid | epoch 314 | valid on 'valid' subset | loss 13.295 | nll_loss 12.835 | ppl 7308.06 | wps 44357.9 | wpb 510.9 | bsz 1 | num_updates 30238 | best_loss 7.962
2022-03-05 10:18:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 314 @ 30238 updates
2022-03-05 10:18:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:18:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:18:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 314 @ 30238 updates, score 13.295) (writing took 2.548869882710278 seconds)
2022-03-05 10:18:40 | INFO | fairseq_cli.train | end of epoch 314 (average epoch stats below)
2022-03-05 10:18:40 | INFO | train | epoch 314 | loss 1.682 | nll_loss 0.645 | ppl 1.56 | wps 24811.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30238 | lr 0.000181854 | gnorm 0.831 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 81042
2022-03-05 10:18:40 | INFO | fairseq.trainer | begin training epoch 315
2022-03-05 10:18:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:18:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:21:22 | INFO | train_inner | epoch 315:     63 / 97 loss=1.68, nll_loss=0.643, ppl=1.56, wps=24589.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30300, lr=0.000181668, gnorm=0.827, loss_scale=16, train_wall=236, gb_free=21, wall=81204
2022-03-05 10:22:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:22:54 | INFO | valid | epoch 315 | valid on 'valid' subset | loss 13.404 | nll_loss 12.95 | ppl 7915.25 | wps 44412.4 | wpb 510.9 | bsz 1 | num_updates 30334 | best_loss 7.962
2022-03-05 10:22:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 315 @ 30334 updates
2022-03-05 10:22:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:22:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:22:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 315 @ 30334 updates, score 13.404) (writing took 2.526593318209052 seconds)
2022-03-05 10:22:56 | INFO | fairseq_cli.train | end of epoch 315 (average epoch stats below)
2022-03-05 10:22:56 | INFO | train | epoch 315 | loss 1.681 | nll_loss 0.644 | ppl 1.56 | wps 24563.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 30334 | lr 0.000181566 | gnorm 0.83 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 81298
2022-03-05 10:22:56 | INFO | fairseq.trainer | begin training epoch 316
2022-03-05 10:22:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:24:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:25:48 | INFO | train_inner | epoch 316:     67 / 97 loss=1.681, nll_loss=0.644, ppl=1.56, wps=24605.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30400, lr=0.000181369, gnorm=0.842, loss_scale=16, train_wall=236, gb_free=21, wall=81470
2022-03-05 10:27:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:27:10 | INFO | valid | epoch 316 | valid on 'valid' subset | loss 13.345 | nll_loss 12.888 | ppl 7577.69 | wps 44289.3 | wpb 510.9 | bsz 1 | num_updates 30430 | best_loss 7.962
2022-03-05 10:27:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 316 @ 30430 updates
2022-03-05 10:27:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:27:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:27:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 316 @ 30430 updates, score 13.345) (writing took 2.649889688938856 seconds)
2022-03-05 10:27:12 | INFO | fairseq_cli.train | end of epoch 316 (average epoch stats below)
2022-03-05 10:27:12 | INFO | train | epoch 316 | loss 1.68 | nll_loss 0.643 | ppl 1.56 | wps 24550.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30430 | lr 0.00018128 | gnorm 0.838 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 81554
2022-03-05 10:27:12 | INFO | fairseq.trainer | begin training epoch 317
2022-03-05 10:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:30:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:30:14 | INFO | train_inner | epoch 317:     71 / 97 loss=1.68, nll_loss=0.643, ppl=1.56, wps=24582.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30500, lr=0.000181071, gnorm=0.834, loss_scale=16, train_wall=236, gb_free=21, wall=81736
2022-03-05 10:31:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:31:26 | INFO | valid | epoch 317 | valid on 'valid' subset | loss 13.41 | nll_loss 12.958 | ppl 7955.72 | wps 44395.9 | wpb 510.9 | bsz 1 | num_updates 30526 | best_loss 7.962
2022-03-05 10:31:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 317 @ 30526 updates
2022-03-05 10:31:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:31:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:31:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 317 @ 30526 updates, score 13.41) (writing took 2.6198853235691786 seconds)
2022-03-05 10:31:28 | INFO | fairseq_cli.train | end of epoch 317 (average epoch stats below)
2022-03-05 10:31:28 | INFO | train | epoch 317 | loss 1.679 | nll_loss 0.641 | ppl 1.56 | wps 24545.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30526 | lr 0.000180994 | gnorm 0.832 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 81810
2022-03-05 10:31:28 | INFO | fairseq.trainer | begin training epoch 318
2022-03-05 10:31:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:34:38 | INFO | train_inner | epoch 318:     74 / 97 loss=1.678, nll_loss=0.641, ppl=1.56, wps=24828.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=30600, lr=0.000180775, gnorm=0.827, loss_scale=16, train_wall=234, gb_free=21, wall=82000
2022-03-05 10:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:35:42 | INFO | valid | epoch 318 | valid on 'valid' subset | loss 13.396 | nll_loss 12.938 | ppl 7849.36 | wps 44519.8 | wpb 510.9 | bsz 1 | num_updates 30623 | best_loss 7.962
2022-03-05 10:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 318 @ 30623 updates
2022-03-05 10:35:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:35:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 318 @ 30623 updates, score 13.396) (writing took 2.58080733101815 seconds)
2022-03-05 10:35:45 | INFO | fairseq_cli.train | end of epoch 318 (average epoch stats below)
2022-03-05 10:35:45 | INFO | train | epoch 318 | loss 1.677 | nll_loss 0.64 | ppl 1.56 | wps 24809.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30623 | lr 0.000180707 | gnorm 0.83 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 82066
2022-03-05 10:35:45 | INFO | fairseq.trainer | begin training epoch 319
2022-03-05 10:35:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:35:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:39:04 | INFO | train_inner | epoch 319:     78 / 97 loss=1.675, nll_loss=0.638, ppl=1.56, wps=24596.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=30700, lr=0.000180481, gnorm=0.831, loss_scale=16, train_wall=236, gb_free=21, wall=82266
2022-03-05 10:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:39:58 | INFO | valid | epoch 319 | valid on 'valid' subset | loss 13.369 | nll_loss 12.913 | ppl 7714.73 | wps 44552.8 | wpb 510.9 | bsz 1 | num_updates 30719 | best_loss 7.962
2022-03-05 10:39:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 319 @ 30719 updates
2022-03-05 10:39:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:40:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 319 @ 30719 updates, score 13.369) (writing took 2.641398737207055 seconds)
2022-03-05 10:40:01 | INFO | fairseq_cli.train | end of epoch 319 (average epoch stats below)
2022-03-05 10:40:01 | INFO | train | epoch 319 | loss 1.676 | nll_loss 0.639 | ppl 1.56 | wps 24552.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30719 | lr 0.000180425 | gnorm 0.836 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 82322
2022-03-05 10:40:01 | INFO | fairseq.trainer | begin training epoch 320
2022-03-05 10:40:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:41:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:43:31 | INFO | train_inner | epoch 320:     82 / 97 loss=1.676, nll_loss=0.639, ppl=1.56, wps=24593.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30800, lr=0.000180187, gnorm=0.839, loss_scale=16, train_wall=236, gb_free=21, wall=82533
2022-03-05 10:44:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:44:14 | INFO | valid | epoch 320 | valid on 'valid' subset | loss 13.412 | nll_loss 12.956 | ppl 7947.32 | wps 44521.7 | wpb 510.9 | bsz 1 | num_updates 30815 | best_loss 7.962
2022-03-05 10:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 320 @ 30815 updates
2022-03-05 10:44:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:44:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 320 @ 30815 updates, score 13.412) (writing took 2.5526049714535475 seconds)
2022-03-05 10:44:17 | INFO | fairseq_cli.train | end of epoch 320 (average epoch stats below)
2022-03-05 10:44:17 | INFO | train | epoch 320 | loss 1.674 | nll_loss 0.637 | ppl 1.55 | wps 24561.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 30815 | lr 0.000180144 | gnorm 0.832 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 82578
2022-03-05 10:44:17 | INFO | fairseq.trainer | begin training epoch 321
2022-03-05 10:44:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:47:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:47:57 | INFO | train_inner | epoch 321:     86 / 97 loss=1.673, nll_loss=0.636, ppl=1.55, wps=24587.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30900, lr=0.000179896, gnorm=0.834, loss_scale=16, train_wall=236, gb_free=21, wall=82799
2022-03-05 10:48:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:48:30 | INFO | valid | epoch 321 | valid on 'valid' subset | loss 13.395 | nll_loss 12.94 | ppl 7860.54 | wps 44372.7 | wpb 510.9 | bsz 1 | num_updates 30911 | best_loss 7.962
2022-03-05 10:48:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 321 @ 30911 updates
2022-03-05 10:48:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:48:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:48:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 321 @ 30911 updates, score 13.395) (writing took 2.5820435285568237 seconds)
2022-03-05 10:48:33 | INFO | fairseq_cli.train | end of epoch 321 (average epoch stats below)
2022-03-05 10:48:33 | INFO | train | epoch 321 | loss 1.673 | nll_loss 0.636 | ppl 1.55 | wps 24541.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30911 | lr 0.000179864 | gnorm 0.832 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 82835
2022-03-05 10:48:33 | INFO | fairseq.trainer | begin training epoch 322
2022-03-05 10:48:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:52:21 | INFO | train_inner | epoch 322:     89 / 97 loss=1.673, nll_loss=0.636, ppl=1.55, wps=24831.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31000, lr=0.000179605, gnorm=0.821, loss_scale=16, train_wall=234, gb_free=21, wall=83063
2022-03-05 10:52:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:52:46 | INFO | valid | epoch 322 | valid on 'valid' subset | loss 13.401 | nll_loss 12.947 | ppl 7897.34 | wps 44380.4 | wpb 510.9 | bsz 1 | num_updates 31008 | best_loss 7.962
2022-03-05 10:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 322 @ 31008 updates
2022-03-05 10:52:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:52:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 322 @ 31008 updates, score 13.401) (writing took 2.648041406646371 seconds)
2022-03-05 10:52:49 | INFO | fairseq_cli.train | end of epoch 322 (average epoch stats below)
2022-03-05 10:52:49 | INFO | train | epoch 322 | loss 1.673 | nll_loss 0.636 | ppl 1.55 | wps 24806.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31008 | lr 0.000179582 | gnorm 0.821 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 83091
2022-03-05 10:52:49 | INFO | fairseq.trainer | begin training epoch 323
2022-03-05 10:52:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:53:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:56:47 | INFO | train_inner | epoch 323:     93 / 97 loss=1.672, nll_loss=0.635, ppl=1.55, wps=24579.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31100, lr=0.000179316, gnorm=0.83, loss_scale=16, train_wall=236, gb_free=21, wall=83329
2022-03-05 10:56:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:57:02 | INFO | valid | epoch 323 | valid on 'valid' subset | loss 13.457 | nll_loss 13.005 | ppl 8221.59 | wps 44808.6 | wpb 510.9 | bsz 1 | num_updates 31104 | best_loss 7.962
2022-03-05 10:57:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 323 @ 31104 updates
2022-03-05 10:57:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:57:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 10:57:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 323 @ 31104 updates, score 13.457) (writing took 2.576940545812249 seconds)
2022-03-05 10:57:05 | INFO | fairseq_cli.train | end of epoch 323 (average epoch stats below)
2022-03-05 10:57:05 | INFO | train | epoch 323 | loss 1.671 | nll_loss 0.634 | ppl 1.55 | wps 24545.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31104 | lr 0.000179305 | gnorm 0.831 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 83347
2022-03-05 10:57:05 | INFO | fairseq.trainer | begin training epoch 324
2022-03-05 10:57:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:58:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:01:14 | INFO | train_inner | epoch 324:     97 / 97 loss=1.671, nll_loss=0.634, ppl=1.55, wps=24585.4, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=31200, lr=0.000179029, gnorm=0.822, loss_scale=16, train_wall=236, gb_free=21, wall=83595
2022-03-05 11:01:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:01:18 | INFO | valid | epoch 324 | valid on 'valid' subset | loss 13.383 | nll_loss 12.931 | ppl 7810.63 | wps 44569.7 | wpb 510.9 | bsz 1 | num_updates 31200 | best_loss 7.962
2022-03-05 11:01:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 324 @ 31200 updates
2022-03-05 11:01:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:01:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:01:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 324 @ 31200 updates, score 13.383) (writing took 2.585250568576157 seconds)
2022-03-05 11:01:21 | INFO | fairseq_cli.train | end of epoch 324 (average epoch stats below)
2022-03-05 11:01:21 | INFO | train | epoch 324 | loss 1.67 | nll_loss 0.633 | ppl 1.55 | wps 24549.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31200 | lr 0.000179029 | gnorm 0.82 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 83603
2022-03-05 11:01:21 | INFO | fairseq.trainer | begin training epoch 325
2022-03-05 11:01:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:04:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:05:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:05:35 | INFO | valid | epoch 325 | valid on 'valid' subset | loss 13.408 | nll_loss 12.956 | ppl 7944.77 | wps 44653.7 | wpb 510.9 | bsz 1 | num_updates 31296 | best_loss 7.962
2022-03-05 11:05:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 325 @ 31296 updates
2022-03-05 11:05:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:05:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:05:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 325 @ 31296 updates, score 13.408) (writing took 2.3675291035324335 seconds)
2022-03-05 11:05:37 | INFO | fairseq_cli.train | end of epoch 325 (average epoch stats below)
2022-03-05 11:05:37 | INFO | train | epoch 325 | loss 1.669 | nll_loss 0.632 | ppl 1.55 | wps 24573.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31296 | lr 0.000178754 | gnorm 0.825 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 83859
2022-03-05 11:05:37 | INFO | fairseq.trainer | begin training epoch 326
2022-03-05 11:05:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:05:47 | INFO | train_inner | epoch 326:      4 / 97 loss=1.668, nll_loss=0.631, ppl=1.55, wps=23928, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=31300, lr=0.000178743, gnorm=0.825, loss_scale=16, train_wall=236, gb_free=21, wall=83869
2022-03-05 11:08:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:09:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:09:50 | INFO | valid | epoch 326 | valid on 'valid' subset | loss 13.324 | nll_loss 12.87 | ppl 7486.79 | wps 44475.3 | wpb 510.9 | bsz 1 | num_updates 31392 | best_loss 7.962
2022-03-05 11:09:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 326 @ 31392 updates
2022-03-05 11:09:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:09:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:09:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 326 @ 31392 updates, score 13.324) (writing took 2.569286786019802 seconds)
2022-03-05 11:09:53 | INFO | fairseq_cli.train | end of epoch 326 (average epoch stats below)
2022-03-05 11:09:53 | INFO | train | epoch 326 | loss 1.667 | nll_loss 0.63 | ppl 1.55 | wps 24565.4 | ups 0.38 | wpb 65493.3 | bsz 127.9 | num_updates 31392 | lr 0.00017848 | gnorm 0.826 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 84115
2022-03-05 11:09:53 | INFO | fairseq.trainer | begin training epoch 327
2022-03-05 11:09:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:10:13 | INFO | train_inner | epoch 327:      8 / 97 loss=1.667, nll_loss=0.63, ppl=1.55, wps=24604.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=31400, lr=0.000178458, gnorm=0.827, loss_scale=8, train_wall=236, gb_free=21, wall=84135
2022-03-05 11:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:14:06 | INFO | valid | epoch 327 | valid on 'valid' subset | loss 13.381 | nll_loss 12.932 | ppl 7813.53 | wps 44338.1 | wpb 510.9 | bsz 1 | num_updates 31489 | best_loss 7.962
2022-03-05 11:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 327 @ 31489 updates
2022-03-05 11:14:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:14:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:14:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 327 @ 31489 updates, score 13.381) (writing took 2.4430149365216494 seconds)
2022-03-05 11:14:09 | INFO | fairseq_cli.train | end of epoch 327 (average epoch stats below)
2022-03-05 11:14:09 | INFO | train | epoch 327 | loss 1.666 | nll_loss 0.63 | ppl 1.55 | wps 24831.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31489 | lr 0.000178205 | gnorm 0.831 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 84370
2022-03-05 11:14:09 | INFO | fairseq.trainer | begin training epoch 328
2022-03-05 11:14:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:14:37 | INFO | train_inner | epoch 328:     11 / 97 loss=1.665, nll_loss=0.628, ppl=1.55, wps=24851.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31500, lr=0.000178174, gnorm=0.826, loss_scale=16, train_wall=234, gb_free=21, wall=84399
2022-03-05 11:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:18:22 | INFO | valid | epoch 328 | valid on 'valid' subset | loss 13.411 | nll_loss 12.961 | ppl 7975.89 | wps 44613.3 | wpb 510.9 | bsz 1 | num_updates 31586 | best_loss 7.962
2022-03-05 11:18:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 328 @ 31586 updates
2022-03-05 11:18:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:18:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:18:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 328 @ 31586 updates, score 13.411) (writing took 2.438298324123025 seconds)
2022-03-05 11:18:25 | INFO | fairseq_cli.train | end of epoch 328 (average epoch stats below)
2022-03-05 11:18:25 | INFO | train | epoch 328 | loss 1.665 | nll_loss 0.629 | ppl 1.55 | wps 24819.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31586 | lr 0.000177931 | gnorm 0.832 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 84626
2022-03-05 11:18:25 | INFO | fairseq.trainer | begin training epoch 329
2022-03-05 11:18:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:19:01 | INFO | train_inner | epoch 329:     14 / 97 loss=1.665, nll_loss=0.628, ppl=1.55, wps=24844.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31600, lr=0.000177892, gnorm=0.833, loss_scale=16, train_wall=234, gb_free=21, wall=84662
2022-03-05 11:19:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:22:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:22:38 | INFO | valid | epoch 329 | valid on 'valid' subset | loss 13.393 | nll_loss 12.94 | ppl 7858.55 | wps 44443.4 | wpb 510.9 | bsz 1 | num_updates 31682 | best_loss 7.962
2022-03-05 11:22:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 329 @ 31682 updates
2022-03-05 11:22:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:22:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt
2022-03-05 11:22:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.04_#1/checkpoint_last.pt (epoch 329 @ 31682 updates, score 13.393) (writing took 2.53092267177999 seconds)
2022-03-05 11:22:41 | INFO | fairseq_cli.train | end of epoch 329 (average epoch stats below)
2022-03-05 11:22:41 | INFO | train | epoch 329 | loss 1.664 | nll_loss 0.628 | ppl 1.55 | wps 24567.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 31682 | lr 0.000177662 | gnorm 0.824 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 84882
2022-03-05 11:22:41 | INFO | fairseq.trainer | begin training epoch 330
2022-03-05 11:22:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:23:27 | INFO | train_inner | epoch 330:     18 / 97 loss=1.664, nll_loss=0.627, ppl=1.54, wps=24607.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31700, lr=0.000177611, gnorm=0.823, loss_scale=16, train_wall=236, gb_free=21, wall=84929
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
