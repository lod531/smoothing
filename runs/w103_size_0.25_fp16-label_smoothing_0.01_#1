Sender: LSF System <lsfadmin@eu-g3-058>
Subject: Job 206754176: <w103_size_0.25_fp16_label_smoothing_0.01_#1> in cluster <euler> Exited

Job <w103_size_0.25_fp16_label_smoothing_0.01_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Tue Mar  1 09:38:55 2022
Job was executed on host(s) <eu-g3-058>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Tue Mar  1 09:39:00 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar  1 09:39:00 2022
Terminated at Wed Mar  2 07:08:18 2022
Results reported at Wed Mar  2 07:08:18 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.01 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 1321671 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   77327.09 sec.
    Max Memory :                                 8264 MB
    Average Memory :                             3560.27 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11736.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   77357 sec.
    Turnaround time :                            77363 sec.

The output (if any) follows:

2022-03-01 09:39:07 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.01, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-01 09:39:08 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-01 09:39:11 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-01 09:39:11 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-01 09:39:11 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-01 09:39:11 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-01 09:39:11 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-01 09:39:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-01 09:39:11 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-01 09:39:14 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-01 09:39:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-01 09:39:14 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-01 09:39:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-01 09:39:14 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-01 09:39:14 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-01 09:39:14 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 09:39:14 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 09:39:14 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-01 09:39:14 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-01 09:39:14 | INFO | fairseq.trainer | begin training epoch 1
2022-03-01 09:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 09:39:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-01 09:39:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-01 09:39:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 09:39:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 09:39:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-01 09:43:57 | INFO | train_inner | epoch 001:    105 / 393 loss=16.933, nll_loss=16.913, ppl=123404, wps=25199.1, ups=0.38, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.392, loss_scale=4, train_wall=278, gb_free=12.3, wall=283
2022-03-01 09:48:17 | INFO | train_inner | epoch 001:    205 / 393 loss=14.489, nll_loss=14.445, ppl=22299.3, wps=25204, ups=0.38, wpb=65535.4, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.542, loss_scale=4, train_wall=255, gb_free=12.3, wall=543
2022-03-01 09:52:37 | INFO | train_inner | epoch 001:    305 / 393 loss=12.395, nll_loss=12.326, ppl=5133.88, wps=25218.4, ups=0.38, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.048, loss_scale=4, train_wall=255, gb_free=12.3, wall=803
2022-03-01 09:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 09:56:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.463 | nll_loss 10.36 | ppl 1314.17 | wps 66232.7 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-01 09:56:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-01 09:56:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 09:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 09:56:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.463) (writing took 8.992957400972955 seconds)
2022-03-01 09:56:37 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-01 09:56:37 | INFO | train | epoch 001 | loss 13.775 | nll_loss 13.72 | ppl 13492.4 | wps 24908.5 | ups 0.38 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.692 | loss_scale 4 | train_wall 1011 | gb_free 12.3 | wall 1043
2022-03-01 09:56:37 | INFO | fairseq.trainer | begin training epoch 2
2022-03-01 09:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 09:57:08 | INFO | train_inner | epoch 002:     12 / 393 loss=10.876, nll_loss=10.782, ppl=1761.19, wps=24083, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.648, loss_scale=4, train_wall=254, gb_free=12.3, wall=1074
2022-03-01 10:01:27 | INFO | train_inner | epoch 002:    112 / 393 loss=10.216, nll_loss=10.105, ppl=1101.57, wps=25265.1, ups=0.39, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.532, loss_scale=4, train_wall=255, gb_free=12.3, wall=1334
2022-03-01 10:05:47 | INFO | train_inner | epoch 002:    212 / 393 loss=9.869, nll_loss=9.748, ppl=859.6, wps=25246.6, ups=0.39, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.578, loss_scale=8, train_wall=255, gb_free=12.3, wall=1593
2022-03-01 10:10:07 | INFO | train_inner | epoch 002:    312 / 393 loss=9.588, nll_loss=9.46, ppl=704.42, wps=25247.1, ups=0.39, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.669, loss_scale=8, train_wall=255, gb_free=12.3, wall=1853
2022-03-01 10:13:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 10:13:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.203 | nll_loss 9.068 | ppl 536.58 | wps 65987.5 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.203
2022-03-01 10:13:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-01 10:13:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 10:13:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 10:13:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.203) (writing took 8.924280262901448 seconds)
2022-03-01 10:13:48 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-01 10:13:48 | INFO | train | epoch 002 | loss 9.802 | nll_loss 9.68 | ppl 820.39 | wps 24947.7 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.617 | loss_scale 8 | train_wall 1000 | gb_free 12.3 | wall 2074
2022-03-01 10:13:48 | INFO | fairseq.trainer | begin training epoch 3
2022-03-01 10:13:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 10:14:37 | INFO | train_inner | epoch 003:     19 / 393 loss=9.34, nll_loss=9.207, ppl=590.97, wps=24094.9, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.753, loss_scale=8, train_wall=254, gb_free=12.3, wall=2124
2022-03-01 10:18:57 | INFO | train_inner | epoch 003:    119 / 393 loss=9.091, nll_loss=8.953, ppl=495.65, wps=25240.6, ups=0.39, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.845, loss_scale=8, train_wall=255, gb_free=12.3, wall=2383
2022-03-01 10:23:17 | INFO | train_inner | epoch 003:    219 / 393 loss=8.894, nll_loss=8.752, ppl=431.26, wps=25235.1, ups=0.39, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.93, loss_scale=8, train_wall=255, gb_free=12.3, wall=2643
2022-03-01 10:27:36 | INFO | train_inner | epoch 003:    319 / 393 loss=8.716, nll_loss=8.571, ppl=380.22, wps=25227.9, ups=0.38, wpb=65535.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.917, loss_scale=16, train_wall=255, gb_free=12.3, wall=2903
2022-03-01 10:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 10:30:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.441 | nll_loss 8.291 | ppl 313.15 | wps 66034 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 8.441
2022-03-01 10:30:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-01 10:30:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 10:30:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 10:31:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 3 @ 1174 updates, score 8.441) (writing took 8.902272386010736 seconds)
2022-03-01 10:31:00 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-01 10:31:00 | INFO | train | epoch 003 | loss 8.854 | nll_loss 8.712 | ppl 419.29 | wps 24936.3 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.902 | loss_scale 16 | train_wall 1000 | gb_free 12.3 | wall 3106
2022-03-01 10:31:00 | INFO | fairseq.trainer | begin training epoch 4
2022-03-01 10:31:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 10:32:07 | INFO | train_inner | epoch 004:     26 / 393 loss=8.54, nll_loss=8.392, ppl=335.84, wps=24097.8, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.93, loss_scale=16, train_wall=254, gb_free=12.3, wall=3173
2022-03-01 10:36:27 | INFO | train_inner | epoch 004:    126 / 393 loss=8.362, nll_loss=8.211, ppl=296.36, wps=25241.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.933, loss_scale=16, train_wall=255, gb_free=12.3, wall=3433
2022-03-01 10:40:47 | INFO | train_inner | epoch 004:    226 / 393 loss=8.234, nll_loss=8.08, ppl=270.66, wps=25230.9, ups=0.38, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.954, loss_scale=16, train_wall=255, gb_free=12.3, wall=3693
2022-03-01 10:45:06 | INFO | train_inner | epoch 004:    326 / 393 loss=8.115, nll_loss=7.96, ppl=249.02, wps=25253.8, ups=0.39, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.966, loss_scale=16, train_wall=255, gb_free=12.3, wall=3952
2022-03-01 10:47:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 10:48:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.912 | nll_loss 7.753 | ppl 215.67 | wps 66064.4 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 7.912
2022-03-01 10:48:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-01 10:48:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 10:48:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 10:48:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 4 @ 1567 updates, score 7.912) (writing took 8.970043612993322 seconds)
2022-03-01 10:48:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-01 10:48:11 | INFO | train | epoch 004 | loss 8.211 | nll_loss 8.057 | ppl 266.35 | wps 24940 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.952 | loss_scale 32 | train_wall 1000 | gb_free 12.3 | wall 4137
2022-03-01 10:48:11 | INFO | fairseq.trainer | begin training epoch 5
2022-03-01 10:48:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 10:49:37 | INFO | train_inner | epoch 005:     33 / 393 loss=7.963, nll_loss=7.805, ppl=223.67, wps=24094.6, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.956, loss_scale=32, train_wall=254, gb_free=12.3, wall=4223
2022-03-01 10:53:57 | INFO | train_inner | epoch 005:    133 / 393 loss=7.823, nll_loss=7.663, ppl=202.7, wps=25223.1, ups=0.38, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.959, loss_scale=32, train_wall=255, gb_free=12.3, wall=4483
2022-03-01 10:58:17 | INFO | train_inner | epoch 005:    233 / 393 loss=7.728, nll_loss=7.567, ppl=189.61, wps=25223.3, ups=0.38, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.946, loss_scale=32, train_wall=255, gb_free=12.3, wall=4743
2022-03-01 11:02:36 | INFO | train_inner | epoch 005:    333 / 393 loss=7.624, nll_loss=7.461, ppl=176.17, wps=25220, ups=0.38, wpb=65530.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.92, loss_scale=32, train_wall=255, gb_free=12.3, wall=5003
2022-03-01 11:05:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:05:14 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.499 | nll_loss 7.331 | ppl 161 | wps 65848 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 7.499
2022-03-01 11:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-01 11:05:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:05:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:05:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 5 @ 1960 updates, score 7.499) (writing took 9.903955615009181 seconds)
2022-03-01 11:05:24 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-01 11:05:24 | INFO | train | epoch 005 | loss 7.711 | nll_loss 7.549 | ppl 187.31 | wps 24900.4 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.942 | loss_scale 32 | train_wall 1001 | gb_free 12.3 | wall 5171
2022-03-01 11:05:24 | INFO | fairseq.trainer | begin training epoch 6
2022-03-01 11:05:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:07:08 | INFO | train_inner | epoch 006:     40 / 393 loss=7.496, nll_loss=7.331, ppl=161.05, wps=23987, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.921, loss_scale=32, train_wall=254, gb_free=12.3, wall=5275
2022-03-01 11:09:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-01 11:11:31 | INFO | train_inner | epoch 006:    141 / 393 loss=7.377, nll_loss=7.21, ppl=148.04, wps=24977.3, ups=0.38, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.907, loss_scale=32, train_wall=257, gb_free=12.3, wall=5537
2022-03-01 11:15:51 | INFO | train_inner | epoch 006:    241 / 393 loss=7.304, nll_loss=7.135, ppl=140.59, wps=25205.1, ups=0.38, wpb=65535.4, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.874, loss_scale=32, train_wall=255, gb_free=12.3, wall=5797
2022-03-01 11:20:11 | INFO | train_inner | epoch 006:    341 / 393 loss=7.231, nll_loss=7.061, ppl=133.57, wps=25223.5, ups=0.38, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.903, loss_scale=32, train_wall=255, gb_free=12.3, wall=6057
2022-03-01 11:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:22:28 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.182 | nll_loss 7.011 | ppl 128.96 | wps 66046.8 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 7.182
2022-03-01 11:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-01 11:22:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:22:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:22:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 6 @ 2352 updates, score 7.182) (writing took 10.383957777987234 seconds)
2022-03-01 11:22:38 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-01 11:22:38 | INFO | train | epoch 006 | loss 7.298 | nll_loss 7.129 | ppl 140 | wps 24819 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.89 | loss_scale 32 | train_wall 1001 | gb_free 12.3 | wall 6205
2022-03-01 11:22:38 | INFO | fairseq.trainer | begin training epoch 7
2022-03-01 11:22:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:24:43 | INFO | train_inner | epoch 007:     48 / 393 loss=7.1, nll_loss=6.929, ppl=121.81, wps=23951.3, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.861, loss_scale=32, train_wall=254, gb_free=12.3, wall=6329
2022-03-01 11:29:03 | INFO | train_inner | epoch 007:    148 / 393 loss=6.995, nll_loss=6.822, ppl=113.13, wps=25217.5, ups=0.38, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.853, loss_scale=32, train_wall=255, gb_free=12.3, wall=6589
2022-03-01 11:32:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-01 11:33:25 | INFO | train_inner | epoch 007:    249 / 393 loss=6.953, nll_loss=6.779, ppl=109.81, wps=24968, ups=0.38, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.841, loss_scale=32, train_wall=258, gb_free=12.3, wall=6852
2022-03-01 11:37:45 | INFO | train_inner | epoch 007:    349 / 393 loss=6.894, nll_loss=6.718, ppl=105.3, wps=25216.9, ups=0.38, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.822, loss_scale=32, train_wall=255, gb_free=12.3, wall=7112
2022-03-01 11:39:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:39:42 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.959 | nll_loss 6.785 | ppl 110.25 | wps 65672.8 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 6.959
2022-03-01 11:39:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-01 11:39:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:39:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:39:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 7 @ 2744 updates, score 6.959) (writing took 9.141485166037455 seconds)
2022-03-01 11:39:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-01 11:39:51 | INFO | train | epoch 007 | loss 6.949 | nll_loss 6.775 | ppl 109.5 | wps 24848.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.845 | loss_scale 32 | train_wall 1001 | gb_free 12.3 | wall 7237
2022-03-01 11:39:51 | INFO | fairseq.trainer | begin training epoch 8
2022-03-01 11:39:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:42:17 | INFO | train_inner | epoch 008:     56 / 393 loss=6.779, nll_loss=6.602, ppl=97.17, wps=24020.4, ups=0.37, wpb=65238.4, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.836, loss_scale=32, train_wall=254, gb_free=12.3, wall=7383
2022-03-01 11:44:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 11:46:40 | INFO | train_inner | epoch 008:    157 / 393 loss=6.676, nll_loss=6.497, ppl=90.31, wps=24921, ups=0.38, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.798, loss_scale=16, train_wall=258, gb_free=12.3, wall=7646
2022-03-01 11:51:00 | INFO | train_inner | epoch 008:    257 / 393 loss=6.654, nll_loss=6.475, ppl=88.96, wps=25195.2, ups=0.38, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.786, loss_scale=16, train_wall=255, gb_free=12.3, wall=7906
2022-03-01 11:55:20 | INFO | train_inner | epoch 008:    357 / 393 loss=6.626, nll_loss=6.446, ppl=87.17, wps=25229.1, ups=0.38, wpb=65535.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.807, loss_scale=16, train_wall=255, gb_free=12.3, wall=8166
2022-03-01 11:56:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:56:55 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.75 | nll_loss 6.569 | ppl 94.93 | wps 66096.9 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 6.75
2022-03-01 11:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-01 11:56:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:57:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 11:57:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 8 @ 3136 updates, score 6.75) (writing took 9.011093734996393 seconds)
2022-03-01 11:57:04 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-01 11:57:04 | INFO | train | epoch 008 | loss 6.657 | nll_loss 6.478 | ppl 89.15 | wps 24830.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.799 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 8271
2022-03-01 11:57:04 | INFO | fairseq.trainer | begin training epoch 9
2022-03-01 11:57:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:59:51 | INFO | train_inner | epoch 009:     64 / 393 loss=6.498, nll_loss=6.316, ppl=79.7, wps=24030.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.791, loss_scale=16, train_wall=254, gb_free=12.3, wall=8438
2022-03-01 12:04:11 | INFO | train_inner | epoch 009:    164 / 393 loss=6.434, nll_loss=6.251, ppl=76.15, wps=25218.8, ups=0.38, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.779, loss_scale=16, train_wall=255, gb_free=12.3, wall=8697
2022-03-01 12:08:31 | INFO | train_inner | epoch 009:    264 / 393 loss=6.416, nll_loss=6.232, ppl=75.18, wps=25211.9, ups=0.38, wpb=65535.4, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.76, loss_scale=32, train_wall=255, gb_free=12.3, wall=8957
2022-03-01 12:09:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 12:12:54 | INFO | train_inner | epoch 009:    365 / 393 loss=6.388, nll_loss=6.204, ppl=73.72, wps=24969.4, ups=0.38, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.755, loss_scale=16, train_wall=257, gb_free=12.3, wall=9220
2022-03-01 12:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 12:14:09 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.64 | nll_loss 6.448 | ppl 87.31 | wps 63908.6 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 6.64
2022-03-01 12:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-01 12:14:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 12:14:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 12:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 9 @ 3528 updates, score 6.64) (writing took 9.610664357081987 seconds)
2022-03-01 12:14:18 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-01 12:14:18 | INFO | train | epoch 009 | loss 6.414 | nll_loss 6.231 | ppl 75.12 | wps 24819.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.77 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 9305
2022-03-01 12:14:18 | INFO | fairseq.trainer | begin training epoch 10
2022-03-01 12:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 12:17:26 | INFO | train_inner | epoch 010:     72 / 393 loss=6.255, nll_loss=6.069, ppl=67.14, wps=23915.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.746, loss_scale=16, train_wall=255, gb_free=12.3, wall=9493
2022-03-01 12:21:47 | INFO | train_inner | epoch 010:    172 / 393 loss=6.207, nll_loss=6.021, ppl=64.92, wps=25147.9, ups=0.38, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.754, loss_scale=16, train_wall=256, gb_free=12.3, wall=9753
2022-03-01 12:26:07 | INFO | train_inner | epoch 010:    272 / 393 loss=6.215, nll_loss=6.028, ppl=65.28, wps=25216.8, ups=0.38, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.734, loss_scale=16, train_wall=255, gb_free=12.3, wall=10013
2022-03-01 12:30:28 | INFO | train_inner | epoch 010:    372 / 393 loss=6.207, nll_loss=6.02, ppl=64.89, wps=25126.4, ups=0.38, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.725, loss_scale=16, train_wall=256, gb_free=12.3, wall=10274
2022-03-01 12:31:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 12:31:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.517 | nll_loss 6.331 | ppl 80.49 | wps 65864.6 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 6.517
2022-03-01 12:31:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-01 12:31:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 12:31:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 12:31:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 10 @ 3921 updates, score 6.517) (writing took 9.175829963060096 seconds)
2022-03-01 12:31:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-01 12:31:34 | INFO | train | epoch 010 | loss 6.208 | nll_loss 6.021 | ppl 64.96 | wps 24851 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.74 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 10340
2022-03-01 12:31:34 | INFO | fairseq.trainer | begin training epoch 11
2022-03-01 12:31:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 12:34:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 12:35:01 | INFO | train_inner | epoch 011:     80 / 393 loss=6.044, nll_loss=5.855, ppl=57.87, wps=23839.9, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.737, loss_scale=16, train_wall=256, gb_free=12.3, wall=10548
2022-03-01 12:39:21 | INFO | train_inner | epoch 011:    180 / 393 loss=6.023, nll_loss=5.833, ppl=56.99, wps=25216.7, ups=0.38, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.713, loss_scale=16, train_wall=255, gb_free=12.3, wall=10807
2022-03-01 12:43:41 | INFO | train_inner | epoch 011:    280 / 393 loss=6.024, nll_loss=5.834, ppl=57.05, wps=25237, ups=0.39, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.681, loss_scale=16, train_wall=255, gb_free=12.3, wall=11067
2022-03-01 12:48:01 | INFO | train_inner | epoch 011:    380 / 393 loss=6.039, nll_loss=5.849, ppl=57.63, wps=25236.1, ups=0.39, wpb=65530.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.67, loss_scale=16, train_wall=255, gb_free=12.3, wall=11327
2022-03-01 12:48:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 12:48:37 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.401 | nll_loss 6.215 | ppl 74.31 | wps 65680.1 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 6.401
2022-03-01 12:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-01 12:48:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 12:48:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 12:48:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 11 @ 4313 updates, score 6.401) (writing took 9.377164153032936 seconds)
2022-03-01 12:48:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-01 12:48:46 | INFO | train | epoch 011 | loss 6.023 | nll_loss 5.833 | ppl 56.99 | wps 24855.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.693 | loss_scale 16 | train_wall 1000 | gb_free 12.3 | wall 11372
2022-03-01 12:48:46 | INFO | fairseq.trainer | begin training epoch 12
2022-03-01 12:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 12:52:32 | INFO | train_inner | epoch 012:     87 / 393 loss=5.849, nll_loss=5.657, ppl=50.46, wps=24050.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.661, loss_scale=16, train_wall=254, gb_free=12.3, wall=11598
2022-03-01 12:56:52 | INFO | train_inner | epoch 012:    187 / 393 loss=5.846, nll_loss=5.653, ppl=50.31, wps=25220.7, ups=0.38, wpb=65530.2, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.66, loss_scale=16, train_wall=255, gb_free=12.3, wall=11858
2022-03-01 12:58:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 13:01:14 | INFO | train_inner | epoch 012:    288 / 393 loss=5.86, nll_loss=5.667, ppl=50.82, wps=24964.9, ups=0.38, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.654, loss_scale=16, train_wall=258, gb_free=12.3, wall=12120
2022-03-01 13:05:34 | INFO | train_inner | epoch 012:    388 / 393 loss=5.855, nll_loss=5.663, ppl=50.65, wps=25206, ups=0.38, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.641, loss_scale=16, train_wall=255, gb_free=12.3, wall=12380
2022-03-01 13:05:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:05:49 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.333 | nll_loss 6.144 | ppl 70.72 | wps 65794.1 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 6.333
2022-03-01 13:05:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-01 13:05:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:05:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:05:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 12 @ 4705 updates, score 6.333) (writing took 9.386047253967263 seconds)
2022-03-01 13:05:59 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-01 13:05:59 | INFO | train | epoch 012 | loss 5.848 | nll_loss 5.655 | ppl 50.39 | wps 24842.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.654 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 12405
2022-03-01 13:05:59 | INFO | fairseq.trainer | begin training epoch 13
2022-03-01 13:05:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:10:06 | INFO | train_inner | epoch 013:     95 / 393 loss=5.677, nll_loss=5.482, ppl=44.7, wps=24030, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.637, loss_scale=16, train_wall=254, gb_free=12.3, wall=12652
2022-03-01 13:14:26 | INFO | train_inner | epoch 013:    195 / 393 loss=5.696, nll_loss=5.5, ppl=45.26, wps=25227.7, ups=0.38, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.638, loss_scale=16, train_wall=255, gb_free=12.3, wall=12912
2022-03-01 13:18:45 | INFO | train_inner | epoch 013:    295 / 393 loss=5.71, nll_loss=5.515, ppl=45.73, wps=25219, ups=0.38, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.616, loss_scale=16, train_wall=255, gb_free=12.3, wall=13172
2022-03-01 13:20:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 13:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:23:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.322 | nll_loss 6.125 | ppl 69.82 | wps 65433.2 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 6.322
2022-03-01 13:23:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-01 13:23:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:23:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 13 @ 5097 updates, score 6.322) (writing took 9.253904978977516 seconds)
2022-03-01 13:23:11 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-01 13:23:11 | INFO | train | epoch 013 | loss 5.701 | nll_loss 5.506 | ppl 45.44 | wps 24851.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.632 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 13438
2022-03-01 13:23:11 | INFO | fairseq.trainer | begin training epoch 14
2022-03-01 13:23:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:23:19 | INFO | train_inner | epoch 014:      3 / 393 loss=5.725, nll_loss=5.53, ppl=46.19, wps=23817.7, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.647, loss_scale=16, train_wall=256, gb_free=12.3, wall=13446
2022-03-01 13:27:39 | INFO | train_inner | epoch 014:    103 / 393 loss=5.521, nll_loss=5.323, ppl=40.02, wps=25206.9, ups=0.38, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.623, loss_scale=16, train_wall=255, gb_free=12.3, wall=13706
2022-03-01 13:31:59 | INFO | train_inner | epoch 014:    203 / 393 loss=5.574, nll_loss=5.377, ppl=41.55, wps=25220.3, ups=0.38, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.621, loss_scale=16, train_wall=255, gb_free=12.3, wall=13965
2022-03-01 13:36:19 | INFO | train_inner | epoch 014:    303 / 393 loss=5.602, nll_loss=5.405, ppl=42.37, wps=25239.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.622, loss_scale=16, train_wall=255, gb_free=12.3, wall=14225
2022-03-01 13:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:40:15 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.273 | nll_loss 6.079 | ppl 67.61 | wps 66048.8 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 6.273
2022-03-01 13:40:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-01 13:40:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:40:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:40:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 14 @ 5490 updates, score 6.273) (writing took 8.898461756063625 seconds)
2022-03-01 13:40:24 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-01 13:40:24 | INFO | train | epoch 014 | loss 5.575 | nll_loss 5.377 | ppl 41.57 | wps 24923 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.623 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 14470
2022-03-01 13:40:24 | INFO | fairseq.trainer | begin training epoch 15
2022-03-01 13:40:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:40:50 | INFO | train_inner | epoch 015:     10 / 393 loss=5.586, nll_loss=5.389, ppl=41.9, wps=24086, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.619, loss_scale=16, train_wall=254, gb_free=12.3, wall=14496
2022-03-01 13:43:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 13:45:12 | INFO | train_inner | epoch 015:    111 / 393 loss=5.415, nll_loss=5.215, ppl=37.14, wps=24971.5, ups=0.38, wpb=65535.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.612, loss_scale=16, train_wall=257, gb_free=12.3, wall=14758
2022-03-01 13:49:32 | INFO | train_inner | epoch 015:    211 / 393 loss=5.459, nll_loss=5.259, ppl=38.3, wps=25215, ups=0.38, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.611, loss_scale=16, train_wall=255, gb_free=12.3, wall=15018
2022-03-01 13:53:52 | INFO | train_inner | epoch 015:    311 / 393 loss=5.491, nll_loss=5.292, ppl=39.18, wps=25225.6, ups=0.38, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.611, loss_scale=16, train_wall=255, gb_free=12.3, wall=15278
2022-03-01 13:57:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:57:27 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.251 | nll_loss 6.058 | ppl 66.65 | wps 65884.6 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 6.251
2022-03-01 13:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-01 13:57:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:57:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 13:57:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 15 @ 5882 updates, score 6.251) (writing took 8.91954600403551 seconds)
2022-03-01 13:57:36 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-01 13:57:36 | INFO | train | epoch 015 | loss 5.464 | nll_loss 5.265 | ppl 38.44 | wps 24854.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.61 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 15502
2022-03-01 13:57:36 | INFO | fairseq.trainer | begin training epoch 16
2022-03-01 13:57:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:58:23 | INFO | train_inner | epoch 016:     18 / 393 loss=5.469, nll_loss=5.27, ppl=38.58, wps=24071.3, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.61, loss_scale=16, train_wall=254, gb_free=12.3, wall=15549
2022-03-01 14:02:43 | INFO | train_inner | epoch 016:    118 / 393 loss=5.311, nll_loss=5.109, ppl=34.51, wps=25227.9, ups=0.38, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.605, loss_scale=16, train_wall=255, gb_free=12.3, wall=15809
2022-03-01 14:07:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 14:07:05 | INFO | train_inner | epoch 016:    219 / 393 loss=5.36, nll_loss=5.159, ppl=35.72, wps=24963.5, ups=0.38, wpb=65530.9, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.642, loss_scale=16, train_wall=258, gb_free=12.3, wall=16071
2022-03-01 14:11:25 | INFO | train_inner | epoch 016:    319 / 393 loss=5.399, nll_loss=5.198, ppl=36.71, wps=25216.6, ups=0.38, wpb=65535.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.616, loss_scale=16, train_wall=255, gb_free=12.3, wall=16331
2022-03-01 14:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 14:14:40 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.25 | nll_loss 6.056 | ppl 66.53 | wps 65456.3 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 6.25
2022-03-01 14:14:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-01 14:14:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 14:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-01 14:14:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 16 @ 6274 updates, score 6.25) (writing took 9.054447176982649 seconds)
2022-03-01 14:14:49 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-01 14:14:49 | INFO | train | epoch 016 | loss 5.366 | nll_loss 5.164 | ppl 35.86 | wps 24852 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.621 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 16535
2022-03-01 14:14:49 | INFO | fairseq.trainer | begin training epoch 17
2022-03-01 14:14:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 14:15:56 | INFO | train_inner | epoch 017:     26 / 393 loss=5.359, nll_loss=5.157, ppl=35.68, wps=24052.7, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.616, loss_scale=16, train_wall=254, gb_free=12.3, wall=16603
2022-03-01 14:20:16 | INFO | train_inner | epoch 017:    126 / 393 loss=5.234, nll_loss=5.03, ppl=32.67, wps=25198, ups=0.38, wpb=65530.2, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.626, loss_scale=16, train_wall=255, gb_free=12.3, wall=16863
2022-03-01 14:24:36 | INFO | train_inner | epoch 017:    226 / 393 loss=5.28, nll_loss=5.076, ppl=33.74, wps=25208, ups=0.38, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.608, loss_scale=16, train_wall=255, gb_free=12.3, wall=17123
2022-03-01 14:28:56 | INFO | train_inner | epoch 017:    326 / 393 loss=5.306, nll_loss=5.103, ppl=34.37, wps=25209.2, ups=0.38, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.612, loss_scale=16, train_wall=255, gb_free=12.3, wall=17383
2022-03-01 14:30:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 14:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 14:31:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.253 | nll_loss 6.057 | ppl 66.57 | wps 65456.2 | wpb 2034.1 | bsz 4 | num_updates 6666 | best_loss 6.25
2022-03-01 14:31:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6666 updates
2022-03-01 14:31:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 14:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 14:31:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 17 @ 6666 updates, score 6.253) (writing took 4.111728797899559 seconds)
2022-03-01 14:31:57 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-01 14:31:57 | INFO | train | epoch 017 | loss 5.278 | nll_loss 5.075 | ppl 33.71 | wps 24957 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 6666 | lr 0.000387318 | gnorm 0.617 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 17563
2022-03-01 14:31:57 | INFO | fairseq.trainer | begin training epoch 18
2022-03-01 14:31:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 14:33:25 | INFO | train_inner | epoch 018:     34 / 393 loss=5.263, nll_loss=5.059, ppl=33.35, wps=24252.1, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.617, loss_scale=16, train_wall=257, gb_free=12.3, wall=17652
2022-03-01 14:37:45 | INFO | train_inner | epoch 018:    134 / 393 loss=5.142, nll_loss=4.937, ppl=30.63, wps=25232.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.618, loss_scale=16, train_wall=255, gb_free=12.3, wall=17911
2022-03-01 14:42:05 | INFO | train_inner | epoch 018:    234 / 393 loss=5.197, nll_loss=4.992, ppl=31.82, wps=25215.9, ups=0.38, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.629, loss_scale=16, train_wall=255, gb_free=12.3, wall=18171
2022-03-01 14:46:25 | INFO | train_inner | epoch 018:    334 / 393 loss=5.244, nll_loss=5.04, ppl=32.89, wps=25206.3, ups=0.38, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.626, loss_scale=16, train_wall=255, gb_free=12.3, wall=18431
2022-03-01 14:48:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 14:49:01 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.259 | nll_loss 6.061 | ppl 66.74 | wps 66064 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 6.25
2022-03-01 14:49:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-01 14:49:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 14:49:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 14:49:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 18 @ 7059 updates, score 6.259) (writing took 4.187084345030598 seconds)
2022-03-01 14:49:05 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-01 14:49:05 | INFO | train | epoch 018 | loss 5.198 | nll_loss 4.994 | ppl 31.86 | wps 25024.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.622 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 18591
2022-03-01 14:49:05 | INFO | fairseq.trainer | begin training epoch 19
2022-03-01 14:49:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 14:50:51 | INFO | train_inner | epoch 019:     41 / 393 loss=5.174, nll_loss=4.969, ppl=31.32, wps=24489.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.617, loss_scale=16, train_wall=254, gb_free=12.3, wall=18698
2022-03-01 14:54:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 14:55:14 | INFO | train_inner | epoch 019:    142 / 393 loss=5.085, nll_loss=4.879, ppl=29.42, wps=24969.3, ups=0.38, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.637, loss_scale=16, train_wall=257, gb_free=12.3, wall=18960
2022-03-01 14:59:34 | INFO | train_inner | epoch 019:    242 / 393 loss=5.124, nll_loss=4.917, ppl=30.22, wps=25219.2, ups=0.38, wpb=65530.9, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.616, loss_scale=16, train_wall=255, gb_free=12.3, wall=19220
2022-03-01 15:03:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 15:03:56 | INFO | train_inner | epoch 019:    343 / 393 loss=5.161, nll_loss=4.955, ppl=31.03, wps=24964.1, ups=0.38, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.636, loss_scale=8, train_wall=258, gb_free=12.3, wall=19483
2022-03-01 15:06:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:06:08 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.276 | nll_loss 6.079 | ppl 67.61 | wps 65768.2 | wpb 2034.1 | bsz 4 | num_updates 7450 | best_loss 6.25
2022-03-01 15:06:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7450 updates
2022-03-01 15:06:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:06:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:06:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 19 @ 7450 updates, score 6.276) (writing took 4.38294403697364 seconds)
2022-03-01 15:06:13 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-01 15:06:13 | INFO | train | epoch 019 | loss 5.124 | nll_loss 4.918 | ppl 30.23 | wps 24900.2 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 7450 | lr 0.000366372 | gnorm 0.626 | loss_scale 8 | train_wall 1001 | gb_free 12.3 | wall 19619
2022-03-01 15:06:13 | INFO | fairseq.trainer | begin training epoch 20
2022-03-01 15:06:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 15:08:23 | INFO | train_inner | epoch 020:     50 / 393 loss=5.082, nll_loss=4.875, ppl=29.34, wps=24483.4, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.617, loss_scale=8, train_wall=254, gb_free=12.3, wall=19749
2022-03-01 15:12:43 | INFO | train_inner | epoch 020:    150 / 393 loss=5.016, nll_loss=4.808, ppl=28.02, wps=25169.7, ups=0.38, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.62, loss_scale=8, train_wall=255, gb_free=12.3, wall=20009
2022-03-01 15:17:04 | INFO | train_inner | epoch 020:    250 / 393 loss=5.06, nll_loss=4.853, ppl=28.89, wps=25163.8, ups=0.38, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.644, loss_scale=8, train_wall=255, gb_free=12.3, wall=20270
2022-03-01 15:21:23 | INFO | train_inner | epoch 020:    350 / 393 loss=5.108, nll_loss=4.902, ppl=29.89, wps=25234.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.629, loss_scale=8, train_wall=255, gb_free=12.3, wall=20530
2022-03-01 15:23:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:23:17 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.285 | nll_loss 6.085 | ppl 67.87 | wps 65497.6 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 6.25
2022-03-01 15:23:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-01 15:23:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:23:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 20 @ 7843 updates, score 6.285) (writing took 5.252547867014073 seconds)
2022-03-01 15:23:23 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-01 15:23:23 | INFO | train | epoch 020 | loss 5.058 | nll_loss 4.85 | ppl 28.84 | wps 24984.8 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.628 | loss_scale 8 | train_wall 1001 | gb_free 12.3 | wall 20649
2022-03-01 15:23:23 | INFO | fairseq.trainer | begin training epoch 21
2022-03-01 15:23:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 15:25:51 | INFO | train_inner | epoch 021:     57 / 393 loss=5.008, nll_loss=4.799, ppl=27.85, wps=24385.7, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.628, loss_scale=16, train_wall=254, gb_free=12.3, wall=20797
2022-03-01 15:30:11 | INFO | train_inner | epoch 021:    157 / 393 loss=4.96, nll_loss=4.75, ppl=26.92, wps=25168.3, ups=0.38, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.636, loss_scale=16, train_wall=255, gb_free=12.3, wall=21058
2022-03-01 15:34:31 | INFO | train_inner | epoch 021:    257 / 393 loss=5.01, nll_loss=4.801, ppl=27.88, wps=25195.7, ups=0.38, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.634, loss_scale=16, train_wall=255, gb_free=12.3, wall=21318
2022-03-01 15:38:52 | INFO | train_inner | epoch 021:    357 / 393 loss=5.025, nll_loss=4.816, ppl=28.18, wps=25188.7, ups=0.38, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.635, loss_scale=16, train_wall=255, gb_free=12.3, wall=21578
2022-03-01 15:40:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:40:27 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.291 | nll_loss 6.095 | ppl 68.34 | wps 65122.7 | wpb 2034.1 | bsz 4 | num_updates 8236 | best_loss 6.25
2022-03-01 15:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8236 updates
2022-03-01 15:40:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:40:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:40:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 21 @ 8236 updates, score 6.291) (writing took 4.338752602925524 seconds)
2022-03-01 15:40:32 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-01 15:40:32 | INFO | train | epoch 021 | loss 4.995 | nll_loss 4.786 | ppl 27.59 | wps 24995 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 8236 | lr 0.000348451 | gnorm 0.639 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 21678
2022-03-01 15:40:32 | INFO | fairseq.trainer | begin training epoch 22
2022-03-01 15:40:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 15:43:18 | INFO | train_inner | epoch 022:     64 / 393 loss=4.939, nll_loss=4.729, ppl=26.52, wps=24467.4, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.654, loss_scale=16, train_wall=254, gb_free=12.3, wall=21844
2022-03-01 15:47:38 | INFO | train_inner | epoch 022:    164 / 393 loss=4.903, nll_loss=4.693, ppl=25.87, wps=25218.8, ups=0.38, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.637, loss_scale=16, train_wall=255, gb_free=12.3, wall=22104
2022-03-01 15:49:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 15:52:01 | INFO | train_inner | epoch 022:    265 / 393 loss=4.948, nll_loss=4.738, ppl=26.69, wps=24964.9, ups=0.38, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.653, loss_scale=16, train_wall=258, gb_free=12.3, wall=22367
2022-03-01 15:56:20 | INFO | train_inner | epoch 022:    365 / 393 loss=4.99, nll_loss=4.781, ppl=27.48, wps=25224.7, ups=0.38, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.643, loss_scale=16, train_wall=255, gb_free=12.3, wall=22627
2022-03-01 15:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:57:35 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.303 | nll_loss 6.105 | ppl 68.82 | wps 65429 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 6.25
2022-03-01 15:57:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-01 15:57:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:57:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 15:57:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 22 @ 8628 updates, score 6.303) (writing took 4.690603911993094 seconds)
2022-03-01 15:57:40 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-01 15:57:40 | INFO | train | epoch 022 | loss 4.936 | nll_loss 4.726 | ppl 26.47 | wps 24953.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.645 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 22706
2022-03-01 15:57:40 | INFO | fairseq.trainer | begin training epoch 23
2022-03-01 15:57:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:00:47 | INFO | train_inner | epoch 023:     72 / 393 loss=4.857, nll_loss=4.646, ppl=25.04, wps=24451.5, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.638, loss_scale=16, train_wall=254, gb_free=12.3, wall=22893
2022-03-01 16:03:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 16:05:10 | INFO | train_inner | epoch 023:    173 / 393 loss=4.847, nll_loss=4.635, ppl=24.85, wps=24962.5, ups=0.38, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.663, loss_scale=8, train_wall=258, gb_free=12.3, wall=23156
2022-03-01 16:09:29 | INFO | train_inner | epoch 023:    273 / 393 loss=4.903, nll_loss=4.692, ppl=25.85, wps=25243.3, ups=0.39, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.653, loss_scale=8, train_wall=255, gb_free=12.3, wall=23416
2022-03-01 16:13:49 | INFO | train_inner | epoch 023:    373 / 393 loss=4.939, nll_loss=4.729, ppl=26.52, wps=25237.6, ups=0.39, wpb=65530.2, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.644, loss_scale=8, train_wall=255, gb_free=12.3, wall=23675
2022-03-01 16:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 16:14:43 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.322 | nll_loss 6.125 | ppl 69.8 | wps 65763.6 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 6.25
2022-03-01 16:14:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-01 16:14:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 16:14:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 16:14:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 23 @ 9020 updates, score 6.322) (writing took 4.2113983780145645 seconds)
2022-03-01 16:14:47 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-01 16:14:47 | INFO | train | epoch 023 | loss 4.881 | nll_loss 4.67 | ppl 25.46 | wps 24980.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.648 | loss_scale 8 | train_wall 1000 | gb_free 12.3 | wall 23734
2022-03-01 16:14:47 | INFO | fairseq.trainer | begin training epoch 24
2022-03-01 16:14:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:18:15 | INFO | train_inner | epoch 024:     80 / 393 loss=4.791, nll_loss=4.579, ppl=23.9, wps=24500.5, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.658, loss_scale=8, train_wall=254, gb_free=12.3, wall=23942
2022-03-01 16:22:35 | INFO | train_inner | epoch 024:    180 / 393 loss=4.8, nll_loss=4.587, ppl=24.03, wps=25228, ups=0.38, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.667, loss_scale=8, train_wall=255, gb_free=12.3, wall=24201
2022-03-01 16:26:55 | INFO | train_inner | epoch 024:    280 / 393 loss=4.856, nll_loss=4.644, ppl=25, wps=25234, ups=0.39, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.649, loss_scale=16, train_wall=255, gb_free=12.3, wall=24461
2022-03-01 16:31:15 | INFO | train_inner | epoch 024:    380 / 393 loss=4.89, nll_loss=4.678, ppl=25.6, wps=25223.4, ups=0.38, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.663, loss_scale=16, train_wall=255, gb_free=12.3, wall=24721
2022-03-01 16:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 16:31:51 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.347 | nll_loss 6.149 | ppl 70.99 | wps 65797 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 6.25
2022-03-01 16:31:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-01 16:31:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 16:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 16:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 24 @ 9413 updates, score 6.347) (writing took 4.171278947964311 seconds)
2022-03-01 16:31:55 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-01 16:31:55 | INFO | train | epoch 024 | loss 4.831 | nll_loss 4.619 | ppl 24.57 | wps 25038.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.663 | loss_scale 16 | train_wall 1001 | gb_free 12.3 | wall 24761
2022-03-01 16:31:55 | INFO | fairseq.trainer | begin training epoch 25
2022-03-01 16:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:35:41 | INFO | train_inner | epoch 025:     87 / 393 loss=4.731, nll_loss=4.517, ppl=22.89, wps=24491.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.66, loss_scale=16, train_wall=254, gb_free=12.3, wall=24987
2022-03-01 16:40:01 | INFO | train_inner | epoch 025:    187 / 393 loss=4.756, nll_loss=4.542, ppl=23.3, wps=25186, ups=0.38, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.662, loss_scale=16, train_wall=255, gb_free=12.3, wall=25248
2022-03-01 16:44:21 | INFO | train_inner | epoch 025:    287 / 393 loss=4.803, nll_loss=4.589, ppl=24.08, wps=25276.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.671, loss_scale=16, train_wall=254, gb_free=12.3, wall=25507
2022-03-01 16:48:40 | INFO | train_inner | epoch 025:    387 / 393 loss=4.852, nll_loss=4.64, ppl=24.93, wps=25276.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.661, loss_scale=32, train_wall=254, gb_free=12.3, wall=25766
2022-03-01 16:48:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 16:48:58 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.362 | nll_loss 6.166 | ppl 71.78 | wps 66011 | wpb 2034.1 | bsz 4 | num_updates 9806 | best_loss 6.25
2022-03-01 16:48:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9806 updates
2022-03-01 16:48:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 16:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 16:49:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 25 @ 9806 updates, score 6.362) (writing took 4.501379319000989 seconds)
2022-03-01 16:49:02 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-01 16:49:02 | INFO | train | epoch 025 | loss 4.782 | nll_loss 4.569 | ppl 23.73 | wps 25044.7 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 9806 | lr 0.000319341 | gnorm 0.661 | loss_scale 32 | train_wall 1000 | gb_free 12.3 | wall 25788
2022-03-01 16:49:02 | INFO | fairseq.trainer | begin training epoch 26
2022-03-01 16:49:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:49:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 16:53:08 | INFO | train_inner | epoch 026:     95 / 393 loss=4.674, nll_loss=4.459, ppl=21.99, wps=24285.8, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.671, loss_scale=16, train_wall=256, gb_free=12.3, wall=26035
2022-03-01 16:57:28 | INFO | train_inner | epoch 026:    195 / 393 loss=4.72, nll_loss=4.505, ppl=22.71, wps=25272.4, ups=0.39, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.658, loss_scale=16, train_wall=254, gb_free=12.3, wall=26294
2022-03-01 17:01:47 | INFO | train_inner | epoch 026:    295 / 393 loss=4.754, nll_loss=4.54, ppl=23.27, wps=25275.8, ups=0.39, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.667, loss_scale=16, train_wall=254, gb_free=12.3, wall=26553
2022-03-01 17:05:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 17:06:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:06:03 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.377 | nll_loss 6.176 | ppl 72.31 | wps 66205.8 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 6.25
2022-03-01 17:06:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-01 17:06:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:06:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:06:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 26 @ 10197 updates, score 6.377) (writing took 4.402601000969298 seconds)
2022-03-01 17:06:08 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-01 17:06:08 | INFO | train | epoch 026 | loss 4.737 | nll_loss 4.523 | ppl 22.99 | wps 24956 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.663 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 26814
2022-03-01 17:06:08 | INFO | fairseq.trainer | begin training epoch 27
2022-03-01 17:06:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:06:16 | INFO | train_inner | epoch 027:      3 / 393 loss=4.802, nll_loss=4.589, ppl=24.07, wps=24301.7, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.658, loss_scale=8, train_wall=256, gb_free=12.3, wall=26822
2022-03-01 17:10:35 | INFO | train_inner | epoch 027:    103 / 393 loss=4.619, nll_loss=4.402, ppl=21.15, wps=25293.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.671, loss_scale=8, train_wall=254, gb_free=12.3, wall=27081
2022-03-01 17:14:54 | INFO | train_inner | epoch 027:    203 / 393 loss=4.68, nll_loss=4.464, ppl=22.07, wps=25288.3, ups=0.39, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.69, loss_scale=8, train_wall=254, gb_free=12.3, wall=27340
2022-03-01 17:19:13 | INFO | train_inner | epoch 027:    303 / 393 loss=4.726, nll_loss=4.511, ppl=22.8, wps=25282.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.666, loss_scale=8, train_wall=254, gb_free=12.3, wall=27599
2022-03-01 17:23:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:23:08 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.392 | nll_loss 6.194 | ppl 73.22 | wps 66168.6 | wpb 2034.1 | bsz 4 | num_updates 10590 | best_loss 6.25
2022-03-01 17:23:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10590 updates
2022-03-01 17:23:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:23:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:23:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 27 @ 10590 updates, score 6.392) (writing took 4.5737010310404 seconds)
2022-03-01 17:23:13 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-01 17:23:13 | INFO | train | epoch 027 | loss 4.694 | nll_loss 4.479 | ppl 22.3 | wps 25089.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 10590 | lr 0.000307293 | gnorm 0.68 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 27839
2022-03-01 17:23:13 | INFO | fairseq.trainer | begin training epoch 28
2022-03-01 17:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:23:39 | INFO | train_inner | epoch 028:     10 / 393 loss=4.743, nll_loss=4.529, ppl=23.08, wps=24521.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.688, loss_scale=8, train_wall=253, gb_free=12.3, wall=27865
2022-03-01 17:27:58 | INFO | train_inner | epoch 028:    110 / 393 loss=4.579, nll_loss=4.362, ppl=20.56, wps=25276, ups=0.39, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.673, loss_scale=16, train_wall=254, gb_free=12.3, wall=28125
2022-03-01 17:32:18 | INFO | train_inner | epoch 028:    210 / 393 loss=4.635, nll_loss=4.418, ppl=21.38, wps=25277.2, ups=0.39, wpb=65530.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.675, loss_scale=16, train_wall=254, gb_free=12.3, wall=28384
2022-03-01 17:34:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 17:36:40 | INFO | train_inner | epoch 028:    311 / 393 loss=4.686, nll_loss=4.471, ppl=22.17, wps=25020.5, ups=0.38, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.686, loss_scale=8, train_wall=257, gb_free=12.3, wall=28646
2022-03-01 17:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:40:14 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.427 | nll_loss 6.228 | ppl 74.94 | wps 66019.6 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 6.25
2022-03-01 17:40:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-01 17:40:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:40:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:40:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 28 @ 10982 updates, score 6.427) (writing took 4.498826795024797 seconds)
2022-03-01 17:40:19 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-01 17:40:19 | INFO | train | epoch 028 | loss 4.654 | nll_loss 4.438 | ppl 21.67 | wps 25017.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.684 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 28865
2022-03-01 17:40:19 | INFO | fairseq.trainer | begin training epoch 29
2022-03-01 17:40:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:41:06 | INFO | train_inner | epoch 029:     18 / 393 loss=4.703, nll_loss=4.487, ppl=22.43, wps=24520.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.708, loss_scale=8, train_wall=253, gb_free=12.3, wall=28912
2022-03-01 17:45:25 | INFO | train_inner | epoch 029:    118 / 393 loss=4.551, nll_loss=4.333, ppl=20.16, wps=25282.3, ups=0.39, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.677, loss_scale=8, train_wall=254, gb_free=12.3, wall=29171
2022-03-01 17:49:44 | INFO | train_inner | epoch 029:    218 / 393 loss=4.602, nll_loss=4.385, ppl=20.9, wps=25283, ups=0.39, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.681, loss_scale=8, train_wall=254, gb_free=12.3, wall=29430
2022-03-01 17:54:03 | INFO | train_inner | epoch 029:    318 / 393 loss=4.657, nll_loss=4.441, ppl=21.71, wps=25276.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.702, loss_scale=8, train_wall=254, gb_free=12.3, wall=29690
2022-03-01 17:57:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:57:20 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.447 | nll_loss 6.25 | ppl 76.09 | wps 65888.5 | wpb 2034.1 | bsz 4 | num_updates 11375 | best_loss 6.25
2022-03-01 17:57:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11375 updates
2022-03-01 17:57:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 17:57:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 29 @ 11375 updates, score 6.447) (writing took 4.294378639082424 seconds)
2022-03-01 17:57:24 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-01 17:57:24 | INFO | train | epoch 029 | loss 4.615 | nll_loss 4.398 | ppl 21.08 | wps 25087.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 11375 | lr 0.0002965 | gnorm 0.689 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 29891
2022-03-01 17:57:24 | INFO | fairseq.trainer | begin training epoch 30
2022-03-01 17:57:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:58:29 | INFO | train_inner | epoch 030:     25 / 393 loss=4.633, nll_loss=4.417, ppl=21.36, wps=24543.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.69, loss_scale=16, train_wall=253, gb_free=12.3, wall=29955
2022-03-01 18:02:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 18:02:51 | INFO | train_inner | epoch 030:    126 / 393 loss=4.524, nll_loss=4.306, ppl=19.78, wps=25032.8, ups=0.38, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.697, loss_scale=8, train_wall=257, gb_free=12.3, wall=30217
2022-03-01 18:07:10 | INFO | train_inner | epoch 030:    226 / 393 loss=4.562, nll_loss=4.344, ppl=20.31, wps=25285.6, ups=0.39, wpb=65530.2, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.686, loss_scale=8, train_wall=254, gb_free=12.3, wall=30476
2022-03-01 18:11:29 | INFO | train_inner | epoch 030:    326 / 393 loss=4.619, nll_loss=4.401, ppl=21.13, wps=25271.2, ups=0.39, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.699, loss_scale=8, train_wall=254, gb_free=12.3, wall=30736
2022-03-01 18:14:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 18:14:25 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.47 | nll_loss 6.273 | ppl 77.32 | wps 66352.8 | wpb 2034.1 | bsz 4 | num_updates 11767 | best_loss 6.25
2022-03-01 18:14:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11767 updates
2022-03-01 18:14:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 18:14:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 18:14:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 30 @ 11767 updates, score 6.47) (writing took 4.34478784003295 seconds)
2022-03-01 18:14:30 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-01 18:14:30 | INFO | train | epoch 030 | loss 4.577 | nll_loss 4.359 | ppl 20.53 | wps 25021.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 11767 | lr 0.000291519 | gnorm 0.693 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 30916
2022-03-01 18:14:30 | INFO | fairseq.trainer | begin training epoch 31
2022-03-01 18:14:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:15:55 | INFO | train_inner | epoch 031:     33 / 393 loss=4.58, nll_loss=4.362, ppl=20.57, wps=24526.6, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.696, loss_scale=8, train_wall=253, gb_free=12.3, wall=31002
2022-03-01 18:20:15 | INFO | train_inner | epoch 031:    133 / 393 loss=4.488, nll_loss=4.268, ppl=19.27, wps=25286, ups=0.39, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.72, loss_scale=8, train_wall=254, gb_free=12.3, wall=31261
2022-03-01 18:24:34 | INFO | train_inner | epoch 031:    233 / 393 loss=4.545, nll_loss=4.327, ppl=20.07, wps=25279.4, ups=0.39, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.703, loss_scale=8, train_wall=254, gb_free=12.3, wall=31520
2022-03-01 18:25:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 18:28:56 | INFO | train_inner | epoch 031:    334 / 393 loss=4.586, nll_loss=4.368, ppl=20.65, wps=25034.1, ups=0.38, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.712, loss_scale=8, train_wall=257, gb_free=12.3, wall=31782
2022-03-01 18:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 18:31:31 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.498 | nll_loss 6.298 | ppl 78.68 | wps 66039.5 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 6.25
2022-03-01 18:31:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-01 18:31:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 18:31:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 18:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 31 @ 12159 updates, score 6.498) (writing took 4.060769469942898 seconds)
2022-03-01 18:31:35 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-01 18:31:35 | INFO | train | epoch 031 | loss 4.542 | nll_loss 4.324 | ppl 20.02 | wps 25030.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.709 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 31941
2022-03-01 18:31:35 | INFO | fairseq.trainer | begin training epoch 32
2022-03-01 18:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:33:21 | INFO | train_inner | epoch 032:     41 / 393 loss=4.524, nll_loss=4.305, ppl=19.77, wps=24552.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.702, loss_scale=8, train_wall=253, gb_free=12.3, wall=32048
2022-03-01 18:37:41 | INFO | train_inner | epoch 032:    141 / 393 loss=4.453, nll_loss=4.233, ppl=18.8, wps=25252.3, ups=0.39, wpb=65530.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.695, loss_scale=8, train_wall=255, gb_free=12.3, wall=32307
2022-03-01 18:42:01 | INFO | train_inner | epoch 032:    241 / 393 loss=4.517, nll_loss=4.298, ppl=19.67, wps=25246.8, ups=0.39, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.7, loss_scale=8, train_wall=255, gb_free=12.3, wall=32567
2022-03-01 18:46:20 | INFO | train_inner | epoch 032:    341 / 393 loss=4.555, nll_loss=4.336, ppl=20.2, wps=25239.3, ups=0.39, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.705, loss_scale=8, train_wall=255, gb_free=12.3, wall=32826
2022-03-01 18:48:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 18:48:37 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.498 | nll_loss 6.296 | ppl 78.58 | wps 65596.7 | wpb 2034.1 | bsz 4 | num_updates 12552 | best_loss 6.25
2022-03-01 18:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12552 updates
2022-03-01 18:48:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 18:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 18:48:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 32 @ 12552 updates, score 6.498) (writing took 4.411412936984561 seconds)
2022-03-01 18:48:42 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-01 18:48:42 | INFO | train | epoch 032 | loss 4.509 | nll_loss 4.29 | ppl 19.56 | wps 25053 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 12552 | lr 0.000282256 | gnorm 0.701 | loss_scale 16 | train_wall 1000 | gb_free 12.3 | wall 32968
2022-03-01 18:48:42 | INFO | fairseq.trainer | begin training epoch 33
2022-03-01 18:48:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:50:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 18:50:49 | INFO | train_inner | epoch 033:     49 / 393 loss=4.497, nll_loss=4.278, ppl=19.4, wps=24258.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.709, loss_scale=8, train_wall=256, gb_free=12.3, wall=33095
2022-03-01 18:55:09 | INFO | train_inner | epoch 033:    149 / 393 loss=4.435, nll_loss=4.214, ppl=18.56, wps=25271.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.715, loss_scale=8, train_wall=254, gb_free=12.3, wall=33355
2022-03-01 18:59:28 | INFO | train_inner | epoch 033:    249 / 393 loss=4.471, nll_loss=4.251, ppl=19.04, wps=25229.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.714, loss_scale=8, train_wall=255, gb_free=12.3, wall=33615
2022-03-01 19:03:48 | INFO | train_inner | epoch 033:    349 / 393 loss=4.525, nll_loss=4.305, ppl=19.77, wps=25245.6, ups=0.39, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.732, loss_scale=8, train_wall=255, gb_free=12.3, wall=33874
2022-03-01 19:05:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:05:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.527 | nll_loss 6.324 | ppl 80.12 | wps 65809 | wpb 2034.1 | bsz 4 | num_updates 12944 | best_loss 6.25
2022-03-01 19:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12944 updates
2022-03-01 19:05:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:05:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:05:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 33 @ 12944 updates, score 6.527) (writing took 4.205281811999157 seconds)
2022-03-01 19:05:49 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-01 19:05:49 | INFO | train | epoch 033 | loss 4.477 | nll_loss 4.256 | ppl 19.11 | wps 24994.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 12944 | lr 0.000277949 | gnorm 0.722 | loss_scale 8 | train_wall 1000 | gb_free 12.3 | wall 33995
2022-03-01 19:05:49 | INFO | fairseq.trainer | begin training epoch 34
2022-03-01 19:05:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 19:08:14 | INFO | train_inner | epoch 034:     56 / 393 loss=4.448, nll_loss=4.227, ppl=18.73, wps=24510.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.721, loss_scale=8, train_wall=254, gb_free=12.3, wall=34140
2022-03-01 19:12:34 | INFO | train_inner | epoch 034:    156 / 393 loss=4.395, nll_loss=4.173, ppl=18.04, wps=25172.7, ups=0.38, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.714, loss_scale=8, train_wall=255, gb_free=12.3, wall=34401
2022-03-01 19:14:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 19:16:57 | INFO | train_inner | epoch 034:    257 / 393 loss=4.458, nll_loss=4.237, ppl=18.86, wps=24996, ups=0.38, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.716, loss_scale=8, train_wall=257, gb_free=12.3, wall=34663
2022-03-01 19:21:16 | INFO | train_inner | epoch 034:    357 / 393 loss=4.5, nll_loss=4.28, ppl=19.42, wps=25243.8, ups=0.39, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.74, loss_scale=8, train_wall=255, gb_free=12.3, wall=34922
2022-03-01 19:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:22:52 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.556 | nll_loss 6.356 | ppl 81.89 | wps 66270.4 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 6.25
2022-03-01 19:22:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-01 19:22:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:22:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:22:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 34 @ 13336 updates, score 6.556) (writing took 4.213964367983863 seconds)
2022-03-01 19:22:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-01 19:22:56 | INFO | train | epoch 034 | loss 4.445 | nll_loss 4.224 | ppl 18.69 | wps 24976.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.721 | loss_scale 8 | train_wall 1000 | gb_free 12.3 | wall 35022
2022-03-01 19:22:56 | INFO | fairseq.trainer | begin training epoch 35
2022-03-01 19:22:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 19:25:42 | INFO | train_inner | epoch 035:     64 / 393 loss=4.406, nll_loss=4.184, ppl=18.18, wps=24517.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.709, loss_scale=8, train_wall=254, gb_free=12.3, wall=35189
2022-03-01 19:30:02 | INFO | train_inner | epoch 035:    164 / 393 loss=4.374, nll_loss=4.152, ppl=17.78, wps=25235.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.716, loss_scale=8, train_wall=255, gb_free=12.3, wall=35448
2022-03-01 19:34:21 | INFO | train_inner | epoch 035:    264 / 393 loss=4.432, nll_loss=4.21, ppl=18.51, wps=25259, ups=0.39, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.729, loss_scale=8, train_wall=255, gb_free=12.3, wall=35708
2022-03-01 19:36:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 19:38:43 | INFO | train_inner | epoch 035:    365 / 393 loss=4.47, nll_loss=4.249, ppl=19.01, wps=25025.5, ups=0.38, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.739, loss_scale=8, train_wall=257, gb_free=12.3, wall=35970
2022-03-01 19:39:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:39:58 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.574 | nll_loss 6.373 | ppl 82.89 | wps 66005.6 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 6.25
2022-03-01 19:39:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-01 19:39:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:40:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:40:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 35 @ 13728 updates, score 6.574) (writing took 4.569544670986943 seconds)
2022-03-01 19:40:03 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-01 19:40:03 | INFO | train | epoch 035 | loss 4.415 | nll_loss 4.194 | ppl 18.3 | wps 24992.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.724 | loss_scale 8 | train_wall 1000 | gb_free 12.3 | wall 36049
2022-03-01 19:40:03 | INFO | fairseq.trainer | begin training epoch 36
2022-03-01 19:40:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 19:43:09 | INFO | train_inner | epoch 036:     72 / 393 loss=4.363, nll_loss=4.14, ppl=17.63, wps=24519.5, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.745, loss_scale=8, train_wall=253, gb_free=12.3, wall=36236
2022-03-01 19:47:29 | INFO | train_inner | epoch 036:    172 / 393 loss=4.355, nll_loss=4.133, ppl=17.54, wps=25282.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.734, loss_scale=8, train_wall=254, gb_free=12.3, wall=36495
2022-03-01 19:51:48 | INFO | train_inner | epoch 036:    272 / 393 loss=4.401, nll_loss=4.179, ppl=18.11, wps=25276.3, ups=0.39, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.745, loss_scale=8, train_wall=254, gb_free=12.3, wall=36754
2022-03-01 19:56:07 | INFO | train_inner | epoch 036:    372 / 393 loss=4.445, nll_loss=4.224, ppl=18.69, wps=25273, ups=0.39, wpb=65535.4, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.74, loss_scale=8, train_wall=254, gb_free=12.3, wall=37013
2022-03-01 19:57:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:57:04 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.597 | nll_loss 6.396 | ppl 84.23 | wps 65847 | wpb 2034.1 | bsz 4 | num_updates 14121 | best_loss 6.25
2022-03-01 19:57:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14121 updates
2022-03-01 19:57:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 19:57:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 36 @ 14121 updates, score 6.597) (writing took 4.241672034957446 seconds)
2022-03-01 19:57:08 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-01 19:57:08 | INFO | train | epoch 036 | loss 4.387 | nll_loss 4.165 | ppl 17.94 | wps 25088.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 14121 | lr 0.000266114 | gnorm 0.74 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 37074
2022-03-01 19:57:08 | INFO | fairseq.trainer | begin training epoch 37
2022-03-01 19:57:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:00:33 | INFO | train_inner | epoch 037:     79 / 393 loss=4.307, nll_loss=4.084, ppl=16.96, wps=24548.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.733, loss_scale=16, train_wall=253, gb_free=12.3, wall=37279
2022-03-01 20:02:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 20:04:55 | INFO | train_inner | epoch 037:    180 / 393 loss=4.329, nll_loss=4.106, ppl=17.22, wps=25029.5, ups=0.38, wpb=65530.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.75, loss_scale=8, train_wall=257, gb_free=12.3, wall=37541
2022-03-01 20:09:14 | INFO | train_inner | epoch 037:    280 / 393 loss=4.382, nll_loss=4.159, ppl=17.87, wps=25272, ups=0.39, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.731, loss_scale=8, train_wall=254, gb_free=12.3, wall=37800
2022-03-01 20:13:33 | INFO | train_inner | epoch 037:    380 / 393 loss=4.432, nll_loss=4.21, ppl=18.51, wps=25278.6, ups=0.39, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.751, loss_scale=8, train_wall=254, gb_free=12.3, wall=38060
2022-03-01 20:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 20:14:09 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.625 | nll_loss 6.422 | ppl 85.76 | wps 66182.9 | wpb 2034.1 | bsz 4 | num_updates 14513 | best_loss 6.25
2022-03-01 20:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14513 updates
2022-03-01 20:14:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 20:14:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 20:14:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 37 @ 14513 updates, score 6.625) (writing took 4.176787054981105 seconds)
2022-03-01 20:14:13 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-01 20:14:13 | INFO | train | epoch 037 | loss 4.36 | nll_loss 4.137 | ppl 17.59 | wps 25026.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 14513 | lr 0.000262495 | gnorm 0.744 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 38100
2022-03-01 20:14:13 | INFO | fairseq.trainer | begin training epoch 38
2022-03-01 20:14:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:17:59 | INFO | train_inner | epoch 038:     87 / 393 loss=4.28, nll_loss=4.056, ppl=16.63, wps=24560.8, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.743, loss_scale=8, train_wall=253, gb_free=12.3, wall=38325
2022-03-01 20:22:18 | INFO | train_inner | epoch 038:    187 / 393 loss=4.304, nll_loss=4.08, ppl=16.91, wps=25283.1, ups=0.39, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.753, loss_scale=8, train_wall=254, gb_free=12.3, wall=38585
2022-03-01 20:26:38 | INFO | train_inner | epoch 038:    287 / 393 loss=4.357, nll_loss=4.134, ppl=17.56, wps=25276.4, ups=0.39, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.757, loss_scale=16, train_wall=254, gb_free=12.3, wall=38844
2022-03-01 20:27:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 20:30:59 | INFO | train_inner | epoch 038:    388 / 393 loss=4.401, nll_loss=4.179, ppl=18.12, wps=25020.3, ups=0.38, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.752, loss_scale=8, train_wall=257, gb_free=12.3, wall=39106
2022-03-01 20:31:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 20:31:15 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.629 | nll_loss 6.428 | ppl 86.11 | wps 66058.9 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 6.25
2022-03-01 20:31:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-01 20:31:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 20:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 20:31:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 38 @ 14905 updates, score 6.629) (writing took 4.444089432014152 seconds)
2022-03-01 20:31:19 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-01 20:31:19 | INFO | train | epoch 038 | loss 4.334 | nll_loss 4.11 | ppl 17.27 | wps 25019.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.75 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 39125
2022-03-01 20:31:19 | INFO | fairseq.trainer | begin training epoch 39
2022-03-01 20:31:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:35:26 | INFO | train_inner | epoch 039:     95 / 393 loss=4.233, nll_loss=4.008, ppl=16.08, wps=24454.5, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.745, loss_scale=8, train_wall=254, gb_free=12.3, wall=39373
2022-03-01 20:39:47 | INFO | train_inner | epoch 039:    195 / 393 loss=4.29, nll_loss=4.066, ppl=16.74, wps=25156.1, ups=0.38, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.753, loss_scale=8, train_wall=255, gb_free=12.3, wall=39633
2022-03-01 20:44:08 | INFO | train_inner | epoch 039:    295 / 393 loss=4.342, nll_loss=4.118, ppl=17.37, wps=25133.7, ups=0.38, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.752, loss_scale=8, train_wall=256, gb_free=12.3, wall=39894
2022-03-01 20:48:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 20:48:25 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.652 | nll_loss 6.453 | ppl 87.6 | wps 65235.4 | wpb 2034.1 | bsz 4 | num_updates 15298 | best_loss 6.25
2022-03-01 20:48:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15298 updates
2022-03-01 20:48:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 20:48:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 20:48:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 39 @ 15298 updates, score 6.652) (writing took 4.365849248948507 seconds)
2022-03-01 20:48:29 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-01 20:48:29 | INFO | train | epoch 039 | loss 4.308 | nll_loss 4.084 | ppl 16.96 | wps 24970.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 15298 | lr 0.000255672 | gnorm 0.751 | loss_scale 8 | train_wall 1002 | gb_free 12.3 | wall 40156
2022-03-01 20:48:29 | INFO | fairseq.trainer | begin training epoch 40
2022-03-01 20:48:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:48:35 | INFO | train_inner | epoch 040:      2 / 393 loss=4.373, nll_loss=4.15, ppl=17.75, wps=24428.4, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.757, loss_scale=8, train_wall=254, gb_free=12.3, wall=40161
2022-03-01 20:52:56 | INFO | train_inner | epoch 040:    102 / 393 loss=4.208, nll_loss=3.982, ppl=15.8, wps=25107.7, ups=0.38, wpb=65530.9, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.753, loss_scale=16, train_wall=256, gb_free=12.3, wall=40422
2022-03-01 20:57:16 | INFO | train_inner | epoch 040:    202 / 393 loss=4.26, nll_loss=4.035, ppl=16.39, wps=25144.2, ups=0.38, wpb=65535.4, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.759, loss_scale=16, train_wall=256, gb_free=12.3, wall=40683
2022-03-01 20:59:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 21:01:40 | INFO | train_inner | epoch 040:    303 / 393 loss=4.315, nll_loss=4.09, ppl=17.03, wps=24888.3, ups=0.38, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.763, loss_scale=8, train_wall=258, gb_free=12.3, wall=40946
2022-03-01 21:05:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:05:35 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.692 | nll_loss 6.492 | ppl 89.99 | wps 66059.6 | wpb 2034.1 | bsz 4 | num_updates 15690 | best_loss 6.25
2022-03-01 21:05:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15690 updates
2022-03-01 21:05:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:05:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:05:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 40 @ 15690 updates, score 6.692) (writing took 4.66050035005901 seconds)
2022-03-01 21:05:40 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-01 21:05:40 | INFO | train | epoch 040 | loss 4.284 | nll_loss 4.059 | ppl 16.67 | wps 24900.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 15690 | lr 0.000252458 | gnorm 0.762 | loss_scale 8 | train_wall 1002 | gb_free 12.3 | wall 41186
2022-03-01 21:05:40 | INFO | fairseq.trainer | begin training epoch 41
2022-03-01 21:05:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:06:06 | INFO | train_inner | epoch 041:     10 / 393 loss=4.341, nll_loss=4.118, ppl=17.36, wps=24499.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.77, loss_scale=8, train_wall=253, gb_free=12.3, wall=41212
2022-03-01 21:10:25 | INFO | train_inner | epoch 041:    110 / 393 loss=4.183, nll_loss=3.956, ppl=15.52, wps=25266.9, ups=0.39, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.772, loss_scale=8, train_wall=254, gb_free=12.3, wall=41472
2022-03-01 21:14:45 | INFO | train_inner | epoch 041:    210 / 393 loss=4.25, nll_loss=4.024, ppl=16.27, wps=25266.3, ups=0.39, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.785, loss_scale=8, train_wall=254, gb_free=12.3, wall=41731
2022-03-01 21:19:04 | INFO | train_inner | epoch 041:    310 / 393 loss=4.292, nll_loss=4.067, ppl=16.76, wps=25276.2, ups=0.39, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.766, loss_scale=8, train_wall=254, gb_free=12.3, wall=41990
2022-03-01 21:22:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 21:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:22:42 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.68 | nll_loss 6.481 | ppl 89.31 | wps 65631.8 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 6.25
2022-03-01 21:22:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-01 21:22:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:22:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 41 @ 16082 updates, score 6.68) (writing took 4.61481169401668 seconds)
2022-03-01 21:22:46 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-01 21:22:46 | INFO | train | epoch 041 | loss 4.261 | nll_loss 4.036 | ppl 16.4 | wps 25004.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.779 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 42212
2022-03-01 21:22:46 | INFO | fairseq.trainer | begin training epoch 42
2022-03-01 21:22:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:23:33 | INFO | train_inner | epoch 042:     18 / 393 loss=4.311, nll_loss=4.086, ppl=16.99, wps=24254, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.795, loss_scale=8, train_wall=256, gb_free=12.3, wall=42259
2022-03-01 21:27:53 | INFO | train_inner | epoch 042:    118 / 393 loss=4.177, nll_loss=3.95, ppl=15.46, wps=25161.2, ups=0.38, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.775, loss_scale=8, train_wall=255, gb_free=12.3, wall=42520
2022-03-01 21:32:14 | INFO | train_inner | epoch 042:    218 / 393 loss=4.231, nll_loss=4.005, ppl=16.05, wps=25172.7, ups=0.38, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.772, loss_scale=8, train_wall=255, gb_free=12.3, wall=42780
2022-03-01 21:36:34 | INFO | train_inner | epoch 042:    318 / 393 loss=4.274, nll_loss=4.049, ppl=16.55, wps=25166.3, ups=0.38, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.771, loss_scale=8, train_wall=255, gb_free=12.3, wall=43040
2022-03-01 21:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:39:52 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.72 | nll_loss 6.516 | ppl 91.54 | wps 65517.8 | wpb 2034.1 | bsz 4 | num_updates 16475 | best_loss 6.25
2022-03-01 21:39:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16475 updates
2022-03-01 21:39:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:39:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:39:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 42 @ 16475 updates, score 6.72) (writing took 4.412899729097262 seconds)
2022-03-01 21:39:56 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-01 21:39:56 | INFO | train | epoch 042 | loss 4.238 | nll_loss 4.013 | ppl 16.14 | wps 24972 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 16475 | lr 0.00024637 | gnorm 0.774 | loss_scale 8 | train_wall 1002 | gb_free 12.3 | wall 43243
2022-03-01 21:39:56 | INFO | fairseq.trainer | begin training epoch 43
2022-03-01 21:39:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:41:02 | INFO | train_inner | epoch 043:     25 / 393 loss=4.254, nll_loss=4.029, ppl=16.32, wps=24375.1, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.785, loss_scale=8, train_wall=255, gb_free=12.3, wall=43308
2022-03-01 21:45:23 | INFO | train_inner | epoch 043:    125 / 393 loss=4.157, nll_loss=3.93, ppl=15.24, wps=25132.5, ups=0.38, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.778, loss_scale=16, train_wall=256, gb_free=12.3, wall=43569
2022-03-01 21:49:43 | INFO | train_inner | epoch 043:    225 / 393 loss=4.21, nll_loss=3.983, ppl=15.81, wps=25131.5, ups=0.38, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.783, loss_scale=16, train_wall=256, gb_free=12.3, wall=43830
2022-03-01 21:53:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 21:54:07 | INFO | train_inner | epoch 043:    326 / 393 loss=4.263, nll_loss=4.038, ppl=16.43, wps=24874.9, ups=0.38, wpb=65530.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.769, loss_scale=8, train_wall=258, gb_free=12.3, wall=44093
2022-03-01 21:56:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:57:03 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.735 | nll_loss 6.531 | ppl 92.45 | wps 65819.5 | wpb 2034.1 | bsz 4 | num_updates 16867 | best_loss 6.25
2022-03-01 21:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16867 updates
2022-03-01 21:57:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:57:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 21:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 43 @ 16867 updates, score 6.735) (writing took 4.571204248000868 seconds)
2022-03-01 21:57:07 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-01 21:57:07 | INFO | train | epoch 043 | loss 4.217 | nll_loss 3.99 | ppl 15.89 | wps 24891.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 16867 | lr 0.00024349 | gnorm 0.778 | loss_scale 8 | train_wall 1003 | gb_free 12.3 | wall 44274
2022-03-01 21:57:07 | INFO | fairseq.trainer | begin training epoch 44
2022-03-01 21:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:58:33 | INFO | train_inner | epoch 044:     33 / 393 loss=4.222, nll_loss=3.996, ppl=15.95, wps=24521.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.782, loss_scale=8, train_wall=253, gb_free=12.3, wall=44359
2022-03-01 22:02:52 | INFO | train_inner | epoch 044:    133 / 393 loss=4.13, nll_loss=3.902, ppl=14.95, wps=25281.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.771, loss_scale=8, train_wall=254, gb_free=12.3, wall=44618
2022-03-01 22:07:11 | INFO | train_inner | epoch 044:    233 / 393 loss=4.194, nll_loss=3.966, ppl=15.63, wps=25267.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.792, loss_scale=8, train_wall=254, gb_free=12.3, wall=44878
2022-03-01 22:11:31 | INFO | train_inner | epoch 044:    333 / 393 loss=4.239, nll_loss=4.013, ppl=16.14, wps=25276.5, ups=0.39, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.79, loss_scale=8, train_wall=254, gb_free=12.3, wall=45137
2022-03-01 22:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 22:14:09 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.75 | nll_loss 6.548 | ppl 93.56 | wps 65768.7 | wpb 2034.1 | bsz 4 | num_updates 17260 | best_loss 6.25
2022-03-01 22:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17260 updates
2022-03-01 22:14:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 22:14:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 22:14:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 44 @ 17260 updates, score 6.75) (writing took 4.774964657030068 seconds)
2022-03-01 22:14:13 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-01 22:14:13 | INFO | train | epoch 044 | loss 4.195 | nll_loss 3.968 | ppl 15.65 | wps 25072.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 17260 | lr 0.000240702 | gnorm 0.789 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 45300
2022-03-01 22:14:13 | INFO | fairseq.trainer | begin training epoch 45
2022-03-01 22:14:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 22:15:57 | INFO | train_inner | epoch 045:     40 / 393 loss=4.206, nll_loss=3.98, ppl=15.78, wps=24490.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.797, loss_scale=16, train_wall=253, gb_free=12.3, wall=45403
2022-03-01 22:20:16 | INFO | train_inner | epoch 045:    140 / 393 loss=4.127, nll_loss=3.899, ppl=14.92, wps=25278.8, ups=0.39, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.769, loss_scale=16, train_wall=254, gb_free=12.3, wall=45663
2022-03-01 22:24:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 22:24:38 | INFO | train_inner | epoch 045:    241 / 393 loss=4.172, nll_loss=3.945, ppl=15.4, wps=25023.7, ups=0.38, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.793, loss_scale=8, train_wall=257, gb_free=12.3, wall=45925
2022-03-01 22:28:58 | INFO | train_inner | epoch 045:    341 / 393 loss=4.224, nll_loss=3.997, ppl=15.97, wps=25277.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.789, loss_scale=8, train_wall=254, gb_free=12.3, wall=46184
2022-03-01 22:31:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 22:31:15 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.761 | nll_loss 6.561 | ppl 94.44 | wps 65979.1 | wpb 2034.1 | bsz 4 | num_updates 17652 | best_loss 6.25
2022-03-01 22:31:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17652 updates
2022-03-01 22:31:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 22:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 22:31:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 45 @ 17652 updates, score 6.761) (writing took 4.666022137040272 seconds)
2022-03-01 22:31:19 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-01 22:31:19 | INFO | train | epoch 045 | loss 4.175 | nll_loss 3.947 | ppl 15.43 | wps 25014 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 17652 | lr 0.000238014 | gnorm 0.787 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 46325
2022-03-01 22:31:19 | INFO | fairseq.trainer | begin training epoch 46
2022-03-01 22:31:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 22:33:24 | INFO | train_inner | epoch 046:     48 / 393 loss=4.167, nll_loss=3.939, ppl=15.34, wps=24515.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.8, loss_scale=8, train_wall=253, gb_free=12.3, wall=46450
2022-03-01 22:37:43 | INFO | train_inner | epoch 046:    148 / 393 loss=4.103, nll_loss=3.874, ppl=14.66, wps=25253.9, ups=0.39, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.786, loss_scale=8, train_wall=255, gb_free=12.3, wall=46710
2022-03-01 22:42:03 | INFO | train_inner | epoch 046:    248 / 393 loss=4.156, nll_loss=3.928, ppl=15.22, wps=25234.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.8, loss_scale=8, train_wall=255, gb_free=12.3, wall=46969
2022-03-01 22:46:23 | INFO | train_inner | epoch 046:    348 / 393 loss=4.21, nll_loss=3.983, ppl=15.82, wps=25209.6, ups=0.38, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.792, loss_scale=16, train_wall=255, gb_free=12.3, wall=47229
2022-03-01 22:48:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 22:48:22 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.792 | nll_loss 6.592 | ppl 96.46 | wps 66005 | wpb 2034.1 | bsz 4 | num_updates 18045 | best_loss 6.25
2022-03-01 22:48:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18045 updates
2022-03-01 22:48:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 22:48:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 22:48:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 46 @ 18045 updates, score 6.792) (writing took 4.747389730997384 seconds)
2022-03-01 22:48:27 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-01 22:48:27 | INFO | train | epoch 046 | loss 4.155 | nll_loss 3.927 | ppl 15.21 | wps 25042.4 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 18045 | lr 0.000235408 | gnorm 0.792 | loss_scale 16 | train_wall 1000 | gb_free 12.3 | wall 47353
2022-03-01 22:48:27 | INFO | fairseq.trainer | begin training epoch 47
2022-03-01 22:48:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 22:50:49 | INFO | train_inner | epoch 047:     55 / 393 loss=4.13, nll_loss=3.902, ppl=14.95, wps=24500, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.791, loss_scale=16, train_wall=253, gb_free=12.3, wall=47495
2022-03-01 22:54:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 22:55:11 | INFO | train_inner | epoch 047:    156 / 393 loss=4.086, nll_loss=3.856, ppl=14.48, wps=25014.5, ups=0.38, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.792, loss_scale=8, train_wall=257, gb_free=12.3, wall=47757
2022-03-01 22:59:31 | INFO | train_inner | epoch 047:    256 / 393 loss=4.144, nll_loss=3.916, ppl=15.09, wps=25268.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.794, loss_scale=8, train_wall=254, gb_free=12.3, wall=48017
2022-03-01 23:03:50 | INFO | train_inner | epoch 047:    356 / 393 loss=4.196, nll_loss=3.969, ppl=15.66, wps=25274.7, ups=0.39, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.829, loss_scale=8, train_wall=254, gb_free=12.3, wall=48276
2022-03-01 23:05:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:05:28 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.807 | nll_loss 6.605 | ppl 97.32 | wps 66238.6 | wpb 2034.1 | bsz 4 | num_updates 18437 | best_loss 6.25
2022-03-01 23:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18437 updates
2022-03-01 23:05:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:05:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 47 @ 18437 updates, score 6.807) (writing took 4.603394987992942 seconds)
2022-03-01 23:05:33 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-01 23:05:33 | INFO | train | epoch 047 | loss 4.136 | nll_loss 3.908 | ppl 15.01 | wps 25008.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 18437 | lr 0.000232892 | gnorm 0.803 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 48379
2022-03-01 23:05:33 | INFO | fairseq.trainer | begin training epoch 48
2022-03-01 23:05:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 23:08:16 | INFO | train_inner | epoch 048:     63 / 393 loss=4.101, nll_loss=3.872, ppl=14.64, wps=24516.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.805, loss_scale=8, train_wall=253, gb_free=12.3, wall=48542
2022-03-01 23:12:35 | INFO | train_inner | epoch 048:    163 / 393 loss=4.083, nll_loss=3.853, ppl=14.45, wps=25279, ups=0.39, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.824, loss_scale=8, train_wall=254, gb_free=12.3, wall=48802
2022-03-01 23:16:55 | INFO | train_inner | epoch 048:    263 / 393 loss=4.126, nll_loss=3.898, ppl=14.91, wps=25265.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.794, loss_scale=16, train_wall=255, gb_free=12.3, wall=49061
2022-03-01 23:17:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 23:21:17 | INFO | train_inner | epoch 048:    364 / 393 loss=4.178, nll_loss=3.951, ppl=15.46, wps=25024, ups=0.38, wpb=65530.9, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.826, loss_scale=8, train_wall=257, gb_free=12.3, wall=49323
2022-03-01 23:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:22:34 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.845 | nll_loss 6.644 | ppl 100.02 | wps 65828.6 | wpb 2034.1 | bsz 4 | num_updates 18829 | best_loss 6.25
2022-03-01 23:22:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18829 updates
2022-03-01 23:22:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:22:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:22:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 48 @ 18829 updates, score 6.845) (writing took 4.636002357001416 seconds)
2022-03-01 23:22:39 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-01 23:22:39 | INFO | train | epoch 048 | loss 4.118 | nll_loss 3.889 | ppl 14.82 | wps 25011.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 18829 | lr 0.000230455 | gnorm 0.812 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 49405
2022-03-01 23:22:39 | INFO | fairseq.trainer | begin training epoch 49
2022-03-01 23:22:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 23:25:43 | INFO | train_inner | epoch 049:     71 / 393 loss=4.065, nll_loss=3.836, ppl=14.28, wps=24506.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.792, loss_scale=8, train_wall=253, gb_free=12.3, wall=49589
2022-03-01 23:30:02 | INFO | train_inner | epoch 049:    171 / 393 loss=4.065, nll_loss=3.835, ppl=14.27, wps=25284, ups=0.39, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.812, loss_scale=8, train_wall=254, gb_free=12.3, wall=49848
2022-03-01 23:34:21 | INFO | train_inner | epoch 049:    271 / 393 loss=4.114, nll_loss=3.885, ppl=14.77, wps=25284, ups=0.39, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.814, loss_scale=8, train_wall=254, gb_free=12.3, wall=50107
2022-03-01 23:38:40 | INFO | train_inner | epoch 049:    371 / 393 loss=4.165, nll_loss=3.937, ppl=15.31, wps=25279.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.824, loss_scale=8, train_wall=254, gb_free=12.3, wall=50367
2022-03-01 23:39:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:39:40 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.836 | nll_loss 6.636 | ppl 99.43 | wps 66306.4 | wpb 2034.1 | bsz 4 | num_updates 19222 | best_loss 6.25
2022-03-01 23:39:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19222 updates
2022-03-01 23:39:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:39:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:39:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 49 @ 19222 updates, score 6.836) (writing took 4.640098426025361 seconds)
2022-03-01 23:39:44 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-01 23:39:44 | INFO | train | epoch 049 | loss 4.1 | nll_loss 3.871 | ppl 14.63 | wps 25081.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 19222 | lr 0.000228087 | gnorm 0.811 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 50431
2022-03-01 23:39:44 | INFO | fairseq.trainer | begin training epoch 50
2022-03-01 23:39:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 23:43:07 | INFO | train_inner | epoch 050:     78 / 393 loss=4.041, nll_loss=3.811, ppl=14.03, wps=24503.1, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.806, loss_scale=16, train_wall=253, gb_free=12.3, wall=50633
2022-03-01 23:43:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 23:47:29 | INFO | train_inner | epoch 050:    179 / 393 loss=4.039, nll_loss=3.808, ppl=14.01, wps=25020.2, ups=0.38, wpb=65530.2, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.803, loss_scale=8, train_wall=257, gb_free=12.3, wall=50895
2022-03-01 23:51:48 | INFO | train_inner | epoch 050:    279 / 393 loss=4.106, nll_loss=3.877, ppl=14.69, wps=25275.6, ups=0.39, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.814, loss_scale=8, train_wall=254, gb_free=12.3, wall=51154
2022-03-01 23:56:07 | INFO | train_inner | epoch 050:    379 / 393 loss=4.156, nll_loss=3.928, ppl=15.22, wps=25280.1, ups=0.39, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.846, loss_scale=8, train_wall=254, gb_free=12.3, wall=51413
2022-03-01 23:56:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:56:46 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.872 | nll_loss 6.671 | ppl 101.88 | wps 65792.8 | wpb 2034.1 | bsz 4 | num_updates 19614 | best_loss 6.25
2022-03-01 23:56:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19614 updates
2022-03-01 23:56:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:56:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-01 23:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 50 @ 19614 updates, score 6.872) (writing took 4.658473500981927 seconds)
2022-03-01 23:56:50 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-01 23:56:50 | INFO | train | epoch 050 | loss 4.083 | nll_loss 3.853 | ppl 14.45 | wps 25010.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 19614 | lr 0.000225796 | gnorm 0.817 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 51457
2022-03-01 23:56:50 | INFO | fairseq.trainer | begin training epoch 51
2022-03-01 23:56:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:00:33 | INFO | train_inner | epoch 051:     86 / 393 loss=4.011, nll_loss=3.78, ppl=13.74, wps=24513.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.819, loss_scale=8, train_wall=253, gb_free=12.3, wall=51680
2022-03-02 00:04:53 | INFO | train_inner | epoch 051:    186 / 393 loss=4.037, nll_loss=3.806, ppl=13.99, wps=25266, ups=0.39, wpb=65530.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.828, loss_scale=8, train_wall=254, gb_free=12.3, wall=51939
2022-03-02 00:09:12 | INFO | train_inner | epoch 051:    286 / 393 loss=4.098, nll_loss=3.868, ppl=14.6, wps=25261, ups=0.39, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.816, loss_scale=16, train_wall=255, gb_free=12.3, wall=52198
2022-03-02 00:13:31 | INFO | train_inner | epoch 051:    386 / 393 loss=4.129, nll_loss=3.9, ppl=14.93, wps=25268.4, ups=0.39, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.816, loss_scale=16, train_wall=254, gb_free=12.3, wall=52458
2022-03-02 00:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 00:13:52 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.876 | nll_loss 6.674 | ppl 102.14 | wps 66071.7 | wpb 2034.1 | bsz 4 | num_updates 20007 | best_loss 6.25
2022-03-02 00:13:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20007 updates
2022-03-02 00:13:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 00:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 00:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 51 @ 20007 updates, score 6.876) (writing took 4.644297051941976 seconds)
2022-03-02 00:13:56 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-02 00:13:56 | INFO | train | epoch 051 | loss 4.066 | nll_loss 3.836 | ppl 14.28 | wps 25070.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 20007 | lr 0.000223568 | gnorm 0.821 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 52483
2022-03-02 00:13:56 | INFO | fairseq.trainer | begin training epoch 52
2022-03-02 00:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:15:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 00:18:00 | INFO | train_inner | epoch 052:     94 / 393 loss=3.983, nll_loss=3.752, ppl=13.47, wps=24270.3, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.819, loss_scale=8, train_wall=256, gb_free=12.3, wall=52727
2022-03-02 00:22:20 | INFO | train_inner | epoch 052:    194 / 393 loss=4.03, nll_loss=3.799, ppl=13.92, wps=25268.2, ups=0.39, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.835, loss_scale=8, train_wall=254, gb_free=12.3, wall=52986
2022-03-02 00:26:39 | INFO | train_inner | epoch 052:    294 / 393 loss=4.071, nll_loss=3.841, ppl=14.33, wps=25273, ups=0.39, wpb=65530.9, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.833, loss_scale=8, train_wall=254, gb_free=12.3, wall=53245
2022-03-02 00:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 00:30:58 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.905 | nll_loss 6.705 | ppl 104.34 | wps 66037.2 | wpb 2034.1 | bsz 4 | num_updates 20399 | best_loss 6.25
2022-03-02 00:30:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20399 updates
2022-03-02 00:30:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 00:31:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 00:31:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 52 @ 20399 updates, score 6.905) (writing took 4.706143154064193 seconds)
2022-03-02 00:31:03 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-02 00:31:03 | INFO | train | epoch 052 | loss 4.05 | nll_loss 3.82 | ppl 14.12 | wps 25006.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 20399 | lr 0.000221409 | gnorm 0.83 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 53509
2022-03-02 00:31:03 | INFO | fairseq.trainer | begin training epoch 53
2022-03-02 00:31:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:31:05 | INFO | train_inner | epoch 053:      1 / 393 loss=4.123, nll_loss=3.894, ppl=14.87, wps=24495.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.84, loss_scale=8, train_wall=253, gb_free=12.3, wall=53512
2022-03-02 00:35:25 | INFO | train_inner | epoch 053:    101 / 393 loss=3.955, nll_loss=3.723, ppl=13.2, wps=25261.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.827, loss_scale=8, train_wall=255, gb_free=12.3, wall=53771
2022-03-02 00:39:44 | INFO | train_inner | epoch 053:    201 / 393 loss=4.017, nll_loss=3.786, ppl=13.79, wps=25280.8, ups=0.39, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.841, loss_scale=16, train_wall=254, gb_free=12.3, wall=54030
2022-03-02 00:44:03 | INFO | train_inner | epoch 053:    301 / 393 loss=4.065, nll_loss=3.834, ppl=14.26, wps=25284.1, ups=0.39, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.826, loss_scale=16, train_wall=254, gb_free=12.3, wall=54289
2022-03-02 00:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 00:48:04 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.925 | nll_loss 6.724 | ppl 105.7 | wps 66395.8 | wpb 2034.1 | bsz 4 | num_updates 20792 | best_loss 6.25
2022-03-02 00:48:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20792 updates
2022-03-02 00:48:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 00:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 00:48:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 53 @ 20792 updates, score 6.925) (writing took 4.517808350967243 seconds)
2022-03-02 00:48:08 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-02 00:48:08 | INFO | train | epoch 053 | loss 4.035 | nll_loss 3.804 | ppl 13.96 | wps 25081.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 20792 | lr 0.000219307 | gnorm 0.836 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 54535
2022-03-02 00:48:08 | INFO | fairseq.trainer | begin training epoch 54
2022-03-02 00:48:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:48:29 | INFO | train_inner | epoch 054:      8 / 393 loss=4.093, nll_loss=3.863, ppl=14.55, wps=24528.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.849, loss_scale=16, train_wall=253, gb_free=12.3, wall=54555
2022-03-02 00:52:48 | INFO | train_inner | epoch 054:    108 / 393 loss=3.943, nll_loss=3.71, ppl=13.09, wps=25281.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.826, loss_scale=16, train_wall=254, gb_free=12.3, wall=54815
2022-03-02 00:57:08 | INFO | train_inner | epoch 054:    208 / 393 loss=4.003, nll_loss=3.771, ppl=13.65, wps=25271.5, ups=0.39, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.848, loss_scale=16, train_wall=254, gb_free=12.3, wall=55074
2022-03-02 00:59:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 01:01:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 01:01:32 | INFO | train_inner | epoch 054:    310 / 393 loss=4.046, nll_loss=3.815, ppl=14.07, wps=24778.4, ups=0.38, wpb=65535.4, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.857, loss_scale=8, train_wall=260, gb_free=12.3, wall=55338
2022-03-02 01:05:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:05:10 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.912 | nll_loss 6.709 | ppl 104.61 | wps 66086.6 | wpb 2034.1 | bsz 4 | num_updates 21183 | best_loss 6.25
2022-03-02 01:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21183 updates
2022-03-02 01:05:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:05:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:05:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 54 @ 21183 updates, score 6.912) (writing took 4.687830534996465 seconds)
2022-03-02 01:05:14 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-02 01:05:14 | INFO | train | epoch 054 | loss 4.018 | nll_loss 3.787 | ppl 13.8 | wps 24946.2 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 21183 | lr 0.000217273 | gnorm 0.842 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 55561
2022-03-02 01:05:14 | INFO | fairseq.trainer | begin training epoch 55
2022-03-02 01:05:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:05:59 | INFO | train_inner | epoch 055:     17 / 393 loss=4.069, nll_loss=3.839, ppl=14.31, wps=24495.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.837, loss_scale=8, train_wall=253, gb_free=12.3, wall=55605
2022-03-02 01:10:18 | INFO | train_inner | epoch 055:    117 / 393 loss=3.941, nll_loss=3.708, ppl=13.07, wps=25272.1, ups=0.39, wpb=65530.2, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.84, loss_scale=8, train_wall=254, gb_free=12.3, wall=55864
2022-03-02 01:14:37 | INFO | train_inner | epoch 055:    217 / 393 loss=3.994, nll_loss=3.762, ppl=13.57, wps=25277.2, ups=0.39, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.851, loss_scale=8, train_wall=254, gb_free=12.3, wall=56123
2022-03-02 01:18:56 | INFO | train_inner | epoch 055:    317 / 393 loss=4.035, nll_loss=3.804, ppl=13.97, wps=25272.2, ups=0.39, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.871, loss_scale=8, train_wall=254, gb_free=12.3, wall=56383
2022-03-02 01:22:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:22:16 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.961 | nll_loss 6.759 | ppl 108.34 | wps 66115.6 | wpb 2034.1 | bsz 4 | num_updates 21576 | best_loss 6.25
2022-03-02 01:22:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21576 updates
2022-03-02 01:22:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:22:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 55 @ 21576 updates, score 6.961) (writing took 4.679784993990324 seconds)
2022-03-02 01:22:20 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-02 01:22:20 | INFO | train | epoch 055 | loss 4.004 | nll_loss 3.773 | ppl 13.67 | wps 25072.7 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 21576 | lr 0.000215285 | gnorm 0.855 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 56587
2022-03-02 01:22:20 | INFO | fairseq.trainer | begin training epoch 56
2022-03-02 01:22:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:23:23 | INFO | train_inner | epoch 056:     24 / 393 loss=4.043, nll_loss=3.812, ppl=14.05, wps=24499.2, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.859, loss_scale=8, train_wall=253, gb_free=12.3, wall=56649
2022-03-02 01:27:42 | INFO | train_inner | epoch 056:    124 / 393 loss=3.929, nll_loss=3.696, ppl=12.96, wps=25267.7, ups=0.39, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.829, loss_scale=16, train_wall=254, gb_free=12.3, wall=56908
2022-03-02 01:32:01 | INFO | train_inner | epoch 056:    224 / 393 loss=3.982, nll_loss=3.75, ppl=13.45, wps=25277.3, ups=0.39, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.847, loss_scale=16, train_wall=254, gb_free=12.3, wall=57168
2022-03-02 01:34:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 01:36:23 | INFO | train_inner | epoch 056:    325 / 393 loss=4.026, nll_loss=3.795, ppl=13.88, wps=25030.1, ups=0.38, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.862, loss_scale=8, train_wall=257, gb_free=12.3, wall=57429
2022-03-02 01:39:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:39:22 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.999 | nll_loss 6.796 | ppl 111.14 | wps 65922.5 | wpb 2034.1 | bsz 4 | num_updates 21968 | best_loss 6.25
2022-03-02 01:39:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21968 updates
2022-03-02 01:39:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:39:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:39:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 56 @ 21968 updates, score 6.999) (writing took 4.549257135018706 seconds)
2022-03-02 01:39:26 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-02 01:39:26 | INFO | train | epoch 056 | loss 3.99 | nll_loss 3.758 | ppl 13.53 | wps 25012.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 21968 | lr 0.000213356 | gnorm 0.846 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 57613
2022-03-02 01:39:26 | INFO | fairseq.trainer | begin training epoch 57
2022-03-02 01:39:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:40:49 | INFO | train_inner | epoch 057:     32 / 393 loss=4.005, nll_loss=3.773, ppl=13.67, wps=24514.2, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.846, loss_scale=8, train_wall=253, gb_free=12.3, wall=57696
2022-03-02 01:45:09 | INFO | train_inner | epoch 057:    132 / 393 loss=3.924, nll_loss=3.691, ppl=12.91, wps=25271.2, ups=0.39, wpb=65530.9, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.849, loss_scale=8, train_wall=254, gb_free=12.3, wall=57955
2022-03-02 01:49:28 | INFO | train_inner | epoch 057:    232 / 393 loss=3.971, nll_loss=3.738, ppl=13.35, wps=25267, ups=0.39, wpb=65535.4, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.856, loss_scale=8, train_wall=255, gb_free=12.3, wall=58214
2022-03-02 01:53:47 | INFO | train_inner | epoch 057:    332 / 393 loss=4.013, nll_loss=3.782, ppl=13.75, wps=25279.1, ups=0.39, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.862, loss_scale=8, train_wall=254, gb_free=12.3, wall=58474
2022-03-02 01:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:56:28 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.992 | nll_loss 6.791 | ppl 110.75 | wps 66039.1 | wpb 2034.1 | bsz 4 | num_updates 22361 | best_loss 6.25
2022-03-02 01:56:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22361 updates
2022-03-02 01:56:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 01:56:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 57 @ 22361 updates, score 6.992) (writing took 4.5572077840333804 seconds)
2022-03-02 01:56:32 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-02 01:56:32 | INFO | train | epoch 057 | loss 3.976 | nll_loss 3.744 | ppl 13.39 | wps 25076.7 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 22361 | lr 0.000211473 | gnorm 0.854 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 58639
2022-03-02 01:56:32 | INFO | fairseq.trainer | begin training epoch 58
2022-03-02 01:56:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:58:14 | INFO | train_inner | epoch 058:     39 / 393 loss=3.982, nll_loss=3.75, ppl=13.45, wps=24512.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.85, loss_scale=16, train_wall=253, gb_free=12.3, wall=58740
2022-03-02 02:02:33 | INFO | train_inner | epoch 058:    139 / 393 loss=3.908, nll_loss=3.674, ppl=12.76, wps=25272.1, ups=0.39, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.86, loss_scale=16, train_wall=254, gb_free=12.3, wall=58999
2022-03-02 02:06:52 | INFO | train_inner | epoch 058:    239 / 393 loss=3.96, nll_loss=3.727, ppl=13.24, wps=25275.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.852, loss_scale=16, train_wall=254, gb_free=12.3, wall=59258
2022-03-02 02:11:11 | INFO | train_inner | epoch 058:    339 / 393 loss=4.019, nll_loss=3.787, ppl=13.81, wps=25274.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.851, loss_scale=16, train_wall=254, gb_free=12.3, wall=59518
2022-03-02 02:13:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 02:13:34 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.99 | nll_loss 6.788 | ppl 110.49 | wps 65900 | wpb 2034.1 | bsz 4 | num_updates 22754 | best_loss 6.25
2022-03-02 02:13:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22754 updates
2022-03-02 02:13:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 02:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 02:13:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 58 @ 22754 updates, score 6.99) (writing took 4.491696445038542 seconds)
2022-03-02 02:13:38 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-02 02:13:38 | INFO | train | epoch 058 | loss 3.962 | nll_loss 3.73 | ppl 13.27 | wps 25077.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 22754 | lr 0.000209639 | gnorm 0.854 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 59664
2022-03-02 02:13:38 | INFO | fairseq.trainer | begin training epoch 59
2022-03-02 02:13:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 02:15:37 | INFO | train_inner | epoch 059:     46 / 393 loss=3.954, nll_loss=3.721, ppl=13.19, wps=24523.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.868, loss_scale=16, train_wall=253, gb_free=12.3, wall=59784
2022-03-02 02:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 02:19:59 | INFO | train_inner | epoch 059:    147 / 393 loss=3.906, nll_loss=3.672, ppl=12.74, wps=25030.6, ups=0.38, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.852, loss_scale=16, train_wall=257, gb_free=12.3, wall=60046
2022-03-02 02:24:19 | INFO | train_inner | epoch 059:    247 / 393 loss=3.945, nll_loss=3.712, ppl=13.1, wps=25262.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.861, loss_scale=16, train_wall=254, gb_free=12.3, wall=60305
2022-03-02 02:28:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 02:28:40 | INFO | train_inner | epoch 059:    348 / 393 loss=4, nll_loss=3.768, ppl=13.63, wps=25035.9, ups=0.38, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.874, loss_scale=8, train_wall=257, gb_free=12.3, wall=60567
2022-03-02 02:30:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 02:30:39 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.027 | nll_loss 6.825 | ppl 113.36 | wps 66009.4 | wpb 2034.1 | bsz 4 | num_updates 23145 | best_loss 6.25
2022-03-02 02:30:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23145 updates
2022-03-02 02:30:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 02:30:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 02:30:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 59 @ 23145 updates, score 7.027) (writing took 4.458157193032093 seconds)
2022-03-02 02:30:44 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-02 02:30:44 | INFO | train | epoch 059 | loss 3.949 | nll_loss 3.716 | ppl 13.14 | wps 24953.5 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 23145 | lr 0.00020786 | gnorm 0.87 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 60690
2022-03-02 02:30:44 | INFO | fairseq.trainer | begin training epoch 60
2022-03-02 02:30:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 02:33:07 | INFO | train_inner | epoch 060:     55 / 393 loss=3.934, nll_loss=3.701, ppl=13, wps=24523.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.879, loss_scale=8, train_wall=253, gb_free=12.3, wall=60833
2022-03-02 02:37:26 | INFO | train_inner | epoch 060:    155 / 393 loss=3.891, nll_loss=3.657, ppl=12.61, wps=25273.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.876, loss_scale=8, train_wall=254, gb_free=12.3, wall=61092
2022-03-02 02:41:45 | INFO | train_inner | epoch 060:    255 / 393 loss=3.943, nll_loss=3.71, ppl=13.09, wps=25268.3, ups=0.39, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.876, loss_scale=8, train_wall=254, gb_free=12.3, wall=61351
2022-03-02 02:46:04 | INFO | train_inner | epoch 060:    355 / 393 loss=3.988, nll_loss=3.755, ppl=13.51, wps=25280, ups=0.39, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.879, loss_scale=8, train_wall=254, gb_free=12.3, wall=61611
2022-03-02 02:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 02:47:45 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.039 | nll_loss 6.837 | ppl 114.32 | wps 66066.7 | wpb 2034.1 | bsz 4 | num_updates 23538 | best_loss 6.25
2022-03-02 02:47:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23538 updates
2022-03-02 02:47:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 02:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 02:47:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 60 @ 23538 updates, score 7.039) (writing took 4.671274831984192 seconds)
2022-03-02 02:47:50 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-02 02:47:50 | INFO | train | epoch 060 | loss 3.936 | nll_loss 3.702 | ppl 13.02 | wps 25075.8 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 23538 | lr 0.000206118 | gnorm 0.873 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 61716
2022-03-02 02:47:50 | INFO | fairseq.trainer | begin training epoch 61
2022-03-02 02:47:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 02:50:31 | INFO | train_inner | epoch 061:     62 / 393 loss=3.906, nll_loss=3.673, ppl=12.75, wps=24507.1, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.863, loss_scale=16, train_wall=253, gb_free=12.3, wall=61877
2022-03-02 02:54:50 | INFO | train_inner | epoch 061:    162 / 393 loss=3.892, nll_loss=3.658, ppl=12.62, wps=25278.2, ups=0.39, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.869, loss_scale=16, train_wall=254, gb_free=12.3, wall=62136
2022-03-02 02:59:09 | INFO | train_inner | epoch 061:    262 / 393 loss=3.923, nll_loss=3.689, ppl=12.9, wps=25265.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.865, loss_scale=16, train_wall=255, gb_free=12.3, wall=62396
2022-03-02 03:03:28 | INFO | train_inner | epoch 061:    362 / 393 loss=3.977, nll_loss=3.745, ppl=13.41, wps=25284.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.891, loss_scale=16, train_wall=254, gb_free=12.3, wall=62655
2022-03-02 03:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:04:51 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.034 | nll_loss 6.834 | ppl 114.05 | wps 65994.8 | wpb 2034.1 | bsz 4 | num_updates 23931 | best_loss 6.25
2022-03-02 03:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23931 updates
2022-03-02 03:04:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:04:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 61 @ 23931 updates, score 7.034) (writing took 4.564860336948186 seconds)
2022-03-02 03:04:56 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-02 03:04:56 | INFO | train | epoch 061 | loss 3.923 | nll_loss 3.69 | ppl 12.9 | wps 25078.7 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 23931 | lr 0.000204418 | gnorm 0.873 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 62742
2022-03-02 03:04:56 | INFO | fairseq.trainer | begin training epoch 62
2022-03-02 03:04:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 03:07:55 | INFO | train_inner | epoch 062:     69 / 393 loss=3.89, nll_loss=3.656, ppl=12.6, wps=24521.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.861, loss_scale=16, train_wall=253, gb_free=12.3, wall=62921
2022-03-02 03:09:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 03:12:16 | INFO | train_inner | epoch 062:    170 / 393 loss=3.874, nll_loss=3.639, ppl=12.46, wps=25028, ups=0.38, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.878, loss_scale=8, train_wall=257, gb_free=12.3, wall=63183
2022-03-02 03:16:36 | INFO | train_inner | epoch 062:    270 / 393 loss=3.923, nll_loss=3.689, ppl=12.9, wps=25271.1, ups=0.39, wpb=65535.4, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.883, loss_scale=8, train_wall=254, gb_free=12.3, wall=63442
2022-03-02 03:20:55 | INFO | train_inner | epoch 062:    370 / 393 loss=3.971, nll_loss=3.738, ppl=13.35, wps=25267.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.897, loss_scale=8, train_wall=254, gb_free=12.3, wall=63701
2022-03-02 03:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:21:57 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.064 | nll_loss 6.862 | ppl 116.35 | wps 65960.7 | wpb 2034.1 | bsz 4 | num_updates 24323 | best_loss 6.25
2022-03-02 03:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24323 updates
2022-03-02 03:21:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:22:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:22:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 62 @ 24323 updates, score 7.064) (writing took 4.545965412980877 seconds)
2022-03-02 03:22:01 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-02 03:22:01 | INFO | train | epoch 062 | loss 3.911 | nll_loss 3.677 | ppl 12.79 | wps 25015 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 24323 | lr 0.000202764 | gnorm 0.878 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 63768
2022-03-02 03:22:01 | INFO | fairseq.trainer | begin training epoch 63
2022-03-02 03:22:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 03:25:21 | INFO | train_inner | epoch 063:     77 / 393 loss=3.86, nll_loss=3.625, ppl=12.34, wps=24517, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.872, loss_scale=8, train_wall=253, gb_free=12.3, wall=63968
2022-03-02 03:29:41 | INFO | train_inner | epoch 063:    177 / 393 loss=3.862, nll_loss=3.627, ppl=12.36, wps=25272, ups=0.39, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.886, loss_scale=8, train_wall=254, gb_free=12.3, wall=64227
2022-03-02 03:34:00 | INFO | train_inner | epoch 063:    277 / 393 loss=3.92, nll_loss=3.686, ppl=12.87, wps=25276.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.877, loss_scale=16, train_wall=254, gb_free=12.3, wall=64486
2022-03-02 03:38:19 | INFO | train_inner | epoch 063:    377 / 393 loss=3.962, nll_loss=3.729, ppl=13.26, wps=25279.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.885, loss_scale=16, train_wall=254, gb_free=12.3, wall=64745
2022-03-02 03:38:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:39:03 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.09 | nll_loss 6.89 | ppl 118.58 | wps 65751.4 | wpb 2034.1 | bsz 4 | num_updates 24716 | best_loss 6.25
2022-03-02 03:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24716 updates
2022-03-02 03:39:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:39:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 63 @ 24716 updates, score 7.09) (writing took 4.558333223918453 seconds)
2022-03-02 03:39:07 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-02 03:39:07 | INFO | train | epoch 063 | loss 3.899 | nll_loss 3.665 | ppl 12.68 | wps 25077.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 24716 | lr 0.000201146 | gnorm 0.882 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 64794
2022-03-02 03:39:07 | INFO | fairseq.trainer | begin training epoch 64
2022-03-02 03:39:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 03:42:45 | INFO | train_inner | epoch 064:     84 / 393 loss=3.837, nll_loss=3.601, ppl=12.13, wps=24509.4, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.866, loss_scale=16, train_wall=253, gb_free=12.3, wall=65012
2022-03-02 03:47:05 | INFO | train_inner | epoch 064:    184 / 393 loss=3.86, nll_loss=3.625, ppl=12.33, wps=25279, ups=0.39, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.887, loss_scale=16, train_wall=254, gb_free=12.3, wall=65271
2022-03-02 03:51:24 | INFO | train_inner | epoch 064:    284 / 393 loss=3.91, nll_loss=3.676, ppl=12.78, wps=25264.5, ups=0.39, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.893, loss_scale=16, train_wall=254, gb_free=12.3, wall=65530
2022-03-02 03:54:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 03:55:46 | INFO | train_inner | epoch 064:    385 / 393 loss=3.953, nll_loss=3.72, ppl=13.17, wps=25011.3, ups=0.38, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.885, loss_scale=16, train_wall=257, gb_free=12.3, wall=65792
2022-03-02 03:56:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:56:09 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.099 | nll_loss 6.896 | ppl 119.12 | wps 65564.8 | wpb 2034.1 | bsz 4 | num_updates 25108 | best_loss 6.25
2022-03-02 03:56:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25108 updates
2022-03-02 03:56:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:56:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 03:56:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 64 @ 25108 updates, score 7.099) (writing took 4.470264901057817 seconds)
2022-03-02 03:56:13 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-02 03:56:13 | INFO | train | epoch 064 | loss 3.887 | nll_loss 3.653 | ppl 12.58 | wps 25008.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 25108 | lr 0.000199569 | gnorm 0.883 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 65820
2022-03-02 03:56:13 | INFO | fairseq.trainer | begin training epoch 65
2022-03-02 03:56:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:00:12 | INFO | train_inner | epoch 065:     92 / 393 loss=3.81, nll_loss=3.574, ppl=11.91, wps=24508.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.888, loss_scale=16, train_wall=253, gb_free=12.3, wall=66058
2022-03-02 04:04:32 | INFO | train_inner | epoch 065:    192 / 393 loss=3.852, nll_loss=3.617, ppl=12.27, wps=25264.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.886, loss_scale=16, train_wall=254, gb_free=12.3, wall=66318
2022-03-02 04:04:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 04:08:53 | INFO | train_inner | epoch 065:    293 / 393 loss=3.903, nll_loss=3.668, ppl=12.71, wps=25024.2, ups=0.38, wpb=65530.9, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.89, loss_scale=8, train_wall=257, gb_free=12.3, wall=66580
2022-03-02 04:13:12 | INFO | train_inner | epoch 065:    393 / 393 loss=3.949, nll_loss=3.715, ppl=13.14, wps=25275.1, ups=0.39, wpb=65249.3, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.904, loss_scale=8, train_wall=253, gb_free=12.3, wall=66838
2022-03-02 04:13:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 04:13:15 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.109 | nll_loss 6.907 | ppl 120.02 | wps 65890.5 | wpb 2034.1 | bsz 4 | num_updates 25500 | best_loss 6.25
2022-03-02 04:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25500 updates
2022-03-02 04:13:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 04:13:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 04:13:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 65 @ 25500 updates, score 7.109) (writing took 4.602340926998295 seconds)
2022-03-02 04:13:20 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-02 04:13:20 | INFO | train | epoch 065 | loss 3.877 | nll_loss 3.642 | ppl 12.48 | wps 25007.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 25500 | lr 0.00019803 | gnorm 0.892 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 66846
2022-03-02 04:13:20 | INFO | fairseq.trainer | begin training epoch 66
2022-03-02 04:13:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:17:39 | INFO | train_inner | epoch 066:    100 / 393 loss=3.79, nll_loss=3.554, ppl=11.74, wps=24505.3, ups=0.37, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.886, loss_scale=8, train_wall=255, gb_free=12.3, wall=67105
2022-03-02 04:21:58 | INFO | train_inner | epoch 066:    200 / 393 loss=3.843, nll_loss=3.608, ppl=12.19, wps=25264.2, ups=0.39, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.903, loss_scale=8, train_wall=255, gb_free=12.3, wall=67365
2022-03-02 04:26:18 | INFO | train_inner | epoch 066:    300 / 393 loss=3.9, nll_loss=3.665, ppl=12.69, wps=25265.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.915, loss_scale=8, train_wall=255, gb_free=12.3, wall=67624
2022-03-02 04:30:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 04:30:21 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.118 | nll_loss 6.915 | ppl 120.72 | wps 65719.6 | wpb 2034.1 | bsz 4 | num_updates 25893 | best_loss 6.25
2022-03-02 04:30:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25893 updates
2022-03-02 04:30:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 04:30:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 04:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 66 @ 25893 updates, score 7.118) (writing took 4.601792649016716 seconds)
2022-03-02 04:30:26 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-02 04:30:26 | INFO | train | epoch 066 | loss 3.866 | nll_loss 3.631 | ppl 12.39 | wps 25065.8 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 25893 | lr 0.000196521 | gnorm 0.902 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 67872
2022-03-02 04:30:26 | INFO | fairseq.trainer | begin training epoch 67
2022-03-02 04:30:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:30:44 | INFO | train_inner | epoch 067:      7 / 393 loss=3.924, nll_loss=3.691, ppl=12.91, wps=24498.6, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.904, loss_scale=16, train_wall=253, gb_free=12.3, wall=67890
2022-03-02 04:35:03 | INFO | train_inner | epoch 067:    107 / 393 loss=3.788, nll_loss=3.551, ppl=11.72, wps=25273.8, ups=0.39, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.892, loss_scale=16, train_wall=254, gb_free=12.3, wall=68150
2022-03-02 04:39:23 | INFO | train_inner | epoch 067:    207 / 393 loss=3.834, nll_loss=3.598, ppl=12.11, wps=25266.2, ups=0.39, wpb=65530.2, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.904, loss_scale=16, train_wall=255, gb_free=12.3, wall=68409
2022-03-02 04:43:42 | INFO | train_inner | epoch 067:    307 / 393 loss=3.889, nll_loss=3.655, ppl=12.59, wps=25280, ups=0.39, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.913, loss_scale=16, train_wall=254, gb_free=12.3, wall=68668
2022-03-02 04:47:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 04:47:27 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.155 | nll_loss 6.953 | ppl 123.91 | wps 66029.7 | wpb 2034.1 | bsz 4 | num_updates 26286 | best_loss 6.25
2022-03-02 04:47:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26286 updates
2022-03-02 04:47:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 04:47:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 04:47:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 67 @ 26286 updates, score 7.155) (writing took 4.5858508800156415 seconds)
2022-03-02 04:47:32 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-02 04:47:32 | INFO | train | epoch 067 | loss 3.855 | nll_loss 3.62 | ppl 12.3 | wps 25078.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 26286 | lr 0.000195046 | gnorm 0.905 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 68898
2022-03-02 04:47:32 | INFO | fairseq.trainer | begin training epoch 68
2022-03-02 04:47:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:48:08 | INFO | train_inner | epoch 068:     14 / 393 loss=3.901, nll_loss=3.667, ppl=12.7, wps=24524.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.908, loss_scale=16, train_wall=253, gb_free=12.3, wall=68934
2022-03-02 04:49:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 04:52:30 | INFO | train_inner | epoch 068:    115 / 393 loss=3.782, nll_loss=3.545, ppl=11.68, wps=25028, ups=0.38, wpb=65530.2, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.896, loss_scale=16, train_wall=257, gb_free=12.3, wall=69196
2022-03-02 04:56:49 | INFO | train_inner | epoch 068:    215 / 393 loss=3.833, nll_loss=3.597, ppl=12.1, wps=25278.9, ups=0.39, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.915, loss_scale=16, train_wall=254, gb_free=12.3, wall=69455
2022-03-02 04:58:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 05:01:11 | INFO | train_inner | epoch 068:    316 / 393 loss=3.879, nll_loss=3.644, ppl=12.5, wps=25023.2, ups=0.38, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.916, loss_scale=8, train_wall=257, gb_free=12.3, wall=69717
2022-03-02 05:04:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:04:33 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.161 | nll_loss 6.961 | ppl 124.57 | wps 65821.5 | wpb 2034.1 | bsz 4 | num_updates 26677 | best_loss 6.25
2022-03-02 05:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26677 updates
2022-03-02 05:04:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:04:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:04:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 68 @ 26677 updates, score 7.161) (writing took 4.510071696946397 seconds)
2022-03-02 05:04:37 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-02 05:04:37 | INFO | train | epoch 068 | loss 3.845 | nll_loss 3.609 | ppl 12.2 | wps 24952.6 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 26677 | lr 0.000193612 | gnorm 0.905 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 69924
2022-03-02 05:04:38 | INFO | fairseq.trainer | begin training epoch 69
2022-03-02 05:04:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:05:37 | INFO | train_inner | epoch 069:     23 / 393 loss=3.876, nll_loss=3.641, ppl=12.47, wps=24520.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.9, loss_scale=8, train_wall=253, gb_free=12.3, wall=69983
2022-03-02 05:09:56 | INFO | train_inner | epoch 069:    123 / 393 loss=3.781, nll_loss=3.544, ppl=11.67, wps=25277, ups=0.39, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.908, loss_scale=8, train_wall=254, gb_free=12.3, wall=70243
2022-03-02 05:14:16 | INFO | train_inner | epoch 069:    223 / 393 loss=3.828, nll_loss=3.592, ppl=12.06, wps=25273.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.918, loss_scale=8, train_wall=254, gb_free=12.3, wall=70502
2022-03-02 05:18:35 | INFO | train_inner | epoch 069:    323 / 393 loss=3.863, nll_loss=3.628, ppl=12.36, wps=25283.1, ups=0.39, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.929, loss_scale=8, train_wall=254, gb_free=12.3, wall=70761
2022-03-02 05:21:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:21:39 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.165 | nll_loss 6.964 | ppl 124.82 | wps 65894.5 | wpb 2034.1 | bsz 4 | num_updates 27070 | best_loss 6.25
2022-03-02 05:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27070 updates
2022-03-02 05:21:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:21:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:21:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 69 @ 27070 updates, score 7.165) (writing took 4.470191325061023 seconds)
2022-03-02 05:21:43 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-02 05:21:43 | INFO | train | epoch 069 | loss 3.836 | nll_loss 3.6 | ppl 12.12 | wps 25086.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 27070 | lr 0.000192201 | gnorm 0.917 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 70949
2022-03-02 05:21:43 | INFO | fairseq.trainer | begin training epoch 70
2022-03-02 05:21:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:23:01 | INFO | train_inner | epoch 070:     30 / 393 loss=3.863, nll_loss=3.628, ppl=12.36, wps=24534, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.906, loss_scale=16, train_wall=253, gb_free=12.3, wall=71027
2022-03-02 05:27:20 | INFO | train_inner | epoch 070:    130 / 393 loss=3.766, nll_loss=3.529, ppl=11.54, wps=25261.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.905, loss_scale=16, train_wall=255, gb_free=12.3, wall=71287
2022-03-02 05:31:40 | INFO | train_inner | epoch 070:    230 / 393 loss=3.821, nll_loss=3.585, ppl=12, wps=25271.3, ups=0.39, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.917, loss_scale=16, train_wall=254, gb_free=12.3, wall=71546
2022-03-02 05:35:59 | INFO | train_inner | epoch 070:    330 / 393 loss=3.864, nll_loss=3.628, ppl=12.37, wps=25269.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.914, loss_scale=16, train_wall=254, gb_free=12.3, wall=71805
2022-03-02 05:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:38:45 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.194 | nll_loss 6.992 | ppl 127.28 | wps 65667.9 | wpb 2034.1 | bsz 4 | num_updates 27463 | best_loss 6.25
2022-03-02 05:38:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27463 updates
2022-03-02 05:38:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:38:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:38:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 70 @ 27463 updates, score 7.194) (writing took 4.556929897982627 seconds)
2022-03-02 05:38:49 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-02 05:38:49 | INFO | train | epoch 070 | loss 3.826 | nll_loss 3.589 | ppl 12.04 | wps 25072.4 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 27463 | lr 0.000190821 | gnorm 0.912 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 71975
2022-03-02 05:38:49 | INFO | fairseq.trainer | begin training epoch 71
2022-03-02 05:38:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:40:25 | INFO | train_inner | epoch 071:     37 / 393 loss=3.841, nll_loss=3.606, ppl=12.17, wps=24523.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.907, loss_scale=16, train_wall=253, gb_free=12.3, wall=72071
2022-03-02 05:43:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 05:44:47 | INFO | train_inner | epoch 071:    138 / 393 loss=3.761, nll_loss=3.524, ppl=11.5, wps=25019.6, ups=0.38, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.917, loss_scale=16, train_wall=257, gb_free=12.3, wall=72333
2022-03-02 05:49:06 | INFO | train_inner | epoch 071:    238 / 393 loss=3.822, nll_loss=3.586, ppl=12.01, wps=25270.6, ups=0.39, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.929, loss_scale=16, train_wall=254, gb_free=12.3, wall=72593
2022-03-02 05:53:26 | INFO | train_inner | epoch 071:    338 / 393 loss=3.847, nll_loss=3.611, ppl=12.22, wps=25247.3, ups=0.39, wpb=65530.2, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.914, loss_scale=16, train_wall=255, gb_free=12.3, wall=72852
2022-03-02 05:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:55:51 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.187 | nll_loss 6.985 | ppl 126.71 | wps 66118.5 | wpb 2034.1 | bsz 4 | num_updates 27855 | best_loss 6.25
2022-03-02 05:55:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27855 updates
2022-03-02 05:55:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 05:55:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 71 @ 27855 updates, score 7.187) (writing took 4.461207718937658 seconds)
2022-03-02 05:55:55 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-02 05:55:55 | INFO | train | epoch 071 | loss 3.816 | nll_loss 3.58 | ppl 11.95 | wps 25003.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 27855 | lr 0.000189473 | gnorm 0.918 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 73002
2022-03-02 05:55:55 | INFO | fairseq.trainer | begin training epoch 72
2022-03-02 05:55:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:55:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 05:57:55 | INFO | train_inner | epoch 072:     46 / 393 loss=3.817, nll_loss=3.581, ppl=11.96, wps=24279.1, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.93, loss_scale=8, train_wall=256, gb_free=12.3, wall=73121
2022-03-02 06:02:14 | INFO | train_inner | epoch 072:    146 / 393 loss=3.763, nll_loss=3.525, ppl=11.52, wps=25277.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.93, loss_scale=8, train_wall=254, gb_free=12.3, wall=73380
2022-03-02 06:06:33 | INFO | train_inner | epoch 072:    246 / 393 loss=3.814, nll_loss=3.577, ppl=11.94, wps=25269, ups=0.39, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.92, loss_scale=8, train_wall=254, gb_free=12.3, wall=73639
2022-03-02 06:10:53 | INFO | train_inner | epoch 072:    346 / 393 loss=3.846, nll_loss=3.611, ppl=12.21, wps=25275.2, ups=0.39, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.927, loss_scale=8, train_wall=254, gb_free=12.3, wall=73899
2022-03-02 06:12:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:12:57 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.204 | nll_loss 7.004 | ppl 128.31 | wps 65766.3 | wpb 2034.1 | bsz 4 | num_updates 28247 | best_loss 6.25
2022-03-02 06:12:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28247 updates
2022-03-02 06:12:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 06:13:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 06:13:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 72 @ 28247 updates, score 7.204) (writing took 4.650251257931814 seconds)
2022-03-02 06:13:01 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-02 06:13:01 | INFO | train | epoch 072 | loss 3.806 | nll_loss 3.57 | ppl 11.87 | wps 25014.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 28247 | lr 0.000188154 | gnorm 0.93 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 74027
2022-03-02 06:13:01 | INFO | fairseq.trainer | begin training epoch 73
2022-03-02 06:13:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 06:15:19 | INFO | train_inner | epoch 073:     53 / 393 loss=3.793, nll_loss=3.556, ppl=11.76, wps=24514, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.939, loss_scale=8, train_wall=253, gb_free=12.3, wall=74165
2022-03-02 06:19:38 | INFO | train_inner | epoch 073:    153 / 393 loss=3.763, nll_loss=3.525, ppl=11.51, wps=25281.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.931, loss_scale=16, train_wall=254, gb_free=12.3, wall=74424
2022-03-02 06:23:57 | INFO | train_inner | epoch 073:    253 / 393 loss=3.794, nll_loss=3.557, ppl=11.77, wps=25277.2, ups=0.39, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.934, loss_scale=16, train_wall=254, gb_free=12.3, wall=74683
2022-03-02 06:28:17 | INFO | train_inner | epoch 073:    353 / 393 loss=3.854, nll_loss=3.618, ppl=12.28, wps=25268.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.931, loss_scale=16, train_wall=254, gb_free=12.3, wall=74943
2022-03-02 06:29:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:30:02 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.218 | nll_loss 7.018 | ppl 129.62 | wps 65997.4 | wpb 2034.1 | bsz 4 | num_updates 28640 | best_loss 6.25
2022-03-02 06:30:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28640 updates
2022-03-02 06:30:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 06:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 06:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 73 @ 28640 updates, score 7.218) (writing took 4.538726162980311 seconds)
2022-03-02 06:30:07 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-02 06:30:07 | INFO | train | epoch 073 | loss 3.798 | nll_loss 3.561 | ppl 11.81 | wps 25080 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 28640 | lr 0.000186859 | gnorm 0.933 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 75053
2022-03-02 06:30:07 | INFO | fairseq.trainer | begin training epoch 74
2022-03-02 06:30:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 06:32:43 | INFO | train_inner | epoch 074:     60 / 393 loss=3.772, nll_loss=3.535, ppl=11.59, wps=24521.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.944, loss_scale=16, train_wall=253, gb_free=12.3, wall=75209
2022-03-02 06:37:02 | INFO | train_inner | epoch 074:    160 / 393 loss=3.742, nll_loss=3.504, ppl=11.35, wps=25273, ups=0.39, wpb=65535.4, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.918, loss_scale=16, train_wall=254, gb_free=12.3, wall=75468
2022-03-02 06:40:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 06:41:24 | INFO | train_inner | epoch 074:    261 / 393 loss=3.804, nll_loss=3.567, ppl=11.86, wps=25022.4, ups=0.38, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.92, loss_scale=16, train_wall=257, gb_free=12.3, wall=75730
2022-03-02 06:45:43 | INFO | train_inner | epoch 074:    361 / 393 loss=3.847, nll_loss=3.611, ppl=12.22, wps=25281.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.947, loss_scale=16, train_wall=254, gb_free=12.3, wall=75989
2022-03-02 06:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:47:08 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 7.233 | nll_loss 7.03 | ppl 130.69 | wps 66016 | wpb 2034.1 | bsz 4 | num_updates 29032 | best_loss 6.25
2022-03-02 06:47:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 29032 updates
2022-03-02 06:47:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 06:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 06:47:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 74 @ 29032 updates, score 7.233) (writing took 4.532632930087857 seconds)
2022-03-02 06:47:13 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-02 06:47:13 | INFO | train | epoch 074 | loss 3.789 | nll_loss 3.552 | ppl 11.73 | wps 25014.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 29032 | lr 0.000185593 | gnorm 0.931 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 76079
2022-03-02 06:47:13 | INFO | fairseq.trainer | begin training epoch 75
2022-03-02 06:47:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 06:50:09 | INFO | train_inner | epoch 075:     68 / 393 loss=3.757, nll_loss=3.519, ppl=11.47, wps=24515.9, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=29100, lr=0.000185376, gnorm=0.944, loss_scale=16, train_wall=253, gb_free=12.3, wall=76255
2022-03-02 06:54:28 | INFO | train_inner | epoch 075:    168 / 393 loss=3.745, nll_loss=3.507, ppl=11.37, wps=25273, ups=0.39, wpb=65530.9, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.916, loss_scale=16, train_wall=254, gb_free=12.3, wall=76515
2022-03-02 06:58:48 | INFO | train_inner | epoch 075:    268 / 393 loss=3.789, nll_loss=3.551, ppl=11.72, wps=25268.7, ups=0.39, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.923, loss_scale=16, train_wall=255, gb_free=12.3, wall=76774
2022-03-02 07:02:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 07:03:10 | INFO | train_inner | epoch 075:    369 / 393 loss=3.84, nll_loss=3.604, ppl=12.16, wps=25025.6, ups=0.38, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.957, loss_scale=16, train_wall=257, gb_free=12.3, wall=77036
2022-03-02 07:04:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 07:04:14 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 7.256 | nll_loss 7.055 | ppl 132.99 | wps 66010.2 | wpb 2034.1 | bsz 4 | num_updates 29424 | best_loss 6.25
2022-03-02 07:04:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 29424 updates
2022-03-02 07:04:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 07:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-02 07:04:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 75 @ 29424 updates, score 7.256) (writing took 4.4163966110209 seconds)
2022-03-02 07:04:19 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-02 07:04:19 | INFO | train | epoch 075 | loss 3.78 | nll_loss 3.543 | ppl 11.66 | wps 25017.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 29424 | lr 0.000184353 | gnorm 0.934 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 77105
2022-03-02 07:04:19 | INFO | fairseq.trainer | begin training epoch 76
2022-03-02 07:04:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 07:07:36 | INFO | train_inner | epoch 076:     76 / 393 loss=3.732, nll_loss=3.494, ppl=11.27, wps=24510, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=29500, lr=0.000184115, gnorm=0.918, loss_scale=16, train_wall=254, gb_free=12.3, wall=77302
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
