Sender: LSF System <lsfadmin@eu-g2-08>
Subject: Job 207132997: <w103_size_0.0625_fp16_cross_entropy_#2> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_cross_entropy_#2> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:35:22 2022
Job was executed on host(s) <eu-g2-08>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:35:42 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:35:42 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   97827.02 sec.
    Max Memory :                                 8538 MB
    Average Memory :                             2254.25 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11462.00 MB
    Max Swap :                                   2547 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   92980 sec.
    Turnaround time :                            93000 sec.

The output (if any) follows:

2022-03-04 09:35:55 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:35:56 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 09:35:58 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 09:35:58 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:35:58 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:35:58 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-04 09:35:58 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 09:35:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:35:58 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 09:36:07 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:36:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:36:07 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-03-04 09:36:07 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:36:07 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:36:07 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:36:07 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 09:36:07 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 09:36:07 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:36:07 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 09:36:07 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:36:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:36:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:36:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:36:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:36:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:36:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:41:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:41:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.542 | ppl 23859.4 | wps 37697.3 | wpb 510.9 | bsz 1 | num_updates 92
2022-03-04 09:41:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 92 updates
2022-03-04 09:41:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:41:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 1 @ 92 updates, score 14.542) (writing took 6.251618495211005 seconds)
2022-03-04 09:41:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:41:13 | INFO | train | epoch 001 | loss 16.234 | ppl 77089.3 | wps 21621.1 | ups 0.33 | wpb 65489.2 | bsz 127.9 | num_updates 92 | lr 1.15977e-05 | gnorm 3.582 | loss_scale 4 | train_wall 269 | gb_free 8.2 | wall 306
2022-03-04 09:41:13 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:41:36 | INFO | train_inner | epoch 002:      8 / 97 loss=16.104, ppl=70422, wps=21724.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.443, loss_scale=4, train_wall=290, gb_free=8.2, wall=329
2022-03-04 09:45:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:45:57 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.758 | ppl 6928.81 | wps 39880.9 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 12.758
2022-03-04 09:45:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 09:45:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:46:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:46:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 2 @ 189 updates, score 12.758) (writing took 6.454901281744242 seconds)
2022-03-04 09:46:04 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 09:46:04 | INFO | train | epoch 002 | loss 13.86 | ppl 14869.9 | wps 21864.1 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.646 | loss_scale 8 | train_wall 254 | gb_free 8.2 | wall 597
2022-03-04 09:46:04 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 09:46:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:46:35 | INFO | train_inner | epoch 003:     11 / 97 loss=13.681, ppl=13136.9, wps=21887.5, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.601, loss_scale=8, train_wall=261, gb_free=8.2, wall=628
2022-03-04 09:50:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:50:48 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.065 | ppl 2141.72 | wps 40171.9 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.065
2022-03-04 09:50:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 09:50:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:50:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.065) (writing took 6.666826609522104 seconds)
2022-03-04 09:50:54 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 09:50:54 | INFO | train | epoch 003 | loss 11.968 | ppl 4007.43 | wps 21846.7 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 1.093 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 888
2022-03-04 09:50:54 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 09:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:51:34 | INFO | train_inner | epoch 004:     14 / 97 loss=11.75, ppl=3444.63, wps=21875.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=1.026, loss_scale=16, train_wall=261, gb_free=8.2, wall=928
2022-03-04 09:55:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:55:38 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.233 | ppl 1203.62 | wps 37939.6 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 10.233
2022-03-04 09:55:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 09:55:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:55:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 09:56:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 4 @ 383 updates, score 10.233) (writing took 23.7743221540004 seconds)
2022-03-04 09:56:02 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 09:56:02 | INFO | train | epoch 004 | loss 10.63 | ppl 1584.76 | wps 20659.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.634 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 1195
2022-03-04 09:56:02 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 09:56:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:56:50 | INFO | train_inner | epoch 005:     17 / 97 loss=10.503, ppl=1451.51, wps=20731, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.58, loss_scale=32, train_wall=261, gb_free=8.2, wall=1244
2022-03-04 10:00:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:00:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.856 | ppl 926.56 | wps 38773.9 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 9.856
2022-03-04 10:00:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 10:00:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:00:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:01:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 5 @ 480 updates, score 9.856) (writing took 70.90960405953228 seconds)
2022-03-04 10:01:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:01:57 | INFO | train | epoch 005 | loss 10.045 | ppl 1056.24 | wps 17911.8 | ups 0.27 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.509 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 1550
2022-03-04 10:01:57 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:01:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:02:54 | INFO | train_inner | epoch 006:     20 / 97 loss=9.966, ppl=999.88, wps=18026.5, ups=0.28, wpb=65495, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.522, loss_scale=32, train_wall=261, gb_free=8.2, wall=1607
2022-03-04 10:03:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:06:40 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.573 | ppl 761.56 | wps 40342.4 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 9.573
2022-03-04 10:06:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 10:06:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:06:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 6 @ 576 updates, score 9.573) (writing took 6.990995392203331 seconds)
2022-03-04 10:06:47 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 10:06:47 | INFO | train | epoch 006 | loss 9.695 | ppl 828.64 | wps 21609.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.564 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 1841
2022-03-04 10:06:48 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 10:06:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:07:56 | INFO | train_inner | epoch 007:     24 / 97 loss=9.624, ppl=789.01, wps=21643.6, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.574, loss_scale=32, train_wall=264, gb_free=8.2, wall=1910
2022-03-04 10:10:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:11:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:11:32 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.319 | ppl 638.65 | wps 39652.9 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 9.319
2022-03-04 10:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 10:11:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:11:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 7 @ 672 updates, score 9.319) (writing took 6.985155913978815 seconds)
2022-03-04 10:11:39 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 10:11:39 | INFO | train | epoch 007 | loss 9.4 | ppl 675.46 | wps 21581.8 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.619 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 2132
2022-03-04 10:11:39 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 10:11:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:12:59 | INFO | train_inner | epoch 008:     28 / 97 loss=9.324, ppl=640.96, wps=21647.5, ups=0.33, wpb=65495, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.654, loss_scale=32, train_wall=264, gb_free=8.2, wall=2212
2022-03-04 10:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:16:23 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.099 | ppl 548.51 | wps 38632.1 | wpb 510.9 | bsz 1 | num_updates 769 | best_loss 9.099
2022-03-04 10:16:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 769 updates
2022-03-04 10:16:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:16:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:16:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 8 @ 769 updates, score 9.099) (writing took 7.198349215090275 seconds)
2022-03-04 10:16:30 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 10:16:30 | INFO | train | epoch 008 | loss 9.126 | ppl 558.64 | wps 21817 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 769 | lr 9.62058e-05 | gnorm 0.73 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 2423
2022-03-04 10:16:30 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 10:16:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:16:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:18:01 | INFO | train_inner | epoch 009:     32 / 97 loss=9.045, ppl=528.15, wps=21671.5, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.765, loss_scale=32, train_wall=263, gb_free=8.2, wall=2514
2022-03-04 10:21:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:21:14 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.906 | ppl 479.75 | wps 40720 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 8.906
2022-03-04 10:21:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 10:21:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:26:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 9 @ 865 updates, score 8.906) (writing took 334.599053312093 seconds)
2022-03-04 10:26:49 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 10:26:49 | INFO | train | epoch 009 | loss 8.872 | ppl 468.61 | wps 10160.6 | ups 0.16 | wpb 65491.1 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.822 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 3042
2022-03-04 10:26:49 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 10:26:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:28:28 | INFO | train_inner | epoch 010:     35 / 97 loss=8.79, ppl=442.49, wps=10441.2, ups=0.16, wpb=65490.8, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.837, loss_scale=32, train_wall=261, gb_free=8.2, wall=3142
2022-03-04 10:29:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:31:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:31:32 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.719 | ppl 421.3 | wps 38932 | wpb 510.9 | bsz 1 | num_updates 961 | best_loss 8.719
2022-03-04 10:31:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 961 updates
2022-03-04 10:31:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:31:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:31:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 10 @ 961 updates, score 8.719) (writing took 6.989107474684715 seconds)
2022-03-04 10:31:39 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 10:31:39 | INFO | train | epoch 010 | loss 8.64 | ppl 398.89 | wps 21630.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 961 | lr 0.000120201 | gnorm 0.865 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 3333
2022-03-04 10:31:39 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 10:31:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:33:30 | INFO | train_inner | epoch 011:     39 / 97 loss=8.556, ppl=376.34, wps=21679.9, ups=0.33, wpb=65495, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.874, loss_scale=32, train_wall=263, gb_free=8.2, wall=3444
2022-03-04 10:35:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:36:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:36:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:36:22 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.572 | ppl 380.58 | wps 38530.2 | wpb 510.9 | bsz 1 | num_updates 1056 | best_loss 8.572
2022-03-04 10:36:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1056 updates
2022-03-04 10:36:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:36:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:36:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 11 @ 1056 updates, score 8.572) (writing took 6.621718633919954 seconds)
2022-03-04 10:36:29 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 10:36:29 | INFO | train | epoch 011 | loss 8.43 | ppl 344.78 | wps 21509 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 1056 | lr 0.000132074 | gnorm 0.944 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 3622
2022-03-04 10:36:29 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 10:36:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:38:33 | INFO | train_inner | epoch 012:     44 / 97 loss=8.34, ppl=323.95, wps=21651.8, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.936, loss_scale=16, train_wall=264, gb_free=8.2, wall=3746
2022-03-04 10:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:41:11 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.434 | ppl 345.85 | wps 39312.7 | wpb 510.9 | bsz 1 | num_updates 1153 | best_loss 8.434
2022-03-04 10:41:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1153 updates
2022-03-04 10:41:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:41:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:41:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 12 @ 1153 updates, score 8.434) (writing took 6.325535768643022 seconds)
2022-03-04 10:41:17 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 10:41:17 | INFO | train | epoch 012 | loss 8.233 | ppl 300.82 | wps 22035.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 1153 | lr 0.000144196 | gnorm 0.893 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 3910
2022-03-04 10:41:17 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 10:41:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:43:34 | INFO | train_inner | epoch 013:     47 / 97 loss=8.149, ppl=283.79, wps=21728.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.9, loss_scale=32, train_wall=264, gb_free=8.2, wall=4048
2022-03-04 10:46:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:46:09 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.297 | ppl 314.44 | wps 36545.3 | wpb 510.9 | bsz 1 | num_updates 1250 | best_loss 8.297
2022-03-04 10:46:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1250 updates
2022-03-04 10:46:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 13 @ 1250 updates, score 8.297) (writing took 6.080685645341873 seconds)
2022-03-04 10:46:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 10:46:15 | INFO | train | epoch 013 | loss 8.05 | ppl 265 | wps 21343 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 1250 | lr 0.000156319 | gnorm 0.947 | loss_scale 32 | train_wall 260 | gb_free 8.2 | wall 4208
2022-03-04 10:46:15 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 10:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:48:42 | INFO | train_inner | epoch 014:     50 / 97 loss=7.954, ppl=247.96, wps=21268.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.975, loss_scale=32, train_wall=269, gb_free=8.2, wall=4355
2022-03-04 10:49:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:51:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:51:07 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.195 | ppl 293.08 | wps 36892 | wpb 510.9 | bsz 1 | num_updates 1346 | best_loss 8.195
2022-03-04 10:51:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1346 updates
2022-03-04 10:51:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:51:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:51:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 14 @ 1346 updates, score 8.195) (writing took 5.890014240518212 seconds)
2022-03-04 10:51:13 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 10:51:13 | INFO | train | epoch 014 | loss 7.876 | ppl 234.95 | wps 21047.5 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 1346 | lr 0.000168316 | gnorm 0.976 | loss_scale 32 | train_wall 261 | gb_free 8.2 | wall 4507
2022-03-04 10:51:13 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 10:51:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:53:53 | INFO | train_inner | epoch 015:     54 / 97 loss=7.792, ppl=221.56, wps=21062.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=0.988, loss_scale=32, train_wall=272, gb_free=8.2, wall=4666
2022-03-04 10:55:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:56:06 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.089 | ppl 272.36 | wps 38053.3 | wpb 510.9 | bsz 1 | num_updates 1442 | best_loss 8.089
2022-03-04 10:56:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1442 updates
2022-03-04 10:56:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:56:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 10:56:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 15 @ 1442 updates, score 8.089) (writing took 5.735757792368531 seconds)
2022-03-04 10:56:12 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 10:56:12 | INFO | train | epoch 015 | loss 7.709 | ppl 209.23 | wps 21047.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 1442 | lr 0.000180314 | gnorm 0.977 | loss_scale 32 | train_wall 261 | gb_free 8.2 | wall 4805
2022-03-04 10:56:12 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 10:56:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:59:00 | INFO | train_inner | epoch 016:     58 / 97 loss=7.609, ppl=195.18, wps=21364.7, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=0.97, loss_scale=32, train_wall=268, gb_free=8.2, wall=4973
2022-03-04 11:00:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:00:58 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.993 | ppl 254.73 | wps 39616.8 | wpb 510.9 | bsz 1 | num_updates 1539 | best_loss 7.993
2022-03-04 11:00:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1539 updates
2022-03-04 11:00:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:01:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 16 @ 1539 updates, score 7.993) (writing took 14.24669536948204 seconds)
2022-03-04 11:01:12 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 11:01:12 | INFO | train | epoch 016 | loss 7.543 | ppl 186.52 | wps 21196.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 1539 | lr 0.000192437 | gnorm 0.968 | loss_scale 32 | train_wall 255 | gb_free 8.2 | wall 5105
2022-03-04 11:01:12 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 11:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:02:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:04:10 | INFO | train_inner | epoch 017:     62 / 97 loss=7.439, ppl=173.57, wps=21092.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=1.01, loss_scale=32, train_wall=264, gb_free=8.2, wall=5283
2022-03-04 11:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:05:56 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.93 | ppl 243.89 | wps 40006.1 | wpb 510.9 | bsz 1 | num_updates 1635 | best_loss 7.93
2022-03-04 11:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1635 updates
2022-03-04 11:05:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:06:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 17 @ 1635 updates, score 7.93) (writing took 7.22423442453146 seconds)
2022-03-04 11:06:03 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 11:06:03 | INFO | train | epoch 017 | loss 7.382 | ppl 166.81 | wps 21581.8 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 1635 | lr 0.000204434 | gnorm 1.019 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 5396
2022-03-04 11:06:03 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 11:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:09:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:09:13 | INFO | train_inner | epoch 018:     66 / 97 loss=7.278, ppl=155.17, wps=21640.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=0.991, loss_scale=32, train_wall=264, gb_free=8.2, wall=5586
2022-03-04 11:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:10:48 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.839 | ppl 228.93 | wps 38858.9 | wpb 510.9 | bsz 1 | num_updates 1731 | best_loss 7.839
2022-03-04 11:10:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1731 updates
2022-03-04 11:10:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:11:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 18 @ 1731 updates, score 7.839) (writing took 17.973693130537868 seconds)
2022-03-04 11:11:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 11:11:06 | INFO | train | epoch 018 | loss 7.223 | ppl 149.42 | wps 20754.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 1731 | lr 0.000216432 | gnorm 0.979 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 5699
2022-03-04 11:11:06 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 11:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:14:24 | INFO | train_inner | epoch 019:     69 / 97 loss=7.116, ppl=138.75, wps=21062.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.956, loss_scale=32, train_wall=261, gb_free=8.2, wall=5897
2022-03-04 11:15:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:15:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:15:51 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.79 | ppl 221.32 | wps 39133.9 | wpb 510.9 | bsz 1 | num_updates 1827 | best_loss 7.79
2022-03-04 11:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1827 updates
2022-03-04 11:15:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:15:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:15:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 19 @ 1827 updates, score 7.79) (writing took 6.985848953947425 seconds)
2022-03-04 11:15:57 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 11:15:57 | INFO | train | epoch 019 | loss 7.068 | ppl 134.21 | wps 21568.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 1827 | lr 0.000228429 | gnorm 0.996 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 5991
2022-03-04 11:15:58 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 11:15:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:19:26 | INFO | train_inner | epoch 020:     73 / 97 loss=6.956, ppl=124.16, wps=21640.7, ups=0.33, wpb=65495, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=0.996, loss_scale=16, train_wall=264, gb_free=8.2, wall=6200
2022-03-04 11:20:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:20:41 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.753 | ppl 215.72 | wps 39529.2 | wpb 510.9 | bsz 1 | num_updates 1924 | best_loss 7.753
2022-03-04 11:20:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1924 updates
2022-03-04 11:20:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:20:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 20 @ 1924 updates, score 7.753) (writing took 5.628352154046297 seconds)
2022-03-04 11:20:47 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 11:20:47 | INFO | train | epoch 020 | loss 6.915 | ppl 120.65 | wps 21935.3 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 1924 | lr 0.000240552 | gnorm 0.958 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 6280
2022-03-04 11:20:47 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 11:20:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:24:25 | INFO | train_inner | epoch 021:     76 / 97 loss=6.799, ppl=111.37, wps=21908.8, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.96, loss_scale=32, train_wall=262, gb_free=8.2, wall=6499
2022-03-04 11:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:25:32 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.716 | ppl 210.19 | wps 39867.1 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 7.716
2022-03-04 11:25:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 11:25:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 21 @ 2021 updates, score 7.716) (writing took 6.033454608172178 seconds)
2022-03-04 11:25:38 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 11:25:38 | INFO | train | epoch 021 | loss 6.765 | ppl 108.78 | wps 21861.2 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 0.98 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 6571
2022-03-04 11:25:38 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 11:25:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:28:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:29:27 | INFO | train_inner | epoch 022:     80 / 97 loss=6.655, ppl=100.74, wps=21713.4, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=1.013, loss_scale=32, train_wall=264, gb_free=8.2, wall=6800
2022-03-04 11:30:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:30:22 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.674 | ppl 204.16 | wps 39727.9 | wpb 510.9 | bsz 1 | num_updates 2117 | best_loss 7.674
2022-03-04 11:30:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2117 updates
2022-03-04 11:30:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:30:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt
2022-03-04 11:30:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_best.pt (epoch 22 @ 2117 updates, score 7.674) (writing took 5.919523824006319 seconds)
2022-03-04 11:30:28 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 11:30:28 | INFO | train | epoch 022 | loss 6.622 | ppl 98.48 | wps 21689.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2117 | lr 0.000264672 | gnorm 1.008 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 6861
2022-03-04 11:30:28 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 11:30:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:34:27 | INFO | train_inner | epoch 023:     83 / 97 loss=6.502, ppl=90.66, wps=21829.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=0.963, loss_scale=32, train_wall=263, gb_free=8.2, wall=7100
2022-03-04 11:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:35:14 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.69 | ppl 206.54 | wps 38236.2 | wpb 510.9 | bsz 1 | num_updates 2214 | best_loss 7.674
2022-03-04 11:35:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2214 updates
2022-03-04 11:35:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:35:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 23 @ 2214 updates, score 7.69) (writing took 3.3492546044290066 seconds)
2022-03-04 11:35:17 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 11:35:17 | INFO | train | epoch 023 | loss 6.48 | ppl 89.27 | wps 21959.6 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 2214 | lr 0.000276795 | gnorm 0.961 | loss_scale 64 | train_wall 255 | gb_free 8.2 | wall 7150
2022-03-04 11:35:17 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 11:35:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:35:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:39:30 | INFO | train_inner | epoch 024:     87 / 97 loss=6.365, ppl=82.41, wps=21625.9, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.978, loss_scale=32, train_wall=268, gb_free=8.2, wall=7403
2022-03-04 11:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:40:05 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.703 | ppl 208.35 | wps 37181.6 | wpb 510.9 | bsz 1 | num_updates 2310 | best_loss 7.674
2022-03-04 11:40:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2310 updates
2022-03-04 11:40:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:40:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:40:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 24 @ 2310 updates, score 7.703) (writing took 3.4617003817111254 seconds)
2022-03-04 11:40:09 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 11:40:09 | INFO | train | epoch 024 | loss 6.343 | ppl 81.16 | wps 21558.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2310 | lr 0.000288792 | gnorm 0.976 | loss_scale 32 | train_wall 257 | gb_free 8.2 | wall 7442
2022-03-04 11:40:09 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 11:40:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:42:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:44:34 | INFO | train_inner | epoch 025:     91 / 97 loss=6.22, ppl=74.55, wps=21509.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=1.007, loss_scale=32, train_wall=269, gb_free=8.2, wall=7708
2022-03-04 11:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:44:58 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.741 | ppl 213.95 | wps 37133.6 | wpb 510.9 | bsz 1 | num_updates 2406 | best_loss 7.674
2022-03-04 11:44:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2406 updates
2022-03-04 11:44:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:45:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:45:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 25 @ 2406 updates, score 7.741) (writing took 3.35705578699708 seconds)
2022-03-04 11:45:01 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 11:45:01 | INFO | train | epoch 025 | loss 6.213 | ppl 74.2 | wps 21479.8 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2406 | lr 0.00030079 | gnorm 1.006 | loss_scale 32 | train_wall 258 | gb_free 8.2 | wall 7734
2022-03-04 11:45:01 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 11:45:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:48:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:49:39 | INFO | train_inner | epoch 026:     95 / 97 loss=6.09, ppl=68.1, wps=21509.5, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=0.969, loss_scale=32, train_wall=269, gb_free=8.2, wall=8012
2022-03-04 11:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:49:51 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.759 | ppl 216.59 | wps 36401.2 | wpb 510.9 | bsz 1 | num_updates 2502 | best_loss 7.674
2022-03-04 11:49:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2502 updates
2022-03-04 11:49:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:49:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:49:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 26 @ 2502 updates, score 7.759) (writing took 3.4606828931719065 seconds)
2022-03-04 11:49:54 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 11:49:54 | INFO | train | epoch 026 | loss 6.081 | ppl 67.69 | wps 21466.6 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2502 | lr 0.000312787 | gnorm 0.974 | loss_scale 32 | train_wall 258 | gb_free 8.2 | wall 8027
2022-03-04 11:49:54 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 11:49:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:54:44 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.875 | ppl 234.76 | wps 37116.4 | wpb 510.9 | bsz 1 | num_updates 2599 | best_loss 7.674
2022-03-04 11:54:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2599 updates
2022-03-04 11:54:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:54:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:54:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 27 @ 2599 updates, score 7.875) (writing took 3.490556988865137 seconds)
2022-03-04 11:54:47 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 11:54:47 | INFO | train | epoch 027 | loss 5.955 | ppl 62.02 | wps 21670.4 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 2599 | lr 0.00032491 | gnorm 0.989 | loss_scale 32 | train_wall 258 | gb_free 8.2 | wall 8320
2022-03-04 11:54:47 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 11:54:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:54:50 | INFO | train_inner | epoch 028:      1 / 97 loss=5.957, ppl=62.14, wps=21017.1, ups=0.32, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=0.991, loss_scale=32, train_wall=266, gb_free=8.2, wall=8324
2022-03-04 11:55:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:59:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:59:36 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.916 | ppl 241.58 | wps 37063.9 | wpb 510.9 | bsz 1 | num_updates 2695 | best_loss 7.674
2022-03-04 11:59:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2695 updates
2022-03-04 11:59:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:59:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 11:59:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 28 @ 2695 updates, score 7.916) (writing took 3.5303934160619974 seconds)
2022-03-04 11:59:39 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 11:59:39 | INFO | train | epoch 028 | loss 5.834 | ppl 57.02 | wps 21526.6 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2695 | lr 0.000336908 | gnorm 1.019 | loss_scale 32 | train_wall 258 | gb_free 8.2 | wall 8613
2022-03-04 11:59:39 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 11:59:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:59:54 | INFO | train_inner | epoch 029:      5 / 97 loss=5.825, ppl=56.69, wps=21550.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=1.013, loss_scale=32, train_wall=268, gb_free=8.2, wall=8627
2022-03-04 12:02:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:04:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:04:29 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.068 | ppl 268.29 | wps 36800 | wpb 510.9 | bsz 1 | num_updates 2791 | best_loss 7.674
2022-03-04 12:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2791 updates
2022-03-04 12:04:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:04:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:04:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 29 @ 2791 updates, score 8.068) (writing took 3.65103841945529 seconds)
2022-03-04 12:04:32 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 12:04:32 | INFO | train | epoch 029 | loss 5.712 | ppl 52.43 | wps 21452.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 2791 | lr 0.000348905 | gnorm 1.04 | loss_scale 32 | train_wall 258 | gb_free 8.2 | wall 8906
2022-03-04 12:04:32 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 12:04:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:04:59 | INFO | train_inner | epoch 030:      9 / 97 loss=5.701, ppl=52.01, wps=21489.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=1.038, loss_scale=32, train_wall=269, gb_free=8.2, wall=8932
2022-03-04 12:09:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:09:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:09:22 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.12 | ppl 278.29 | wps 37085.3 | wpb 510.9 | bsz 1 | num_updates 2886 | best_loss 7.674
2022-03-04 12:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2886 updates
2022-03-04 12:09:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:09:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 30 @ 2886 updates, score 8.12) (writing took 3.497733909636736 seconds)
2022-03-04 12:09:25 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 12:09:25 | INFO | train | epoch 030 | loss 5.591 | ppl 48.2 | wps 21244.6 | ups 0.32 | wpb 65490.6 | bsz 127.9 | num_updates 2886 | lr 0.000360778 | gnorm 1.019 | loss_scale 16 | train_wall 258 | gb_free 8.2 | wall 9198
2022-03-04 12:09:25 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 12:09:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:10:07 | INFO | train_inner | epoch 031:     14 / 97 loss=5.571, ppl=47.52, wps=21293.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=1.028, loss_scale=16, train_wall=272, gb_free=8.2, wall=9240
2022-03-04 12:14:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:14:14 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.126 | ppl 279.41 | wps 36804.3 | wpb 510.9 | bsz 1 | num_updates 2983 | best_loss 7.674
2022-03-04 12:14:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2983 updates
2022-03-04 12:14:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 31 @ 2983 updates, score 8.126) (writing took 3.5459913667291403 seconds)
2022-03-04 12:14:18 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 12:14:18 | INFO | train | epoch 031 | loss 5.476 | ppl 44.52 | wps 21717 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 2983 | lr 0.0003729 | gnorm 1.018 | loss_scale 16 | train_wall 258 | gb_free 8.2 | wall 9491
2022-03-04 12:14:18 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 12:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:15:08 | INFO | train_inner | epoch 032:     17 / 97 loss=5.457, ppl=43.94, wps=21741.7, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=1.029, loss_scale=16, train_wall=266, gb_free=8.2, wall=9541
2022-03-04 12:18:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:19:03 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.249 | ppl 304.26 | wps 38988.5 | wpb 510.9 | bsz 1 | num_updates 3080 | best_loss 7.674
2022-03-04 12:19:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3080 updates
2022-03-04 12:19:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:19:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:19:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 32 @ 3080 updates, score 8.249) (writing took 3.3842608369886875 seconds)
2022-03-04 12:19:06 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 12:19:06 | INFO | train | epoch 032 | loss 5.36 | ppl 41.07 | wps 22002.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 3080 | lr 0.000385023 | gnorm 1.064 | loss_scale 32 | train_wall 255 | gb_free 8.2 | wall 9780
2022-03-04 12:19:06 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 12:19:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:20:05 | INFO | train_inner | epoch 033:     20 / 97 loss=5.336, ppl=40.4, wps=22030.2, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=1.059, loss_scale=32, train_wall=263, gb_free=8.2, wall=9838
2022-03-04 12:20:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:23:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:23:53 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.346 | ppl 325.44 | wps 38623.1 | wpb 510.9 | bsz 1 | num_updates 3176 | best_loss 7.674
2022-03-04 12:23:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3176 updates
2022-03-04 12:23:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:23:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 33 @ 3176 updates, score 8.346) (writing took 3.5952640287578106 seconds)
2022-03-04 12:23:57 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 12:23:57 | INFO | train | epoch 033 | loss 5.248 | ppl 38 | wps 21661.4 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3176 | lr 0.000397021 | gnorm 1.051 | loss_scale 16 | train_wall 256 | gb_free 8.2 | wall 10070
2022-03-04 12:23:57 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 12:23:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:25:08 | INFO | train_inner | epoch 034:     24 / 97 loss=5.218, ppl=37.22, wps=21652.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=1.063, loss_scale=16, train_wall=267, gb_free=8.2, wall=10141
2022-03-04 12:26:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:28:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:28:46 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.608 | ppl 390.15 | wps 37007.2 | wpb 510.9 | bsz 1 | num_updates 3272 | best_loss 7.674
2022-03-04 12:28:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3272 updates
2022-03-04 12:28:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:28:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 34 @ 3272 updates, score 8.608) (writing took 3.288527263328433 seconds)
2022-03-04 12:28:50 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 12:28:50 | INFO | train | epoch 034 | loss 5.136 | ppl 35.15 | wps 21454.4 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3272 | lr 0.000409018 | gnorm 1.076 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 10363
2022-03-04 12:28:50 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 12:28:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:30:12 | INFO | train_inner | epoch 035:     28 / 97 loss=5.096, ppl=34.21, wps=21487, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=1.065, loss_scale=16, train_wall=269, gb_free=8.2, wall=10446
2022-03-04 12:33:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:33:38 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.537 | ppl 371.46 | wps 38389.3 | wpb 510.9 | bsz 1 | num_updates 3369 | best_loss 7.674
2022-03-04 12:33:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3369 updates
2022-03-04 12:33:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:33:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:33:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 35 @ 3369 updates, score 8.537) (writing took 3.255027277395129 seconds)
2022-03-04 12:33:42 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 12:33:42 | INFO | train | epoch 035 | loss 5.024 | ppl 32.54 | wps 21767.2 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 3369 | lr 0.000421141 | gnorm 1.09 | loss_scale 32 | train_wall 258 | gb_free 8.2 | wall 10655
2022-03-04 12:33:42 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 12:33:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:35:12 | INFO | train_inner | epoch 036:     31 / 97 loss=4.995, ppl=31.88, wps=21885.5, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=1.102, loss_scale=32, train_wall=264, gb_free=8.2, wall=10745
2022-03-04 12:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:38:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:38:27 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.506 | ppl 363.5 | wps 36927.7 | wpb 510.9 | bsz 1 | num_updates 3465 | best_loss 7.674
2022-03-04 12:38:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3465 updates
2022-03-04 12:38:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:38:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:38:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 36 @ 3465 updates, score 8.506) (writing took 4.553576301783323 seconds)
2022-03-04 12:38:31 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 12:38:31 | INFO | train | epoch 036 | loss 4.911 | ppl 30.08 | wps 21704.6 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3465 | lr 0.000433138 | gnorm 1.077 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 10945
2022-03-04 12:38:31 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 12:38:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:40:13 | INFO | train_inner | epoch 037:     35 / 97 loss=4.873, ppl=29.3, wps=21751.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=1.092, loss_scale=16, train_wall=264, gb_free=8.2, wall=11046
2022-03-04 12:43:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:43:14 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.648 | ppl 401.11 | wps 38668.2 | wpb 510.9 | bsz 1 | num_updates 3562 | best_loss 7.674
2022-03-04 12:43:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3562 updates
2022-03-04 12:43:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:43:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:43:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 37 @ 3562 updates, score 8.648) (writing took 4.437871487811208 seconds)
2022-03-04 12:43:19 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 12:43:19 | INFO | train | epoch 037 | loss 4.807 | ppl 27.98 | wps 22093.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 3562 | lr 0.000445261 | gnorm 1.119 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 11232
2022-03-04 12:43:19 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 12:43:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:45:09 | INFO | train_inner | epoch 038:     38 / 97 loss=4.76, ppl=27.09, wps=22071.4, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=1.113, loss_scale=32, train_wall=261, gb_free=8.2, wall=11343
2022-03-04 12:47:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:47:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:48:03 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.815 | ppl 450.27 | wps 39641.8 | wpb 510.9 | bsz 1 | num_updates 3658 | best_loss 7.674
2022-03-04 12:48:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3658 updates
2022-03-04 12:48:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:48:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:48:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 38 @ 3658 updates, score 8.815) (writing took 4.232916057109833 seconds)
2022-03-04 12:48:07 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 12:48:07 | INFO | train | epoch 038 | loss 4.703 | ppl 26.04 | wps 21818.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3658 | lr 0.000457259 | gnorm 1.155 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 11520
2022-03-04 12:48:07 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 12:48:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:50:09 | INFO | train_inner | epoch 039:     42 / 97 loss=4.666, ppl=25.39, wps=21883.9, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.168, loss_scale=16, train_wall=263, gb_free=8.2, wall=11642
2022-03-04 12:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:52:53 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.788 | ppl 441.91 | wps 39691.4 | wpb 510.9 | bsz 1 | num_updates 3755 | best_loss 7.674
2022-03-04 12:52:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3755 updates
2022-03-04 12:52:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:52:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:52:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 39 @ 3755 updates, score 8.788) (writing took 4.444421151652932 seconds)
2022-03-04 12:52:57 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 12:52:57 | INFO | train | epoch 039 | loss 4.595 | ppl 24.17 | wps 21896.9 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 3755 | lr 0.000469381 | gnorm 1.105 | loss_scale 16 | train_wall 255 | gb_free 8.2 | wall 11810
2022-03-04 12:52:57 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 12:52:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:54:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:55:11 | INFO | train_inner | epoch 040:     46 / 97 loss=4.537, ppl=23.22, wps=21698.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.076, loss_scale=16, train_wall=266, gb_free=8.2, wall=11944
2022-03-04 12:57:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:57:42 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.88 | ppl 471.18 | wps 39955.5 | wpb 510.9 | bsz 1 | num_updates 3851 | best_loss 7.674
2022-03-04 12:57:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3851 updates
2022-03-04 12:57:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:57:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 12:57:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 40 @ 3851 updates, score 8.88) (writing took 4.149572776630521 seconds)
2022-03-04 12:57:46 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 12:57:46 | INFO | train | epoch 040 | loss 4.495 | ppl 22.55 | wps 21769.4 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3851 | lr 0.000481379 | gnorm 1.141 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 12099
2022-03-04 12:57:46 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 12:57:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:00:08 | INFO | train_inner | epoch 041:     49 / 97 loss=4.455, ppl=21.94, wps=22041.6, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.209, loss_scale=16, train_wall=262, gb_free=8.2, wall=12241
2022-03-04 13:01:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:02:30 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.891 | ppl 474.6 | wps 39364.4 | wpb 510.9 | bsz 1 | num_updates 3947 | best_loss 7.674
2022-03-04 13:02:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3947 updates
2022-03-04 13:02:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 41 @ 3947 updates, score 8.891) (writing took 4.373146990314126 seconds)
2022-03-04 13:02:34 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 13:02:34 | INFO | train | epoch 041 | loss 4.397 | ppl 21.07 | wps 21800.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 3947 | lr 0.000493376 | gnorm 1.178 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 12388
2022-03-04 13:02:34 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 13:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:05:08 | INFO | train_inner | epoch 042:     53 / 97 loss=4.337, ppl=20.21, wps=21841.2, ups=0.33, wpb=65495, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.122, loss_scale=16, train_wall=264, gb_free=8.2, wall=12541
2022-03-04 13:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:07:18 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.115 | ppl 554.6 | wps 39855.4 | wpb 510.9 | bsz 1 | num_updates 4044 | best_loss 7.674
2022-03-04 13:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4044 updates
2022-03-04 13:07:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:07:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 42 @ 4044 updates, score 9.115) (writing took 4.420129258185625 seconds)
2022-03-04 13:07:23 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 13:07:23 | INFO | train | epoch 042 | loss 4.289 | ppl 19.55 | wps 22038.1 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4044 | lr 0.000497272 | gnorm 1.137 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 12676
2022-03-04 13:07:23 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 13:07:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:10:04 | INFO | train_inner | epoch 043:     56 / 97 loss=4.229, ppl=18.76, wps=22077.2, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.163, loss_scale=32, train_wall=261, gb_free=8.2, wall=12837
2022-03-04 13:10:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:12:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:12:06 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.953 | ppl 495.75 | wps 39849.1 | wpb 510.9 | bsz 1 | num_updates 4140 | best_loss 7.674
2022-03-04 13:12:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4140 updates
2022-03-04 13:12:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:12:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:12:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 43 @ 4140 updates, score 8.953) (writing took 4.3409755025058985 seconds)
2022-03-04 13:12:11 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 13:12:11 | INFO | train | epoch 043 | loss 4.184 | ppl 18.17 | wps 21829.4 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 4140 | lr 0.000491473 | gnorm 1.162 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 12964
2022-03-04 13:12:11 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 13:12:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:15:03 | INFO | train_inner | epoch 044:     60 / 97 loss=4.117, ppl=17.35, wps=21892.3, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.121, loss_scale=16, train_wall=263, gb_free=8.2, wall=13137
2022-03-04 13:16:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:16:55 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.344 | ppl 649.79 | wps 39310.6 | wpb 510.9 | bsz 1 | num_updates 4237 | best_loss 7.674
2022-03-04 13:16:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4237 updates
2022-03-04 13:16:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:16:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:16:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 44 @ 4237 updates, score 9.344) (writing took 4.298341780900955 seconds)
2022-03-04 13:16:59 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 13:16:59 | INFO | train | epoch 044 | loss 4.077 | ppl 16.87 | wps 22038 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4237 | lr 0.000485815 | gnorm 1.121 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 13252
2022-03-04 13:16:59 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 13:16:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:20:01 | INFO | train_inner | epoch 045:     63 / 97 loss=4.016, ppl=16.17, wps=22038.7, ups=0.34, wpb=65495, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=1.149, loss_scale=32, train_wall=261, gb_free=8.2, wall=13434
2022-03-04 13:20:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:21:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:21:43 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.107 | ppl 551.25 | wps 39554.7 | wpb 510.9 | bsz 1 | num_updates 4333 | best_loss 7.674
2022-03-04 13:21:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4333 updates
2022-03-04 13:21:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:21:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 45 @ 4333 updates, score 9.107) (writing took 3.2264988590031862 seconds)
2022-03-04 13:21:46 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 13:21:46 | INFO | train | epoch 045 | loss 3.972 | ppl 15.69 | wps 21870 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 4333 | lr 0.000480403 | gnorm 1.12 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 13540
2022-03-04 13:21:46 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 13:21:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:24:59 | INFO | train_inner | epoch 046:     67 / 97 loss=3.908, ppl=15.01, wps=21911.3, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.107, loss_scale=16, train_wall=264, gb_free=8.2, wall=13733
2022-03-04 13:26:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:26:31 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.459 | ppl 704.01 | wps 39707.8 | wpb 510.9 | bsz 1 | num_updates 4430 | best_loss 7.674
2022-03-04 13:26:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4430 updates
2022-03-04 13:26:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:26:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:26:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 46 @ 4430 updates, score 9.459) (writing took 3.3552502542734146 seconds)
2022-03-04 13:26:34 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 13:26:34 | INFO | train | epoch 046 | loss 3.876 | ppl 14.69 | wps 22077.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4430 | lr 0.000475114 | gnorm 1.121 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 13827
2022-03-04 13:26:34 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 13:26:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:29:57 | INFO | train_inner | epoch 047:     70 / 97 loss=3.808, ppl=14, wps=22026.4, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=1.096, loss_scale=32, train_wall=263, gb_free=8.2, wall=14030
2022-03-04 13:31:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:31:20 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.546 | ppl 747.35 | wps 39708.7 | wpb 510.9 | bsz 1 | num_updates 4527 | best_loss 7.674
2022-03-04 13:31:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4527 updates
2022-03-04 13:31:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:31:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:31:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 47 @ 4527 updates, score 9.546) (writing took 3.8976879622787237 seconds)
2022-03-04 13:31:23 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 13:31:23 | INFO | train | epoch 047 | loss 3.776 | ppl 13.7 | wps 21955.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4527 | lr 0.000469997 | gnorm 1.096 | loss_scale 32 | train_wall 255 | gb_free 8.2 | wall 14117
2022-03-04 13:31:23 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 13:31:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:33:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:33:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:35:00 | INFO | train_inner | epoch 048:     75 / 97 loss=3.714, ppl=13.12, wps=21619.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.113, loss_scale=16, train_wall=267, gb_free=8.2, wall=14333
2022-03-04 13:36:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:36:08 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.564 | ppl 757.09 | wps 38834.2 | wpb 510.9 | bsz 1 | num_updates 4622 | best_loss 7.674
2022-03-04 13:36:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4622 updates
2022-03-04 13:36:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:36:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:36:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 48 @ 4622 updates, score 9.564) (writing took 3.314028788357973 seconds)
2022-03-04 13:36:11 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 13:36:11 | INFO | train | epoch 048 | loss 3.686 | ppl 12.87 | wps 21624.2 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 4622 | lr 0.000465141 | gnorm 1.119 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 14404
2022-03-04 13:36:11 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 13:36:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:39:56 | INFO | train_inner | epoch 049:     78 / 97 loss=3.618, ppl=12.28, wps=22112.5, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=1.098, loss_scale=16, train_wall=261, gb_free=8.2, wall=14629
2022-03-04 13:40:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:40:55 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.507 | ppl 727.39 | wps 39506.7 | wpb 510.9 | bsz 1 | num_updates 4719 | best_loss 7.674
2022-03-04 13:40:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4719 updates
2022-03-04 13:40:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:40:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 49 @ 4719 updates, score 9.507) (writing took 3.2425274066627026 seconds)
2022-03-04 13:40:59 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 13:40:59 | INFO | train | epoch 049 | loss 3.6 | ppl 12.12 | wps 22088.6 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4719 | lr 0.000460336 | gnorm 1.104 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 14692
2022-03-04 13:40:59 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 13:40:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:42:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:44:55 | INFO | train_inner | epoch 050:     82 / 97 loss=3.531, ppl=11.56, wps=21884, ups=0.33, wpb=65495, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=1.127, loss_scale=16, train_wall=264, gb_free=8.2, wall=14928
2022-03-04 13:45:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:45:44 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.674 | ppl 817.14 | wps 38536.9 | wpb 510.9 | bsz 1 | num_updates 4815 | best_loss 7.674
2022-03-04 13:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4815 updates
2022-03-04 13:45:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:45:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 50 @ 4815 updates, score 9.674) (writing took 4.2458136305212975 seconds)
2022-03-04 13:45:48 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 13:45:48 | INFO | train | epoch 050 | loss 3.515 | ppl 11.43 | wps 21744.7 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 4815 | lr 0.000455724 | gnorm 1.111 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 14981
2022-03-04 13:45:48 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 13:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:49:53 | INFO | train_inner | epoch 051:     85 / 97 loss=3.452, ppl=10.94, wps=22010.4, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.116, loss_scale=32, train_wall=262, gb_free=8.2, wall=15226
2022-03-04 13:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:50:32 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.813 | ppl 899.72 | wps 39463.8 | wpb 510.9 | bsz 1 | num_updates 4912 | best_loss 7.674
2022-03-04 13:50:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4912 updates
2022-03-04 13:50:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:50:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:50:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 51 @ 4912 updates, score 9.813) (writing took 3.9652121290564537 seconds)
2022-03-04 13:50:36 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 13:50:36 | INFO | train | epoch 051 | loss 3.433 | ppl 10.8 | wps 22027.4 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 4912 | lr 0.000451202 | gnorm 1.114 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 15270
2022-03-04 13:50:36 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 13:50:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:53:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:54:51 | INFO | train_inner | epoch 052:     89 / 97 loss=3.362, ppl=10.28, wps=21969.7, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1.105, loss_scale=16, train_wall=263, gb_free=8.2, wall=15524
2022-03-04 13:55:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:55:19 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.926 | ppl 973.07 | wps 40450.8 | wpb 510.9 | bsz 1 | num_updates 5008 | best_loss 7.674
2022-03-04 13:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5008 updates
2022-03-04 13:55:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:55:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 13:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 52 @ 5008 updates, score 9.926) (writing took 4.37408984452486 seconds)
2022-03-04 13:55:23 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 13:55:23 | INFO | train | epoch 052 | loss 3.35 | ppl 10.2 | wps 21911.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 5008 | lr 0.000446856 | gnorm 1.107 | loss_scale 16 | train_wall 252 | gb_free 8.2 | wall 15556
2022-03-04 13:55:23 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 13:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:59:46 | INFO | train_inner | epoch 053:     92 / 97 loss=3.285, ppl=9.75, wps=22198.1, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=1.112, loss_scale=16, train_wall=260, gb_free=8.2, wall=15819
2022-03-04 14:00:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:00:05 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.979 | ppl 1009.3 | wps 40760.1 | wpb 510.9 | bsz 1 | num_updates 5105 | best_loss 7.674
2022-03-04 14:00:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5105 updates
2022-03-04 14:00:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:00:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 53 @ 5105 updates, score 9.979) (writing took 3.5074991546571255 seconds)
2022-03-04 14:00:09 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 14:00:09 | INFO | train | epoch 053 | loss 3.278 | ppl 9.7 | wps 22250.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5105 | lr 0.000442591 | gnorm 1.119 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 15842
2022-03-04 14:00:09 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 14:00:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:04:41 | INFO | train_inner | epoch 054:     95 / 97 loss=3.211, ppl=9.26, wps=22196.3, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5200, lr=0.000438529, gnorm=1.128, loss_scale=32, train_wall=260, gb_free=8.2, wall=16114
2022-03-04 14:04:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:04:52 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 10.174 | ppl 1154.88 | wps 39448.2 | wpb 510.9 | bsz 1 | num_updates 5202 | best_loss 7.674
2022-03-04 14:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5202 updates
2022-03-04 14:04:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 54 @ 5202 updates, score 10.174) (writing took 3.301689339801669 seconds)
2022-03-04 14:04:55 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 14:04:55 | INFO | train | epoch 054 | loss 3.204 | ppl 9.21 | wps 22162.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5202 | lr 0.000438445 | gnorm 1.125 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 16129
2022-03-04 14:04:55 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 14:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:05:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:09:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:09:40 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 10.179 | ppl 1159.01 | wps 39428.9 | wpb 510.9 | bsz 1 | num_updates 5298 | best_loss 7.674
2022-03-04 14:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5298 updates
2022-03-04 14:09:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:09:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:09:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 55 @ 5298 updates, score 10.179) (writing took 3.146646738052368 seconds)
2022-03-04 14:09:44 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 14:09:44 | INFO | train | epoch 055 | loss 3.13 | ppl 8.75 | wps 21813.9 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 5298 | lr 0.000434454 | gnorm 1.104 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 16417
2022-03-04 14:09:44 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 14:09:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:09:49 | INFO | train_inner | epoch 056:      2 / 97 loss=3.129, ppl=8.75, wps=21217.6, ups=0.32, wpb=65451.9, bsz=127.8, num_updates=5300, lr=0.000434372, gnorm=1.106, loss_scale=16, train_wall=265, gb_free=8.2, wall=16423
2022-03-04 14:14:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:14:28 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 10.249 | ppl 1217.29 | wps 40102.9 | wpb 510.9 | bsz 1 | num_updates 5395 | best_loss 7.674
2022-03-04 14:14:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5395 updates
2022-03-04 14:14:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:14:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 56 @ 5395 updates, score 10.249) (writing took 3.1835948955267668 seconds)
2022-03-04 14:14:31 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 14:14:31 | INFO | train | epoch 056 | loss 3.062 | ppl 8.35 | wps 22082.8 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5395 | lr 0.000430531 | gnorm 1.108 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 16704
2022-03-04 14:14:31 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 14:14:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:14:46 | INFO | train_inner | epoch 057:      5 / 97 loss=3.057, ppl=8.32, wps=22107, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=1.103, loss_scale=32, train_wall=262, gb_free=8.2, wall=16719
2022-03-04 14:18:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:19:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:19:15 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 10.36 | ppl 1313.92 | wps 39687.9 | wpb 510.9 | bsz 1 | num_updates 5491 | best_loss 7.674
2022-03-04 14:19:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5491 updates
2022-03-04 14:19:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:19:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:19:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 57 @ 5491 updates, score 10.36) (writing took 4.365019455552101 seconds)
2022-03-04 14:19:19 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 14:19:19 | INFO | train | epoch 057 | loss 2.995 | ppl 7.97 | wps 21818 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 5491 | lr 0.000426751 | gnorm 1.109 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 16993
2022-03-04 14:19:19 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 14:19:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:19:45 | INFO | train_inner | epoch 058:      9 / 97 loss=2.986, ppl=7.92, wps=21871, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.105, loss_scale=32, train_wall=263, gb_free=8.2, wall=17018
2022-03-04 14:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:24:03 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 10.443 | ppl 1391.71 | wps 39454.1 | wpb 510.9 | bsz 1 | num_updates 5588 | best_loss 7.674
2022-03-04 14:24:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5588 updates
2022-03-04 14:24:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:24:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:24:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 58 @ 5588 updates, score 10.443) (writing took 4.423372903838754 seconds)
2022-03-04 14:24:08 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 14:24:08 | INFO | train | epoch 058 | loss 2.933 | ppl 7.64 | wps 22045.1 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5588 | lr 0.000423031 | gnorm 1.101 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 17281
2022-03-04 14:24:08 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 14:24:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:24:42 | INFO | train_inner | epoch 059:     12 / 97 loss=2.924, ppl=7.59, wps=22065.3, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.103, loss_scale=32, train_wall=261, gb_free=8.2, wall=17315
2022-03-04 14:25:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:28:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:28:52 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 10.506 | ppl 1454.67 | wps 39382.9 | wpb 510.9 | bsz 1 | num_updates 5684 | best_loss 7.674
2022-03-04 14:28:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5684 updates
2022-03-04 14:28:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:28:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:28:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 59 @ 5684 updates, score 10.506) (writing took 3.956930635496974 seconds)
2022-03-04 14:28:56 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 14:28:56 | INFO | train | epoch 059 | loss 2.87 | ppl 7.31 | wps 21834.6 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 5684 | lr 0.000419443 | gnorm 1.121 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 17569
2022-03-04 14:28:56 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 14:28:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:29:41 | INFO | train_inner | epoch 060:     16 / 97 loss=2.854, ppl=7.23, wps=21871.1, ups=0.33, wpb=65495, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.125, loss_scale=16, train_wall=264, gb_free=8.2, wall=17615
2022-03-04 14:33:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:33:40 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 10.576 | ppl 1526.81 | wps 39433.2 | wpb 510.9 | bsz 1 | num_updates 5781 | best_loss 7.674
2022-03-04 14:33:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5781 updates
2022-03-04 14:33:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:33:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:33:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 60 @ 5781 updates, score 10.576) (writing took 3.8851835783571005 seconds)
2022-03-04 14:33:44 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 14:33:44 | INFO | train | epoch 060 | loss 2.811 | ppl 7.02 | wps 22023.4 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5781 | lr 0.000415909 | gnorm 1.115 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 17857
2022-03-04 14:33:44 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 14:33:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:34:38 | INFO | train_inner | epoch 061:     19 / 97 loss=2.802, ppl=6.98, wps=22062.1, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.106, loss_scale=32, train_wall=261, gb_free=8.2, wall=17911
2022-03-04 14:37:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:38:28 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 10.717 | ppl 1682.87 | wps 39705.7 | wpb 510.9 | bsz 1 | num_updates 5877 | best_loss 7.674
2022-03-04 14:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5877 updates
2022-03-04 14:38:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:38:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:38:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 61 @ 5877 updates, score 10.717) (writing took 3.206579962745309 seconds)
2022-03-04 14:38:31 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 14:38:31 | INFO | train | epoch 061 | loss 2.753 | ppl 6.74 | wps 21878.7 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 5877 | lr 0.000412498 | gnorm 1.116 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 18145
2022-03-04 14:38:31 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 14:38:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:39:37 | INFO | train_inner | epoch 062:     23 / 97 loss=2.733, ppl=6.65, wps=21912.6, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.109, loss_scale=16, train_wall=264, gb_free=8.2, wall=18210
2022-03-04 14:43:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:43:16 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 10.951 | ppl 1979.93 | wps 39378.3 | wpb 510.9 | bsz 1 | num_updates 5974 | best_loss 7.674
2022-03-04 14:43:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5974 updates
2022-03-04 14:43:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:43:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 62 @ 5974 updates, score 10.951) (writing took 4.334356181323528 seconds)
2022-03-04 14:43:20 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 14:43:20 | INFO | train | epoch 062 | loss 2.698 | ppl 6.49 | wps 22003.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 5974 | lr 0.000409136 | gnorm 1.113 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 18433
2022-03-04 14:43:20 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 14:43:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:44:35 | INFO | train_inner | epoch 063:     26 / 97 loss=2.683, ppl=6.42, wps=22009.4, ups=0.34, wpb=65495, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.122, loss_scale=32, train_wall=262, gb_free=8.2, wall=18508
2022-03-04 14:47:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:48:05 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 10.806 | ppl 1790.6 | wps 38557.2 | wpb 510.9 | bsz 1 | num_updates 6071 | best_loss 7.674
2022-03-04 14:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6071 updates
2022-03-04 14:48:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:48:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 63 @ 6071 updates, score 10.806) (writing took 3.3669720962643623 seconds)
2022-03-04 14:48:08 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 14:48:08 | INFO | train | epoch 063 | loss 2.644 | ppl 6.25 | wps 22027.7 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6071 | lr 0.000405854 | gnorm 1.109 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 18722
2022-03-04 14:48:08 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 14:48:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:49:31 | INFO | train_inner | epoch 064:     29 / 97 loss=2.627, ppl=6.18, wps=22071.3, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.112, loss_scale=32, train_wall=262, gb_free=8.2, wall=18805
2022-03-04 14:50:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:52:53 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10.939 | ppl 1962.62 | wps 37952.7 | wpb 510.9 | bsz 1 | num_updates 6167 | best_loss 7.674
2022-03-04 14:52:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6167 updates
2022-03-04 14:52:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:52:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:52:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 64 @ 6167 updates, score 10.939) (writing took 4.112572306767106 seconds)
2022-03-04 14:52:57 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 14:52:57 | INFO | train | epoch 064 | loss 2.59 | ppl 6.02 | wps 21802.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 6167 | lr 0.000402683 | gnorm 1.114 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 19010
2022-03-04 14:52:57 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 14:52:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:54:31 | INFO | train_inner | epoch 065:     33 / 97 loss=2.571, ppl=5.94, wps=21838.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.122, loss_scale=32, train_wall=264, gb_free=8.2, wall=19105
2022-03-04 14:56:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:57:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:57:41 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 11.032 | ppl 2094.06 | wps 37717.2 | wpb 510.9 | bsz 1 | num_updates 6263 | best_loss 7.674
2022-03-04 14:57:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6263 updates
2022-03-04 14:57:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:57:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 14:57:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 65 @ 6263 updates, score 11.032) (writing took 3.858089027926326 seconds)
2022-03-04 14:57:45 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 14:57:45 | INFO | train | epoch 065 | loss 2.541 | ppl 5.82 | wps 21833.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 6263 | lr 0.000399585 | gnorm 1.122 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 19298
2022-03-04 14:57:45 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 14:57:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:58:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:59:34 | INFO | train_inner | epoch 066:     38 / 97 loss=2.525, ppl=5.76, wps=21660.2, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.126, loss_scale=16, train_wall=266, gb_free=8.2, wall=19407
2022-03-04 15:02:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:02:29 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 11.096 | ppl 2189.34 | wps 37689.1 | wpb 510.9 | bsz 1 | num_updates 6359 | best_loss 7.674
2022-03-04 15:02:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6359 updates
2022-03-04 15:02:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:02:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:02:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 66 @ 6359 updates, score 11.096) (writing took 3.899480516090989 seconds)
2022-03-04 15:02:33 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 15:02:33 | INFO | train | epoch 066 | loss 2.491 | ppl 5.62 | wps 21830.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 6359 | lr 0.000396557 | gnorm 1.121 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 19586
2022-03-04 15:02:33 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 15:02:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:04:31 | INFO | train_inner | epoch 067:     41 / 97 loss=2.47, ppl=5.54, wps=22061.6, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.094, loss_scale=16, train_wall=261, gb_free=8.2, wall=19704
2022-03-04 15:07:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:07:17 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 11.179 | ppl 2318.08 | wps 38729.6 | wpb 510.9 | bsz 1 | num_updates 6456 | best_loss 7.674
2022-03-04 15:07:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6456 updates
2022-03-04 15:07:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:07:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 67 @ 6456 updates, score 11.179) (writing took 4.38679787889123 seconds)
2022-03-04 15:07:21 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 15:07:21 | INFO | train | epoch 067 | loss 2.446 | ppl 5.45 | wps 22003.4 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6456 | lr 0.000393567 | gnorm 1.096 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 19875
2022-03-04 15:07:22 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 15:07:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:09:27 | INFO | train_inner | epoch 068:     44 / 97 loss=2.428, ppl=5.38, wps=22053.3, ups=0.34, wpb=65495, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1.111, loss_scale=32, train_wall=261, gb_free=8.2, wall=20001
2022-03-04 15:10:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:12:05 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 11.315 | ppl 2548.09 | wps 38436.6 | wpb 510.9 | bsz 1 | num_updates 6552 | best_loss 7.674
2022-03-04 15:12:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6552 updates
2022-03-04 15:12:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:12:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 68 @ 6552 updates, score 11.315) (writing took 4.15943399630487 seconds)
2022-03-04 15:12:09 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 15:12:09 | INFO | train | epoch 068 | loss 2.4 | ppl 5.28 | wps 21850.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 6552 | lr 0.000390673 | gnorm 1.128 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 20162
2022-03-04 15:12:09 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 15:12:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:14:27 | INFO | train_inner | epoch 069:     48 / 97 loss=2.382, ppl=5.21, wps=21886.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.139, loss_scale=16, train_wall=263, gb_free=8.2, wall=20300
2022-03-04 15:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:16:53 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 11.332 | ppl 2577.7 | wps 38286.7 | wpb 510.9 | bsz 1 | num_updates 6649 | best_loss 7.674
2022-03-04 15:16:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6649 updates
2022-03-04 15:16:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:16:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 69 @ 6649 updates, score 11.332) (writing took 3.5855840742588043 seconds)
2022-03-04 15:16:57 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 15:16:57 | INFO | train | epoch 069 | loss 2.357 | ppl 5.12 | wps 22111.5 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6649 | lr 0.000387813 | gnorm 1.134 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 20450
2022-03-04 15:16:57 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 15:16:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:19:23 | INFO | train_inner | epoch 070:     51 / 97 loss=2.334, ppl=5.04, wps=22125.2, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.119, loss_scale=32, train_wall=261, gb_free=8.2, wall=20596
2022-03-04 15:21:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:21:41 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 11.385 | ppl 2675.33 | wps 37781.7 | wpb 510.9 | bsz 1 | num_updates 6746 | best_loss 7.674
2022-03-04 15:21:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6746 updates
2022-03-04 15:21:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:21:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:21:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 70 @ 6746 updates, score 11.385) (writing took 3.506474843248725 seconds)
2022-03-04 15:21:44 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 15:21:44 | INFO | train | epoch 070 | loss 2.314 | ppl 4.97 | wps 22089.9 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 6746 | lr 0.000385014 | gnorm 1.126 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 20737
2022-03-04 15:21:44 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 15:21:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:23:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:24:23 | INFO | train_inner | epoch 071:     55 / 97 loss=2.291, ppl=4.89, wps=21808.8, ups=0.33, wpb=65495, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.128, loss_scale=32, train_wall=265, gb_free=8.2, wall=20896
2022-03-04 15:26:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:26:30 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 11.519 | ppl 2934.48 | wps 37617.5 | wpb 510.9 | bsz 1 | num_updates 6842 | best_loss 7.674
2022-03-04 15:26:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6842 updates
2022-03-04 15:26:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:26:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:26:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 71 @ 6842 updates, score 11.519) (writing took 4.436589624732733 seconds)
2022-03-04 15:26:34 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 15:26:34 | INFO | train | epoch 071 | loss 2.269 | ppl 4.82 | wps 21690.7 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 6842 | lr 0.000382304 | gnorm 1.108 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 21027
2022-03-04 15:26:34 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 15:26:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:29:20 | INFO | train_inner | epoch 072:     58 / 97 loss=2.248, ppl=4.75, wps=22079.5, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.11, loss_scale=32, train_wall=261, gb_free=8.2, wall=21193
2022-03-04 15:30:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:31:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:31:17 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 11.554 | ppl 3007.12 | wps 39847.7 | wpb 510.9 | bsz 1 | num_updates 6938 | best_loss 7.674
2022-03-04 15:31:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6938 updates
2022-03-04 15:31:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:31:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:31:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 72 @ 6938 updates, score 11.554) (writing took 3.5761777944862843 seconds)
2022-03-04 15:31:20 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 15:31:20 | INFO | train | epoch 072 | loss 2.233 | ppl 4.7 | wps 21970.9 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 6938 | lr 0.00037965 | gnorm 1.113 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 21313
2022-03-04 15:31:20 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 15:31:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:34:17 | INFO | train_inner | epoch 073:     62 / 97 loss=2.208, ppl=4.62, wps=22048.9, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.122, loss_scale=32, train_wall=262, gb_free=8.2, wall=21490
2022-03-04 15:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:36:03 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 11.693 | ppl 3311.09 | wps 39115.9 | wpb 510.9 | bsz 1 | num_updates 7035 | best_loss 7.674
2022-03-04 15:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7035 updates
2022-03-04 15:36:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:36:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:36:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 73 @ 7035 updates, score 11.693) (writing took 3.444358330219984 seconds)
2022-03-04 15:36:06 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 15:36:06 | INFO | train | epoch 073 | loss 2.193 | ppl 4.57 | wps 22221.6 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7035 | lr 0.000377023 | gnorm 1.102 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 21599
2022-03-04 15:36:06 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 15:36:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:36:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:39:13 | INFO | train_inner | epoch 074:     66 / 97 loss=2.171, ppl=4.5, wps=22114.9, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.11, loss_scale=32, train_wall=261, gb_free=8.2, wall=21786
2022-03-04 15:40:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:40:48 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 11.746 | ppl 3433.78 | wps 38407.6 | wpb 510.9 | bsz 1 | num_updates 7131 | best_loss 7.674
2022-03-04 15:40:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7131 updates
2022-03-04 15:40:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:40:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 74 @ 7131 updates, score 11.746) (writing took 3.440684912726283 seconds)
2022-03-04 15:40:52 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 15:40:52 | INFO | train | epoch 074 | loss 2.159 | ppl 4.47 | wps 22017.7 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 7131 | lr 0.000374477 | gnorm 1.148 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 21885
2022-03-04 15:40:52 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 15:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:43:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:44:12 | INFO | train_inner | epoch 075:     70 / 97 loss=2.133, ppl=4.39, wps=21880.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.135, loss_scale=32, train_wall=264, gb_free=8.2, wall=22085
2022-03-04 15:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:45:36 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 11.803 | ppl 3572.11 | wps 37822.7 | wpb 510.9 | bsz 1 | num_updates 7227 | best_loss 7.674
2022-03-04 15:45:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7227 updates
2022-03-04 15:45:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:45:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 75 @ 7227 updates, score 11.803) (writing took 3.34367872774601 seconds)
2022-03-04 15:45:40 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 15:45:40 | INFO | train | epoch 075 | loss 2.119 | ppl 4.34 | wps 21828.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 7227 | lr 0.000371981 | gnorm 1.125 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 22173
2022-03-04 15:45:40 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 15:45:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:49:09 | INFO | train_inner | epoch 076:     73 / 97 loss=2.096, ppl=4.28, wps=22093.2, ups=0.34, wpb=65495, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.11, loss_scale=32, train_wall=261, gb_free=8.2, wall=22382
2022-03-04 15:49:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:50:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:50:24 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 11.975 | ppl 4026.77 | wps 38391.2 | wpb 510.9 | bsz 1 | num_updates 7323 | best_loss 7.674
2022-03-04 15:50:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7323 updates
2022-03-04 15:50:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:50:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:50:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 76 @ 7323 updates, score 11.975) (writing took 3.126722190529108 seconds)
2022-03-04 15:50:27 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 15:50:27 | INFO | train | epoch 076 | loss 2.084 | ppl 4.24 | wps 21872.1 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 7323 | lr 0.000369535 | gnorm 1.104 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 22460
2022-03-04 15:50:27 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 15:50:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:54:08 | INFO | train_inner | epoch 077:     77 / 97 loss=2.059, ppl=4.17, wps=21875.4, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.114, loss_scale=32, train_wall=264, gb_free=8.2, wall=22681
2022-03-04 15:54:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:55:12 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 12.019 | ppl 4149.42 | wps 38308.5 | wpb 510.9 | bsz 1 | num_updates 7419 | best_loss 7.674
2022-03-04 15:55:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7419 updates
2022-03-04 15:55:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:55:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 15:55:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 77 @ 7419 updates, score 12.019) (writing took 4.413887936621904 seconds)
2022-03-04 15:55:16 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 15:55:16 | INFO | train | epoch 077 | loss 2.051 | ppl 4.14 | wps 21751 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 7419 | lr 0.000367136 | gnorm 1.125 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 22749
2022-03-04 15:55:16 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 15:55:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:59:08 | INFO | train_inner | epoch 078:     81 / 97 loss=2.026, ppl=4.07, wps=21861.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.116, loss_scale=16, train_wall=263, gb_free=8.2, wall=22981
2022-03-04 15:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:00:00 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 12.018 | ppl 4148.03 | wps 36738.7 | wpb 510.9 | bsz 1 | num_updates 7516 | best_loss 7.674
2022-03-04 16:00:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7516 updates
2022-03-04 16:00:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:00:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:00:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 78 @ 7516 updates, score 12.018) (writing took 4.291906436905265 seconds)
2022-03-04 16:00:04 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 16:00:04 | INFO | train | epoch 078 | loss 2.016 | ppl 4.05 | wps 22030.6 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7516 | lr 0.00036476 | gnorm 1.106 | loss_scale 16 | train_wall 253 | gb_free 8.2 | wall 23038
2022-03-04 16:00:04 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 16:00:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:04:06 | INFO | train_inner | epoch 079:     84 / 97 loss=1.993, ppl=3.98, wps=21975.5, ups=0.34, wpb=65495, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.106, loss_scale=32, train_wall=262, gb_free=8.2, wall=23279
2022-03-04 16:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:04:49 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 12.113 | ppl 4431.11 | wps 37398.2 | wpb 510.9 | bsz 1 | num_updates 7613 | best_loss 7.674
2022-03-04 16:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7613 updates
2022-03-04 16:04:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 79 @ 7613 updates, score 12.113) (writing took 4.411656767129898 seconds)
2022-03-04 16:04:54 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 16:04:54 | INFO | train | epoch 079 | loss 1.986 | ppl 3.96 | wps 21966.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7613 | lr 0.000362428 | gnorm 1.101 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 23327
2022-03-04 16:04:54 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 16:04:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:07:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:09:07 | INFO | train_inner | epoch 080:     88 / 97 loss=1.963, ppl=3.9, wps=21758.3, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.136, loss_scale=32, train_wall=264, gb_free=8.2, wall=23580
2022-03-04 16:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:09:38 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 12.244 | ppl 4850.69 | wps 38533.5 | wpb 510.9 | bsz 1 | num_updates 7709 | best_loss 7.674
2022-03-04 16:09:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7709 updates
2022-03-04 16:09:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:09:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:09:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 80 @ 7709 updates, score 12.244) (writing took 3.8383394721895456 seconds)
2022-03-04 16:09:42 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 16:09:42 | INFO | train | epoch 080 | loss 1.954 | ppl 3.87 | wps 21798.9 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 7709 | lr 0.000360165 | gnorm 1.135 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 23615
2022-03-04 16:09:42 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 16:09:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:13:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:14:05 | INFO | train_inner | epoch 081:     92 / 97 loss=1.928, ppl=3.8, wps=21937.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=7800, lr=0.000358057, gnorm=1.109, loss_scale=32, train_wall=263, gb_free=8.2, wall=23878
2022-03-04 16:14:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:14:25 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 12.223 | ppl 4779.53 | wps 38262.2 | wpb 510.9 | bsz 1 | num_updates 7805 | best_loss 7.674
2022-03-04 16:14:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7805 updates
2022-03-04 16:14:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:14:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:14:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 81 @ 7805 updates, score 12.223) (writing took 4.027275338768959 seconds)
2022-03-04 16:14:29 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 16:14:29 | INFO | train | epoch 081 | loss 1.923 | ppl 3.79 | wps 21890 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 7805 | lr 0.000357943 | gnorm 1.11 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 23903
2022-03-04 16:14:29 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 16:14:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:19:02 | INFO | train_inner | epoch 082:     95 / 97 loss=1.898, ppl=3.73, wps=22044.7, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=7900, lr=0.000355784, gnorm=1.119, loss_scale=32, train_wall=262, gb_free=8.2, wall=24176
2022-03-04 16:19:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:19:14 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 12.381 | ppl 5335.24 | wps 38385.3 | wpb 510.9 | bsz 1 | num_updates 7902 | best_loss 7.674
2022-03-04 16:19:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7902 updates
2022-03-04 16:19:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:19:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:19:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 82 @ 7902 updates, score 12.381) (writing took 4.368738602846861 seconds)
2022-03-04 16:19:18 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 16:19:18 | INFO | train | epoch 082 | loss 1.896 | ppl 3.72 | wps 22005.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 7902 | lr 0.000355739 | gnorm 1.12 | loss_scale 32 | train_wall 254 | gb_free 8.2 | wall 24191
2022-03-04 16:19:18 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 16:19:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:20:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:23:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:24:02 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 12.275 | ppl 4955.9 | wps 38317.7 | wpb 510.9 | bsz 1 | num_updates 7998 | best_loss 7.674
2022-03-04 16:24:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7998 updates
2022-03-04 16:24:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:24:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:24:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 83 @ 7998 updates, score 12.275) (writing took 4.279667753726244 seconds)
2022-03-04 16:24:06 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 16:24:06 | INFO | train | epoch 083 | loss 1.865 | ppl 3.64 | wps 21812.6 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 7998 | lr 0.000353598 | gnorm 1.121 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 24479
2022-03-04 16:24:06 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 16:24:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:24:12 | INFO | train_inner | epoch 084:      2 / 97 loss=1.866, ppl=3.64, wps=21125.2, ups=0.32, wpb=65451.9, bsz=127.8, num_updates=8000, lr=0.000353553, gnorm=1.121, loss_scale=32, train_wall=263, gb_free=8.2, wall=24485
2022-03-04 16:26:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:28:49 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 12.469 | ppl 5671.36 | wps 38259.2 | wpb 510.9 | bsz 1 | num_updates 8094 | best_loss 7.674
2022-03-04 16:28:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8094 updates
2022-03-04 16:28:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:28:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:28:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 84 @ 8094 updates, score 12.469) (writing took 4.148157151415944 seconds)
2022-03-04 16:28:53 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 16:28:53 | INFO | train | epoch 084 | loss 1.837 | ppl 3.57 | wps 21897.4 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 8094 | lr 0.000351494 | gnorm 1.116 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 24767
2022-03-04 16:28:53 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 16:28:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:29:11 | INFO | train_inner | epoch 085:      6 / 97 loss=1.833, ppl=3.56, wps=21922.1, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.113, loss_scale=32, train_wall=263, gb_free=8.2, wall=24784
2022-03-04 16:32:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:33:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:33:37 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 12.447 | ppl 5584.37 | wps 39881.9 | wpb 510.9 | bsz 1 | num_updates 8190 | best_loss 7.674
2022-03-04 16:33:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8190 updates
2022-03-04 16:33:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:33:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:33:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 85 @ 8190 updates, score 12.447) (writing took 3.610049458220601 seconds)
2022-03-04 16:33:41 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 16:33:41 | INFO | train | epoch 085 | loss 1.81 | ppl 3.51 | wps 21880.2 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 8190 | lr 0.000349428 | gnorm 1.107 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 25054
2022-03-04 16:33:41 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 16:33:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:34:10 | INFO | train_inner | epoch 086:     10 / 97 loss=1.802, ppl=3.49, wps=21910.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.111, loss_scale=32, train_wall=264, gb_free=8.2, wall=25083
2022-03-04 16:36:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:38:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:38:26 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 12.498 | ppl 5783.19 | wps 38704.7 | wpb 510.9 | bsz 1 | num_updates 8286 | best_loss 7.674
2022-03-04 16:38:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8286 updates
2022-03-04 16:38:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:38:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 86 @ 8286 updates, score 12.498) (writing took 3.6720769815146923 seconds)
2022-03-04 16:38:29 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 16:38:29 | INFO | train | epoch 086 | loss 1.783 | ppl 3.44 | wps 21787.8 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 8286 | lr 0.000347398 | gnorm 1.11 | loss_scale 16 | train_wall 254 | gb_free 8.2 | wall 25342
2022-03-04 16:38:29 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 16:38:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:39:09 | INFO | train_inner | epoch 087:     14 / 97 loss=1.779, ppl=3.43, wps=21849.6, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.108, loss_scale=16, train_wall=264, gb_free=8.2, wall=25383
2022-03-04 16:43:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:43:11 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 12.622 | ppl 6303.4 | wps 40435.2 | wpb 510.9 | bsz 1 | num_updates 8383 | best_loss 7.674
2022-03-04 16:43:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8383 updates
2022-03-04 16:43:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:43:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:43:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 87 @ 8383 updates, score 12.622) (writing took 3.1381902005523443 seconds)
2022-03-04 16:43:14 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 16:43:14 | INFO | train | epoch 087 | loss 1.759 | ppl 3.38 | wps 22291 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8383 | lr 0.000345382 | gnorm 1.102 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 25627
2022-03-04 16:43:14 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 16:43:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:44:03 | INFO | train_inner | epoch 088:     17 / 97 loss=1.751, ppl=3.36, wps=22350, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.1, loss_scale=32, train_wall=259, gb_free=8.2, wall=25676
2022-03-04 16:45:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:47:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:47:56 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 12.658 | ppl 6463.88 | wps 40565.7 | wpb 510.9 | bsz 1 | num_updates 8479 | best_loss 7.674
2022-03-04 16:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8479 updates
2022-03-04 16:47:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:48:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 88 @ 8479 updates, score 12.658) (writing took 4.289009587839246 seconds)
2022-03-04 16:48:00 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 16:48:00 | INFO | train | epoch 088 | loss 1.734 | ppl 3.33 | wps 21992.8 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 8479 | lr 0.000343422 | gnorm 1.108 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 25913
2022-03-04 16:48:00 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 16:48:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:49:00 | INFO | train_inner | epoch 089:     21 / 97 loss=1.728, ppl=3.31, wps=22033.1, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.108, loss_scale=16, train_wall=262, gb_free=8.2, wall=25973
2022-03-04 16:52:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:52:42 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 12.765 | ppl 6961.4 | wps 40211.7 | wpb 510.9 | bsz 1 | num_updates 8576 | best_loss 7.674
2022-03-04 16:52:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8576 updates
2022-03-04 16:52:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:52:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:52:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 89 @ 8576 updates, score 12.765) (writing took 4.380578817799687 seconds)
2022-03-04 16:52:46 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 16:52:46 | INFO | train | epoch 089 | loss 1.712 | ppl 3.28 | wps 22193.2 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8576 | lr 0.000341474 | gnorm 1.107 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 26200
2022-03-04 16:52:46 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 16:52:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:53:55 | INFO | train_inner | epoch 090:     24 / 97 loss=1.705, ppl=3.26, wps=22211.1, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.097, loss_scale=32, train_wall=259, gb_free=8.2, wall=26268
2022-03-04 16:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:57:28 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 12.815 | ppl 7204.43 | wps 40538.3 | wpb 510.9 | bsz 1 | num_updates 8673 | best_loss 7.674
2022-03-04 16:57:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8673 updates
2022-03-04 16:57:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 16:57:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 90 @ 8673 updates, score 12.815) (writing took 4.178647210821509 seconds)
2022-03-04 16:57:32 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 16:57:32 | INFO | train | epoch 090 | loss 1.687 | ppl 3.22 | wps 22206.4 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8673 | lr 0.000339559 | gnorm 1.104 | loss_scale 32 | train_wall 252 | gb_free 8.2 | wall 26486
2022-03-04 16:57:32 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 16:57:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:57:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:58:52 | INFO | train_inner | epoch 091:     28 / 97 loss=1.681, ppl=3.21, wps=22048.4, ups=0.34, wpb=65492.9, bsz=127.9, num_updates=8700, lr=0.000339032, gnorm=1.118, loss_scale=32, train_wall=261, gb_free=8.2, wall=26565
2022-03-04 17:02:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:02:14 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 12.887 | ppl 7575.15 | wps 39932.4 | wpb 510.9 | bsz 1 | num_updates 8769 | best_loss 7.674
2022-03-04 17:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8769 updates
2022-03-04 17:02:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:02:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:02:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 91 @ 8769 updates, score 12.887) (writing took 4.511457299813628 seconds)
2022-03-04 17:02:19 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 17:02:19 | INFO | train | epoch 091 | loss 1.664 | ppl 3.17 | wps 21964.2 | ups 0.34 | wpb 65493.3 | bsz 127.9 | num_updates 8769 | lr 0.000337695 | gnorm 1.131 | loss_scale 32 | train_wall 251 | gb_free 8.2 | wall 26772
2022-03-04 17:02:19 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 17:02:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:03:47 | INFO | train_inner | epoch 092:     31 / 97 loss=1.657, ppl=3.15, wps=22205.1, ups=0.34, wpb=65495, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.13, loss_scale=32, train_wall=259, gb_free=8.2, wall=26860
2022-03-04 17:04:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:06:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:07:00 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 12.892 | ppl 7602.79 | wps 40419.7 | wpb 510.9 | bsz 1 | num_updates 8864 | best_loss 7.674
2022-03-04 17:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8864 updates
2022-03-04 17:07:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 92 @ 8864 updates, score 12.892) (writing took 4.421836007386446 seconds)
2022-03-04 17:07:05 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 17:07:05 | INFO | train | epoch 092 | loss 1.64 | ppl 3.12 | wps 21755.3 | ups 0.33 | wpb 65490.6 | bsz 127.9 | num_updates 8864 | lr 0.000335881 | gnorm 1.111 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 27058
2022-03-04 17:07:05 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 17:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:08:46 | INFO | train_inner | epoch 093:     36 / 97 loss=1.631, ppl=3.1, wps=21871.1, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.106, loss_scale=16, train_wall=263, gb_free=8.2, wall=27159
2022-03-04 17:11:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:11:45 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 12.967 | ppl 8006.09 | wps 41617.5 | wpb 510.9 | bsz 1 | num_updates 8961 | best_loss 7.674
2022-03-04 17:11:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8961 updates
2022-03-04 17:11:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 93 @ 8961 updates, score 12.967) (writing took 3.467349359765649 seconds)
2022-03-04 17:11:49 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 17:11:49 | INFO | train | epoch 093 | loss 1.621 | ppl 3.08 | wps 22366.3 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 8961 | lr 0.000334058 | gnorm 1.102 | loss_scale 16 | train_wall 251 | gb_free 8.2 | wall 27342
2022-03-04 17:11:49 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 17:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:13:39 | INFO | train_inner | epoch 094:     39 / 97 loss=1.609, ppl=3.05, wps=22388.4, ups=0.34, wpb=65495, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.097, loss_scale=32, train_wall=258, gb_free=8.2, wall=27452
2022-03-04 17:14:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:16:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:16:29 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 12.942 | ppl 7868.61 | wps 41276.4 | wpb 510.9 | bsz 1 | num_updates 9057 | best_loss 7.674
2022-03-04 17:16:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9057 updates
2022-03-04 17:16:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:16:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 94 @ 9057 updates, score 12.942) (writing took 3.490743711590767 seconds)
2022-03-04 17:16:32 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 17:16:32 | INFO | train | epoch 094 | loss 1.598 | ppl 3.03 | wps 22153.1 | ups 0.34 | wpb 65491.1 | bsz 127.9 | num_updates 9057 | lr 0.000332283 | gnorm 1.107 | loss_scale 16 | train_wall 250 | gb_free 8.2 | wall 27626
2022-03-04 17:16:32 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 17:16:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:18:33 | INFO | train_inner | epoch 095:     43 / 97 loss=1.593, ppl=3.02, wps=22237.4, ups=0.34, wpb=65490.8, bsz=127.9, num_updates=9100, lr=0.000331497, gnorm=1.122, loss_scale=16, train_wall=260, gb_free=8.2, wall=27746
2022-03-04 17:21:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:21:13 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 13.106 | ppl 8818.42 | wps 38743.4 | wpb 510.9 | bsz 1 | num_updates 9154 | best_loss 7.674
2022-03-04 17:21:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9154 updates
2022-03-04 17:21:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:21:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 95 @ 9154 updates, score 13.106) (writing took 3.350189307704568 seconds)
2022-03-04 17:21:16 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 17:21:16 | INFO | train | epoch 095 | loss 1.579 | ppl 2.99 | wps 22391.1 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 9154 | lr 0.000330518 | gnorm 1.111 | loss_scale 32 | train_wall 250 | gb_free 8.2 | wall 27909
2022-03-04 17:21:16 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 17:21:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:23:29 | INFO | train_inner | epoch 096:     46 / 97 loss=1.571, ppl=2.97, wps=22155.5, ups=0.34, wpb=65495, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.092, loss_scale=32, train_wall=261, gb_free=8.2, wall=28042
2022-03-04 17:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:26:02 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 13.136 | ppl 9003.82 | wps 38956.5 | wpb 510.9 | bsz 1 | num_updates 9251 | best_loss 7.674
2022-03-04 17:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9251 updates
2022-03-04 17:26:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 96 @ 9251 updates, score 13.136) (writing took 3.193097749724984 seconds)
2022-03-04 17:26:06 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 17:26:06 | INFO | train | epoch 096 | loss 1.558 | ppl 2.95 | wps 21946.5 | ups 0.34 | wpb 65491.6 | bsz 127.9 | num_updates 9251 | lr 0.00032878 | gnorm 1.095 | loss_scale 32 | train_wall 255 | gb_free 8.2 | wall 28199
2022-03-04 17:26:06 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 17:26:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:27:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:28:29 | INFO | train_inner | epoch 097:     50 / 97 loss=1.547, ppl=2.92, wps=21835.7, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.102, loss_scale=32, train_wall=265, gb_free=8.2, wall=28342
2022-03-04 17:30:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:30:50 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 13.175 | ppl 9247.03 | wps 39537 | wpb 510.9 | bsz 1 | num_updates 9347 | best_loss 7.674
2022-03-04 17:30:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9347 updates
2022-03-04 17:30:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 97 @ 9347 updates, score 13.175) (writing took 3.152049032971263 seconds)
2022-03-04 17:30:53 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 17:30:53 | INFO | train | epoch 097 | loss 1.537 | ppl 2.9 | wps 21890.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 9347 | lr 0.000327087 | gnorm 1.101 | loss_scale 32 | train_wall 253 | gb_free 8.2 | wall 28486
2022-03-04 17:30:53 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 17:30:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:32:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:33:30 | INFO | train_inner | epoch 098:     54 / 97 loss=1.529, ppl=2.89, wps=21727.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.104, loss_scale=16, train_wall=266, gb_free=8.2, wall=28643
2022-03-04 17:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:35:43 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 13.272 | ppl 9894.41 | wps 37052.6 | wpb 510.9 | bsz 1 | num_updates 9443 | best_loss 7.674
2022-03-04 17:35:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9443 updates
2022-03-04 17:35:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:35:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:35:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 98 @ 9443 updates, score 13.272) (writing took 3.2165754940360785 seconds)
2022-03-04 17:35:46 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 17:35:46 | INFO | train | epoch 098 | loss 1.519 | ppl 2.87 | wps 21431.9 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 9443 | lr 0.000325421 | gnorm 1.102 | loss_scale 16 | train_wall 259 | gb_free 8.2 | wall 28779
2022-03-04 17:35:46 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 17:35:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:38:35 | INFO | train_inner | epoch 099:     57 / 97 loss=1.505, ppl=2.84, wps=21500.6, ups=0.33, wpb=65495, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.083, loss_scale=16, train_wall=269, gb_free=8.2, wall=28948
2022-03-04 17:40:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:40:39 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 13.29 | ppl 10017.1 | wps 37519.1 | wpb 510.9 | bsz 1 | num_updates 9540 | best_loss 7.674
2022-03-04 17:40:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9540 updates
2022-03-04 17:40:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:40:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:40:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 99 @ 9540 updates, score 13.29) (writing took 3.2902043350040913 seconds)
2022-03-04 17:40:43 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 17:40:43 | INFO | train | epoch 099 | loss 1.502 | ppl 2.83 | wps 21431.9 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 9540 | lr 0.000323762 | gnorm 1.092 | loss_scale 32 | train_wall 262 | gb_free 8.2 | wall 29076
2022-03-04 17:40:43 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 17:40:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:43:41 | INFO | train_inner | epoch 100:     60 / 97 loss=1.497, ppl=2.82, wps=21366.2, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.113, loss_scale=32, train_wall=270, gb_free=8.2, wall=29255
2022-03-04 17:45:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:45:39 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 13.32 | ppl 10224 | wps 35854.1 | wpb 510.9 | bsz 1 | num_updates 9637 | best_loss 7.674
2022-03-04 17:45:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9637 updates
2022-03-04 17:45:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:45:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 100 @ 9637 updates, score 13.32) (writing took 3.8293278366327286 seconds)
2022-03-04 17:45:43 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 17:45:43 | INFO | train | epoch 100 | loss 1.483 | ppl 2.8 | wps 21183.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 9637 | lr 0.000322128 | gnorm 1.107 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 29376
2022-03-04 17:45:43 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 17:45:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 17:48:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:48:56 | INFO | train_inner | epoch 101:     65 / 97 loss=1.469, ppl=2.77, wps=20834.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.1, loss_scale=16, train_wall=277, gb_free=8.2, wall=29569
2022-03-04 17:50:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:50:37 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 13.434 | ppl 11065.4 | wps 35736.7 | wpb 510.9 | bsz 1 | num_updates 9732 | best_loss 7.674
2022-03-04 17:50:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9732 updates
2022-03-04 17:50:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:50:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:50:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 101 @ 9732 updates, score 13.434) (writing took 3.7318016923964024 seconds)
2022-03-04 17:50:41 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 17:50:41 | INFO | train | epoch 101 | loss 1.464 | ppl 2.76 | wps 20878.1 | ups 0.32 | wpb 65490.6 | bsz 127.9 | num_updates 9732 | lr 0.000320552 | gnorm 1.101 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 29674
2022-03-04 17:50:41 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 17:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:54:04 | INFO | train_inner | epoch 102:     68 / 97 loss=1.455, ppl=2.74, wps=21239.9, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.105, loss_scale=16, train_wall=271, gb_free=8.2, wall=29877
2022-03-04 17:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:55:36 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 13.438 | ppl 11095.6 | wps 36636.4 | wpb 510.9 | bsz 1 | num_updates 9829 | best_loss 7.674
2022-03-04 17:55:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9829 updates
2022-03-04 17:55:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:55:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 17:55:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 102 @ 9829 updates, score 13.438) (writing took 4.182778537273407 seconds)
2022-03-04 17:55:40 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 17:55:40 | INFO | train | epoch 102 | loss 1.447 | ppl 2.73 | wps 21204.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 9829 | lr 0.000318967 | gnorm 1.103 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 29973
2022-03-04 17:55:40 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 17:55:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:59:13 | INFO | train_inner | epoch 103:     71 / 97 loss=1.435, ppl=2.7, wps=21189.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.091, loss_scale=32, train_wall=272, gb_free=8.2, wall=30186
2022-03-04 18:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:00:37 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 13.523 | ppl 11770.2 | wps 35843.1 | wpb 510.9 | bsz 1 | num_updates 9926 | best_loss 7.674
2022-03-04 18:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9926 updates
2022-03-04 18:00:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:00:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 103 @ 9926 updates, score 13.523) (writing took 3.9773364029824734 seconds)
2022-03-04 18:00:41 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 18:00:41 | INFO | train | epoch 103 | loss 1.43 | ppl 2.69 | wps 21132.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 9926 | lr 0.000317404 | gnorm 1.089 | loss_scale 32 | train_wall 264 | gb_free 8.2 | wall 30274
2022-03-04 18:00:41 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 18:00:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:01:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 18:03:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:04:28 | INFO | train_inner | epoch 104:     76 / 97 loss=1.419, ppl=2.67, wps=20791, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.084, loss_scale=16, train_wall=277, gb_free=8.2, wall=30501
2022-03-04 18:05:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:05:37 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 13.596 | ppl 12379.5 | wps 36661.1 | wpb 510.9 | bsz 1 | num_updates 10021 | best_loss 7.674
2022-03-04 18:05:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10021 updates
2022-03-04 18:05:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:05:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:05:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 104 @ 10021 updates, score 13.596) (writing took 3.8048236407339573 seconds)
2022-03-04 18:05:40 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 18:05:40 | INFO | train | epoch 104 | loss 1.411 | ppl 2.66 | wps 20765.2 | ups 0.32 | wpb 65490.6 | bsz 127.9 | num_updates 10021 | lr 0.000315896 | gnorm 1.087 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 30574
2022-03-04 18:05:40 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 18:05:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:09:37 | INFO | train_inner | epoch 105:     79 / 97 loss=1.401, ppl=2.64, wps=21201.4, ups=0.32, wpb=65495, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.093, loss_scale=16, train_wall=272, gb_free=8.2, wall=30810
2022-03-04 18:10:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:10:36 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 13.644 | ppl 12798.1 | wps 36264.6 | wpb 510.9 | bsz 1 | num_updates 10118 | best_loss 7.674
2022-03-04 18:10:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10118 updates
2022-03-04 18:10:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:10:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:10:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 105 @ 10118 updates, score 13.644) (writing took 3.79999278485775 seconds)
2022-03-04 18:10:40 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 18:10:40 | INFO | train | epoch 105 | loss 1.399 | ppl 2.64 | wps 21196.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 10118 | lr 0.000314378 | gnorm 1.088 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 30873
2022-03-04 18:10:40 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 18:10:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:14:44 | INFO | train_inner | epoch 106:     82 / 97 loss=1.388, ppl=2.62, wps=21305.6, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=10200, lr=0.000313112, gnorm=1.102, loss_scale=32, train_wall=270, gb_free=8.2, wall=31118
2022-03-04 18:15:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:15:36 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 13.675 | ppl 13080.2 | wps 35241.6 | wpb 510.9 | bsz 1 | num_updates 10214 | best_loss 7.674
2022-03-04 18:15:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10214 updates
2022-03-04 18:15:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:15:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:15:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 106 @ 10214 updates, score 13.675) (writing took 4.076090196147561 seconds)
2022-03-04 18:15:40 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 18:15:40 | INFO | train | epoch 106 | loss 1.382 | ppl 2.61 | wps 20981 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10214 | lr 0.000312897 | gnorm 1.109 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 31173
2022-03-04 18:15:40 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 18:15:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:19:57 | INFO | train_inner | epoch 107:     86 / 97 loss=1.369, ppl=2.58, wps=20978, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.087, loss_scale=16, train_wall=274, gb_free=8.2, wall=31430
2022-03-04 18:20:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:20:35 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 13.641 | ppl 12776.4 | wps 36705.6 | wpb 510.9 | bsz 1 | num_updates 10311 | best_loss 7.674
2022-03-04 18:20:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10311 updates
2022-03-04 18:20:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:20:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:20:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 107 @ 10311 updates, score 13.641) (writing took 3.822733771055937 seconds)
2022-03-04 18:20:39 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 18:20:39 | INFO | train | epoch 107 | loss 1.366 | ppl 2.58 | wps 21229.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 10311 | lr 0.000311422 | gnorm 1.088 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 31472
2022-03-04 18:20:39 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 18:20:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:23:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:25:08 | INFO | train_inner | epoch 108:     90 / 97 loss=1.356, ppl=2.56, wps=21039.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.096, loss_scale=16, train_wall=274, gb_free=8.2, wall=31741
2022-03-04 18:25:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:25:35 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 13.713 | ppl 13423.8 | wps 35861.6 | wpb 510.9 | bsz 1 | num_updates 10407 | best_loss 7.674
2022-03-04 18:25:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10407 updates
2022-03-04 18:25:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:25:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 108 @ 10407 updates, score 13.713) (writing took 3.7240654435008764 seconds)
2022-03-04 18:25:38 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 18:25:38 | INFO | train | epoch 108 | loss 1.353 | ppl 2.55 | wps 20992.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10407 | lr 0.000309983 | gnorm 1.083 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 31772
2022-03-04 18:25:38 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 18:25:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:30:17 | INFO | train_inner | epoch 109:     93 / 97 loss=1.343, ppl=2.54, wps=21205.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10500, lr=0.000308607, gnorm=1.089, loss_scale=16, train_wall=272, gb_free=8.2, wall=32050
2022-03-04 18:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:30:35 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 13.844 | ppl 14705.9 | wps 35299.3 | wpb 510.9 | bsz 1 | num_updates 10504 | best_loss 7.674
2022-03-04 18:30:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10504 updates
2022-03-04 18:30:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:30:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 109 @ 10504 updates, score 13.844) (writing took 3.8816690519452095 seconds)
2022-03-04 18:30:38 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 18:30:38 | INFO | train | epoch 109 | loss 1.339 | ppl 2.53 | wps 21171.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 10504 | lr 0.000308548 | gnorm 1.094 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 32072
2022-03-04 18:30:39 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 18:30:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:35:26 | INFO | train_inner | epoch 110:     97 / 97 loss=1.324, ppl=2.5, wps=21167.8, ups=0.32, wpb=65451.9, bsz=127.8, num_updates=10600, lr=0.000307148, gnorm=1.089, loss_scale=16, train_wall=272, gb_free=8.2, wall=32359
2022-03-04 18:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:35:32 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 13.792 | ppl 14185.7 | wps 37190.1 | wpb 510.9 | bsz 1 | num_updates 10600 | best_loss 7.674
2022-03-04 18:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10600 updates
2022-03-04 18:35:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:35:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:35:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 110 @ 10600 updates, score 13.792) (writing took 4.127330284565687 seconds)
2022-03-04 18:35:36 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 18:35:36 | INFO | train | epoch 110 | loss 1.322 | ppl 2.5 | wps 21138.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10600 | lr 0.000307148 | gnorm 1.086 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 32369
2022-03-04 18:35:36 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 18:35:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:40:30 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 13.894 | ppl 15228.3 | wps 35705.3 | wpb 510.9 | bsz 1 | num_updates 10697 | best_loss 7.674
2022-03-04 18:40:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10697 updates
2022-03-04 18:40:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:40:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:40:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 111 @ 10697 updates, score 13.894) (writing took 4.279887426644564 seconds)
2022-03-04 18:40:34 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 18:40:34 | INFO | train | epoch 111 | loss 1.31 | ppl 2.48 | wps 21317.5 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 10697 | lr 0.000305752 | gnorm 1.086 | loss_scale 32 | train_wall 262 | gb_free 8.2 | wall 32667
2022-03-04 18:40:34 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 18:40:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:40:43 | INFO | train_inner | epoch 112:      3 / 97 loss=1.308, ppl=2.48, wps=20654.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.086, loss_scale=32, train_wall=270, gb_free=8.2, wall=32676
2022-03-04 18:45:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:45:30 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 13.926 | ppl 15569.8 | wps 36862.6 | wpb 510.9 | bsz 1 | num_updates 10794 | best_loss 7.674
2022-03-04 18:45:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10794 updates
2022-03-04 18:45:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:45:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:45:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 112 @ 10794 updates, score 13.926) (writing took 3.944685611873865 seconds)
2022-03-04 18:45:34 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 18:45:34 | INFO | train | epoch 112 | loss 1.295 | ppl 2.45 | wps 21186.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 10794 | lr 0.000304375 | gnorm 1.088 | loss_scale 32 | train_wall 264 | gb_free 8.2 | wall 32967
2022-03-04 18:45:34 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 18:45:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:45:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:45:55 | INFO | train_inner | epoch 113:      7 / 97 loss=1.292, ppl=2.45, wps=20997.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.089, loss_scale=16, train_wall=275, gb_free=8.2, wall=32988
2022-03-04 18:50:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:50:30 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 14.04 | ppl 16848.3 | wps 35266.3 | wpb 510.9 | bsz 1 | num_updates 10890 | best_loss 7.674
2022-03-04 18:50:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10890 updates
2022-03-04 18:50:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:50:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:50:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 113 @ 10890 updates, score 14.04) (writing took 4.015799943357706 seconds)
2022-03-04 18:50:34 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 18:50:34 | INFO | train | epoch 113 | loss 1.281 | ppl 2.43 | wps 20918 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10890 | lr 0.00030303 | gnorm 1.08 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 33268
2022-03-04 18:50:34 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 18:50:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:51:05 | INFO | train_inner | epoch 114:     10 / 97 loss=1.277, ppl=2.42, wps=21142.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.076, loss_scale=16, train_wall=272, gb_free=8.2, wall=33298
2022-03-04 18:53:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:55:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:55:30 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 14.055 | ppl 17016 | wps 35928.2 | wpb 510.9 | bsz 1 | num_updates 10986 | best_loss 7.674
2022-03-04 18:55:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10986 updates
2022-03-04 18:55:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:55:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 18:55:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 114 @ 10986 updates, score 14.055) (writing took 3.6937530618160963 seconds)
2022-03-04 18:55:34 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 18:55:34 | INFO | train | epoch 114 | loss 1.268 | ppl 2.41 | wps 20988.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 10986 | lr 0.000301703 | gnorm 1.076 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 33567
2022-03-04 18:55:34 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 18:55:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:56:16 | INFO | train_inner | epoch 115:     14 / 97 loss=1.264, ppl=2.4, wps=21030.7, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.074, loss_scale=16, train_wall=274, gb_free=8.2, wall=33609
2022-03-04 19:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:00:29 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 14.024 | ppl 16660.5 | wps 36062.5 | wpb 510.9 | bsz 1 | num_updates 11083 | best_loss 7.674
2022-03-04 19:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11083 updates
2022-03-04 19:00:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 115 @ 11083 updates, score 14.024) (writing took 3.792999753728509 seconds)
2022-03-04 19:00:33 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 19:00:33 | INFO | train | epoch 115 | loss 1.257 | ppl 2.39 | wps 21238.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11083 | lr 0.00030038 | gnorm 1.072 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 33866
2022-03-04 19:00:33 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 19:00:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:01:24 | INFO | train_inner | epoch 116:     17 / 97 loss=1.252, ppl=2.38, wps=21263.7, ups=0.32, wpb=65495, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.067, loss_scale=32, train_wall=271, gb_free=8.2, wall=33917
2022-03-04 19:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:05:28 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 14.104 | ppl 17604.5 | wps 37340.7 | wpb 510.9 | bsz 1 | num_updates 11180 | best_loss 7.674
2022-03-04 19:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11180 updates
2022-03-04 19:05:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:05:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 116 @ 11180 updates, score 14.104) (writing took 3.85102504119277 seconds)
2022-03-04 19:05:32 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 19:05:32 | INFO | train | epoch 116 | loss 1.242 | ppl 2.37 | wps 21277.9 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11180 | lr 0.000299074 | gnorm 1.057 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 34165
2022-03-04 19:05:32 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 19:05:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:06:31 | INFO | train_inner | epoch 117:     20 / 97 loss=1.242, ppl=2.36, wps=21321.3, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.07, loss_scale=32, train_wall=271, gb_free=8.2, wall=34224
2022-03-04 19:07:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 19:10:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:10:29 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 14.205 | ppl 18881.3 | wps 36646.6 | wpb 510.9 | bsz 1 | num_updates 11276 | best_loss 7.674
2022-03-04 19:10:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11276 updates
2022-03-04 19:10:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:10:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 117 @ 11276 updates, score 14.205) (writing took 3.797367176041007 seconds)
2022-03-04 19:10:32 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 19:10:32 | INFO | train | epoch 117 | loss 1.231 | ppl 2.35 | wps 20898.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11276 | lr 0.000297798 | gnorm 1.09 | loss_scale 32 | train_wall 264 | gb_free 8.2 | wall 34466
2022-03-04 19:10:32 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 19:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:11:44 | INFO | train_inner | epoch 118:     24 / 97 loss=1.222, ppl=2.33, wps=20912.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.077, loss_scale=32, train_wall=276, gb_free=8.2, wall=34538
2022-03-04 19:13:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:15:30 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 14.264 | ppl 19678.7 | wps 36311.4 | wpb 510.9 | bsz 1 | num_updates 11372 | best_loss 7.674
2022-03-04 19:15:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11372 updates
2022-03-04 19:15:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:15:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:15:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 118 @ 11372 updates, score 14.264) (writing took 3.808818819001317 seconds)
2022-03-04 19:15:34 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 19:15:34 | INFO | train | epoch 118 | loss 1.217 | ppl 2.32 | wps 20873.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11372 | lr 0.000296539 | gnorm 1.075 | loss_scale 16 | train_wall 265 | gb_free 8.2 | wall 34767
2022-03-04 19:15:34 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 19:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:16:57 | INFO | train_inner | epoch 119:     28 / 97 loss=1.214, ppl=2.32, wps=20926.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.079, loss_scale=16, train_wall=275, gb_free=8.2, wall=34851
2022-03-04 19:20:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:20:31 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 14.324 | ppl 20504.2 | wps 36880.2 | wpb 510.9 | bsz 1 | num_updates 11468 | best_loss 7.674
2022-03-04 19:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11468 updates
2022-03-04 19:20:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:20:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:20:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 119 @ 11468 updates, score 14.324) (writing took 3.7527238242328167 seconds)
2022-03-04 19:20:34 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 19:20:34 | INFO | train | epoch 119 | loss 1.205 | ppl 2.31 | wps 20912.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11468 | lr 0.000295295 | gnorm 1.081 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 35068
2022-03-04 19:20:34 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 19:20:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:22:10 | INFO | train_inner | epoch 120:     32 / 97 loss=1.203, ppl=2.3, wps=20943.8, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.081, loss_scale=16, train_wall=275, gb_free=8.2, wall=35163
2022-03-04 19:25:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:25:31 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 14.285 | ppl 19968.2 | wps 34887.3 | wpb 510.9 | bsz 1 | num_updates 11565 | best_loss 7.674
2022-03-04 19:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11565 updates
2022-03-04 19:25:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:25:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 120 @ 11565 updates, score 14.285) (writing took 3.74521316960454 seconds)
2022-03-04 19:25:35 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 19:25:35 | INFO | train | epoch 120 | loss 1.194 | ppl 2.29 | wps 21126.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11565 | lr 0.000294054 | gnorm 1.069 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 35368
2022-03-04 19:25:35 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 19:25:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:27:20 | INFO | train_inner | epoch 121:     35 / 97 loss=1.188, ppl=2.28, wps=21138.8, ups=0.32, wpb=65495, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.071, loss_scale=32, train_wall=272, gb_free=8.2, wall=35473
2022-03-04 19:30:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:30:31 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 14.337 | ppl 20688.5 | wps 34913.3 | wpb 510.9 | bsz 1 | num_updates 11662 | best_loss 7.674
2022-03-04 19:30:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11662 updates
2022-03-04 19:30:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:30:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 121 @ 11662 updates, score 14.337) (writing took 3.825442438945174 seconds)
2022-03-04 19:30:35 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 19:30:35 | INFO | train | epoch 121 | loss 1.185 | ppl 2.27 | wps 21150.9 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11662 | lr 0.000292829 | gnorm 1.076 | loss_scale 32 | train_wall 264 | gb_free 8.2 | wall 35669
2022-03-04 19:30:35 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 19:30:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:32:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:32:33 | INFO | train_inner | epoch 122:     39 / 97 loss=1.183, ppl=2.27, wps=20929.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=1.07, loss_scale=16, train_wall=275, gb_free=8.2, wall=35786
2022-03-04 19:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:35:32 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 14.363 | ppl 21072.6 | wps 35822.7 | wpb 510.9 | bsz 1 | num_updates 11758 | best_loss 7.674
2022-03-04 19:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11758 updates
2022-03-04 19:35:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:35:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:35:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 122 @ 11758 updates, score 14.363) (writing took 3.802939197048545 seconds)
2022-03-04 19:35:35 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 19:35:35 | INFO | train | epoch 122 | loss 1.173 | ppl 2.26 | wps 20945.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11758 | lr 0.000291631 | gnorm 1.067 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 35969
2022-03-04 19:35:35 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 19:35:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:37:42 | INFO | train_inner | epoch 123:     42 / 97 loss=1.166, ppl=2.24, wps=21176.6, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.067, loss_scale=16, train_wall=272, gb_free=8.2, wall=36095
2022-03-04 19:39:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:40:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:40:33 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 14.401 | ppl 21633.4 | wps 34543.2 | wpb 510.9 | bsz 1 | num_updates 11854 | best_loss 7.674
2022-03-04 19:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11854 updates
2022-03-04 19:40:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:40:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 123 @ 11854 updates, score 14.401) (writing took 4.170652307569981 seconds)
2022-03-04 19:40:37 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 19:40:37 | INFO | train | epoch 123 | loss 1.162 | ppl 2.24 | wps 20869.5 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 11854 | lr 0.000290447 | gnorm 1.074 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 36270
2022-03-04 19:40:37 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 19:40:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:42:54 | INFO | train_inner | epoch 124:     46 / 97 loss=1.157, ppl=2.23, wps=21011.9, ups=0.32, wpb=65495, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.073, loss_scale=16, train_wall=274, gb_free=8.2, wall=36407
2022-03-04 19:45:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:45:33 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 14.495 | ppl 23094.4 | wps 35195.4 | wpb 510.9 | bsz 1 | num_updates 11951 | best_loss 7.674
2022-03-04 19:45:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11951 updates
2022-03-04 19:45:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:45:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:45:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 124 @ 11951 updates, score 14.495) (writing took 3.766215382143855 seconds)
2022-03-04 19:45:37 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 19:45:37 | INFO | train | epoch 124 | loss 1.151 | ppl 2.22 | wps 21173.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 11951 | lr 0.000289266 | gnorm 1.057 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 36570
2022-03-04 19:45:37 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 19:45:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:47:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:48:06 | INFO | train_inner | epoch 125:     50 / 97 loss=1.147, ppl=2.21, wps=21000.3, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.061, loss_scale=16, train_wall=274, gb_free=8.2, wall=36719
2022-03-04 19:50:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:50:32 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 14.5 | ppl 23172.8 | wps 35750.2 | wpb 510.9 | bsz 1 | num_updates 12047 | best_loss 7.674
2022-03-04 19:50:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12047 updates
2022-03-04 19:50:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:50:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:50:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 125 @ 12047 updates, score 14.5) (writing took 3.6604317482560873 seconds)
2022-03-04 19:50:36 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 19:50:36 | INFO | train | epoch 125 | loss 1.14 | ppl 2.2 | wps 21009.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12047 | lr 0.000288111 | gnorm 1.069 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 36869
2022-03-04 19:50:36 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 19:50:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:53:14 | INFO | train_inner | epoch 126:     53 / 97 loss=1.134, ppl=2.19, wps=21268.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=1.067, loss_scale=16, train_wall=271, gb_free=8.2, wall=37027
2022-03-04 19:54:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:55:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:55:30 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 14.436 | ppl 22165.2 | wps 35613.1 | wpb 510.9 | bsz 1 | num_updates 12143 | best_loss 7.674
2022-03-04 19:55:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12143 updates
2022-03-04 19:55:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:55:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 19:55:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 126 @ 12143 updates, score 14.436) (writing took 3.6928946524858475 seconds)
2022-03-04 19:55:34 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 19:55:34 | INFO | train | epoch 126 | loss 1.131 | ppl 2.19 | wps 21107.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12143 | lr 0.00028697 | gnorm 1.073 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 37167
2022-03-04 19:55:34 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 19:55:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:58:24 | INFO | train_inner | epoch 127:     57 / 97 loss=1.126, ppl=2.18, wps=21081.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12200, lr=0.000286299, gnorm=1.063, loss_scale=16, train_wall=274, gb_free=8.2, wall=37338
2022-03-04 20:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:00:29 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 14.565 | ppl 24244.8 | wps 36342.8 | wpb 510.9 | bsz 1 | num_updates 12240 | best_loss 7.674
2022-03-04 20:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12240 updates
2022-03-04 20:00:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 127 @ 12240 updates, score 14.565) (writing took 3.763071583583951 seconds)
2022-03-04 20:00:33 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 20:00:33 | INFO | train | epoch 127 | loss 1.121 | ppl 2.18 | wps 21243.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 12240 | lr 0.000285831 | gnorm 1.061 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 37466
2022-03-04 20:00:33 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 20:00:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:03:33 | INFO | train_inner | epoch 128:     60 / 97 loss=1.118, ppl=2.17, wps=21206.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=1.061, loss_scale=32, train_wall=272, gb_free=8.2, wall=37646
2022-03-04 20:03:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:05:30 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 14.498 | ppl 23136.2 | wps 35085.6 | wpb 510.9 | bsz 1 | num_updates 12336 | best_loss 7.674
2022-03-04 20:05:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12336 updates
2022-03-04 20:05:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 128 @ 12336 updates, score 14.498) (writing took 3.70267628505826 seconds)
2022-03-04 20:05:34 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 20:05:34 | INFO | train | epoch 128 | loss 1.112 | ppl 2.16 | wps 20889.5 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12336 | lr 0.000284717 | gnorm 1.063 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 37767
2022-03-04 20:05:34 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 20:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:08:46 | INFO | train_inner | epoch 129:     64 / 97 loss=1.105, ppl=2.15, wps=20954.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=1.059, loss_scale=16, train_wall=275, gb_free=8.2, wall=37959
2022-03-04 20:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:10:30 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 14.552 | ppl 24013.2 | wps 35878.4 | wpb 510.9 | bsz 1 | num_updates 12433 | best_loss 7.674
2022-03-04 20:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12433 updates
2022-03-04 20:10:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:10:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:10:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 129 @ 12433 updates, score 14.552) (writing took 3.8166463784873486 seconds)
2022-03-04 20:10:34 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 20:10:34 | INFO | train | epoch 129 | loss 1.1 | ppl 2.14 | wps 21199.7 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 12433 | lr 0.000283604 | gnorm 1.046 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 38067
2022-03-04 20:10:34 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 20:10:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:12:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:13:57 | INFO | train_inner | epoch 130:     68 / 97 loss=1.097, ppl=2.14, wps=21039, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=1.06, loss_scale=16, train_wall=274, gb_free=8.2, wall=38270
2022-03-04 20:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:15:29 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 14.679 | ppl 26224.6 | wps 36269 | wpb 510.9 | bsz 1 | num_updates 12529 | best_loss 7.674
2022-03-04 20:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12529 updates
2022-03-04 20:15:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:15:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 130 @ 12529 updates, score 14.679) (writing took 3.922633992508054 seconds)
2022-03-04 20:15:33 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 20:15:33 | INFO | train | epoch 130 | loss 1.091 | ppl 2.13 | wps 20993.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12529 | lr 0.000282515 | gnorm 1.059 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 38366
2022-03-04 20:15:33 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 20:15:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:19:05 | INFO | train_inner | epoch 131:     71 / 97 loss=1.084, ppl=2.12, wps=21267.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=1.055, loss_scale=16, train_wall=271, gb_free=8.2, wall=38578
2022-03-04 20:20:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:20:29 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 14.762 | ppl 27784.4 | wps 36280.1 | wpb 510.9 | bsz 1 | num_updates 12626 | best_loss 7.674
2022-03-04 20:20:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12626 updates
2022-03-04 20:20:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:20:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:20:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 131 @ 12626 updates, score 14.762) (writing took 3.894249726086855 seconds)
2022-03-04 20:20:32 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 20:20:32 | INFO | train | epoch 131 | loss 1.083 | ppl 2.12 | wps 21217.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 12626 | lr 0.000281428 | gnorm 1.061 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 38666
2022-03-04 20:20:32 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 20:20:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:22:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:24:18 | INFO | train_inner | epoch 132:     75 / 97 loss=1.077, ppl=2.11, wps=20936.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=1.06, loss_scale=16, train_wall=275, gb_free=8.2, wall=38891
2022-03-04 20:25:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:25:29 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 14.756 | ppl 27662.8 | wps 35841.7 | wpb 510.9 | bsz 1 | num_updates 12722 | best_loss 7.674
2022-03-04 20:25:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12722 updates
2022-03-04 20:25:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:25:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:25:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 132 @ 12722 updates, score 14.756) (writing took 3.880259398370981 seconds)
2022-03-04 20:25:33 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 20:25:33 | INFO | train | epoch 132 | loss 1.074 | ppl 2.11 | wps 20906.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12722 | lr 0.000280364 | gnorm 1.057 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 38966
2022-03-04 20:25:33 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 20:25:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:28:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:29:31 | INFO | train_inner | epoch 133:     79 / 97 loss=1.069, ppl=2.1, wps=20913.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=1.078, loss_scale=16, train_wall=275, gb_free=8.2, wall=39204
2022-03-04 20:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:30:30 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 14.783 | ppl 28184.3 | wps 36129.7 | wpb 510.9 | bsz 1 | num_updates 12818 | best_loss 7.674
2022-03-04 20:30:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12818 updates
2022-03-04 20:30:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:30:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 133 @ 12818 updates, score 14.783) (writing took 3.7542914003133774 seconds)
2022-03-04 20:30:34 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 20:30:34 | INFO | train | epoch 133 | loss 1.064 | ppl 2.09 | wps 20886.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 12818 | lr 0.000279312 | gnorm 1.072 | loss_scale 16 | train_wall 265 | gb_free 8.2 | wall 39267
2022-03-04 20:30:34 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 20:30:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:34:39 | INFO | train_inner | epoch 134:     82 / 97 loss=1.055, ppl=2.08, wps=21244.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=1.044, loss_scale=16, train_wall=271, gb_free=8.2, wall=39512
2022-03-04 20:35:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:35:30 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 14.799 | ppl 28501.2 | wps 35091.7 | wpb 510.9 | bsz 1 | num_updates 12915 | best_loss 7.674
2022-03-04 20:35:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12915 updates
2022-03-04 20:35:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:35:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:35:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 134 @ 12915 updates, score 14.799) (writing took 3.757194247096777 seconds)
2022-03-04 20:35:34 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 20:35:34 | INFO | train | epoch 134 | loss 1.055 | ppl 2.08 | wps 21176.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 12915 | lr 0.000278261 | gnorm 1.047 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 39567
2022-03-04 20:35:34 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 20:35:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:39:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:39:51 | INFO | train_inner | epoch 135:     86 / 97 loss=1.052, ppl=2.07, wps=21017.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=1.054, loss_scale=16, train_wall=274, gb_free=8.2, wall=39824
2022-03-04 20:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:40:30 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 14.88 | ppl 30161.3 | wps 35155.4 | wpb 510.9 | bsz 1 | num_updates 13011 | best_loss 7.674
2022-03-04 20:40:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13011 updates
2022-03-04 20:40:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:40:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:40:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 135 @ 13011 updates, score 14.88) (writing took 3.7178971339017153 seconds)
2022-03-04 20:40:33 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 20:40:33 | INFO | train | epoch 135 | loss 1.046 | ppl 2.07 | wps 21016.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13011 | lr 0.000277233 | gnorm 1.052 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 39867
2022-03-04 20:40:33 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 20:40:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:44:59 | INFO | train_inner | epoch 136:     89 / 97 loss=1.037, ppl=2.05, wps=21229.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13100, lr=0.000276289, gnorm=1.038, loss_scale=16, train_wall=272, gb_free=8.2, wall=40132
2022-03-04 20:45:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:45:29 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 15.033 | ppl 33531.2 | wps 36469.5 | wpb 510.9 | bsz 1 | num_updates 13108 | best_loss 7.674
2022-03-04 20:45:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13108 updates
2022-03-04 20:45:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:45:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 136 @ 13108 updates, score 15.033) (writing took 3.727016543969512 seconds)
2022-03-04 20:45:33 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 20:45:33 | INFO | train | epoch 136 | loss 1.037 | ppl 2.05 | wps 21227.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 13108 | lr 0.000276205 | gnorm 1.038 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 40166
2022-03-04 20:45:33 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 20:45:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:47:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:50:11 | INFO | train_inner | epoch 137:     93 / 97 loss=1.034, ppl=2.05, wps=21029.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13200, lr=0.000275241, gnorm=1.046, loss_scale=16, train_wall=274, gb_free=8.2, wall=40444
2022-03-04 20:50:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:50:28 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 14.937 | ppl 31368.8 | wps 36637.3 | wpb 510.9 | bsz 1 | num_updates 13204 | best_loss 7.674
2022-03-04 20:50:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13204 updates
2022-03-04 20:50:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:50:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:50:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 137 @ 13204 updates, score 14.937) (writing took 4.060041006654501 seconds)
2022-03-04 20:50:32 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 20:50:32 | INFO | train | epoch 137 | loss 1.03 | ppl 2.04 | wps 20976.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13204 | lr 0.000275199 | gnorm 1.043 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 40466
2022-03-04 20:50:32 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 20:50:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:55:19 | INFO | train_inner | epoch 138:     96 / 97 loss=1.024, ppl=2.03, wps=21227.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13300, lr=0.000274204, gnorm=1.044, loss_scale=32, train_wall=271, gb_free=8.2, wall=40752
2022-03-04 20:55:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:55:28 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 14.973 | ppl 32152.7 | wps 36946 | wpb 510.9 | bsz 1 | num_updates 13301 | best_loss 7.674
2022-03-04 20:55:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13301 updates
2022-03-04 20:55:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:55:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 20:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 138 @ 13301 updates, score 14.973) (writing took 4.5086383651942015 seconds)
2022-03-04 20:55:32 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 20:55:32 | INFO | train | epoch 138 | loss 1.022 | ppl 2.03 | wps 21164.7 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 13301 | lr 0.000274194 | gnorm 1.044 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 40766
2022-03-04 20:55:32 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 20:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:59:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:00:26 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 14.957 | ppl 31812.2 | wps 37302 | wpb 510.9 | bsz 1 | num_updates 13397 | best_loss 7.674
2022-03-04 21:00:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13397 updates
2022-03-04 21:00:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:00:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:00:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 139 @ 13397 updates, score 14.957) (writing took 4.3467273116111755 seconds)
2022-03-04 21:00:30 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 21:00:30 | INFO | train | epoch 139 | loss 1.012 | ppl 2.02 | wps 21114.1 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13397 | lr 0.00027321 | gnorm 1.032 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 41063
2022-03-04 21:00:30 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 21:00:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:00:39 | INFO | train_inner | epoch 140:      3 / 97 loss=1.012, ppl=2.02, wps=20461.6, ups=0.31, wpb=65451.9, bsz=127.8, num_updates=13400, lr=0.000273179, gnorm=1.032, loss_scale=16, train_wall=272, gb_free=8.2, wall=41072
2022-03-04 21:05:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:05:22 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 15.058 | ppl 34118.1 | wps 36459.3 | wpb 510.9 | bsz 1 | num_updates 13494 | best_loss 7.674
2022-03-04 21:05:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13494 updates
2022-03-04 21:05:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:05:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 140 @ 13494 updates, score 15.058) (writing took 4.380623629316688 seconds)
2022-03-04 21:05:27 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 21:05:27 | INFO | train | epoch 140 | loss 1.007 | ppl 2.01 | wps 21440 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 13494 | lr 0.000272226 | gnorm 1.038 | loss_scale 16 | train_wall 260 | gb_free 8.2 | wall 41360
2022-03-04 21:05:27 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 21:05:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:05:44 | INFO | train_inner | epoch 141:      6 / 97 loss=1.006, ppl=2.01, wps=21463.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1.038, loss_scale=16, train_wall=268, gb_free=8.2, wall=41377
2022-03-04 21:06:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:10:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:10:20 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 15.021 | ppl 33245.1 | wps 36786.8 | wpb 510.9 | bsz 1 | num_updates 13590 | best_loss 7.674
2022-03-04 21:10:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13590 updates
2022-03-04 21:10:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:10:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:10:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 141 @ 13590 updates, score 15.021) (writing took 4.196479203179479 seconds)
2022-03-04 21:10:24 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 21:10:24 | INFO | train | epoch 141 | loss 0.997 | ppl 2 | wps 21128.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13590 | lr 0.000271263 | gnorm 1.039 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 41657
2022-03-04 21:10:24 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 21:10:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:10:53 | INFO | train_inner | epoch 142:     10 / 97 loss=0.994, ppl=1.99, wps=21182.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=1.037, loss_scale=16, train_wall=272, gb_free=8.2, wall=41687
2022-03-04 21:13:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:15:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:15:15 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 15.109 | ppl 35334 | wps 36508.8 | wpb 510.9 | bsz 1 | num_updates 13686 | best_loss 7.674
2022-03-04 21:15:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13686 updates
2022-03-04 21:15:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:15:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:15:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 142 @ 13686 updates, score 15.109) (writing took 4.05989995226264 seconds)
2022-03-04 21:15:19 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 21:15:19 | INFO | train | epoch 142 | loss 0.991 | ppl 1.99 | wps 21297 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 13686 | lr 0.00027031 | gnorm 1.043 | loss_scale 16 | train_wall 260 | gb_free 8.2 | wall 41953
2022-03-04 21:15:19 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 21:15:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:16:00 | INFO | train_inner | epoch 143:     14 / 97 loss=0.989, ppl=1.98, wps=21342.4, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1.043, loss_scale=16, train_wall=270, gb_free=8.2, wall=41994
2022-03-04 21:20:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:20:11 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 15.127 | ppl 35772.8 | wps 35804.3 | wpb 510.9 | bsz 1 | num_updates 13783 | best_loss 7.674
2022-03-04 21:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13783 updates
2022-03-04 21:20:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:20:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 143 @ 13783 updates, score 15.127) (writing took 3.526158457621932 seconds)
2022-03-04 21:20:14 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 21:20:14 | INFO | train | epoch 143 | loss 0.984 | ppl 1.98 | wps 21532 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 13783 | lr 0.000269357 | gnorm 1.034 | loss_scale 32 | train_wall 260 | gb_free 8.2 | wall 42248
2022-03-04 21:20:14 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 21:20:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:21:05 | INFO | train_inner | epoch 144:     17 / 97 loss=0.979, ppl=1.97, wps=21486.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=1.028, loss_scale=32, train_wall=268, gb_free=8.2, wall=42298
2022-03-04 21:22:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:25:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:25:08 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 15.137 | ppl 36022.3 | wps 35258 | wpb 510.9 | bsz 1 | num_updates 13879 | best_loss 7.674
2022-03-04 21:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13879 updates
2022-03-04 21:25:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:25:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:25:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 144 @ 13879 updates, score 15.137) (writing took 3.864566884934902 seconds)
2022-03-04 21:25:12 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 21:25:12 | INFO | train | epoch 144 | loss 0.975 | ppl 1.97 | wps 21151.7 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13879 | lr 0.000268424 | gnorm 1.022 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 42545
2022-03-04 21:25:12 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 21:25:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:26:15 | INFO | train_inner | epoch 145:     21 / 97 loss=0.973, ppl=1.96, wps=21161.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=1.022, loss_scale=16, train_wall=272, gb_free=8.2, wall=42608
2022-03-04 21:29:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:30:06 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 15.192 | ppl 37432.8 | wps 34602.8 | wpb 510.9 | bsz 1 | num_updates 13975 | best_loss 7.674
2022-03-04 21:30:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13975 updates
2022-03-04 21:30:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:30:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:30:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 145 @ 13975 updates, score 15.192) (writing took 3.8878889679908752 seconds)
2022-03-04 21:30:10 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 21:30:10 | INFO | train | epoch 145 | loss 0.969 | ppl 1.96 | wps 21058.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 13975 | lr 0.0002675 | gnorm 1.024 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 42843
2022-03-04 21:30:10 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 21:30:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:31:25 | INFO | train_inner | epoch 146:     25 / 97 loss=0.968, ppl=1.96, wps=21094.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=1.034, loss_scale=16, train_wall=273, gb_free=8.2, wall=42918
2022-03-04 21:34:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:35:02 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 15.269 | ppl 39489.4 | wps 36087.3 | wpb 510.9 | bsz 1 | num_updates 14072 | best_loss 7.674
2022-03-04 21:35:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14072 updates
2022-03-04 21:35:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:35:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:35:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 146 @ 14072 updates, score 15.269) (writing took 3.655660752207041 seconds)
2022-03-04 21:35:06 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 21:35:06 | INFO | train | epoch 146 | loss 0.962 | ppl 1.95 | wps 21489.7 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 14072 | lr 0.000266577 | gnorm 1.044 | loss_scale 16 | train_wall 260 | gb_free 8.2 | wall 43139
2022-03-04 21:35:06 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 21:35:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:36:30 | INFO | train_inner | epoch 147:     28 / 97 loss=0.958, ppl=1.94, wps=21509.3, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=1.035, loss_scale=32, train_wall=268, gb_free=8.2, wall=43223
2022-03-04 21:37:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:39:59 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 15.216 | ppl 38063.8 | wps 35533.5 | wpb 510.9 | bsz 1 | num_updates 14168 | best_loss 7.674
2022-03-04 21:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14168 updates
2022-03-04 21:39:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:40:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:40:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 147 @ 14168 updates, score 15.216) (writing took 3.831945402547717 seconds)
2022-03-04 21:40:03 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 21:40:03 | INFO | train | epoch 147 | loss 0.955 | ppl 1.94 | wps 21151.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14168 | lr 0.000265672 | gnorm 1.04 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 43436
2022-03-04 21:40:03 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 21:40:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:41:39 | INFO | train_inner | epoch 148:     32 / 97 loss=0.951, ppl=1.93, wps=21173.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=1.037, loss_scale=16, train_wall=272, gb_free=8.2, wall=43532
2022-03-04 21:44:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:44:56 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 15.32 | ppl 40903.6 | wps 37504.6 | wpb 510.9 | bsz 1 | num_updates 14265 | best_loss 7.674
2022-03-04 21:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14265 updates
2022-03-04 21:44:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:44:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:44:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 148 @ 14265 updates, score 15.32) (writing took 3.7085171844810247 seconds)
2022-03-04 21:44:59 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 21:44:59 | INFO | train | epoch 148 | loss 0.948 | ppl 1.93 | wps 21429.5 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 14265 | lr 0.000264767 | gnorm 1.024 | loss_scale 32 | train_wall 261 | gb_free 8.2 | wall 43733
2022-03-04 21:44:59 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 21:44:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:46:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:46:46 | INFO | train_inner | epoch 149:     36 / 97 loss=0.948, ppl=1.93, wps=21314.9, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=1.034, loss_scale=16, train_wall=271, gb_free=8.2, wall=43839
2022-03-04 21:49:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:49:52 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 15.277 | ppl 39707 | wps 37369 | wpb 510.9 | bsz 1 | num_updates 14361 | best_loss 7.674
2022-03-04 21:49:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14361 updates
2022-03-04 21:49:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:49:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:49:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 149 @ 14361 updates, score 15.277) (writing took 3.9827721286565065 seconds)
2022-03-04 21:49:56 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-04 21:49:56 | INFO | train | epoch 149 | loss 0.941 | ppl 1.92 | wps 21211.1 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14361 | lr 0.000263881 | gnorm 1.027 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 44029
2022-03-04 21:49:56 | INFO | fairseq.trainer | begin training epoch 150
2022-03-04 21:49:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:51:51 | INFO | train_inner | epoch 150:     39 / 97 loss=0.937, ppl=1.91, wps=21448, ups=0.33, wpb=65495, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=1.024, loss_scale=16, train_wall=269, gb_free=8.2, wall=44145
2022-03-04 21:54:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:54:48 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 15.281 | ppl 39811 | wps 35862.9 | wpb 510.9 | bsz 1 | num_updates 14458 | best_loss 7.674
2022-03-04 21:54:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14458 updates
2022-03-04 21:54:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:54:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:54:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 150 @ 14458 updates, score 15.281) (writing took 4.124680113047361 seconds)
2022-03-04 21:54:53 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-04 21:54:53 | INFO | train | epoch 150 | loss 0.934 | ppl 1.91 | wps 21414.9 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 14458 | lr 0.000262994 | gnorm 1.027 | loss_scale 32 | train_wall 261 | gb_free 8.2 | wall 44326
2022-03-04 21:54:53 | INFO | fairseq.trainer | begin training epoch 151
2022-03-04 21:54:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:56:58 | INFO | train_inner | epoch 151:     42 / 97 loss=0.928, ppl=1.9, wps=21383, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=1.013, loss_scale=32, train_wall=269, gb_free=8.2, wall=44451
2022-03-04 21:57:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:59:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:59:47 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 15.485 | ppl 45850.7 | wps 36627.8 | wpb 510.9 | bsz 1 | num_updates 14554 | best_loss 7.674
2022-03-04 21:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14554 updates
2022-03-04 21:59:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:59:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 21:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 151 @ 14554 updates, score 15.485) (writing took 3.9246901609003544 seconds)
2022-03-04 21:59:51 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-04 21:59:51 | INFO | train | epoch 151 | loss 0.925 | ppl 1.9 | wps 21085.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14554 | lr 0.000262125 | gnorm 1.015 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 44624
2022-03-04 21:59:51 | INFO | fairseq.trainer | begin training epoch 152
2022-03-04 21:59:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:02:08 | INFO | train_inner | epoch 152:     46 / 97 loss=0.922, ppl=1.9, wps=21121.9, ups=0.32, wpb=65495, bsz=127.9, num_updates=14600, lr=0.000261712, gnorm=1.024, loss_scale=16, train_wall=273, gb_free=8.2, wall=44761
2022-03-04 22:04:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:46 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 15.487 | ppl 45928.5 | wps 36945.5 | wpb 510.9 | bsz 1 | num_updates 14651 | best_loss 7.674
2022-03-04 22:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14651 updates
2022-03-04 22:04:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:04:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:04:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 152 @ 14651 updates, score 15.487) (writing took 3.9347641300410032 seconds)
2022-03-04 22:04:50 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-04 22:04:50 | INFO | train | epoch 152 | loss 0.921 | ppl 1.89 | wps 21237.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 14651 | lr 0.000261256 | gnorm 1.032 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 44923
2022-03-04 22:04:50 | INFO | fairseq.trainer | begin training epoch 153
2022-03-04 22:04:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:07:19 | INFO | train_inner | epoch 153:     50 / 97 loss=0.918, ppl=1.89, wps=21058, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=1.041, loss_scale=16, train_wall=274, gb_free=8.2, wall=45072
2022-03-04 22:09:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:09:45 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 15.454 | ppl 44897.1 | wps 36158.2 | wpb 510.9 | bsz 1 | num_updates 14747 | best_loss 7.674
2022-03-04 22:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14747 updates
2022-03-04 22:09:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:09:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:09:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 153 @ 14747 updates, score 15.454) (writing took 3.7073647044599056 seconds)
2022-03-04 22:09:49 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-04 22:09:49 | INFO | train | epoch 153 | loss 0.913 | ppl 1.88 | wps 21027.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14747 | lr 0.000260404 | gnorm 1.031 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 45222
2022-03-04 22:09:49 | INFO | fairseq.trainer | begin training epoch 154
2022-03-04 22:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:12:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:12:29 | INFO | train_inner | epoch 154:     54 / 97 loss=0.91, ppl=1.88, wps=21087.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=1.019, loss_scale=16, train_wall=273, gb_free=8.2, wall=45383
2022-03-04 22:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:14:44 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 15.496 | ppl 46220.2 | wps 36868.9 | wpb 510.9 | bsz 1 | num_updates 14843 | best_loss 7.674
2022-03-04 22:14:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14843 updates
2022-03-04 22:14:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:14:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:14:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 154 @ 14843 updates, score 15.496) (writing took 3.9856119230389595 seconds)
2022-03-04 22:14:48 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-04 22:14:48 | INFO | train | epoch 154 | loss 0.907 | ppl 1.88 | wps 21038.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14843 | lr 0.000259561 | gnorm 1.017 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 45521
2022-03-04 22:14:48 | INFO | fairseq.trainer | begin training epoch 155
2022-03-04 22:14:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:17:37 | INFO | train_inner | epoch 155:     57 / 97 loss=0.906, ppl=1.87, wps=21276, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=1.024, loss_scale=16, train_wall=271, gb_free=8.2, wall=45690
2022-03-04 22:19:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:19:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:19:43 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 15.572 | ppl 48710.8 | wps 36674 | wpb 510.9 | bsz 1 | num_updates 14939 | best_loss 7.674
2022-03-04 22:19:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14939 updates
2022-03-04 22:19:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:19:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 155 @ 14939 updates, score 15.572) (writing took 3.823285259306431 seconds)
2022-03-04 22:19:46 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-04 22:19:46 | INFO | train | epoch 155 | loss 0.902 | ppl 1.87 | wps 21049.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 14939 | lr 0.000258726 | gnorm 1.031 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 45820
2022-03-04 22:19:46 | INFO | fairseq.trainer | begin training epoch 156
2022-03-04 22:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:22:47 | INFO | train_inner | epoch 156:     61 / 97 loss=0.897, ppl=1.86, wps=21134.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=1.017, loss_scale=16, train_wall=273, gb_free=8.2, wall=46000
2022-03-04 22:24:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:24:41 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 15.583 | ppl 49085.1 | wps 35986.1 | wpb 510.9 | bsz 1 | num_updates 15036 | best_loss 7.674
2022-03-04 22:24:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15036 updates
2022-03-04 22:24:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:24:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:24:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 156 @ 15036 updates, score 15.583) (writing took 4.186985144391656 seconds)
2022-03-04 22:24:45 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-04 22:24:45 | INFO | train | epoch 156 | loss 0.896 | ppl 1.86 | wps 21254.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 15036 | lr 0.00025789 | gnorm 1.007 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 46118
2022-03-04 22:24:45 | INFO | fairseq.trainer | begin training epoch 157
2022-03-04 22:24:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:27:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:27:58 | INFO | train_inner | epoch 157:     65 / 97 loss=0.894, ppl=1.86, wps=21056.8, ups=0.32, wpb=65495, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=1.013, loss_scale=16, train_wall=273, gb_free=8.2, wall=46311
2022-03-04 22:29:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:29:40 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 15.627 | ppl 50600.2 | wps 35194.4 | wpb 510.9 | bsz 1 | num_updates 15132 | best_loss 7.674
2022-03-04 22:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15132 updates
2022-03-04 22:29:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:29:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:29:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 157 @ 15132 updates, score 15.627) (writing took 3.7950298823416233 seconds)
2022-03-04 22:29:44 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-04 22:29:44 | INFO | train | epoch 157 | loss 0.891 | ppl 1.85 | wps 21043.8 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15132 | lr 0.00025707 | gnorm 1.016 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 46417
2022-03-04 22:29:44 | INFO | fairseq.trainer | begin training epoch 158
2022-03-04 22:29:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:33:04 | INFO | train_inner | epoch 158:     68 / 97 loss=0.888, ppl=1.85, wps=21420.1, ups=0.33, wpb=65490.8, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=1.016, loss_scale=16, train_wall=269, gb_free=8.2, wall=46617
2022-03-04 22:34:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:34:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:34:36 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 15.58 | ppl 48985.9 | wps 36291.4 | wpb 510.9 | bsz 1 | num_updates 15228 | best_loss 7.674
2022-03-04 22:34:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15228 updates
2022-03-04 22:34:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:34:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:34:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 158 @ 15228 updates, score 15.58) (writing took 3.7040500678122044 seconds)
2022-03-04 22:34:39 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-04 22:34:39 | INFO | train | epoch 158 | loss 0.883 | ppl 1.84 | wps 21290.5 | ups 0.33 | wpb 65491.1 | bsz 127.9 | num_updates 15228 | lr 0.000256259 | gnorm 1.021 | loss_scale 16 | train_wall 260 | gb_free 8.2 | wall 46713
2022-03-04 22:34:39 | INFO | fairseq.trainer | begin training epoch 159
2022-03-04 22:34:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:38:12 | INFO | train_inner | epoch 159:     72 / 97 loss=0.881, ppl=1.84, wps=21261.7, ups=0.32, wpb=65495, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=1.021, loss_scale=16, train_wall=271, gb_free=8.2, wall=46925
2022-03-04 22:39:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:39:32 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 15.649 | ppl 51373.9 | wps 36978.9 | wpb 510.9 | bsz 1 | num_updates 15325 | best_loss 7.674
2022-03-04 22:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15325 updates
2022-03-04 22:39:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:39:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:39:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 159 @ 15325 updates, score 15.649) (writing took 3.7058740239590406 seconds)
2022-03-04 22:39:35 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-04 22:39:35 | INFO | train | epoch 159 | loss 0.879 | ppl 1.84 | wps 21451.7 | ups 0.33 | wpb 65491.6 | bsz 127.9 | num_updates 15325 | lr 0.000255446 | gnorm 1.018 | loss_scale 16 | train_wall 261 | gb_free 8.2 | wall 47009
2022-03-04 22:39:35 | INFO | fairseq.trainer | begin training epoch 160
2022-03-04 22:39:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:41:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:43:21 | INFO | train_inner | epoch 160:     76 / 97 loss=0.873, ppl=1.83, wps=21214.8, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=1.014, loss_scale=16, train_wall=272, gb_free=8.2, wall=47234
2022-03-04 22:44:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:44:29 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 15.646 | ppl 51272.7 | wps 35912.5 | wpb 510.9 | bsz 1 | num_updates 15421 | best_loss 7.674
2022-03-04 22:44:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15421 updates
2022-03-04 22:44:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:44:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 160 @ 15421 updates, score 15.646) (writing took 3.7602614127099514 seconds)
2022-03-04 22:44:33 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-04 22:44:33 | INFO | train | epoch 160 | loss 0.872 | ppl 1.83 | wps 21111 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15421 | lr 0.00025465 | gnorm 1.009 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 47306
2022-03-04 22:44:33 | INFO | fairseq.trainer | begin training epoch 161
2022-03-04 22:44:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:48:28 | INFO | train_inner | epoch 161:     79 / 97 loss=0.87, ppl=1.83, wps=21315.8, ups=0.33, wpb=65492.9, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=1.013, loss_scale=32, train_wall=270, gb_free=8.2, wall=47541
2022-03-04 22:48:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:49:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:49:27 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 15.663 | ppl 51871.2 | wps 35613.6 | wpb 510.9 | bsz 1 | num_updates 15517 | best_loss 7.674
2022-03-04 22:49:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15517 updates
2022-03-04 22:49:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:49:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:49:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 161 @ 15517 updates, score 15.663) (writing took 3.9551554806530476 seconds)
2022-03-04 22:49:31 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-04 22:49:31 | INFO | train | epoch 161 | loss 0.868 | ppl 1.82 | wps 21082.9 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15517 | lr 0.000253861 | gnorm 1.016 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 47605
2022-03-04 22:49:31 | INFO | fairseq.trainer | begin training epoch 162
2022-03-04 22:49:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:53:39 | INFO | train_inner | epoch 162:     83 / 97 loss=0.863, ppl=1.82, wps=21080.3, ups=0.32, wpb=65495, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=1.014, loss_scale=16, train_wall=273, gb_free=8.2, wall=47852
2022-03-04 22:54:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:54:26 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 15.69 | ppl 52860.5 | wps 35295.5 | wpb 510.9 | bsz 1 | num_updates 15614 | best_loss 7.674
2022-03-04 22:54:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15614 updates
2022-03-04 22:54:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:54:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:54:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 162 @ 15614 updates, score 15.69) (writing took 3.6673054564744234 seconds)
2022-03-04 22:54:30 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-04 22:54:30 | INFO | train | epoch 162 | loss 0.861 | ppl 1.82 | wps 21279.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 15614 | lr 0.000253071 | gnorm 1.013 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 47903
2022-03-04 22:54:30 | INFO | fairseq.trainer | begin training epoch 163
2022-03-04 22:54:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:58:49 | INFO | train_inner | epoch 163:     87 / 97 loss=0.859, ppl=1.81, wps=21107.9, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=15700, lr=0.000252377, gnorm=1.016, loss_scale=16, train_wall=273, gb_free=8.2, wall=48162
2022-03-04 22:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:59:25 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 15.697 | ppl 53112.4 | wps 35270.6 | wpb 510.9 | bsz 1 | num_updates 15710 | best_loss 7.674
2022-03-04 22:59:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15710 updates
2022-03-04 22:59:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:59:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 22:59:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 163 @ 15710 updates, score 15.697) (writing took 3.815486775711179 seconds)
2022-03-04 22:59:28 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-04 22:59:28 | INFO | train | epoch 163 | loss 0.856 | ppl 1.81 | wps 21073 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15710 | lr 0.000252297 | gnorm 1.011 | loss_scale 16 | train_wall 262 | gb_free 8.2 | wall 48202
2022-03-04 22:59:28 | INFO | fairseq.trainer | begin training epoch 164
2022-03-04 22:59:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:03:57 | INFO | train_inner | epoch 164:     90 / 97 loss=0.851, ppl=1.8, wps=21260.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=15800, lr=0.000251577, gnorm=0.992, loss_scale=32, train_wall=271, gb_free=8.2, wall=48470
2022-03-04 23:04:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:04:24 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 15.745 | ppl 54906.2 | wps 36357.3 | wpb 510.9 | bsz 1 | num_updates 15807 | best_loss 7.674
2022-03-04 23:04:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15807 updates
2022-03-04 23:04:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:04:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 164 @ 15807 updates, score 15.745) (writing took 3.6037229858338833 seconds)
2022-03-04 23:04:27 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-04 23:04:27 | INFO | train | epoch 164 | loss 0.85 | ppl 1.8 | wps 21258.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 15807 | lr 0.000251522 | gnorm 0.995 | loss_scale 32 | train_wall 263 | gb_free 8.2 | wall 48500
2022-03-04 23:04:27 | INFO | fairseq.trainer | begin training epoch 165
2022-03-04 23:04:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:06:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:09:07 | INFO | train_inner | epoch 165:     94 / 97 loss=0.848, ppl=1.8, wps=21109.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=15900, lr=0.000250785, gnorm=1.009, loss_scale=16, train_wall=273, gb_free=8.2, wall=48780
2022-03-04 23:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:09:22 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 15.807 | ppl 57338.2 | wps 36205.1 | wpb 510.9 | bsz 1 | num_updates 15903 | best_loss 7.674
2022-03-04 23:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15903 updates
2022-03-04 23:09:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:09:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:09:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 165 @ 15903 updates, score 15.807) (writing took 4.059150900691748 seconds)
2022-03-04 23:09:26 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-04 23:09:26 | INFO | train | epoch 165 | loss 0.845 | ppl 1.8 | wps 21034.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15903 | lr 0.000250761 | gnorm 1.007 | loss_scale 16 | train_wall 263 | gb_free 8.2 | wall 48799
2022-03-04 23:09:26 | INFO | fairseq.trainer | begin training epoch 166
2022-03-04 23:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:13:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:14:23 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 15.764 | ppl 55663.8 | wps 35025.7 | wpb 510.9 | bsz 1 | num_updates 15999 | best_loss 7.674
2022-03-04 23:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15999 updates
2022-03-04 23:14:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:14:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:14:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 166 @ 15999 updates, score 15.764) (writing took 3.7479506451636553 seconds)
2022-03-04 23:14:26 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-04 23:14:26 | INFO | train | epoch 166 | loss 0.839 | ppl 1.79 | wps 20940.6 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 15999 | lr 0.000250008 | gnorm 1.008 | loss_scale 16 | train_wall 264 | gb_free 8.2 | wall 49100
2022-03-04 23:14:26 | INFO | fairseq.trainer | begin training epoch 167
2022-03-04 23:14:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:14:29 | INFO | train_inner | epoch 167:      1 / 97 loss=0.84, ppl=1.79, wps=20308.3, ups=0.31, wpb=65451.9, bsz=127.8, num_updates=16000, lr=0.00025, gnorm=1.008, loss_scale=16, train_wall=275, gb_free=8.2, wall=49103
2022-03-04 23:19:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:19:28 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 15.857 | ppl 59367.2 | wps 34884.3 | wpb 510.9 | bsz 1 | num_updates 16096 | best_loss 7.674
2022-03-04 23:19:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16096 updates
2022-03-04 23:19:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:19:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:19:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 167 @ 16096 updates, score 15.857) (writing took 3.8196821808815002 seconds)
2022-03-04 23:19:32 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-04 23:19:32 | INFO | train | epoch 167 | loss 0.836 | ppl 1.78 | wps 20781.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 16096 | lr 0.000249253 | gnorm 1.005 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 49405
2022-03-04 23:19:32 | INFO | fairseq.trainer | begin training epoch 168
2022-03-04 23:19:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:19:44 | INFO | train_inner | epoch 168:      4 / 97 loss=0.835, ppl=1.78, wps=20803.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=1.004, loss_scale=16, train_wall=277, gb_free=8.2, wall=49418
2022-03-04 23:21:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:24:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:24:34 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 15.835 | ppl 58466.6 | wps 34926.9 | wpb 510.9 | bsz 1 | num_updates 16192 | best_loss 7.674
2022-03-04 23:24:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16192 updates
2022-03-04 23:24:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:24:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:24:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 168 @ 16192 updates, score 15.835) (writing took 3.762688636779785 seconds)
2022-03-04 23:24:38 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-04 23:24:38 | INFO | train | epoch 168 | loss 0.831 | ppl 1.78 | wps 20568.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16192 | lr 0.000248513 | gnorm 1.01 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 49711
2022-03-04 23:24:38 | INFO | fairseq.trainer | begin training epoch 169
2022-03-04 23:24:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:25:02 | INFO | train_inner | epoch 169:      8 / 97 loss=0.828, ppl=1.78, wps=20611.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=1.009, loss_scale=16, train_wall=280, gb_free=8.2, wall=49735
2022-03-04 23:29:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:29:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:29:40 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 15.881 | ppl 60352.9 | wps 34501.1 | wpb 510.9 | bsz 1 | num_updates 16288 | best_loss 7.674
2022-03-04 23:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16288 updates
2022-03-04 23:29:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:29:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:29:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 169 @ 16288 updates, score 15.881) (writing took 3.6725467946380377 seconds)
2022-03-04 23:29:44 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-04 23:29:44 | INFO | train | epoch 169 | loss 0.823 | ppl 1.77 | wps 20528.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16288 | lr 0.00024778 | gnorm 0.998 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 50017
2022-03-04 23:29:44 | INFO | fairseq.trainer | begin training epoch 170
2022-03-04 23:29:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:30:21 | INFO | train_inner | epoch 170:     12 / 97 loss=0.821, ppl=1.77, wps=20554.6, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=1.004, loss_scale=16, train_wall=280, gb_free=8.2, wall=50054
2022-03-04 23:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:34:47 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 15.848 | ppl 58979.9 | wps 34301.4 | wpb 510.9 | bsz 1 | num_updates 16385 | best_loss 7.674
2022-03-04 23:34:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16385 updates
2022-03-04 23:34:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:34:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:34:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 170 @ 16385 updates, score 15.848) (writing took 3.6851019244641066 seconds)
2022-03-04 23:34:50 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-04 23:34:50 | INFO | train | epoch 170 | loss 0.821 | ppl 1.77 | wps 20738.9 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 16385 | lr 0.000247045 | gnorm 1.016 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 50324
2022-03-04 23:34:50 | INFO | fairseq.trainer | begin training epoch 171
2022-03-04 23:34:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:35:36 | INFO | train_inner | epoch 171:     15 / 97 loss=0.822, ppl=1.77, wps=20756.4, ups=0.32, wpb=65495, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=1.009, loss_scale=16, train_wall=277, gb_free=8.2, wall=50369
2022-03-04 23:37:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:39:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:39:53 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 16.014 | ppl 66172 | wps 34388.7 | wpb 510.9 | bsz 1 | num_updates 16481 | best_loss 7.674
2022-03-04 23:39:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16481 updates
2022-03-04 23:39:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:39:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:39:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 171 @ 16481 updates, score 16.014) (writing took 3.7544380091130733 seconds)
2022-03-04 23:39:57 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-04 23:39:57 | INFO | train | epoch 171 | loss 0.815 | ppl 1.76 | wps 20492.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16481 | lr 0.000246325 | gnorm 0.995 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 50630
2022-03-04 23:39:57 | INFO | fairseq.trainer | begin training epoch 172
2022-03-04 23:39:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:40:55 | INFO | train_inner | epoch 172:     19 / 97 loss=0.81, ppl=1.75, wps=20529.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=0.989, loss_scale=16, train_wall=280, gb_free=8.2, wall=50688
2022-03-04 23:44:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:45:00 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 15.935 | ppl 62651.4 | wps 34093.4 | wpb 510.9 | bsz 1 | num_updates 16578 | best_loss 7.674
2022-03-04 23:45:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16578 updates
2022-03-04 23:45:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 172 @ 16578 updates, score 15.935) (writing took 3.7341866213828325 seconds)
2022-03-04 23:45:04 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-04 23:45:04 | INFO | train | epoch 172 | loss 0.81 | ppl 1.75 | wps 20708 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 16578 | lr 0.000245603 | gnorm 0.991 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 50937
2022-03-04 23:45:04 | INFO | fairseq.trainer | begin training epoch 173
2022-03-04 23:45:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:45:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:46:14 | INFO | train_inner | epoch 173:     23 / 97 loss=0.81, ppl=1.75, wps=20516, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=0.994, loss_scale=16, train_wall=280, gb_free=8.2, wall=51008
2022-03-04 23:50:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:50:07 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 15.946 | ppl 63146.7 | wps 34211.1 | wpb 510.9 | bsz 1 | num_updates 16674 | best_loss 7.674
2022-03-04 23:50:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16674 updates
2022-03-04 23:50:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:50:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:50:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 173 @ 16674 updates, score 15.946) (writing took 3.760072059929371 seconds)
2022-03-04 23:50:11 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-04 23:50:11 | INFO | train | epoch 173 | loss 0.806 | ppl 1.75 | wps 20475.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16674 | lr 0.000244895 | gnorm 1 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 51244
2022-03-04 23:50:11 | INFO | fairseq.trainer | begin training epoch 174
2022-03-04 23:50:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:51:31 | INFO | train_inner | epoch 174:     26 / 97 loss=0.804, ppl=1.75, wps=20720.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=0.999, loss_scale=16, train_wall=278, gb_free=8.2, wall=51324
2022-03-04 23:52:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:55:12 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 16.022 | ppl 66536.1 | wps 34628.2 | wpb 510.9 | bsz 1 | num_updates 16770 | best_loss 7.674
2022-03-04 23:55:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16770 updates
2022-03-04 23:55:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:55:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-04 23:55:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 174 @ 16770 updates, score 16.022) (writing took 3.738423351198435 seconds)
2022-03-04 23:55:16 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-04 23:55:16 | INFO | train | epoch 174 | loss 0.801 | ppl 1.74 | wps 20610.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16770 | lr 0.000244193 | gnorm 0.995 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 51549
2022-03-04 23:55:16 | INFO | fairseq.trainer | begin training epoch 175
2022-03-04 23:55:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:56:48 | INFO | train_inner | epoch 175:     30 / 97 loss=0.796, ppl=1.74, wps=20656.5, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=0.99, loss_scale=16, train_wall=279, gb_free=8.2, wall=51641
2022-03-05 00:00:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:00:17 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 16.035 | ppl 67131.3 | wps 34343 | wpb 510.9 | bsz 1 | num_updates 16866 | best_loss 7.674
2022-03-05 00:00:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16866 updates
2022-03-05 00:00:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:00:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:00:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 175 @ 16866 updates, score 16.035) (writing took 3.7440138440579176 seconds)
2022-03-05 00:00:21 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-05 00:00:21 | INFO | train | epoch 175 | loss 0.796 | ppl 1.74 | wps 20606 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 0.985 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 51854
2022-03-05 00:00:21 | INFO | fairseq.trainer | begin training epoch 176
2022-03-05 00:00:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:02:05 | INFO | train_inner | epoch 176:     34 / 97 loss=0.795, ppl=1.74, wps=20624.5, ups=0.31, wpb=65495, bsz=127.9, num_updates=16900, lr=0.000243252, gnorm=0.987, loss_scale=16, train_wall=279, gb_free=8.2, wall=51958
2022-03-05 00:05:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:05:24 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 16.026 | ppl 66711.4 | wps 34103.1 | wpb 510.9 | bsz 1 | num_updates 16963 | best_loss 7.674
2022-03-05 00:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16963 updates
2022-03-05 00:05:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:05:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:05:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 176 @ 16963 updates, score 16.026) (writing took 3.7175239827483892 seconds)
2022-03-05 00:05:28 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-05 00:05:28 | INFO | train | epoch 176 | loss 0.791 | ppl 1.73 | wps 20727.7 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 16963 | lr 0.0002428 | gnorm 0.981 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 52161
2022-03-05 00:05:28 | INFO | fairseq.trainer | begin training epoch 177
2022-03-05 00:05:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:07:21 | INFO | train_inner | epoch 177:     37 / 97 loss=0.792, ppl=1.73, wps=20720.4, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=0.988, loss_scale=32, train_wall=278, gb_free=8.2, wall=52274
2022-03-05 00:08:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:10:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:10:32 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 16.087 | ppl 69627.5 | wps 34386.9 | wpb 510.9 | bsz 1 | num_updates 17059 | best_loss 7.674
2022-03-05 00:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17059 updates
2022-03-05 00:10:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:10:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:10:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 177 @ 17059 updates, score 16.087) (writing took 3.7394342105835676 seconds)
2022-03-05 00:10:35 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-05 00:10:35 | INFO | train | epoch 177 | loss 0.788 | ppl 1.73 | wps 20425.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17059 | lr 0.000242116 | gnorm 0.994 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 52469
2022-03-05 00:10:35 | INFO | fairseq.trainer | begin training epoch 178
2022-03-05 00:10:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:12:41 | INFO | train_inner | epoch 178:     41 / 97 loss=0.785, ppl=1.72, wps=20463.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=0.984, loss_scale=16, train_wall=281, gb_free=8.2, wall=52595
2022-03-05 00:15:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:15:39 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 16.154 | ppl 72920.2 | wps 34400.9 | wpb 510.9 | bsz 1 | num_updates 17156 | best_loss 7.674
2022-03-05 00:15:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17156 updates
2022-03-05 00:15:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 178 @ 17156 updates, score 16.154) (writing took 3.7815429773181677 seconds)
2022-03-05 00:15:43 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-05 00:15:43 | INFO | train | epoch 178 | loss 0.784 | ppl 1.72 | wps 20635.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 17156 | lr 0.00024143 | gnorm 0.98 | loss_scale 32 | train_wall 270 | gb_free 8.2 | wall 52776
2022-03-05 00:15:43 | INFO | fairseq.trainer | begin training epoch 179
2022-03-05 00:15:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:17:58 | INFO | train_inner | epoch 179:     44 / 97 loss=0.783, ppl=1.72, wps=20667.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=0.98, loss_scale=32, train_wall=278, gb_free=8.2, wall=52911
2022-03-05 00:18:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:20:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:20:47 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 16.198 | ppl 75158.2 | wps 34037.8 | wpb 510.9 | bsz 1 | num_updates 17252 | best_loss 7.674
2022-03-05 00:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17252 updates
2022-03-05 00:20:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:20:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:20:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 179 @ 17252 updates, score 16.198) (writing took 3.7239587604999542 seconds)
2022-03-05 00:20:51 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-05 00:20:51 | INFO | train | epoch 179 | loss 0.778 | ppl 1.71 | wps 20415.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17252 | lr 0.000240758 | gnorm 0.986 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 53084
2022-03-05 00:20:51 | INFO | fairseq.trainer | begin training epoch 180
2022-03-05 00:20:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:23:19 | INFO | train_inner | epoch 180:     48 / 97 loss=0.775, ppl=1.71, wps=20428.2, ups=0.31, wpb=65495, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=0.989, loss_scale=16, train_wall=282, gb_free=8.2, wall=53232
2022-03-05 00:25:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:25:56 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 16.176 | ppl 74048.6 | wps 34329.7 | wpb 510.9 | bsz 1 | num_updates 17348 | best_loss 7.674
2022-03-05 00:25:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17348 updates
2022-03-05 00:25:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:25:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:25:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 180 @ 17348 updates, score 16.176) (writing took 3.746310021728277 seconds)
2022-03-05 00:25:59 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-05 00:25:59 | INFO | train | epoch 180 | loss 0.776 | ppl 1.71 | wps 20401.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17348 | lr 0.000240091 | gnorm 0.988 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 53393
2022-03-05 00:25:59 | INFO | fairseq.trainer | begin training epoch 181
2022-03-05 00:25:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:28:39 | INFO | train_inner | epoch 181:     52 / 97 loss=0.771, ppl=1.71, wps=20442.1, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=0.979, loss_scale=16, train_wall=282, gb_free=8.2, wall=53552
2022-03-05 00:30:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:31:04 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 16.144 | ppl 72402.8 | wps 34311.5 | wpb 510.9 | bsz 1 | num_updates 17445 | best_loss 7.674
2022-03-05 00:31:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17445 updates
2022-03-05 00:31:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:31:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:31:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 181 @ 17445 updates, score 16.144) (writing took 3.764025965705514 seconds)
2022-03-05 00:31:07 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-05 00:31:07 | INFO | train | epoch 181 | loss 0.77 | ppl 1.71 | wps 20615.2 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 17445 | lr 0.000239422 | gnorm 0.977 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 53701
2022-03-05 00:31:08 | INFO | fairseq.trainer | begin training epoch 182
2022-03-05 00:31:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:33:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:33:59 | INFO | train_inner | epoch 182:     56 / 97 loss=0.771, ppl=1.71, wps=20468.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=0.984, loss_scale=16, train_wall=281, gb_free=8.2, wall=53872
2022-03-05 00:36:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:36:11 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 16.174 | ppl 73923.2 | wps 34081.3 | wpb 510.9 | bsz 1 | num_updates 17541 | best_loss 7.674
2022-03-05 00:36:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17541 updates
2022-03-05 00:36:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:36:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 182 @ 17541 updates, score 16.174) (writing took 3.772570703178644 seconds)
2022-03-05 00:36:15 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-05 00:36:15 | INFO | train | epoch 182 | loss 0.766 | ppl 1.7 | wps 20439.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17541 | lr 0.000238766 | gnorm 0.987 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 54008
2022-03-05 00:36:15 | INFO | fairseq.trainer | begin training epoch 183
2022-03-05 00:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:39:15 | INFO | train_inner | epoch 183:     59 / 97 loss=0.764, ppl=1.7, wps=20704.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=0.985, loss_scale=16, train_wall=278, gb_free=8.2, wall=54189
2022-03-05 00:41:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:41:18 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 16.273 | ppl 79181.3 | wps 34181.5 | wpb 510.9 | bsz 1 | num_updates 17638 | best_loss 7.674
2022-03-05 00:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17638 updates
2022-03-05 00:41:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:41:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:41:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 183 @ 17638 updates, score 16.273) (writing took 3.7597281374037266 seconds)
2022-03-05 00:41:22 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-05 00:41:22 | INFO | train | epoch 183 | loss 0.762 | ppl 1.7 | wps 20715.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 17638 | lr 0.000238109 | gnorm 0.989 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 54315
2022-03-05 00:41:22 | INFO | fairseq.trainer | begin training epoch 184
2022-03-05 00:41:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:41:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:44:34 | INFO | train_inner | epoch 184:     63 / 97 loss=0.758, ppl=1.69, wps=20565, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.987, loss_scale=16, train_wall=280, gb_free=8.2, wall=54507
2022-03-05 00:46:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:46:24 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 16.286 | ppl 79915.9 | wps 34327.7 | wpb 510.9 | bsz 1 | num_updates 17734 | best_loss 7.674
2022-03-05 00:46:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17734 updates
2022-03-05 00:46:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:46:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 184 @ 17734 updates, score 16.286) (writing took 3.756024507805705 seconds)
2022-03-05 00:46:28 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-05 00:46:28 | INFO | train | epoch 184 | loss 0.756 | ppl 1.69 | wps 20559.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17734 | lr 0.000237463 | gnorm 0.973 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 54621
2022-03-05 00:46:28 | INFO | fairseq.trainer | begin training epoch 185
2022-03-05 00:46:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:49:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:49:52 | INFO | train_inner | epoch 185:     67 / 97 loss=0.756, ppl=1.69, wps=20581, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=0.971, loss_scale=16, train_wall=280, gb_free=8.2, wall=54825
2022-03-05 00:51:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:51:30 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 16.24 | ppl 77378.4 | wps 34482.4 | wpb 510.9 | bsz 1 | num_updates 17830 | best_loss 7.674
2022-03-05 00:51:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17830 updates
2022-03-05 00:51:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:51:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:51:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 185 @ 17830 updates, score 16.24) (writing took 3.8170809149742126 seconds)
2022-03-05 00:51:34 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-05 00:51:34 | INFO | train | epoch 185 | loss 0.754 | ppl 1.69 | wps 20535.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17830 | lr 0.000236823 | gnorm 0.978 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 54927
2022-03-05 00:51:34 | INFO | fairseq.trainer | begin training epoch 186
2022-03-05 00:51:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:55:07 | INFO | train_inner | epoch 186:     70 / 97 loss=0.752, ppl=1.68, wps=20779.9, ups=0.32, wpb=65495, bsz=127.9, num_updates=17900, lr=0.00023636, gnorm=0.981, loss_scale=16, train_wall=277, gb_free=8.2, wall=55141
2022-03-05 00:56:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:56:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:56:35 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 16.28 | ppl 79601 | wps 35421.7 | wpb 510.9 | bsz 1 | num_updates 17926 | best_loss 7.674
2022-03-05 00:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17926 updates
2022-03-05 00:56:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:56:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 00:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 186 @ 17926 updates, score 16.28) (writing took 3.700523316860199 seconds)
2022-03-05 00:56:39 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-05 00:56:39 | INFO | train | epoch 186 | loss 0.749 | ppl 1.68 | wps 20603.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 17926 | lr 0.000236188 | gnorm 0.976 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 55232
2022-03-05 00:56:39 | INFO | fairseq.trainer | begin training epoch 187
2022-03-05 00:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:00:24 | INFO | train_inner | epoch 187:     74 / 97 loss=0.745, ppl=1.68, wps=20650.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=0.97, loss_scale=16, train_wall=279, gb_free=8.2, wall=55458
2022-03-05 01:01:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:01:41 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 16.232 | ppl 76992.6 | wps 34664.9 | wpb 510.9 | bsz 1 | num_updates 18023 | best_loss 7.674
2022-03-05 01:01:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18023 updates
2022-03-05 01:01:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:01:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 187 @ 18023 updates, score 16.232) (writing took 3.754925709217787 seconds)
2022-03-05 01:01:44 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-05 01:01:44 | INFO | train | epoch 187 | loss 0.745 | ppl 1.68 | wps 20796 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 18023 | lr 0.000235552 | gnorm 0.971 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 55538
2022-03-05 01:01:44 | INFO | fairseq.trainer | begin training epoch 188
2022-03-05 01:01:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:04:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:05:42 | INFO | train_inner | epoch 188:     78 / 97 loss=0.745, ppl=1.68, wps=20625.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.97, loss_scale=16, train_wall=280, gb_free=8.2, wall=55775
2022-03-05 01:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:06:46 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 16.299 | ppl 80602.6 | wps 35090.4 | wpb 510.9 | bsz 1 | num_updates 18119 | best_loss 7.674
2022-03-05 01:06:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18119 updates
2022-03-05 01:06:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:06:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:06:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 188 @ 18119 updates, score 16.299) (writing took 3.750351754948497 seconds)
2022-03-05 01:06:49 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-05 01:06:49 | INFO | train | epoch 188 | loss 0.742 | ppl 1.67 | wps 20613.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18119 | lr 0.000234927 | gnorm 0.963 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 55843
2022-03-05 01:06:49 | INFO | fairseq.trainer | begin training epoch 189
2022-03-05 01:06:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:10:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:11:00 | INFO | train_inner | epoch 189:     82 / 97 loss=0.74, ppl=1.67, wps=20595.8, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.969, loss_scale=16, train_wall=280, gb_free=8.2, wall=56093
2022-03-05 01:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:11:52 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 16.258 | ppl 78345.4 | wps 34444 | wpb 510.9 | bsz 1 | num_updates 18215 | best_loss 7.674
2022-03-05 01:11:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18215 updates
2022-03-05 01:11:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:11:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 189 @ 18215 updates, score 16.258) (writing took 3.7887862008064985 seconds)
2022-03-05 01:11:56 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 01:11:56 | INFO | train | epoch 189 | loss 0.74 | ppl 1.67 | wps 20519.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18215 | lr 0.000234307 | gnorm 0.974 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 56149
2022-03-05 01:11:56 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 01:11:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:16:15 | INFO | train_inner | epoch 190:     85 / 97 loss=0.737, ppl=1.67, wps=20790, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=18300, lr=0.000233762, gnorm=0.971, loss_scale=16, train_wall=277, gb_free=8.2, wall=56408
2022-03-05 01:16:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:16:57 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 16.34 | ppl 82964.5 | wps 34847.2 | wpb 510.9 | bsz 1 | num_updates 18312 | best_loss 7.674
2022-03-05 01:16:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18312 updates
2022-03-05 01:16:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:17:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 190 @ 18312 updates, score 16.34) (writing took 3.624623203650117 seconds)
2022-03-05 01:17:01 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 01:17:01 | INFO | train | epoch 190 | loss 0.734 | ppl 1.66 | wps 20804 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 18312 | lr 0.000233686 | gnorm 0.968 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 56454
2022-03-05 01:17:01 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 01:17:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:19:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:21:31 | INFO | train_inner | epoch 191:     89 / 97 loss=0.732, ppl=1.66, wps=20698, ups=0.32, wpb=65495, bsz=127.9, num_updates=18400, lr=0.000233126, gnorm=0.967, loss_scale=16, train_wall=279, gb_free=8.2, wall=56725
2022-03-05 01:21:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:22:02 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 16.435 | ppl 88569.5 | wps 34742.3 | wpb 510.9 | bsz 1 | num_updates 18408 | best_loss 7.674
2022-03-05 01:22:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18408 updates
2022-03-05 01:22:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:22:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:22:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 191 @ 18408 updates, score 16.435) (writing took 3.747553799301386 seconds)
2022-03-05 01:22:06 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 01:22:06 | INFO | train | epoch 191 | loss 0.73 | ppl 1.66 | wps 20641.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 18408 | lr 0.000233076 | gnorm 0.97 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 56759
2022-03-05 01:22:06 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 01:22:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:26:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:26:50 | INFO | train_inner | epoch 192:     93 / 97 loss=0.727, ppl=1.66, wps=20536.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=18500, lr=0.000232495, gnorm=0.973, loss_scale=16, train_wall=280, gb_free=8.2, wall=57044
2022-03-05 01:27:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:27:09 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 16.332 | ppl 82499.3 | wps 34230.1 | wpb 510.9 | bsz 1 | num_updates 18504 | best_loss 7.674
2022-03-05 01:27:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18504 updates
2022-03-05 01:27:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:27:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:27:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 192 @ 18504 updates, score 16.332) (writing took 3.713365910574794 seconds)
2022-03-05 01:27:13 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 01:27:13 | INFO | train | epoch 192 | loss 0.726 | ppl 1.65 | wps 20489.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18504 | lr 0.00023247 | gnorm 0.969 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 57066
2022-03-05 01:27:13 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 01:27:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:32:08 | INFO | train_inner | epoch 193:     96 / 97 loss=0.725, ppl=1.65, wps=20622.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=18600, lr=0.000231869, gnorm=0.967, loss_scale=16, train_wall=279, gb_free=8.2, wall=57361
2022-03-05 01:32:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:32:17 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 16.436 | ppl 88679.7 | wps 33879.7 | wpb 510.9 | bsz 1 | num_updates 18601 | best_loss 7.674
2022-03-05 01:32:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18601 updates
2022-03-05 01:32:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:32:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:32:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 193 @ 18601 updates, score 16.436) (writing took 3.7756195552647114 seconds)
2022-03-05 01:32:21 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 01:32:21 | INFO | train | epoch 193 | loss 0.724 | ppl 1.65 | wps 20593.3 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 18601 | lr 0.000231863 | gnorm 0.967 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 57374
2022-03-05 01:32:21 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 01:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:37:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:37:25 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 16.451 | ppl 89575.9 | wps 34107.7 | wpb 510.9 | bsz 1 | num_updates 18697 | best_loss 7.674
2022-03-05 01:37:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18697 updates
2022-03-05 01:37:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:37:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 194 @ 18697 updates, score 16.451) (writing took 3.823676588013768 seconds)
2022-03-05 01:37:29 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 01:37:29 | INFO | train | epoch 194 | loss 0.719 | ppl 1.65 | wps 20420.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18697 | lr 0.000231267 | gnorm 0.964 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 57682
2022-03-05 01:37:29 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 01:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:37:38 | INFO | train_inner | epoch 195:      3 / 97 loss=0.718, ppl=1.65, wps=19813.3, ups=0.3, wpb=65451.9, bsz=127.8, num_updates=18700, lr=0.000231249, gnorm=0.964, loss_scale=16, train_wall=281, gb_free=8.2, wall=57691
2022-03-05 01:41:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:42:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:42:33 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 16.489 | ppl 91962.1 | wps 33892.9 | wpb 510.9 | bsz 1 | num_updates 18793 | best_loss 7.674
2022-03-05 01:42:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18793 updates
2022-03-05 01:42:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:42:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 195 @ 18793 updates, score 16.489) (writing took 3.7892067413777113 seconds)
2022-03-05 01:42:37 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 01:42:37 | INFO | train | epoch 195 | loss 0.716 | ppl 1.64 | wps 20419.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18793 | lr 0.000230676 | gnorm 0.97 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 57990
2022-03-05 01:42:37 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 01:42:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:42:58 | INFO | train_inner | epoch 196:      7 / 97 loss=0.715, ppl=1.64, wps=20450.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.97, loss_scale=16, train_wall=281, gb_free=8.2, wall=58012
2022-03-05 01:47:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:47:41 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 16.403 | ppl 86658.2 | wps 34002.2 | wpb 510.9 | bsz 1 | num_updates 18890 | best_loss 7.674
2022-03-05 01:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18890 updates
2022-03-05 01:47:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:47:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:47:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 196 @ 18890 updates, score 16.403) (writing took 3.674271984025836 seconds)
2022-03-05 01:47:44 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 01:47:44 | INFO | train | epoch 196 | loss 0.713 | ppl 1.64 | wps 20661 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 18890 | lr 0.000230083 | gnorm 0.975 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 58298
2022-03-05 01:47:44 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 01:47:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:48:15 | INFO | train_inner | epoch 197:     10 / 97 loss=0.712, ppl=1.64, wps=20683.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.976, loss_scale=32, train_wall=278, gb_free=8.2, wall=58328
2022-03-05 01:49:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:52:49 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 16.47 | ppl 90771.1 | wps 34237.4 | wpb 510.9 | bsz 1 | num_updates 18986 | best_loss 7.674
2022-03-05 01:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18986 updates
2022-03-05 01:52:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:52:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 197 @ 18986 updates, score 16.47) (writing took 3.816407660022378 seconds)
2022-03-05 01:52:52 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 01:52:52 | INFO | train | epoch 197 | loss 0.707 | ppl 1.63 | wps 20409.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 18986 | lr 0.0002295 | gnorm 0.956 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 58606
2022-03-05 01:52:52 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 01:52:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:53:35 | INFO | train_inner | epoch 198:     14 / 97 loss=0.705, ppl=1.63, wps=20442.3, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.957, loss_scale=16, train_wall=282, gb_free=8.2, wall=58649
2022-03-05 01:57:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:57:56 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 16.611 | ppl 100097 | wps 34374.7 | wpb 510.9 | bsz 1 | num_updates 19082 | best_loss 7.674
2022-03-05 01:57:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19082 updates
2022-03-05 01:57:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:57:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 01:57:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 198 @ 19082 updates, score 16.611) (writing took 3.7296426612883806 seconds)
2022-03-05 01:57:59 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 01:57:59 | INFO | train | epoch 198 | loss 0.706 | ppl 1.63 | wps 20474.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19082 | lr 0.000228922 | gnorm 0.966 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 58913
2022-03-05 01:57:59 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 01:57:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:58:54 | INFO | train_inner | epoch 199:     18 / 97 loss=0.705, ppl=1.63, wps=20530.9, ups=0.31, wpb=65495, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.963, loss_scale=16, train_wall=280, gb_free=8.2, wall=58968
2022-03-05 02:02:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:03:03 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 16.528 | ppl 94477 | wps 34786 | wpb 510.9 | bsz 1 | num_updates 19179 | best_loss 7.674
2022-03-05 02:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19179 updates
2022-03-05 02:03:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:03:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 199 @ 19179 updates, score 16.528) (writing took 3.7250032480806112 seconds)
2022-03-05 02:03:06 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 02:03:06 | INFO | train | epoch 199 | loss 0.702 | ppl 1.63 | wps 20701.7 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 19179 | lr 0.000228343 | gnorm 0.954 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 59220
2022-03-05 02:03:06 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 02:03:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:10 | INFO | train_inner | epoch 200:     21 / 97 loss=0.699, ppl=1.62, wps=20735.2, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.952, loss_scale=32, train_wall=278, gb_free=8.2, wall=59284
2022-03-05 02:06:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:08:09 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 16.603 | ppl 99513.6 | wps 34642.5 | wpb 510.9 | bsz 1 | num_updates 19275 | best_loss 7.674
2022-03-05 02:08:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19275 updates
2022-03-05 02:08:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:08:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:08:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 200 @ 19275 updates, score 16.603) (writing took 3.816868305206299 seconds)
2022-03-05 02:08:13 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 02:08:13 | INFO | train | epoch 200 | loss 0.699 | ppl 1.62 | wps 20509.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19275 | lr 0.000227773 | gnorm 0.957 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 59526
2022-03-05 02:08:13 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 02:08:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:09:29 | INFO | train_inner | epoch 201:     25 / 97 loss=0.699, ppl=1.62, wps=20537.4, ups=0.31, wpb=65495, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.955, loss_scale=16, train_wall=280, gb_free=8.2, wall=59603
2022-03-05 02:13:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:13:16 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 16.564 | ppl 96857 | wps 34495.6 | wpb 510.9 | bsz 1 | num_updates 19372 | best_loss 7.674
2022-03-05 02:13:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19372 updates
2022-03-05 02:13:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:13:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 201 @ 19372 updates, score 16.564) (writing took 3.8063790686428547 seconds)
2022-03-05 02:13:20 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 02:13:20 | INFO | train | epoch 201 | loss 0.697 | ppl 1.62 | wps 20698.4 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 19372 | lr 0.000227202 | gnorm 0.955 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 59833
2022-03-05 02:13:20 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 02:13:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:13:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:14:48 | INFO | train_inner | epoch 202:     29 / 97 loss=0.694, ppl=1.62, wps=20523.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.956, loss_scale=16, train_wall=280, gb_free=8.2, wall=59922
2022-03-05 02:18:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:18:23 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 16.591 | ppl 98682.4 | wps 34205.6 | wpb 510.9 | bsz 1 | num_updates 19468 | best_loss 7.674
2022-03-05 02:18:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19468 updates
2022-03-05 02:18:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:18:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:18:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 202 @ 19468 updates, score 16.591) (writing took 3.780918011441827 seconds)
2022-03-05 02:18:26 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 02:18:26 | INFO | train | epoch 202 | loss 0.692 | ppl 1.62 | wps 20497 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19468 | lr 0.000226641 | gnorm 0.959 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 60140
2022-03-05 02:18:26 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 02:18:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:20:04 | INFO | train_inner | epoch 203:     32 / 97 loss=0.69, ppl=1.61, wps=20747.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.956, loss_scale=16, train_wall=277, gb_free=8.2, wall=60237
2022-03-05 02:20:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:23:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:23:29 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 16.603 | ppl 99566.2 | wps 34290.9 | wpb 510.9 | bsz 1 | num_updates 19564 | best_loss 7.674
2022-03-05 02:23:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19564 updates
2022-03-05 02:23:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:23:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:23:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 203 @ 19564 updates, score 16.603) (writing took 3.8188099451363087 seconds)
2022-03-05 02:23:33 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 02:23:33 | INFO | train | epoch 203 | loss 0.689 | ppl 1.61 | wps 20539.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19564 | lr 0.000226085 | gnorm 0.959 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 60446
2022-03-05 02:23:33 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 02:23:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:25:23 | INFO | train_inner | epoch 204:     36 / 97 loss=0.689, ppl=1.61, wps=20548.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.961, loss_scale=16, train_wall=280, gb_free=8.2, wall=60556
2022-03-05 02:28:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:28:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:28:35 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 16.727 | ppl 108438 | wps 34581.4 | wpb 510.9 | bsz 1 | num_updates 19660 | best_loss 7.674
2022-03-05 02:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19660 updates
2022-03-05 02:28:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:28:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 204 @ 19660 updates, score 16.727) (writing took 3.8049743697047234 seconds)
2022-03-05 02:28:39 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 02:28:39 | INFO | train | epoch 204 | loss 0.685 | ppl 1.61 | wps 20535.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 19660 | lr 0.000225532 | gnorm 0.951 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 60752
2022-03-05 02:28:39 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 02:28:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:30:41 | INFO | train_inner | epoch 205:     40 / 97 loss=0.682, ppl=1.6, wps=20550.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.949, loss_scale=16, train_wall=280, gb_free=8.2, wall=60875
2022-03-05 02:33:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:33:41 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 16.687 | ppl 105534 | wps 35111.4 | wpb 510.9 | bsz 1 | num_updates 19757 | best_loss 7.674
2022-03-05 02:33:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19757 updates
2022-03-05 02:33:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:33:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:33:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 205 @ 19757 updates, score 16.687) (writing took 3.7302553951740265 seconds)
2022-03-05 02:33:44 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 02:33:44 | INFO | train | epoch 205 | loss 0.684 | ppl 1.61 | wps 20779 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 19757 | lr 0.000224978 | gnorm 0.954 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 61058
2022-03-05 02:33:44 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 02:33:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:35:56 | INFO | train_inner | epoch 206:     43 / 97 loss=0.684, ppl=1.61, wps=20849.8, ups=0.32, wpb=65495, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.948, loss_scale=32, train_wall=277, gb_free=8.2, wall=61189
2022-03-05 02:35:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:38:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:38:45 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 16.594 | ppl 98940 | wps 35794.8 | wpb 510.9 | bsz 1 | num_updates 19853 | best_loss 7.674
2022-03-05 02:38:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19853 updates
2022-03-05 02:38:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:38:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:38:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 206 @ 19853 updates, score 16.594) (writing took 3.7243129070848227 seconds)
2022-03-05 02:38:49 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 02:38:49 | INFO | train | epoch 206 | loss 0.68 | ppl 1.6 | wps 20673.3 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 19853 | lr 0.000224433 | gnorm 0.942 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 61362
2022-03-05 02:38:49 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 02:38:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:41:11 | INFO | train_inner | epoch 207:     47 / 97 loss=0.677, ppl=1.6, wps=20765.1, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.948, loss_scale=16, train_wall=278, gb_free=8.2, wall=61504
2022-03-05 02:42:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:43:48 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 16.67 | ppl 104252 | wps 37784.3 | wpb 510.9 | bsz 1 | num_updates 19949 | best_loss 7.674
2022-03-05 02:43:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19949 updates
2022-03-05 02:43:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:43:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 207 @ 19949 updates, score 16.67) (writing took 3.578717416152358 seconds)
2022-03-05 02:43:51 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 02:43:51 | INFO | train | epoch 207 | loss 0.677 | ppl 1.6 | wps 20779.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 19949 | lr 0.000223892 | gnorm 0.955 | loss_scale 16 | train_wall 267 | gb_free 8.2 | wall 61664
2022-03-05 02:43:51 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 02:43:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:46:27 | INFO | train_inner | epoch 208:     51 / 97 loss=0.675, ppl=1.6, wps=20738, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.952, loss_scale=16, train_wall=279, gb_free=8.2, wall=61820
2022-03-05 02:48:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:48:54 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 16.671 | ppl 104350 | wps 34101 | wpb 510.9 | bsz 1 | num_updates 20046 | best_loss 7.674
2022-03-05 02:48:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20046 updates
2022-03-05 02:48:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:48:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:48:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 208 @ 20046 updates, score 16.671) (writing took 3.53331739641726 seconds)
2022-03-05 02:48:57 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 02:48:57 | INFO | train | epoch 208 | loss 0.674 | ppl 1.6 | wps 20750.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 20046 | lr 0.00022335 | gnorm 0.949 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 61971
2022-03-05 02:48:57 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 02:48:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:49:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:51:46 | INFO | train_inner | epoch 209:     55 / 97 loss=0.673, ppl=1.59, wps=20495.9, ups=0.31, wpb=65495, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.946, loss_scale=16, train_wall=281, gb_free=8.2, wall=62140
2022-03-05 02:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:54:01 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 16.651 | ppl 102923 | wps 34294.2 | wpb 510.9 | bsz 1 | num_updates 20142 | best_loss 7.674
2022-03-05 02:54:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20142 updates
2022-03-05 02:54:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:54:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:54:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 209 @ 20142 updates, score 16.651) (writing took 3.8861906733363867 seconds)
2022-03-05 02:54:05 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 02:54:05 | INFO | train | epoch 209 | loss 0.67 | ppl 1.59 | wps 20419.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20142 | lr 0.000222817 | gnorm 0.94 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 62278
2022-03-05 02:54:05 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 02:54:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:57:03 | INFO | train_inner | epoch 210:     58 / 97 loss=0.669, ppl=1.59, wps=20648.3, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.949, loss_scale=32, train_wall=279, gb_free=8.2, wall=62457
2022-03-05 02:57:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:59:09 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 16.825 | ppl 116116 | wps 34204.8 | wpb 510.9 | bsz 1 | num_updates 20238 | best_loss 7.674
2022-03-05 02:59:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20238 updates
2022-03-05 02:59:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:59:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 02:59:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 210 @ 20238 updates, score 16.825) (writing took 3.8612370248883963 seconds)
2022-03-05 02:59:13 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 02:59:13 | INFO | train | epoch 210 | loss 0.667 | ppl 1.59 | wps 20405.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20238 | lr 0.000222288 | gnorm 0.952 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 62587
2022-03-05 02:59:13 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 02:59:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:02:24 | INFO | train_inner | epoch 211:     62 / 97 loss=0.666, ppl=1.59, wps=20432.4, ups=0.31, wpb=65495, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.95, loss_scale=16, train_wall=282, gb_free=8.2, wall=62777
2022-03-05 03:04:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:04:17 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 16.788 | ppl 113155 | wps 34286.3 | wpb 510.9 | bsz 1 | num_updates 20335 | best_loss 7.674
2022-03-05 03:04:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20335 updates
2022-03-05 03:04:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:04:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:04:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 211 @ 20335 updates, score 16.788) (writing took 3.7295913472771645 seconds)
2022-03-05 03:04:21 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 03:04:21 | INFO | train | epoch 211 | loss 0.666 | ppl 1.59 | wps 20631.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 20335 | lr 0.000221757 | gnorm 0.948 | loss_scale 32 | train_wall 270 | gb_free 8.2 | wall 62894
2022-03-05 03:04:21 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 03:04:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:05:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:07:44 | INFO | train_inner | epoch 212:     66 / 97 loss=0.662, ppl=1.58, wps=20465.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.943, loss_scale=16, train_wall=281, gb_free=8.2, wall=63097
2022-03-05 03:09:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:09:25 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 16.771 | ppl 111838 | wps 34100 | wpb 510.9 | bsz 1 | num_updates 20431 | best_loss 7.674
2022-03-05 03:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20431 updates
2022-03-05 03:09:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:09:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:09:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 212 @ 20431 updates, score 16.771) (writing took 3.894976729527116 seconds)
2022-03-05 03:09:29 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 03:09:29 | INFO | train | epoch 212 | loss 0.661 | ppl 1.58 | wps 20406.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20431 | lr 0.000221236 | gnorm 0.945 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 63203
2022-03-05 03:09:29 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 03:09:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:12:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:13:05 | INFO | train_inner | epoch 213:     70 / 97 loss=0.66, ppl=1.58, wps=20433.2, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.939, loss_scale=16, train_wall=282, gb_free=8.2, wall=63418
2022-03-05 03:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:14:34 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 16.803 | ppl 114342 | wps 34048.6 | wpb 510.9 | bsz 1 | num_updates 20527 | best_loss 7.674
2022-03-05 03:14:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20527 updates
2022-03-05 03:14:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:14:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 213 @ 20527 updates, score 16.803) (writing took 3.7964588794857264 seconds)
2022-03-05 03:14:38 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 03:14:38 | INFO | train | epoch 213 | loss 0.658 | ppl 1.58 | wps 20395.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20527 | lr 0.000220718 | gnorm 0.939 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 63511
2022-03-05 03:14:38 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 03:14:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:18:21 | INFO | train_inner | epoch 214:     73 / 97 loss=0.658, ppl=1.58, wps=20680.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.946, loss_scale=16, train_wall=278, gb_free=8.2, wall=63735
2022-03-05 03:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:19:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:19:41 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 16.789 | ppl 113255 | wps 34286.1 | wpb 510.9 | bsz 1 | num_updates 20623 | best_loss 7.674
2022-03-05 03:19:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20623 updates
2022-03-05 03:19:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:19:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 214 @ 20623 updates, score 16.789) (writing took 3.7101714089512825 seconds)
2022-03-05 03:19:45 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 03:19:45 | INFO | train | epoch 214 | loss 0.657 | ppl 1.58 | wps 20482 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20623 | lr 0.000220203 | gnorm 0.945 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 63818
2022-03-05 03:19:45 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 03:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:23:40 | INFO | train_inner | epoch 215:     77 / 97 loss=0.656, ppl=1.58, wps=20566.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.945, loss_scale=16, train_wall=280, gb_free=8.2, wall=64053
2022-03-05 03:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:24:47 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 16.843 | ppl 117533 | wps 34509.8 | wpb 510.9 | bsz 1 | num_updates 20720 | best_loss 7.674
2022-03-05 03:24:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20720 updates
2022-03-05 03:24:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:24:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 215 @ 20720 updates, score 16.843) (writing took 3.6879921332001686 seconds)
2022-03-05 03:24:51 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 03:24:51 | INFO | train | epoch 215 | loss 0.653 | ppl 1.57 | wps 20754.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 20720 | lr 0.000219687 | gnorm 0.937 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 64124
2022-03-05 03:24:51 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 03:24:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:28:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:28:58 | INFO | train_inner | epoch 216:     81 / 97 loss=0.65, ppl=1.57, wps=20548.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.938, loss_scale=16, train_wall=280, gb_free=8.2, wall=64372
2022-03-05 03:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:29:53 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 16.89 | ppl 121432 | wps 34869.8 | wpb 510.9 | bsz 1 | num_updates 20816 | best_loss 7.674
2022-03-05 03:29:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20816 updates
2022-03-05 03:29:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:29:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:29:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 216 @ 20816 updates, score 16.89) (writing took 3.7524743359535933 seconds)
2022-03-05 03:29:57 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 03:29:57 | INFO | train | epoch 216 | loss 0.65 | ppl 1.57 | wps 20503.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 20816 | lr 0.00021918 | gnorm 0.942 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 64431
2022-03-05 03:29:57 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 03:29:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:34:14 | INFO | train_inner | epoch 217:     84 / 97 loss=0.649, ppl=1.57, wps=20748.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=20900, lr=0.000218739, gnorm=0.942, loss_scale=16, train_wall=277, gb_free=8.2, wall=64687
2022-03-05 03:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:35:00 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 16.913 | ppl 123376 | wps 34183.2 | wpb 510.9 | bsz 1 | num_updates 20913 | best_loss 7.674
2022-03-05 03:35:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20913 updates
2022-03-05 03:35:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:35:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:35:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 217 @ 20913 updates, score 16.913) (writing took 3.8650828413665295 seconds)
2022-03-05 03:35:04 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 03:35:04 | INFO | train | epoch 217 | loss 0.647 | ppl 1.57 | wps 20704 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 20913 | lr 0.000218671 | gnorm 0.94 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 64737
2022-03-05 03:35:04 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 03:35:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:35:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:39:33 | INFO | train_inner | epoch 218:     88 / 97 loss=0.645, ppl=1.56, wps=20505.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21000, lr=0.000218218, gnorm=0.94, loss_scale=16, train_wall=280, gb_free=8.2, wall=65007
2022-03-05 03:40:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:40:07 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 16.886 | ppl 121134 | wps 34114.9 | wpb 510.9 | bsz 1 | num_updates 21009 | best_loss 7.674
2022-03-05 03:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 21009 updates
2022-03-05 03:40:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:40:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:40:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 218 @ 21009 updates, score 16.886) (writing took 3.7835906129330397 seconds)
2022-03-05 03:40:11 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 03:40:11 | INFO | train | epoch 218 | loss 0.644 | ppl 1.56 | wps 20473.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21009 | lr 0.000218171 | gnorm 0.941 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 65044
2022-03-05 03:40:11 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 03:40:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:42:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:44:52 | INFO | train_inner | epoch 219:     92 / 97 loss=0.643, ppl=1.56, wps=20554.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21100, lr=0.0002177, gnorm=0.937, loss_scale=16, train_wall=280, gb_free=8.2, wall=65325
2022-03-05 03:45:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:45:14 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 16.908 | ppl 122954 | wps 34490.6 | wpb 510.9 | bsz 1 | num_updates 21105 | best_loss 7.674
2022-03-05 03:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21105 updates
2022-03-05 03:45:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:45:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:45:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 219 @ 21105 updates, score 16.908) (writing took 3.796739688143134 seconds)
2022-03-05 03:45:17 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 03:45:17 | INFO | train | epoch 219 | loss 0.642 | ppl 1.56 | wps 20526.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21105 | lr 0.000217674 | gnorm 0.937 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 65351
2022-03-05 03:45:17 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 03:45:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:50:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:50:11 | INFO | train_inner | epoch 220:     96 / 97 loss=0.641, ppl=1.56, wps=20557.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21200, lr=0.000217186, gnorm=0.939, loss_scale=16, train_wall=280, gb_free=8.2, wall=65644
2022-03-05 03:50:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:50:20 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 16.944 | ppl 126069 | wps 34285 | wpb 510.9 | bsz 1 | num_updates 21201 | best_loss 7.674
2022-03-05 03:50:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21201 updates
2022-03-05 03:50:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:50:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:50:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 220 @ 21201 updates, score 16.944) (writing took 4.5496352184563875 seconds)
2022-03-05 03:50:25 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 03:50:25 | INFO | train | epoch 220 | loss 0.64 | ppl 1.56 | wps 20477.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21201 | lr 0.000217181 | gnorm 0.938 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 65658
2022-03-05 03:50:25 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 03:50:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:55:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:55:27 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 16.996 | ppl 130744 | wps 34804.9 | wpb 510.9 | bsz 1 | num_updates 21298 | best_loss 7.674
2022-03-05 03:55:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21298 updates
2022-03-05 03:55:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:55:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 03:55:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 221 @ 21298 updates, score 16.996) (writing took 3.823529088869691 seconds)
2022-03-05 03:55:31 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 03:55:31 | INFO | train | epoch 221 | loss 0.637 | ppl 1.55 | wps 20727.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 21298 | lr 0.000216686 | gnorm 0.937 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 65964
2022-03-05 03:55:31 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 03:55:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:55:37 | INFO | train_inner | epoch 222:      2 / 97 loss=0.636, ppl=1.55, wps=20043.6, ups=0.31, wpb=65451.9, bsz=127.8, num_updates=21300, lr=0.000216676, gnorm=0.936, loss_scale=16, train_wall=277, gb_free=8.2, wall=65970
2022-03-05 03:57:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:00:33 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 16.972 | ppl 128577 | wps 34955.1 | wpb 510.9 | bsz 1 | num_updates 21394 | best_loss 7.674
2022-03-05 04:00:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21394 updates
2022-03-05 04:00:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:00:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:00:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 222 @ 21394 updates, score 16.972) (writing took 3.785515993833542 seconds)
2022-03-05 04:00:37 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 04:00:37 | INFO | train | epoch 222 | loss 0.634 | ppl 1.55 | wps 20534.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21394 | lr 0.000216199 | gnorm 0.942 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 66270
2022-03-05 04:00:37 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 04:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:00:55 | INFO | train_inner | epoch 223:      6 / 97 loss=0.633, ppl=1.55, wps=20577.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.941, loss_scale=16, train_wall=280, gb_free=8.2, wall=66289
2022-03-05 04:04:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:05:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:05:39 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 16.98 | ppl 129270 | wps 34846.7 | wpb 510.9 | bsz 1 | num_updates 21490 | best_loss 7.674
2022-03-05 04:05:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21490 updates
2022-03-05 04:05:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:05:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:05:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 223 @ 21490 updates, score 16.98) (writing took 3.7195563204586506 seconds)
2022-03-05 04:05:42 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 04:05:42 | INFO | train | epoch 223 | loss 0.631 | ppl 1.55 | wps 20602.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21490 | lr 0.000215716 | gnorm 0.93 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 66576
2022-03-05 04:05:42 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 04:05:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:06:13 | INFO | train_inner | epoch 224:     10 / 97 loss=0.63, ppl=1.55, wps=20635.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.928, loss_scale=16, train_wall=280, gb_free=8.2, wall=66606
2022-03-05 04:10:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:10:44 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 16.991 | ppl 130231 | wps 34757.8 | wpb 510.9 | bsz 1 | num_updates 21587 | best_loss 7.674
2022-03-05 04:10:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21587 updates
2022-03-05 04:10:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:10:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 224 @ 21587 updates, score 16.991) (writing took 3.7961238492280245 seconds)
2022-03-05 04:10:48 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 04:10:48 | INFO | train | epoch 224 | loss 0.629 | ppl 1.55 | wps 20776.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 21587 | lr 0.000215231 | gnorm 0.933 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 66881
2022-03-05 04:10:48 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 04:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:11:28 | INFO | train_inner | epoch 225:     13 / 97 loss=0.627, ppl=1.54, wps=20791.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.932, loss_scale=16, train_wall=277, gb_free=8.2, wall=66921
2022-03-05 04:13:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:15:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:15:51 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 17.009 | ppl 131904 | wps 34394 | wpb 510.9 | bsz 1 | num_updates 21683 | best_loss 7.674
2022-03-05 04:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21683 updates
2022-03-05 04:15:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:15:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 225 @ 21683 updates, score 17.009) (writing took 3.7209880966693163 seconds)
2022-03-05 04:15:55 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 04:15:55 | INFO | train | epoch 225 | loss 0.626 | ppl 1.54 | wps 20515.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21683 | lr 0.000214754 | gnorm 0.924 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 67188
2022-03-05 04:15:55 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 04:15:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:16:46 | INFO | train_inner | epoch 226:     17 / 97 loss=0.625, ppl=1.54, wps=20557.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.927, loss_scale=16, train_wall=280, gb_free=8.2, wall=67240
2022-03-05 04:20:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:20:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:20:57 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 17.001 | ppl 131180 | wps 34544.3 | wpb 510.9 | bsz 1 | num_updates 21779 | best_loss 7.674
2022-03-05 04:20:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21779 updates
2022-03-05 04:20:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:21:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:21:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 226 @ 21779 updates, score 17.001) (writing took 3.8841593880206347 seconds)
2022-03-05 04:21:01 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 04:21:01 | INFO | train | epoch 226 | loss 0.624 | ppl 1.54 | wps 20519.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21779 | lr 0.00021428 | gnorm 0.94 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 67494
2022-03-05 04:21:01 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 04:21:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:22:05 | INFO | train_inner | epoch 227:     21 / 97 loss=0.623, ppl=1.54, wps=20559.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.937, loss_scale=16, train_wall=280, gb_free=8.2, wall=67558
2022-03-05 04:25:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:26:05 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 16.988 | ppl 130004 | wps 34181.2 | wpb 510.9 | bsz 1 | num_updates 21876 | best_loss 7.674
2022-03-05 04:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21876 updates
2022-03-05 04:26:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:26:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:26:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 227 @ 21876 updates, score 16.988) (writing took 3.7548757679760456 seconds)
2022-03-05 04:26:08 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 04:26:08 | INFO | train | epoch 227 | loss 0.621 | ppl 1.54 | wps 20671.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 21876 | lr 0.000213804 | gnorm 0.932 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 67802
2022-03-05 04:26:08 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 04:26:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:27:22 | INFO | train_inner | epoch 228:     24 / 97 loss=0.618, ppl=1.54, wps=20664.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.93, loss_scale=32, train_wall=278, gb_free=8.2, wall=67875
2022-03-05 04:27:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:31:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:31:12 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 17.092 | ppl 139738 | wps 34439.8 | wpb 510.9 | bsz 1 | num_updates 21972 | best_loss 7.674
2022-03-05 04:31:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21972 updates
2022-03-05 04:31:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:31:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:31:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 228 @ 21972 updates, score 17.092) (writing took 3.79195174574852 seconds)
2022-03-05 04:31:16 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 04:31:16 | INFO | train | epoch 228 | loss 0.62 | ppl 1.54 | wps 20422.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 21972 | lr 0.000213337 | gnorm 0.931 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 68109
2022-03-05 04:31:16 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 04:31:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:32:42 | INFO | train_inner | epoch 229:     28 / 97 loss=0.618, ppl=1.54, wps=20460.1, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.927, loss_scale=16, train_wall=281, gb_free=8.2, wall=68195
2022-03-05 04:35:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:36:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:36:20 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 17.034 | ppl 134179 | wps 34185.2 | wpb 510.9 | bsz 1 | num_updates 22068 | best_loss 7.674
2022-03-05 04:36:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22068 updates
2022-03-05 04:36:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:36:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 229 @ 22068 updates, score 17.034) (writing took 3.7536929715424776 seconds)
2022-03-05 04:36:24 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 04:36:24 | INFO | train | epoch 229 | loss 0.615 | ppl 1.53 | wps 20434.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22068 | lr 0.000212872 | gnorm 0.92 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 68417
2022-03-05 04:36:24 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 04:36:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:38:02 | INFO | train_inner | epoch 230:     32 / 97 loss=0.614, ppl=1.53, wps=20469.6, ups=0.31, wpb=65495, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.917, loss_scale=16, train_wall=281, gb_free=8.2, wall=68515
2022-03-05 04:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:41:27 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 17.054 | ppl 136054 | wps 35211.3 | wpb 510.9 | bsz 1 | num_updates 22165 | best_loss 7.674
2022-03-05 04:41:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22165 updates
2022-03-05 04:41:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:41:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:41:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 230 @ 22165 updates, score 17.054) (writing took 4.041351640596986 seconds)
2022-03-05 04:41:31 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 04:41:31 | INFO | train | epoch 230 | loss 0.614 | ppl 1.53 | wps 20673.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 22165 | lr 0.000212406 | gnorm 0.917 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 68724
2022-03-05 04:41:31 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 04:41:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:43:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:43:21 | INFO | train_inner | epoch 231:     36 / 97 loss=0.615, ppl=1.53, wps=20507.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=22200, lr=0.000212238, gnorm=0.923, loss_scale=16, train_wall=281, gb_free=8.2, wall=68835
2022-03-05 04:46:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:46:34 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 17.115 | ppl 141968 | wps 34353.3 | wpb 510.9 | bsz 1 | num_updates 22261 | best_loss 7.674
2022-03-05 04:46:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22261 updates
2022-03-05 04:46:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:46:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:46:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 231 @ 22261 updates, score 17.115) (writing took 3.7468628603965044 seconds)
2022-03-05 04:46:37 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 04:46:37 | INFO | train | epoch 231 | loss 0.611 | ppl 1.53 | wps 20523.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22261 | lr 0.000211947 | gnorm 0.92 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 69031
2022-03-05 04:46:37 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 04:46:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:48:37 | INFO | train_inner | epoch 232:     39 / 97 loss=0.609, ppl=1.52, wps=20769.7, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.917, loss_scale=16, train_wall=277, gb_free=8.2, wall=69150
2022-03-05 04:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:51:40 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 17.083 | ppl 138786 | wps 33787.5 | wpb 510.9 | bsz 1 | num_updates 22358 | best_loss 7.674
2022-03-05 04:51:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22358 updates
2022-03-05 04:51:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:51:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:51:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 232 @ 22358 updates, score 17.083) (writing took 3.7610255740582943 seconds)
2022-03-05 04:51:44 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 04:51:44 | INFO | train | epoch 232 | loss 0.609 | ppl 1.53 | wps 20713.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 22358 | lr 0.000211487 | gnorm 0.925 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 69337
2022-03-05 04:51:44 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 04:51:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:52:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:53:56 | INFO | train_inner | epoch 233:     43 / 97 loss=0.608, ppl=1.52, wps=20521, ups=0.31, wpb=65495, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.926, loss_scale=16, train_wall=281, gb_free=8.2, wall=69469
2022-03-05 04:56:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:56:47 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 17.079 | ppl 138402 | wps 34258.9 | wpb 510.9 | bsz 1 | num_updates 22454 | best_loss 7.674
2022-03-05 04:56:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22454 updates
2022-03-05 04:56:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:56:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 04:56:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 233 @ 22454 updates, score 17.079) (writing took 3.790989961475134 seconds)
2022-03-05 04:56:51 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 04:56:51 | INFO | train | epoch 233 | loss 0.606 | ppl 1.52 | wps 20510.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22454 | lr 0.000211034 | gnorm 0.916 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 69644
2022-03-05 04:56:51 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 04:56:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:59:12 | INFO | train_inner | epoch 234:     46 / 97 loss=0.605, ppl=1.52, wps=20741.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.915, loss_scale=16, train_wall=278, gb_free=8.2, wall=69785
2022-03-05 04:59:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:01:54 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 17.149 | ppl 145336 | wps 34813.6 | wpb 510.9 | bsz 1 | num_updates 22550 | best_loss 7.674
2022-03-05 05:01:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22550 updates
2022-03-05 05:01:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:01:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 234 @ 22550 updates, score 17.149) (writing took 3.6961174607276917 seconds)
2022-03-05 05:01:57 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 05:01:57 | INFO | train | epoch 234 | loss 0.605 | ppl 1.52 | wps 20500.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22550 | lr 0.000210585 | gnorm 0.923 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 69951
2022-03-05 05:01:57 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 05:01:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:04:31 | INFO | train_inner | epoch 235:     50 / 97 loss=0.603, ppl=1.52, wps=20530.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.923, loss_scale=16, train_wall=281, gb_free=8.2, wall=70104
2022-03-05 05:06:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:07:00 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 17.184 | ppl 148869 | wps 34516.6 | wpb 510.9 | bsz 1 | num_updates 22647 | best_loss 7.674
2022-03-05 05:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22647 updates
2022-03-05 05:07:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:07:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:07:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 235 @ 22647 updates, score 17.184) (writing took 3.8194383960217237 seconds)
2022-03-05 05:07:04 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 05:07:04 | INFO | train | epoch 235 | loss 0.602 | ppl 1.52 | wps 20722.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 22647 | lr 0.000210133 | gnorm 0.918 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 70257
2022-03-05 05:07:04 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 05:07:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:08:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:09:49 | INFO | train_inner | epoch 236:     54 / 97 loss=0.601, ppl=1.52, wps=20541.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.924, loss_scale=16, train_wall=280, gb_free=8.2, wall=70423
2022-03-05 05:12:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:12:07 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 17.201 | ppl 150619 | wps 34486.5 | wpb 510.9 | bsz 1 | num_updates 22743 | best_loss 7.674
2022-03-05 05:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22743 updates
2022-03-05 05:12:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:12:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:12:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 236 @ 22743 updates, score 17.201) (writing took 3.8209629133343697 seconds)
2022-03-05 05:12:11 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 05:12:11 | INFO | train | epoch 236 | loss 0.599 | ppl 1.52 | wps 20474.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22743 | lr 0.000209689 | gnorm 0.92 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 70564
2022-03-05 05:12:11 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 05:12:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:15:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:15:09 | INFO | train_inner | epoch 237:     58 / 97 loss=0.599, ppl=1.51, wps=20509.2, ups=0.31, wpb=65495, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.919, loss_scale=16, train_wall=281, gb_free=8.2, wall=70742
2022-03-05 05:17:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:17:14 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 17.183 | ppl 148794 | wps 34521.3 | wpb 510.9 | bsz 1 | num_updates 22839 | best_loss 7.674
2022-03-05 05:17:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22839 updates
2022-03-05 05:17:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:17:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:17:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 237 @ 22839 updates, score 17.183) (writing took 3.7540729977190495 seconds)
2022-03-05 05:17:18 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 05:17:18 | INFO | train | epoch 237 | loss 0.597 | ppl 1.51 | wps 20481.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 22839 | lr 0.000209248 | gnorm 0.923 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 70871
2022-03-05 05:17:18 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 05:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:20:24 | INFO | train_inner | epoch 238:     61 / 97 loss=0.596, ppl=1.51, wps=20787.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.921, loss_scale=16, train_wall=277, gb_free=8.2, wall=71057
2022-03-05 05:22:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:22:20 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 17.31 | ppl 162501 | wps 34787.2 | wpb 510.9 | bsz 1 | num_updates 22936 | best_loss 7.674
2022-03-05 05:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22936 updates
2022-03-05 05:22:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:22:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:22:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 238 @ 22936 updates, score 17.31) (writing took 3.7117588482797146 seconds)
2022-03-05 05:22:23 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 05:22:23 | INFO | train | epoch 238 | loss 0.595 | ppl 1.51 | wps 20804.9 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 22936 | lr 0.000208805 | gnorm 0.921 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 71177
2022-03-05 05:22:23 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 05:22:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:22:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:25:41 | INFO | train_inner | epoch 239:     65 / 97 loss=0.595, ppl=1.51, wps=20647, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.919, loss_scale=16, train_wall=279, gb_free=8.2, wall=71374
2022-03-05 05:27:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:27:25 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 17.285 | ppl 159690 | wps 34720.1 | wpb 510.9 | bsz 1 | num_updates 23032 | best_loss 7.674
2022-03-05 05:27:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23032 updates
2022-03-05 05:27:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:27:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:27:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 239 @ 23032 updates, score 17.285) (writing took 3.618729155510664 seconds)
2022-03-05 05:27:28 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 05:27:28 | INFO | train | epoch 239 | loss 0.593 | ppl 1.51 | wps 20622.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23032 | lr 0.00020837 | gnorm 0.914 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 71481
2022-03-05 05:27:28 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 05:27:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:30:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:30:59 | INFO | train_inner | epoch 240:     69 / 97 loss=0.59, ppl=1.5, wps=20616.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.909, loss_scale=16, train_wall=280, gb_free=8.2, wall=71692
2022-03-05 05:32:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:32:31 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 17.301 | ppl 161522 | wps 34159.7 | wpb 510.9 | bsz 1 | num_updates 23128 | best_loss 7.674
2022-03-05 05:32:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23128 updates
2022-03-05 05:32:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:32:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:32:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 240 @ 23128 updates, score 17.301) (writing took 3.8047361355274916 seconds)
2022-03-05 05:32:35 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 05:32:35 | INFO | train | epoch 240 | loss 0.589 | ppl 1.5 | wps 20506.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23128 | lr 0.000207937 | gnorm 0.908 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 71788
2022-03-05 05:32:35 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 05:32:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:36:16 | INFO | train_inner | epoch 241:     72 / 97 loss=0.59, ppl=1.51, wps=20632.3, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.91, loss_scale=16, train_wall=279, gb_free=8.2, wall=72009
2022-03-05 05:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:37:39 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 17.242 | ppl 155054 | wps 34155.3 | wpb 510.9 | bsz 1 | num_updates 23225 | best_loss 7.674
2022-03-05 05:37:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23225 updates
2022-03-05 05:37:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:37:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:37:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 241 @ 23225 updates, score 17.242) (writing took 3.8553983066231012 seconds)
2022-03-05 05:37:43 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 05:37:43 | INFO | train | epoch 241 | loss 0.589 | ppl 1.5 | wps 20591.5 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 23225 | lr 0.000207502 | gnorm 0.908 | loss_scale 32 | train_wall 271 | gb_free 8.2 | wall 72097
2022-03-05 05:37:43 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 05:37:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:41:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:41:37 | INFO | train_inner | epoch 242:     76 / 97 loss=0.588, ppl=1.5, wps=20407, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.912, loss_scale=16, train_wall=282, gb_free=8.2, wall=72330
2022-03-05 05:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:42:48 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 17.305 | ppl 161982 | wps 34319.8 | wpb 510.9 | bsz 1 | num_updates 23321 | best_loss 7.674
2022-03-05 05:42:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23321 updates
2022-03-05 05:42:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:42:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:42:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 242 @ 23321 updates, score 17.305) (writing took 3.7636534199118614 seconds)
2022-03-05 05:42:52 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 05:42:52 | INFO | train | epoch 242 | loss 0.587 | ppl 1.5 | wps 20389.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23321 | lr 0.000207074 | gnorm 0.913 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 72405
2022-03-05 05:42:52 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 05:42:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:46:54 | INFO | train_inner | epoch 243:     79 / 97 loss=0.584, ppl=1.5, wps=20671.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.903, loss_scale=16, train_wall=278, gb_free=8.2, wall=72647
2022-03-05 05:47:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:47:55 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 17.284 | ppl 159632 | wps 34696.4 | wpb 510.9 | bsz 1 | num_updates 23418 | best_loss 7.674
2022-03-05 05:47:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23418 updates
2022-03-05 05:47:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:47:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:47:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 243 @ 23418 updates, score 17.284) (writing took 3.8130575865507126 seconds)
2022-03-05 05:47:59 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 05:47:59 | INFO | train | epoch 243 | loss 0.584 | ppl 1.5 | wps 20671.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 23418 | lr 0.000206645 | gnorm 0.903 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 72712
2022-03-05 05:47:59 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 05:47:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:48:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:52:13 | INFO | train_inner | epoch 244:     83 / 97 loss=0.583, ppl=1.5, wps=20533.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23500, lr=0.000206284, gnorm=0.903, loss_scale=16, train_wall=280, gb_free=8.2, wall=72966
2022-03-05 05:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:53:02 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 17.346 | ppl 166637 | wps 34140.7 | wpb 510.9 | bsz 1 | num_updates 23514 | best_loss 7.674
2022-03-05 05:53:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23514 updates
2022-03-05 05:53:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:53:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:53:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 244 @ 23514 updates, score 17.346) (writing took 3.7757144793868065 seconds)
2022-03-05 05:53:06 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 05:53:06 | INFO | train | epoch 244 | loss 0.582 | ppl 1.5 | wps 20490.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23514 | lr 0.000206223 | gnorm 0.905 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 73019
2022-03-05 05:53:06 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 05:53:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:57:31 | INFO | train_inner | epoch 245:     87 / 97 loss=0.581, ppl=1.5, wps=20564.8, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23600, lr=0.000205847, gnorm=0.913, loss_scale=16, train_wall=280, gb_free=8.2, wall=73285
2022-03-05 05:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:58:08 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 17.318 | ppl 163429 | wps 34376.8 | wpb 510.9 | bsz 1 | num_updates 23610 | best_loss 7.674
2022-03-05 05:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23610 updates
2022-03-05 05:58:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:58:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 05:58:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 245 @ 23610 updates, score 17.318) (writing took 3.81958164088428 seconds)
2022-03-05 05:58:12 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 05:58:12 | INFO | train | epoch 245 | loss 0.58 | ppl 1.49 | wps 20530.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23610 | lr 0.000205803 | gnorm 0.911 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 73325
2022-03-05 05:58:12 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 05:58:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:02:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:02:51 | INFO | train_inner | epoch 246:     91 / 97 loss=0.578, ppl=1.49, wps=20511.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=0.904, loss_scale=16, train_wall=281, gb_free=8.2, wall=73604
2022-03-05 06:03:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:03:15 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 17.332 | ppl 164996 | wps 34622.4 | wpb 510.9 | bsz 1 | num_updates 23706 | best_loss 7.674
2022-03-05 06:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23706 updates
2022-03-05 06:03:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:03:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:03:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 246 @ 23706 updates, score 17.332) (writing took 3.659464132040739 seconds)
2022-03-05 06:03:19 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 06:03:19 | INFO | train | epoch 246 | loss 0.577 | ppl 1.49 | wps 20483.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23706 | lr 0.000205386 | gnorm 0.902 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 73632
2022-03-05 06:03:19 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 06:03:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:08:07 | INFO | train_inner | epoch 247:     94 / 97 loss=0.576, ppl=1.49, wps=20720.8, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=23800, lr=0.00020498, gnorm=0.907, loss_scale=16, train_wall=278, gb_free=8.2, wall=73920
2022-03-05 06:08:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:08:22 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 17.32 | ppl 163593 | wps 34239.5 | wpb 510.9 | bsz 1 | num_updates 23803 | best_loss 7.674
2022-03-05 06:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23803 updates
2022-03-05 06:08:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:08:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 247 @ 23803 updates, score 17.32) (writing took 3.830906419083476 seconds)
2022-03-05 06:08:26 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 06:08:26 | INFO | train | epoch 247 | loss 0.576 | ppl 1.49 | wps 20694.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 23803 | lr 0.000204967 | gnorm 0.909 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 73939
2022-03-05 06:08:26 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 06:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:10:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:13:29 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 17.414 | ppl 174619 | wps 34093.1 | wpb 510.9 | bsz 1 | num_updates 23899 | best_loss 7.674
2022-03-05 06:13:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23899 updates
2022-03-05 06:13:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:13:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 248 @ 23899 updates, score 17.414) (writing took 3.902312558144331 seconds)
2022-03-05 06:13:33 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 06:13:33 | INFO | train | epoch 248 | loss 0.573 | ppl 1.49 | wps 20493.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 23899 | lr 0.000204555 | gnorm 0.898 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 74246
2022-03-05 06:13:33 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 06:13:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:13:36 | INFO | train_inner | epoch 249:      1 / 97 loss=0.573, ppl=1.49, wps=19882.4, ups=0.3, wpb=65451.9, bsz=127.8, num_updates=23900, lr=0.000204551, gnorm=0.899, loss_scale=16, train_wall=280, gb_free=8.2, wall=74249
2022-03-05 06:18:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:18:35 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 17.376 | ppl 170053 | wps 34420.9 | wpb 510.9 | bsz 1 | num_updates 23996 | best_loss 7.674
2022-03-05 06:18:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23996 updates
2022-03-05 06:18:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:18:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:18:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 249 @ 23996 updates, score 17.376) (writing took 3.7866904959082603 seconds)
2022-03-05 06:18:39 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 06:18:39 | INFO | train | epoch 249 | loss 0.572 | ppl 1.49 | wps 20755.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 23996 | lr 0.000204141 | gnorm 0.911 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 74552
2022-03-05 06:18:39 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 06:18:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:18:51 | INFO | train_inner | epoch 250:      4 / 97 loss=0.571, ppl=1.49, wps=20776.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.91, loss_scale=32, train_wall=277, gb_free=8.2, wall=74564
2022-03-05 06:23:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:23:42 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 17.415 | ppl 174784 | wps 34757.7 | wpb 510.9 | bsz 1 | num_updates 24093 | best_loss 7.674
2022-03-05 06:23:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24093 updates
2022-03-05 06:23:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:23:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:23:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 250 @ 24093 updates, score 17.415) (writing took 3.775340288877487 seconds)
2022-03-05 06:23:45 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 06:23:45 | INFO | train | epoch 250 | loss 0.567 | ppl 1.48 | wps 20718.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 24093 | lr 0.00020373 | gnorm 0.892 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 74859
2022-03-05 06:23:45 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 06:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-05 06:24:10 | INFO | train_inner | epoch 251:      8 / 97 loss=0.567, ppl=1.48, wps=20545, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.892, loss_scale=32, train_wall=280, gb_free=8.2, wall=74883
2022-03-05 06:24:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:28:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:28:49 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 17.367 | ppl 169008 | wps 34562.7 | wpb 510.9 | bsz 1 | num_updates 24188 | best_loss 7.674
2022-03-05 06:28:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24188 updates
2022-03-05 06:28:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:28:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:28:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 251 @ 24188 updates, score 17.367) (writing took 3.7580487318336964 seconds)
2022-03-05 06:28:52 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 06:28:52 | INFO | train | epoch 251 | loss 0.567 | ppl 1.48 | wps 20267.7 | ups 0.31 | wpb 65490.6 | bsz 127.9 | num_updates 24188 | lr 0.000203329 | gnorm 0.891 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 75166
2022-03-05 06:28:52 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 06:28:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:29:29 | INFO | train_inner | epoch 252:     12 / 97 loss=0.566, ppl=1.48, wps=20508.1, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.888, loss_scale=16, train_wall=281, gb_free=8.2, wall=75202
2022-03-05 06:31:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:33:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:33:56 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 17.423 | ppl 175731 | wps 34979.6 | wpb 510.9 | bsz 1 | num_updates 24284 | best_loss 7.674
2022-03-05 06:33:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24284 updates
2022-03-05 06:33:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:33:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:33:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 252 @ 24284 updates, score 17.423) (writing took 3.7822914719581604 seconds)
2022-03-05 06:33:59 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 06:33:59 | INFO | train | epoch 252 | loss 0.567 | ppl 1.48 | wps 20480.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24284 | lr 0.000202927 | gnorm 0.906 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 75473
2022-03-05 06:33:59 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 06:33:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:34:48 | INFO | train_inner | epoch 253:     16 / 97 loss=0.566, ppl=1.48, wps=20527.1, ups=0.31, wpb=65495, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.906, loss_scale=16, train_wall=281, gb_free=8.2, wall=75522
2022-03-05 06:38:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:39:02 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 17.428 | ppl 176284 | wps 34689 | wpb 510.9 | bsz 1 | num_updates 24381 | best_loss 7.674
2022-03-05 06:39:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24381 updates
2022-03-05 06:39:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 253 @ 24381 updates, score 17.428) (writing took 3.7117351852357388 seconds)
2022-03-05 06:39:06 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 06:39:06 | INFO | train | epoch 253 | loss 0.563 | ppl 1.48 | wps 20737.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 24381 | lr 0.000202523 | gnorm 0.89 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 75779
2022-03-05 06:39:06 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 06:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:39:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:40:07 | INFO | train_inner | epoch 254:     20 / 97 loss=0.562, ppl=1.48, wps=20574.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.892, loss_scale=16, train_wall=280, gb_free=8.2, wall=75840
2022-03-05 06:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:44:07 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 17.425 | ppl 175960 | wps 34648.7 | wpb 510.9 | bsz 1 | num_updates 24477 | best_loss 7.674
2022-03-05 06:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24477 updates
2022-03-05 06:44:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:44:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:44:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 254 @ 24477 updates, score 17.425) (writing took 3.8800652362406254 seconds)
2022-03-05 06:44:11 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 06:44:11 | INFO | train | epoch 254 | loss 0.562 | ppl 1.48 | wps 20585.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24477 | lr 0.000202125 | gnorm 0.899 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 76084
2022-03-05 06:44:11 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 06:44:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:45:21 | INFO | train_inner | epoch 255:     23 / 97 loss=0.561, ppl=1.48, wps=20835.6, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.895, loss_scale=16, train_wall=276, gb_free=8.2, wall=76154
2022-03-05 06:47:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:49:12 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 17.407 | ppl 173754 | wps 35092.9 | wpb 510.9 | bsz 1 | num_updates 24573 | best_loss 7.674
2022-03-05 06:49:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24573 updates
2022-03-05 06:49:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:49:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 255 @ 24573 updates, score 17.407) (writing took 3.711538737639785 seconds)
2022-03-05 06:49:16 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 06:49:16 | INFO | train | epoch 255 | loss 0.559 | ppl 1.47 | wps 20644.6 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 24573 | lr 0.00020173 | gnorm 0.898 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 76389
2022-03-05 06:49:16 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 06:49:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:50:38 | INFO | train_inner | epoch 256:     27 / 97 loss=0.557, ppl=1.47, wps=20681.9, ups=0.32, wpb=65495, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.897, loss_scale=16, train_wall=279, gb_free=8.2, wall=76471
2022-03-05 06:54:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:54:18 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 17.463 | ppl 180718 | wps 33170.1 | wpb 510.9 | bsz 1 | num_updates 24670 | best_loss 7.674
2022-03-05 06:54:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24670 updates
2022-03-05 06:54:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:54:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 256 @ 24670 updates, score 17.463) (writing took 3.7437045630067587 seconds)
2022-03-05 06:54:22 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 06:54:22 | INFO | train | epoch 256 | loss 0.558 | ppl 1.47 | wps 20734.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 24670 | lr 0.000201333 | gnorm 0.895 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 76695
2022-03-05 06:54:22 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 06:54:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:55:54 | INFO | train_inner | epoch 257:     30 / 97 loss=0.559, ppl=1.47, wps=20676.4, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.895, loss_scale=32, train_wall=278, gb_free=8.2, wall=76788
2022-03-05 06:56:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:59:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:59:27 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 17.512 | ppl 186973 | wps 34209.2 | wpb 510.9 | bsz 1 | num_updates 24766 | best_loss 7.674
2022-03-05 06:59:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24766 updates
2022-03-05 06:59:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:59:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 06:59:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 257 @ 24766 updates, score 17.512) (writing took 3.7060748860239983 seconds)
2022-03-05 06:59:31 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 06:59:31 | INFO | train | epoch 257 | loss 0.556 | ppl 1.47 | wps 20367.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24766 | lr 0.000200943 | gnorm 0.898 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 77004
2022-03-05 06:59:31 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 06:59:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:01:15 | INFO | train_inner | epoch 258:     34 / 97 loss=0.555, ppl=1.47, wps=20417.4, ups=0.31, wpb=65495, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.9, loss_scale=16, train_wall=282, gb_free=8.2, wall=77108
2022-03-05 07:03:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:04:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:04:35 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 17.506 | ppl 186080 | wps 34449.9 | wpb 510.9 | bsz 1 | num_updates 24862 | best_loss 7.674
2022-03-05 07:04:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24862 updates
2022-03-05 07:04:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:04:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 258 @ 24862 updates, score 17.506) (writing took 3.737156143411994 seconds)
2022-03-05 07:04:39 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 07:04:39 | INFO | train | epoch 258 | loss 0.553 | ppl 1.47 | wps 20423.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 24862 | lr 0.000200554 | gnorm 0.89 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 77312
2022-03-05 07:04:39 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 07:04:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:06:35 | INFO | train_inner | epoch 259:     38 / 97 loss=0.551, ppl=1.47, wps=20461.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.889, loss_scale=16, train_wall=281, gb_free=8.2, wall=77428
2022-03-05 07:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:09:43 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 17.484 | ppl 183261 | wps 34151.3 | wpb 510.9 | bsz 1 | num_updates 24959 | best_loss 7.674
2022-03-05 07:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24959 updates
2022-03-05 07:09:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:09:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:09:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 259 @ 24959 updates, score 17.484) (writing took 3.8190869837999344 seconds)
2022-03-05 07:09:47 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 07:09:47 | INFO | train | epoch 259 | loss 0.551 | ppl 1.47 | wps 20616.6 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 24959 | lr 0.000200164 | gnorm 0.888 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 77620
2022-03-05 07:09:47 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 07:09:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:10:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:11:55 | INFO | train_inner | epoch 260:     42 / 97 loss=0.55, ppl=1.46, wps=20453.9, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=25000, lr=0.0002, gnorm=0.889, loss_scale=16, train_wall=281, gb_free=8.2, wall=77749
2022-03-05 07:14:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:14:51 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 17.516 | ppl 187473 | wps 34081.3 | wpb 510.9 | bsz 1 | num_updates 25055 | best_loss 7.674
2022-03-05 07:14:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25055 updates
2022-03-05 07:14:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:14:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 260 @ 25055 updates, score 17.516) (writing took 3.789215689525008 seconds)
2022-03-05 07:14:54 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 07:14:54 | INFO | train | epoch 260 | loss 0.551 | ppl 1.47 | wps 20438.1 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25055 | lr 0.00019978 | gnorm 0.898 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 77928
2022-03-05 07:14:54 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 07:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:17:13 | INFO | train_inner | epoch 261:     45 / 97 loss=0.552, ppl=1.47, wps=20648.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.899, loss_scale=16, train_wall=279, gb_free=8.2, wall=78066
2022-03-05 07:18:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:19:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:19:58 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 17.503 | ppl 185688 | wps 34008.2 | wpb 510.9 | bsz 1 | num_updates 25151 | best_loss 7.674
2022-03-05 07:19:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25151 updates
2022-03-05 07:19:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:20:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:20:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 261 @ 25151 updates, score 17.503) (writing took 3.7894905395805836 seconds)
2022-03-05 07:20:02 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 07:20:02 | INFO | train | epoch 261 | loss 0.549 | ppl 1.46 | wps 20420.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25151 | lr 0.000199399 | gnorm 0.892 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 78235
2022-03-05 07:20:02 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 07:20:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:22:33 | INFO | train_inner | epoch 262:     49 / 97 loss=0.546, ppl=1.46, wps=20454.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.885, loss_scale=16, train_wall=281, gb_free=8.2, wall=78386
2022-03-05 07:25:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:25:06 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 17.431 | ppl 176673 | wps 33947.8 | wpb 510.9 | bsz 1 | num_updates 25248 | best_loss 7.674
2022-03-05 07:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25248 updates
2022-03-05 07:25:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:25:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:25:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 262 @ 25248 updates, score 17.431) (writing took 3.8076674938201904 seconds)
2022-03-05 07:25:10 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 07:25:10 | INFO | train | epoch 262 | loss 0.547 | ppl 1.46 | wps 20632.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 25248 | lr 0.000199015 | gnorm 0.894 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 78543
2022-03-05 07:25:10 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 07:25:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:25:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:27:53 | INFO | train_inner | epoch 263:     53 / 97 loss=0.547, ppl=1.46, wps=20459.6, ups=0.31, wpb=65495, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.904, loss_scale=16, train_wall=281, gb_free=8.2, wall=78706
2022-03-05 07:30:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:30:13 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 17.465 | ppl 180908 | wps 34260.4 | wpb 510.9 | bsz 1 | num_updates 25344 | best_loss 7.674
2022-03-05 07:30:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25344 updates
2022-03-05 07:30:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:30:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:30:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 263 @ 25344 updates, score 17.465) (writing took 3.774174068123102 seconds)
2022-03-05 07:30:17 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 07:30:17 | INFO | train | epoch 263 | loss 0.544 | ppl 1.46 | wps 20498.3 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25344 | lr 0.000198638 | gnorm 0.893 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 78850
2022-03-05 07:30:17 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 07:30:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:33:10 | INFO | train_inner | epoch 264:     57 / 97 loss=0.542, ppl=1.46, wps=20661.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.885, loss_scale=16, train_wall=279, gb_free=8.2, wall=79023
2022-03-05 07:35:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:35:18 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 17.524 | ppl 188526 | wps 34588.1 | wpb 510.9 | bsz 1 | num_updates 25440 | best_loss 7.674
2022-03-05 07:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25440 updates
2022-03-05 07:35:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:35:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:35:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 264 @ 25440 updates, score 17.524) (writing took 3.849463701248169 seconds)
2022-03-05 07:35:21 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 07:35:21 | INFO | train | epoch 264 | loss 0.542 | ppl 1.46 | wps 20642.4 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 25440 | lr 0.000198263 | gnorm 0.882 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 79155
2022-03-05 07:35:21 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 07:35:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:38:24 | INFO | train_inner | epoch 265:     60 / 97 loss=0.54, ppl=1.45, wps=20839.6, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.882, loss_scale=16, train_wall=277, gb_free=8.2, wall=79337
2022-03-05 07:40:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:40:23 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 17.501 | ppl 185554 | wps 34318.1 | wpb 510.9 | bsz 1 | num_updates 25537 | best_loss 7.674
2022-03-05 07:40:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25537 updates
2022-03-05 07:40:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:40:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 265 @ 25537 updates, score 17.501) (writing took 3.8171130158007145 seconds)
2022-03-05 07:40:27 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 07:40:27 | INFO | train | epoch 265 | loss 0.541 | ppl 1.45 | wps 20809.2 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 25537 | lr 0.000197886 | gnorm 0.887 | loss_scale 32 | train_wall 268 | gb_free 8.2 | wall 79460
2022-03-05 07:40:27 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 07:40:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:43:40 | INFO | train_inner | epoch 266:     63 / 97 loss=0.541, ppl=1.46, wps=20749.9, ups=0.32, wpb=65495, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.883, loss_scale=32, train_wall=278, gb_free=8.2, wall=79653
2022-03-05 07:44:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:45:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:45:30 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 17.588 | ppl 196988 | wps 34972.3 | wpb 510.9 | bsz 1 | num_updates 25633 | best_loss 7.674
2022-03-05 07:45:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25633 updates
2022-03-05 07:45:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 266 @ 25633 updates, score 17.588) (writing took 3.7460357900708914 seconds)
2022-03-05 07:45:33 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 07:45:33 | INFO | train | epoch 266 | loss 0.539 | ppl 1.45 | wps 20506.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25633 | lr 0.000197515 | gnorm 0.879 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 79767
2022-03-05 07:45:33 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 07:45:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:48:58 | INFO | train_inner | epoch 267:     67 / 97 loss=0.539, ppl=1.45, wps=20554.8, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.887, loss_scale=16, train_wall=280, gb_free=8.2, wall=79972
2022-03-05 07:50:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:50:36 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 17.599 | ppl 198585 | wps 34561.3 | wpb 510.9 | bsz 1 | num_updates 25730 | best_loss 7.674
2022-03-05 07:50:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25730 updates
2022-03-05 07:50:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:50:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:50:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 267 @ 25730 updates, score 17.599) (writing took 3.764607710763812 seconds)
2022-03-05 07:50:40 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 07:50:40 | INFO | train | epoch 267 | loss 0.538 | ppl 1.45 | wps 20736.3 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 25730 | lr 0.000197142 | gnorm 0.888 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 80073
2022-03-05 07:50:40 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 07:50:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:53:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:54:17 | INFO | train_inner | epoch 268:     71 / 97 loss=0.536, ppl=1.45, wps=20582.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.889, loss_scale=16, train_wall=280, gb_free=8.2, wall=80290
2022-03-05 07:55:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:55:42 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 17.547 | ppl 191498 | wps 35061.5 | wpb 510.9 | bsz 1 | num_updates 25826 | best_loss 7.674
2022-03-05 07:55:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25826 updates
2022-03-05 07:55:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:55:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 07:55:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 268 @ 25826 updates, score 17.547) (writing took 3.7367572002112865 seconds)
2022-03-05 07:55:46 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 07:55:46 | INFO | train | epoch 268 | loss 0.535 | ppl 1.45 | wps 20547.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 25826 | lr 0.000196776 | gnorm 0.89 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 80379
2022-03-05 07:55:46 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 07:55:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:59:32 | INFO | train_inner | epoch 269:     74 / 97 loss=0.536, ppl=1.45, wps=20741, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.884, loss_scale=16, train_wall=278, gb_free=8.2, wall=80606
2022-03-05 08:00:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:00:48 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 17.603 | ppl 199069 | wps 35152.4 | wpb 510.9 | bsz 1 | num_updates 25923 | best_loss 7.674
2022-03-05 08:00:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25923 updates
2022-03-05 08:00:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:00:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:00:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 269 @ 25923 updates, score 17.603) (writing took 3.715704817324877 seconds)
2022-03-05 08:00:52 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 08:00:52 | INFO | train | epoch 269 | loss 0.535 | ppl 1.45 | wps 20725.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 25923 | lr 0.000196407 | gnorm 0.884 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 80685
2022-03-05 08:00:52 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 08:00:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:01:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:04:50 | INFO | train_inner | epoch 270:     78 / 97 loss=0.534, ppl=1.45, wps=20646.5, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.887, loss_scale=16, train_wall=279, gb_free=8.2, wall=80923
2022-03-05 08:05:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:05:53 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 17.57 | ppl 194540 | wps 34851.5 | wpb 510.9 | bsz 1 | num_updates 26019 | best_loss 7.674
2022-03-05 08:05:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26019 updates
2022-03-05 08:05:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:05:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:05:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 270 @ 26019 updates, score 17.57) (writing took 3.7303818706423044 seconds)
2022-03-05 08:05:57 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 08:05:57 | INFO | train | epoch 270 | loss 0.532 | ppl 1.45 | wps 20608.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26019 | lr 0.000196045 | gnorm 0.886 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 80990
2022-03-05 08:05:57 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 08:05:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:08:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:10:07 | INFO | train_inner | epoch 271:     82 / 97 loss=0.53, ppl=1.44, wps=20655.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=26100, lr=0.00019574, gnorm=0.88, loss_scale=16, train_wall=279, gb_free=8.2, wall=81240
2022-03-05 08:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:10:58 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 17.595 | ppl 197978 | wps 34960.6 | wpb 510.9 | bsz 1 | num_updates 26115 | best_loss 7.674
2022-03-05 08:10:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26115 updates
2022-03-05 08:10:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:11:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 271 @ 26115 updates, score 17.595) (writing took 3.7862165421247482 seconds)
2022-03-05 08:11:02 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 08:11:02 | INFO | train | epoch 271 | loss 0.53 | ppl 1.44 | wps 20621 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26115 | lr 0.000195684 | gnorm 0.882 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 81295
2022-03-05 08:11:02 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 08:11:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:15:23 | INFO | train_inner | epoch 272:     85 / 97 loss=0.531, ppl=1.44, wps=20717.4, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=26200, lr=0.000195366, gnorm=0.881, loss_scale=32, train_wall=278, gb_free=8.2, wall=81556
2022-03-05 08:15:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:16:06 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 17.681 | ppl 210121 | wps 35201.8 | wpb 510.9 | bsz 1 | num_updates 26212 | best_loss 7.674
2022-03-05 08:16:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26212 updates
2022-03-05 08:16:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:16:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:16:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 272 @ 26212 updates, score 17.681) (writing took 3.6489492263644934 seconds)
2022-03-05 08:16:09 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 08:16:09 | INFO | train | epoch 272 | loss 0.53 | ppl 1.44 | wps 20681 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 26212 | lr 0.000195321 | gnorm 0.88 | loss_scale 32 | train_wall 270 | gb_free 8.2 | wall 81603
2022-03-05 08:16:09 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 08:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:18:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:20:43 | INFO | train_inner | epoch 273:     89 / 97 loss=0.528, ppl=1.44, wps=20483, ups=0.31, wpb=65495, bsz=127.9, num_updates=26300, lr=0.000194994, gnorm=0.885, loss_scale=16, train_wall=281, gb_free=8.2, wall=81876
2022-03-05 08:21:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:21:13 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 17.697 | ppl 212551 | wps 34268 | wpb 510.9 | bsz 1 | num_updates 26308 | best_loss 7.674
2022-03-05 08:21:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26308 updates
2022-03-05 08:21:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:21:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:21:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 273 @ 26308 updates, score 17.697) (writing took 3.7598553001880646 seconds)
2022-03-05 08:21:17 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 08:21:17 | INFO | train | epoch 273 | loss 0.528 | ppl 1.44 | wps 20427.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26308 | lr 0.000194965 | gnorm 0.884 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 81910
2022-03-05 08:21:17 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 08:21:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:25:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:26:02 | INFO | train_inner | epoch 274:     93 / 97 loss=0.528, ppl=1.44, wps=20487.2, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.889, loss_scale=16, train_wall=281, gb_free=8.2, wall=82195
2022-03-05 08:26:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:26:21 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 17.667 | ppl 208134 | wps 34051.6 | wpb 510.9 | bsz 1 | num_updates 26404 | best_loss 7.674
2022-03-05 08:26:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26404 updates
2022-03-05 08:26:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:26:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 274 @ 26404 updates, score 17.667) (writing took 3.8180202450603247 seconds)
2022-03-05 08:26:25 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 08:26:25 | INFO | train | epoch 274 | loss 0.526 | ppl 1.44 | wps 20442.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26404 | lr 0.00019461 | gnorm 0.889 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 82218
2022-03-05 08:26:25 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 08:26:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:31:19 | INFO | train_inner | epoch 275:     96 / 97 loss=0.524, ppl=1.44, wps=20643.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=26500, lr=0.000194257, gnorm=0.872, loss_scale=16, train_wall=279, gb_free=8.2, wall=82513
2022-03-05 08:31:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:31:29 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 17.777 | ppl 224642 | wps 34033.6 | wpb 510.9 | bsz 1 | num_updates 26501 | best_loss 7.674
2022-03-05 08:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26501 updates
2022-03-05 08:31:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:31:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 275 @ 26501 updates, score 17.777) (writing took 3.796233668923378 seconds)
2022-03-05 08:31:33 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 08:31:33 | INFO | train | epoch 275 | loss 0.524 | ppl 1.44 | wps 20621 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 26501 | lr 0.000194254 | gnorm 0.871 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 82526
2022-03-05 08:31:33 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 08:31:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:33:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:36:37 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 17.673 | ppl 209042 | wps 34225 | wpb 510.9 | bsz 1 | num_updates 26597 | best_loss 7.674
2022-03-05 08:36:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26597 updates
2022-03-05 08:36:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:36:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:36:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 276 @ 26597 updates, score 17.673) (writing took 3.8956042006611824 seconds)
2022-03-05 08:36:40 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 08:36:40 | INFO | train | epoch 276 | loss 0.523 | ppl 1.44 | wps 20426 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26597 | lr 0.000193903 | gnorm 0.89 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 82834
2022-03-05 08:36:40 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 08:36:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:36:50 | INFO | train_inner | epoch 277:      3 / 97 loss=0.522, ppl=1.44, wps=19811.7, ups=0.3, wpb=65451.9, bsz=127.8, num_updates=26600, lr=0.000193892, gnorm=0.889, loss_scale=16, train_wall=281, gb_free=8.2, wall=82843
2022-03-05 08:40:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:41:45 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 17.777 | ppl 224569 | wps 33889.2 | wpb 510.9 | bsz 1 | num_updates 26693 | best_loss 7.674
2022-03-05 08:41:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26693 updates
2022-03-05 08:41:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:41:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:41:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 277 @ 26693 updates, score 17.777) (writing took 3.8181988317519426 seconds)
2022-03-05 08:41:49 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 08:41:49 | INFO | train | epoch 277 | loss 0.519 | ppl 1.43 | wps 20402.9 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26693 | lr 0.000193554 | gnorm 0.877 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 83142
2022-03-05 08:41:49 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 08:41:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:42:10 | INFO | train_inner | epoch 278:      7 / 97 loss=0.519, ppl=1.43, wps=20443.7, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.877, loss_scale=16, train_wall=281, gb_free=8.2, wall=83163
2022-03-05 08:46:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:46:53 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 17.784 | ppl 225657 | wps 34191.2 | wpb 510.9 | bsz 1 | num_updates 26790 | best_loss 7.674
2022-03-05 08:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26790 updates
2022-03-05 08:46:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:46:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:46:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 278 @ 26790 updates, score 17.784) (writing took 3.750082517042756 seconds)
2022-03-05 08:46:56 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 08:46:56 | INFO | train | epoch 278 | loss 0.518 | ppl 1.43 | wps 20639.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 26790 | lr 0.000193203 | gnorm 0.876 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 83450
2022-03-05 08:46:56 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 08:46:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:47:27 | INFO | train_inner | epoch 279:     10 / 97 loss=0.518, ppl=1.43, wps=20659.6, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.879, loss_scale=16, train_wall=278, gb_free=8.2, wall=83480
2022-03-05 08:50:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:52:00 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 17.789 | ppl 226399 | wps 34415.3 | wpb 510.9 | bsz 1 | num_updates 26886 | best_loss 7.674
2022-03-05 08:52:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26886 updates
2022-03-05 08:52:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:52:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:52:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 279 @ 26886 updates, score 17.789) (writing took 3.803554732352495 seconds)
2022-03-05 08:52:04 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 08:52:04 | INFO | train | epoch 279 | loss 0.516 | ppl 1.43 | wps 20460 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 26886 | lr 0.000192858 | gnorm 0.872 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 83757
2022-03-05 08:52:04 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 08:52:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:52:47 | INFO | train_inner | epoch 280:     14 / 97 loss=0.515, ppl=1.43, wps=20510.6, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.869, loss_scale=16, train_wall=281, gb_free=8.2, wall=83800
2022-03-05 08:57:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:57:06 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 17.776 | ppl 224460 | wps 34699.4 | wpb 510.9 | bsz 1 | num_updates 26983 | best_loss 7.674
2022-03-05 08:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26983 updates
2022-03-05 08:57:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 08:57:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 280 @ 26983 updates, score 17.776) (writing took 3.781656051054597 seconds)
2022-03-05 08:57:10 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 08:57:10 | INFO | train | epoch 280 | loss 0.517 | ppl 1.43 | wps 20735 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 26983 | lr 0.000192511 | gnorm 0.881 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 84063
2022-03-05 08:57:10 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 08:57:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:58:02 | INFO | train_inner | epoch 281:     17 / 97 loss=0.516, ppl=1.43, wps=20753.1, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.879, loss_scale=32, train_wall=277, gb_free=8.2, wall=84115
2022-03-05 08:59:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:02:13 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 17.803 | ppl 228656 | wps 34341.7 | wpb 510.9 | bsz 1 | num_updates 27079 | best_loss 7.674
2022-03-05 09:02:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27079 updates
2022-03-05 09:02:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:02:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:02:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 281 @ 27079 updates, score 17.803) (writing took 3.7514464985579252 seconds)
2022-03-05 09:02:17 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 09:02:17 | INFO | train | epoch 281 | loss 0.514 | ppl 1.43 | wps 20483.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27079 | lr 0.000192169 | gnorm 0.876 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 84370
2022-03-05 09:02:17 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 09:02:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:03:21 | INFO | train_inner | epoch 282:     21 / 97 loss=0.513, ppl=1.43, wps=20508.1, ups=0.31, wpb=65495, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.873, loss_scale=16, train_wall=281, gb_free=8.2, wall=84435
2022-03-05 09:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:07:20 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 17.783 | ppl 225589 | wps 34796.6 | wpb 510.9 | bsz 1 | num_updates 27176 | best_loss 7.674
2022-03-05 09:07:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27176 updates
2022-03-05 09:07:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:07:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:07:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 282 @ 27176 updates, score 17.783) (writing took 3.6645304560661316 seconds)
2022-03-05 09:07:23 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 09:07:23 | INFO | train | epoch 282 | loss 0.513 | ppl 1.43 | wps 20745.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 27176 | lr 0.000191826 | gnorm 0.867 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 84676
2022-03-05 09:07:23 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 09:07:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:08:36 | INFO | train_inner | epoch 283:     24 / 97 loss=0.513, ppl=1.43, wps=20801, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.871, loss_scale=32, train_wall=277, gb_free=8.2, wall=84750
2022-03-05 09:09:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:12:25 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 17.725 | ppl 216698 | wps 34824.5 | wpb 510.9 | bsz 1 | num_updates 27272 | best_loss 7.674
2022-03-05 09:12:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27272 updates
2022-03-05 09:12:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:12:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 283 @ 27272 updates, score 17.725) (writing took 3.730315800756216 seconds)
2022-03-05 09:12:28 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 09:12:28 | INFO | train | epoch 283 | loss 0.511 | ppl 1.42 | wps 20607.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27272 | lr 0.000191488 | gnorm 0.87 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 84982
2022-03-05 09:12:28 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 09:12:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:13:54 | INFO | train_inner | epoch 284:     28 / 97 loss=0.509, ppl=1.42, wps=20641.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.869, loss_scale=16, train_wall=279, gb_free=8.2, wall=85067
2022-03-05 09:16:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:17:30 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 17.722 | ppl 216130 | wps 34671 | wpb 510.9 | bsz 1 | num_updates 27368 | best_loss 7.674
2022-03-05 09:17:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27368 updates
2022-03-05 09:17:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:17:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:17:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 284 @ 27368 updates, score 17.722) (writing took 3.7140461448580027 seconds)
2022-03-05 09:17:33 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 09:17:33 | INFO | train | epoch 284 | loss 0.509 | ppl 1.42 | wps 20609 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27368 | lr 0.000191152 | gnorm 0.876 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 85287
2022-03-05 09:17:33 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 09:17:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:19:11 | INFO | train_inner | epoch 285:     32 / 97 loss=0.51, ppl=1.42, wps=20613.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.879, loss_scale=16, train_wall=280, gb_free=8.2, wall=85385
2022-03-05 09:22:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:22:35 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 17.776 | ppl 224414 | wps 34918.8 | wpb 510.9 | bsz 1 | num_updates 27465 | best_loss 7.674
2022-03-05 09:22:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27465 updates
2022-03-05 09:22:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:22:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:22:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 285 @ 27465 updates, score 17.776) (writing took 3.735032768920064 seconds)
2022-03-05 09:22:39 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 09:22:39 | INFO | train | epoch 285 | loss 0.508 | ppl 1.42 | wps 20800.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 27465 | lr 0.000190814 | gnorm 0.872 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 85592
2022-03-05 09:22:39 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 09:22:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:24:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:24:28 | INFO | train_inner | epoch 286:     36 / 97 loss=0.507, ppl=1.42, wps=20687.7, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=27500, lr=0.000190693, gnorm=0.864, loss_scale=16, train_wall=279, gb_free=8.2, wall=85701
2022-03-05 09:27:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:39 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 17.876 | ppl 240563 | wps 34772.3 | wpb 510.9 | bsz 1 | num_updates 27561 | best_loss 7.674
2022-03-05 09:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27561 updates
2022-03-05 09:27:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:27:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:27:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 286 @ 27561 updates, score 17.876) (writing took 3.718343848362565 seconds)
2022-03-05 09:27:43 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 09:27:43 | INFO | train | epoch 286 | loss 0.507 | ppl 1.42 | wps 20653 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 27561 | lr 0.000190481 | gnorm 0.874 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 85896
2022-03-05 09:27:43 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 09:27:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:29:42 | INFO | train_inner | epoch 287:     39 / 97 loss=0.505, ppl=1.42, wps=20843.7, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.873, loss_scale=16, train_wall=277, gb_free=8.2, wall=86015
2022-03-05 09:32:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:32:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:32:46 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 17.855 | ppl 237103 | wps 34319.2 | wpb 510.9 | bsz 1 | num_updates 27657 | best_loss 7.674
2022-03-05 09:32:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27657 updates
2022-03-05 09:32:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:32:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:32:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 287 @ 27657 updates, score 17.855) (writing took 3.739987950772047 seconds)
2022-03-05 09:32:49 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 09:32:49 | INFO | train | epoch 287 | loss 0.504 | ppl 1.42 | wps 20528.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27657 | lr 0.00019015 | gnorm 0.859 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 86203
2022-03-05 09:32:49 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 09:32:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:35:01 | INFO | train_inner | epoch 288:     43 / 97 loss=0.505, ppl=1.42, wps=20552.1, ups=0.31, wpb=65495, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.861, loss_scale=16, train_wall=280, gb_free=8.2, wall=86334
2022-03-05 09:37:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:37:52 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 17.766 | ppl 222932 | wps 34822.3 | wpb 510.9 | bsz 1 | num_updates 27754 | best_loss 7.674
2022-03-05 09:37:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27754 updates
2022-03-05 09:37:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:37:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:37:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 288 @ 27754 updates, score 17.766) (writing took 3.714992731809616 seconds)
2022-03-05 09:37:56 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 09:37:56 | INFO | train | epoch 288 | loss 0.504 | ppl 1.42 | wps 20719.1 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 27754 | lr 0.000189818 | gnorm 0.864 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 86509
2022-03-05 09:37:56 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 09:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:40:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:40:19 | INFO | train_inner | epoch 289:     47 / 97 loss=0.504, ppl=1.42, wps=20596.2, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.865, loss_scale=16, train_wall=280, gb_free=8.2, wall=86652
2022-03-05 09:42:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:42:59 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 17.813 | ppl 230286 | wps 34359.3 | wpb 510.9 | bsz 1 | num_updates 27850 | best_loss 7.674
2022-03-05 09:42:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27850 updates
2022-03-05 09:42:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:43:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 289 @ 27850 updates, score 17.813) (writing took 3.905829856172204 seconds)
2022-03-05 09:43:03 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 09:43:03 | INFO | train | epoch 289 | loss 0.503 | ppl 1.42 | wps 20513.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 27850 | lr 0.00018949 | gnorm 0.869 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 86816
2022-03-05 09:43:03 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 09:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:45:35 | INFO | train_inner | epoch 290:     50 / 97 loss=0.501, ppl=1.42, wps=20692.1, ups=0.32, wpb=65495, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.868, loss_scale=16, train_wall=278, gb_free=8.2, wall=86969
2022-03-05 09:47:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:48:06 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 17.922 | ppl 248394 | wps 34247.9 | wpb 510.9 | bsz 1 | num_updates 27947 | best_loss 7.674
2022-03-05 09:48:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27947 updates
2022-03-05 09:48:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 290 @ 27947 updates, score 17.922) (writing took 3.837878817692399 seconds)
2022-03-05 09:48:10 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 09:48:10 | INFO | train | epoch 290 | loss 0.5 | ppl 1.41 | wps 20694 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 27947 | lr 0.000189161 | gnorm 0.871 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 87123
2022-03-05 09:48:10 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 09:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:50:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:50:56 | INFO | train_inner | epoch 291:     54 / 97 loss=0.5, ppl=1.41, wps=20452.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.868, loss_scale=16, train_wall=281, gb_free=8.2, wall=87289
2022-03-05 09:53:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:53:14 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 17.79 | ppl 226653 | wps 34088.2 | wpb 510.9 | bsz 1 | num_updates 28043 | best_loss 7.674
2022-03-05 09:53:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28043 updates
2022-03-05 09:53:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:53:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 291 @ 28043 updates, score 17.79) (writing took 3.8017550241202116 seconds)
2022-03-05 09:53:17 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 09:53:17 | INFO | train | epoch 291 | loss 0.499 | ppl 1.41 | wps 20419.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28043 | lr 0.000188837 | gnorm 0.864 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 87431
2022-03-05 09:53:17 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 09:53:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:56:13 | INFO | train_inner | epoch 292:     57 / 97 loss=0.498, ppl=1.41, wps=20654.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.859, loss_scale=16, train_wall=279, gb_free=8.2, wall=87606
2022-03-05 09:58:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:58:22 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 17.756 | ppl 221380 | wps 34211.3 | wpb 510.9 | bsz 1 | num_updates 28140 | best_loss 7.674
2022-03-05 09:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28140 updates
2022-03-05 09:58:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:58:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 09:58:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 292 @ 28140 updates, score 17.756) (writing took 3.7993730399757624 seconds)
2022-03-05 09:58:25 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 09:58:25 | INFO | train | epoch 292 | loss 0.497 | ppl 1.41 | wps 20625.2 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 28140 | lr 0.000188512 | gnorm 0.856 | loss_scale 32 | train_wall 270 | gb_free 8.2 | wall 87739
2022-03-05 09:58:25 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 09:58:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:01:30 | INFO | train_inner | epoch 293:     60 / 97 loss=0.496, ppl=1.41, wps=20639.5, ups=0.32, wpb=65490.8, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.866, loss_scale=32, train_wall=279, gb_free=8.2, wall=87923
2022-03-05 10:02:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:03:30 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 17.824 | ppl 232113 | wps 34207.6 | wpb 510.9 | bsz 1 | num_updates 28236 | best_loss 7.674
2022-03-05 10:03:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28236 updates
2022-03-05 10:03:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:03:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:03:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 293 @ 28236 updates, score 17.824) (writing took 3.7963548190891743 seconds)
2022-03-05 10:03:33 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 10:03:33 | INFO | train | epoch 293 | loss 0.497 | ppl 1.41 | wps 20413.6 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28236 | lr 0.000188191 | gnorm 0.87 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 88047
2022-03-05 10:03:33 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 10:03:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:06:50 | INFO | train_inner | epoch 294:     64 / 97 loss=0.497, ppl=1.41, wps=20451.9, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.867, loss_scale=16, train_wall=281, gb_free=8.2, wall=88243
2022-03-05 10:08:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:08:38 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 17.881 | ppl 241409 | wps 34273 | wpb 510.9 | bsz 1 | num_updates 28333 | best_loss 7.674
2022-03-05 10:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28333 updates
2022-03-05 10:08:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:08:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 294 @ 28333 updates, score 17.881) (writing took 3.7739014234393835 seconds)
2022-03-05 10:08:42 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 10:08:42 | INFO | train | epoch 294 | loss 0.496 | ppl 1.41 | wps 20609.3 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 28333 | lr 0.000187868 | gnorm 0.865 | loss_scale 16 | train_wall 271 | gb_free 8.2 | wall 88355
2022-03-05 10:08:42 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 10:08:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:10:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:12:10 | INFO | train_inner | epoch 295:     68 / 97 loss=0.494, ppl=1.41, wps=20450.1, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.863, loss_scale=16, train_wall=282, gb_free=8.2, wall=88564
2022-03-05 10:13:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:13:45 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 17.866 | ppl 238817 | wps 34375.4 | wpb 510.9 | bsz 1 | num_updates 28429 | best_loss 7.674
2022-03-05 10:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28429 updates
2022-03-05 10:13:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:13:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:13:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 295 @ 28429 updates, score 17.866) (writing took 3.7465459275990725 seconds)
2022-03-05 10:13:49 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 10:13:49 | INFO | train | epoch 295 | loss 0.493 | ppl 1.41 | wps 20475.2 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28429 | lr 0.000187551 | gnorm 0.861 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 88662
2022-03-05 10:13:49 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 10:13:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:17:26 | INFO | train_inner | epoch 296:     71 / 97 loss=0.494, ppl=1.41, wps=20735.9, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.858, loss_scale=32, train_wall=278, gb_free=8.2, wall=88880
2022-03-05 10:18:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:18:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:18:52 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 17.984 | ppl 259165 | wps 34446.8 | wpb 510.9 | bsz 1 | num_updates 28525 | best_loss 7.674
2022-03-05 10:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28525 updates
2022-03-05 10:18:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:18:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:18:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 296 @ 28525 updates, score 17.984) (writing took 3.780638651922345 seconds)
2022-03-05 10:18:56 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 10:18:56 | INFO | train | epoch 296 | loss 0.492 | ppl 1.41 | wps 20483 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28525 | lr 0.000187235 | gnorm 0.86 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 88969
2022-03-05 10:18:56 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 10:18:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:22:45 | INFO | train_inner | epoch 297:     75 / 97 loss=0.491, ppl=1.41, wps=20529.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28600, lr=0.000186989, gnorm=0.863, loss_scale=16, train_wall=280, gb_free=8.2, wall=89199
2022-03-05 10:23:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:23:59 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 17.89 | ppl 242984 | wps 34644.2 | wpb 510.9 | bsz 1 | num_updates 28622 | best_loss 7.674
2022-03-05 10:23:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28622 updates
2022-03-05 10:23:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:24:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:24:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 297 @ 28622 updates, score 17.89) (writing took 3.780995711684227 seconds)
2022-03-05 10:24:03 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 10:24:03 | INFO | train | epoch 297 | loss 0.491 | ppl 1.41 | wps 20689.6 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 28622 | lr 0.000186918 | gnorm 0.861 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 89276
2022-03-05 10:24:03 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 10:24:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:27:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:28:04 | INFO | train_inner | epoch 298:     79 / 97 loss=0.49, ppl=1.4, wps=20527.3, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.863, loss_scale=16, train_wall=281, gb_free=8.2, wall=89518
2022-03-05 10:28:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:29:06 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 17.95 | ppl 253205 | wps 34374.6 | wpb 510.9 | bsz 1 | num_updates 28718 | best_loss 7.674
2022-03-05 10:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28718 updates
2022-03-05 10:29:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:29:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 298 @ 28718 updates, score 17.95) (writing took 3.771812694147229 seconds)
2022-03-05 10:29:10 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 10:29:10 | INFO | train | epoch 298 | loss 0.488 | ppl 1.4 | wps 20488.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28718 | lr 0.000186605 | gnorm 0.857 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 89583
2022-03-05 10:29:10 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 10:29:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:33:20 | INFO | train_inner | epoch 299:     82 / 97 loss=0.488, ppl=1.4, wps=20727.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=28800, lr=0.000186339, gnorm=0.851, loss_scale=16, train_wall=278, gb_free=8.2, wall=89834
2022-03-05 10:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:34:13 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 17.925 | ppl 248918 | wps 34048.6 | wpb 510.9 | bsz 1 | num_updates 28815 | best_loss 7.674
2022-03-05 10:34:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28815 updates
2022-03-05 10:34:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:34:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:34:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 299 @ 28815 updates, score 17.925) (writing took 3.7420266084372997 seconds)
2022-03-05 10:34:16 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 10:34:16 | INFO | train | epoch 299 | loss 0.488 | ppl 1.4 | wps 20715.5 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 28815 | lr 0.00018629 | gnorm 0.852 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 89890
2022-03-05 10:34:16 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 10:34:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:38:39 | INFO | train_inner | epoch 300:     86 / 97 loss=0.488, ppl=1.4, wps=20557.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.857, loss_scale=16, train_wall=280, gb_free=8.2, wall=90152
2022-03-05 10:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:39:19 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 17.948 | ppl 252904 | wps 34439.4 | wpb 510.9 | bsz 1 | num_updates 28911 | best_loss 7.674
2022-03-05 10:39:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28911 updates
2022-03-05 10:39:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:39:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:39:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 300 @ 28911 updates, score 17.948) (writing took 3.7505651228129864 seconds)
2022-03-05 10:39:22 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 10:39:22 | INFO | train | epoch 300 | loss 0.487 | ppl 1.4 | wps 20534.8 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 28911 | lr 0.000185981 | gnorm 0.858 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 90196
2022-03-05 10:39:22 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 10:39:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:43:52 | INFO | train_inner | epoch 301:     89 / 97 loss=0.486, ppl=1.4, wps=20893.1, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=29000, lr=0.000185695, gnorm=0.853, loss_scale=16, train_wall=276, gb_free=8.2, wall=90466
2022-03-05 10:44:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:44:23 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 17.93 | ppl 249661 | wps 34851 | wpb 510.9 | bsz 1 | num_updates 29008 | best_loss 7.674
2022-03-05 10:44:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 29008 updates
2022-03-05 10:44:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:44:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:44:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 301 @ 29008 updates, score 17.93) (writing took 3.6627609115093946 seconds)
2022-03-05 10:44:26 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 10:44:26 | INFO | train | epoch 301 | loss 0.485 | ppl 1.4 | wps 20902.7 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 29008 | lr 0.00018567 | gnorm 0.853 | loss_scale 32 | train_wall 267 | gb_free 8.2 | wall 90500
2022-03-05 10:44:26 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 10:44:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:44:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:49:08 | INFO | train_inner | epoch 302:     93 / 97 loss=0.484, ppl=1.4, wps=20777.2, ups=0.32, wpb=65492.9, bsz=127.9, num_updates=29100, lr=0.000185376, gnorm=0.853, loss_scale=16, train_wall=278, gb_free=8.2, wall=90781
2022-03-05 10:49:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:49:26 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 17.98 | ppl 258551 | wps 34484.9 | wpb 510.9 | bsz 1 | num_updates 29104 | best_loss 7.674
2022-03-05 10:49:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29104 updates
2022-03-05 10:49:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:49:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:49:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 302 @ 29104 updates, score 17.98) (writing took 3.6393863670527935 seconds)
2022-03-05 10:49:30 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 10:49:30 | INFO | train | epoch 302 | loss 0.484 | ppl 1.4 | wps 20734.2 | ups 0.32 | wpb 65491.1 | bsz 127.9 | num_updates 29104 | lr 0.000185363 | gnorm 0.853 | loss_scale 16 | train_wall 267 | gb_free 8.2 | wall 90803
2022-03-05 10:49:30 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 10:49:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:51:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:54:25 | INFO | train_inner | epoch 303:     97 / 97 loss=0.484, ppl=1.4, wps=20627.8, ups=0.32, wpb=65451.9, bsz=127.8, num_updates=29200, lr=0.000185058, gnorm=0.851, loss_scale=16, train_wall=279, gb_free=8.2, wall=91098
2022-03-05 10:54:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:54:31 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 18.06 | ppl 273315 | wps 34322.7 | wpb 510.9 | bsz 1 | num_updates 29200 | best_loss 7.674
2022-03-05 10:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29200 updates
2022-03-05 10:54:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:54:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:54:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 303 @ 29200 updates, score 18.06) (writing took 3.699699692428112 seconds)
2022-03-05 10:54:35 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 10:54:35 | INFO | train | epoch 303 | loss 0.482 | ppl 1.4 | wps 20587 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29200 | lr 0.000185058 | gnorm 0.85 | loss_scale 16 | train_wall 268 | gb_free 8.2 | wall 91108
2022-03-05 10:54:35 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 10:54:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:59:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:59:38 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 18.047 | ppl 270828 | wps 34198.8 | wpb 510.9 | bsz 1 | num_updates 29297 | best_loss 7.674
2022-03-05 10:59:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29297 updates
2022-03-05 10:59:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:59:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 10:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 304 @ 29297 updates, score 18.047) (writing took 3.8208151180297136 seconds)
2022-03-05 10:59:42 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 10:59:42 | INFO | train | epoch 304 | loss 0.481 | ppl 1.4 | wps 20702.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 29297 | lr 0.000184752 | gnorm 0.852 | loss_scale 32 | train_wall 269 | gb_free 8.2 | wall 91415
2022-03-05 10:59:42 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 10:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:59:51 | INFO | train_inner | epoch 305:      3 / 97 loss=0.48, ppl=1.39, wps=20074.5, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=29300, lr=0.000184742, gnorm=0.851, loss_scale=32, train_wall=278, gb_free=8.2, wall=91424
2022-03-05 11:03:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:04:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:04:45 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 18.005 | ppl 263100 | wps 34184 | wpb 510.9 | bsz 1 | num_updates 29393 | best_loss 7.674
2022-03-05 11:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29393 updates
2022-03-05 11:04:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:04:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 305 @ 29393 updates, score 18.005) (writing took 3.831796346232295 seconds)
2022-03-05 11:04:48 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 11:04:48 | INFO | train | epoch 305 | loss 0.479 | ppl 1.39 | wps 20510.4 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29393 | lr 0.00018445 | gnorm 0.849 | loss_scale 16 | train_wall 269 | gb_free 8.2 | wall 91722
2022-03-05 11:04:48 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 11:04:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:05:10 | INFO | train_inner | epoch 306:      7 / 97 loss=0.478, ppl=1.39, wps=20539.2, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.846, loss_scale=16, train_wall=280, gb_free=8.2, wall=91743
2022-03-05 11:09:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:09:53 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 17.978 | ppl 258169 | wps 34079.5 | wpb 510.9 | bsz 1 | num_updates 29490 | best_loss 7.674
2022-03-05 11:09:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29490 updates
2022-03-05 11:09:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:09:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 306 @ 29490 updates, score 17.978) (writing took 3.758071146905422 seconds)
2022-03-05 11:09:57 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 11:09:57 | INFO | train | epoch 306 | loss 0.478 | ppl 1.39 | wps 20603.4 | ups 0.31 | wpb 65491.6 | bsz 127.9 | num_updates 29490 | lr 0.000184146 | gnorm 0.854 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 92030
2022-03-05 11:09:57 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 11:09:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:10:28 | INFO | train_inner | epoch 307:     10 / 97 loss=0.478, ppl=1.39, wps=20626.4, ups=0.31, wpb=65492.9, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.857, loss_scale=32, train_wall=279, gb_free=8.2, wall=92061
2022-03-05 11:12:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:14:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:15:01 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 18.022 | ppl 266201 | wps 34316.3 | wpb 510.9 | bsz 1 | num_updates 29586 | best_loss 7.674
2022-03-05 11:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29586 updates
2022-03-05 11:15:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:15:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 307 @ 29586 updates, score 18.022) (writing took 3.763911921530962 seconds)
2022-03-05 11:15:04 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 11:15:04 | INFO | train | epoch 307 | loss 0.476 | ppl 1.39 | wps 20433.5 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29586 | lr 0.000183847 | gnorm 0.855 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 92338
2022-03-05 11:15:04 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 11:15:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:15:48 | INFO | train_inner | epoch 308:     14 / 97 loss=0.476, ppl=1.39, wps=20461.4, ups=0.31, wpb=65490.8, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.855, loss_scale=16, train_wall=281, gb_free=8.2, wall=92381
2022-03-05 11:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:20:08 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 18.039 | ppl 269401 | wps 34210.6 | wpb 510.9 | bsz 1 | num_updates 29683 | best_loss 7.674
2022-03-05 11:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29683 updates
2022-03-05 11:20:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:20:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 308 @ 29683 updates, score 18.039) (writing took 3.829460769891739 seconds)
2022-03-05 11:20:12 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 11:20:12 | INFO | train | epoch 308 | loss 0.476 | ppl 1.39 | wps 20649.8 | ups 0.32 | wpb 65491.6 | bsz 127.9 | num_updates 29683 | lr 0.000183546 | gnorm 0.849 | loss_scale 32 | train_wall 270 | gb_free 8.2 | wall 92645
2022-03-05 11:20:12 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 11:20:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:21:04 | INFO | train_inner | epoch 309:     17 / 97 loss=0.475, ppl=1.39, wps=20676.3, ups=0.32, wpb=65495, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.844, loss_scale=32, train_wall=278, gb_free=8.2, wall=92698
2022-03-05 11:21:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:25:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:25:15 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 18.045 | ppl 270419 | wps 34779.2 | wpb 510.9 | bsz 1 | num_updates 29779 | best_loss 7.674
2022-03-05 11:25:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 29779 updates
2022-03-05 11:25:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt
2022-03-05 11:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-cross_entropy_#2/checkpoint_last.pt (epoch 309 @ 29779 updates, score 18.045) (writing took 3.7635550536215305 seconds)
2022-03-05 11:25:19 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-03-05 11:25:19 | INFO | train | epoch 309 | loss 0.474 | ppl 1.39 | wps 20472.7 | ups 0.31 | wpb 65491.1 | bsz 127.9 | num_updates 29779 | lr 0.00018325 | gnorm 0.853 | loss_scale 16 | train_wall 270 | gb_free 8.2 | wall 92952
2022-03-05 11:25:19 | INFO | fairseq.trainer | begin training epoch 310
2022-03-05 11:25:19 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
