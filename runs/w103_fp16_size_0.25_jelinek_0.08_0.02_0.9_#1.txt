Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 206859225: <w103_fp16_size_0.25_jelinek_0.08_0.02_0.9_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.08_0.02_0.9_#1> was submitted from host <eu-login-36> by user <andriusb> in cluster <euler> at Wed Mar  2 08:33:00 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Wed Mar  2 10:37:06 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar  2 10:37:06 2022
Terminated at Thu Mar  3 09:44:41 2022
Results reported at Thu Mar  3 09:44:41 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.08, 0.02, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321671 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   83209.00 sec.
    Max Memory :                                 8479 MB
    Average Memory :                             3098.62 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11521.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   83255 sec.
    Turnaround time :                            90701 sec.

The output (if any) follows:

2022-03-02 10:37:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.08, 0.02, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-02 10:37:16 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-02 10:37:20 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 660/450337 [00:00<01:08, 6586.92it/s]  0%|          | 1319/450337 [00:00<01:17, 5789.51it/s]  0%|          | 1905/450337 [00:00<01:19, 5627.63it/s]  1%|          | 2471/450337 [00:00<01:20, 5572.73it/s]  1%|          | 3192/450337 [00:00<01:12, 6130.00it/s]  1%|          | 3809/450337 [00:00<01:14, 5980.37it/s]  1%|          | 4519/450337 [00:00<01:10, 6329.97it/s]  1%|          | 5236/450337 [00:00<01:07, 6586.99it/s]  1%|▏         | 5924/450337 [00:00<01:06, 6665.37it/s]  1%|▏         | 6593/450337 [00:01<01:13, 6064.64it/s]  2%|▏         | 7211/450337 [00:01<01:12, 6081.14it/s]  2%|▏         | 7827/450337 [00:01<01:13, 6055.03it/s]  2%|▏         | 8438/450337 [00:01<01:15, 5861.94it/s]  2%|▏         | 9087/450337 [00:01<01:13, 6040.55it/s]  2%|▏         | 9696/450337 [00:01<01:14, 5928.80it/s]  2%|▏         | 10331/450337 [00:01<01:12, 6048.49it/s]  2%|▏         | 10939/450337 [00:01<01:13, 5949.65it/s]  3%|▎         | 11541/450337 [00:01<01:13, 5965.41it/s]  3%|▎         | 12171/450337 [00:02<01:12, 6059.00it/s]  3%|▎         | 12779/450337 [00:02<01:13, 5916.76it/s]  3%|▎         | 13373/450337 [00:02<01:13, 5913.93it/s]  3%|▎         | 14022/450337 [00:02<01:11, 6080.87it/s]  3%|▎         | 14632/450337 [00:02<01:11, 6078.19it/s]  3%|▎         | 15302/450337 [00:02<01:09, 6259.91it/s]  4%|▎         | 15929/450337 [00:02<01:13, 5937.84it/s]  4%|▎         | 16527/450337 [00:02<01:13, 5869.29it/s]  4%|▍         | 17133/450337 [00:02<01:13, 5921.60it/s]  4%|▍         | 17728/450337 [00:02<01:13, 5909.53it/s]  4%|▍         | 18321/450337 [00:03<01:13, 5893.10it/s]  4%|▍         | 19039/450337 [00:03<01:08, 6270.96it/s]  4%|▍         | 19704/450337 [00:03<01:07, 6382.29it/s]  5%|▍         | 20344/450337 [00:03<01:11, 6024.37it/s]  5%|▍         | 20976/450337 [00:03<01:10, 6108.33it/s]  5%|▍         | 21591/450337 [00:03<01:13, 5823.73it/s]  5%|▍         | 22214/450337 [00:03<01:12, 5936.97it/s]  5%|▌         | 22928/450337 [00:03<01:08, 6279.99it/s]  5%|▌         | 23561/450337 [00:03<01:08, 6251.07it/s]  5%|▌         | 24295/450337 [00:03<01:04, 6566.46it/s]  6%|▌         | 24959/450337 [00:04<01:04, 6582.36it/s]  6%|▌         | 25620/450337 [00:04<01:05, 6530.15it/s]  6%|▌         | 26275/450337 [00:04<01:08, 6205.71it/s]  6%|▌         | 26900/450337 [00:04<01:10, 5986.98it/s]  6%|▌         | 27503/450337 [00:04<01:13, 5769.57it/s]  6%|▌         | 28140/450337 [00:04<01:11, 5933.65it/s]  6%|▋         | 28824/450337 [00:04<01:08, 6186.95it/s]  7%|▋         | 29447/450337 [00:04<01:11, 5865.92it/s]  7%|▋         | 30119/450337 [00:04<01:08, 6101.52it/s]  7%|▋         | 30735/450337 [00:05<01:12, 5809.22it/s]  7%|▋         | 31322/450337 [00:05<01:12, 5759.13it/s]  7%|▋         | 31906/450337 [00:05<01:12, 5780.58it/s]  7%|▋         | 32487/450337 [00:05<01:12, 5758.31it/s]  7%|▋         | 33065/450337 [00:05<01:14, 5567.09it/s]  7%|▋         | 33662/450337 [00:05<01:13, 5679.28it/s]  8%|▊         | 34234/450337 [00:05<01:13, 5690.92it/s]  8%|▊         | 34951/450337 [00:05<01:07, 6123.17it/s]  8%|▊         | 35566/450337 [00:05<01:10, 5885.82it/s]  8%|▊         | 36170/450337 [00:06<01:09, 5926.87it/s]  8%|▊         | 36769/450337 [00:06<01:09, 5942.96it/s]  8%|▊         | 37366/450337 [00:06<01:13, 5650.45it/s]  8%|▊         | 37935/450337 [00:06<01:13, 5585.01it/s]  9%|▊         | 38523/450337 [00:06<01:12, 5661.73it/s]  9%|▊         | 39139/450337 [00:06<01:10, 5805.73it/s]  9%|▉         | 39722/450337 [00:06<01:10, 5802.65it/s]  9%|▉         | 40385/450337 [00:06<01:07, 6042.51it/s]  9%|▉         | 41004/450337 [00:06<01:07, 6084.60it/s]  9%|▉         | 41614/450337 [00:06<01:08, 5965.86it/s]  9%|▉         | 42212/450337 [00:07<01:12, 5657.19it/s]  9%|▉         | 42782/450337 [00:07<01:12, 5656.14it/s] 10%|▉         | 43351/450337 [00:07<01:12, 5602.72it/s] 10%|▉         | 44016/450337 [00:07<01:08, 5904.67it/s] 10%|▉         | 44678/450337 [00:07<01:06, 6110.05it/s] 10%|█         | 45292/450337 [00:07<01:07, 5962.04it/s] 10%|█         | 45950/450337 [00:07<01:05, 6139.07it/s] 10%|█         | 46611/450337 [00:07<01:04, 6275.84it/s] 11%|█         | 47401/450337 [00:07<00:59, 6754.48it/s] 11%|█         | 48207/450337 [00:07<00:56, 7138.61it/s] 11%|█         | 48923/450337 [00:08<00:58, 6890.50it/s] 11%|█         | 49637/450337 [00:08<00:57, 6962.73it/s] 11%|█         | 50336/450337 [00:08<01:02, 6376.68it/s] 11%|█▏        | 50985/450337 [00:08<01:04, 6209.48it/s] 11%|█▏        | 51614/450337 [00:08<01:04, 6208.90it/s] 12%|█▏        | 52454/450337 [00:08<00:58, 6821.23it/s] 12%|█▏        | 53144/450337 [00:08<01:01, 6499.00it/s] 12%|█▏        | 53802/450337 [00:08<01:01, 6409.26it/s] 12%|█▏        | 54448/450337 [00:08<01:06, 5964.76it/s] 12%|█▏        | 55062/450337 [00:09<01:05, 6009.37it/s] 12%|█▏        | 55748/450337 [00:09<01:03, 6237.36it/s] 13%|█▎        | 56378/450337 [00:09<01:03, 6158.52it/s] 13%|█▎        | 56998/450337 [00:09<01:07, 5819.54it/s] 13%|█▎        | 57586/450337 [00:09<01:07, 5788.69it/s] 13%|█▎        | 58202/450337 [00:09<01:06, 5890.31it/s] 13%|█▎        | 58915/450337 [00:09<01:02, 6243.23it/s] 13%|█▎        | 59544/450337 [00:09<01:05, 5961.19it/s] 13%|█▎        | 60145/450337 [00:09<01:05, 5936.36it/s] 14%|█▎        | 60829/450337 [00:10<01:02, 6195.09it/s] 14%|█▎        | 61542/450337 [00:10<01:00, 6456.47it/s] 14%|█▍        | 62191/450337 [00:10<01:02, 6202.99it/s] 14%|█▍        | 62816/450337 [00:10<01:02, 6209.24it/s] 14%|█▍        | 63440/450337 [00:10<01:03, 6107.07it/s] 14%|█▍        | 64053/450337 [00:10<01:05, 5941.63it/s] 14%|█▍        | 64650/450337 [00:10<01:05, 5918.33it/s] 14%|█▍        | 65244/450337 [00:10<01:05, 5883.99it/s] 15%|█▍        | 65888/450337 [00:10<01:03, 6044.15it/s] 15%|█▍        | 66494/450337 [00:10<01:03, 6040.62it/s] 15%|█▍        | 67099/450337 [00:11<01:04, 5974.58it/s] 15%|█▌        | 67698/450337 [00:11<01:06, 5789.07it/s] 15%|█▌        | 68279/450337 [00:11<01:06, 5781.83it/s] 15%|█▌        | 69009/450337 [00:11<01:01, 6214.77it/s] 15%|█▌        | 69633/450337 [00:11<01:01, 6213.46it/s] 16%|█▌        | 70260/450337 [00:11<01:01, 6225.72it/s] 16%|█▌        | 70884/450337 [00:11<01:02, 6079.76it/s] 16%|█▌        | 71506/450337 [00:11<01:01, 6117.59it/s] 16%|█▌        | 72133/450337 [00:11<01:01, 6158.06it/s] 16%|█▌        | 72839/450337 [00:11<00:58, 6418.71it/s] 16%|█▋        | 73557/450337 [00:12<00:56, 6639.15it/s] 17%|█▋        | 74360/450337 [00:12<00:53, 7048.94it/s] 17%|█▋        | 75066/450337 [00:12<00:55, 6747.04it/s] 17%|█▋        | 75744/450337 [00:12<00:58, 6383.40it/s] 17%|█▋        | 76423/450337 [00:12<00:57, 6494.46it/s] 17%|█▋        | 77077/450337 [00:12<00:59, 6251.18it/s] 17%|█▋        | 77740/450337 [00:12<00:58, 6356.49it/s] 17%|█▋        | 78380/450337 [00:12<01:00, 6119.59it/s] 18%|█▊        | 78996/450337 [00:12<01:04, 5786.42it/s] 18%|█▊        | 79580/450337 [00:13<01:04, 5705.73it/s] 18%|█▊        | 80154/450337 [00:13<01:05, 5640.91it/s] 18%|█▊        | 80807/450337 [00:13<01:02, 5885.76it/s] 18%|█▊        | 81473/450337 [00:13<01:00, 6107.13it/s] 18%|█▊        | 82087/450337 [00:13<01:02, 5900.27it/s] 18%|█▊        | 82724/450337 [00:13<01:00, 6034.18it/s] 19%|█▊        | 83371/450337 [00:13<00:59, 6160.59it/s] 19%|█▊        | 84020/450337 [00:13<00:58, 6250.91it/s] 19%|█▉        | 84647/450337 [00:13<00:59, 6163.00it/s] 19%|█▉        | 85265/450337 [00:14<01:01, 5935.45it/s] 19%|█▉        | 85943/450337 [00:14<00:59, 6171.84it/s] 19%|█▉        | 86563/450337 [00:14<01:00, 6061.76it/s] 19%|█▉        | 87265/450337 [00:14<00:57, 6337.14it/s] 20%|█▉        | 87902/450337 [00:14<00:57, 6332.08it/s] 20%|█▉        | 88537/450337 [00:14<00:59, 6052.62it/s] 20%|█▉        | 89146/450337 [00:14<01:00, 5935.66it/s] 20%|█▉        | 89782/450337 [00:14<00:59, 6051.36it/s] 20%|██        | 90390/450337 [00:14<01:02, 5798.69it/s] 20%|██        | 90974/450337 [00:14<01:03, 5692.28it/s] 20%|██        | 91660/450337 [00:15<00:59, 6015.92it/s] 20%|██        | 92279/450337 [00:15<00:59, 6057.06it/s] 21%|██        | 92913/450337 [00:15<00:58, 6138.02it/s] 21%|██        | 93529/450337 [00:15<00:58, 6109.92it/s] 21%|██        | 94142/450337 [00:15<00:59, 5974.12it/s] 21%|██        | 94855/450337 [00:15<00:56, 6301.31it/s] 21%|██        | 95487/450337 [00:15<00:57, 6129.24it/s] 21%|██▏       | 96102/450337 [00:15<00:58, 6088.56it/s] 22%|██▏       | 96923/450337 [00:15<00:52, 6699.29it/s] 22%|██▏       | 97596/450337 [00:16<00:56, 6221.67it/s] 22%|██▏       | 98256/450337 [00:16<00:55, 6326.88it/s] 22%|██▏       | 98896/450337 [00:16<00:57, 6075.57it/s] 22%|██▏       | 99510/450337 [00:16<00:59, 5870.10it/s] 22%|██▏       | 100102/450337 [00:16<00:59, 5860.60it/s] 22%|██▏       | 100692/450337 [00:16<01:00, 5735.88it/s] 23%|██▎       | 101345/450337 [00:16<00:58, 5960.28it/s] 23%|██▎       | 102010/450337 [00:16<00:56, 6145.53it/s] 23%|██▎       | 102628/450337 [00:16<00:57, 6038.72it/s] 23%|██▎       | 103234/450337 [00:16<00:57, 6001.96it/s] 23%|██▎       | 103836/450337 [00:17<00:59, 5869.55it/s] 23%|██▎       | 104449/450337 [00:17<00:58, 5941.88it/s] 23%|██▎       | 105045/450337 [00:17<00:59, 5840.15it/s] 23%|██▎       | 105685/450337 [00:17<00:57, 6000.22it/s] 24%|██▎       | 106287/450337 [00:17<00:57, 5939.37it/s] 24%|██▎       | 106882/450337 [00:17<00:58, 5830.40it/s] 24%|██▍       | 107466/450337 [00:17<00:59, 5770.42it/s] 24%|██▍       | 108136/450337 [00:17<00:56, 6036.26it/s] 24%|██▍       | 108741/450337 [00:17<01:00, 5621.62it/s] 24%|██▍       | 109324/450337 [00:18<01:00, 5678.04it/s] 24%|██▍       | 109929/450337 [00:18<00:58, 5780.38it/s] 25%|██▍       | 110562/450337 [00:18<00:57, 5939.57it/s] 25%|██▍       | 111231/450337 [00:18<00:55, 6153.85it/s] 25%|██▍       | 111850/450337 [00:18<00:56, 5969.87it/s] 25%|██▍       | 112527/450337 [00:18<00:54, 6198.71it/s] 25%|██▌       | 113150/450337 [00:18<00:56, 6012.73it/s] 25%|██▌       | 113755/450337 [00:18<00:57, 5811.68it/s] 25%|██▌       | 114405/450337 [00:18<00:55, 6000.41it/s] 26%|██▌       | 115017/450337 [00:18<00:55, 6029.63it/s] 26%|██▌       | 115623/450337 [00:19<01:00, 5540.76it/s] 26%|██▌       | 116248/450337 [00:19<00:58, 5734.39it/s] 26%|██▌       | 116830/450337 [00:19<00:58, 5663.21it/s] 26%|██▌       | 117402/450337 [00:19<00:58, 5668.80it/s] 26%|██▌       | 117995/450337 [00:19<00:57, 5744.00it/s] 26%|██▋       | 118651/450337 [00:19<00:55, 5981.74it/s] 26%|██▋       | 119252/450337 [00:19<00:56, 5874.42it/s] 27%|██▋       | 119842/450337 [00:19<00:56, 5801.68it/s] 27%|██▋       | 120618/450337 [00:19<00:51, 6371.00it/s] 27%|██▋       | 121258/450337 [00:20<00:53, 6182.58it/s] 27%|██▋       | 121880/450337 [00:20<00:55, 5923.22it/s] 27%|██▋       | 122484/450337 [00:20<00:55, 5954.84it/s] 27%|██▋       | 123083/450337 [00:20<00:55, 5918.81it/s] 27%|██▋       | 123677/450337 [00:20<00:55, 5876.84it/s] 28%|██▊       | 124275/450337 [00:20<00:55, 5904.47it/s] 28%|██▊       | 124867/450337 [00:20<00:55, 5852.25it/s] 28%|██▊       | 125453/450337 [00:20<00:55, 5843.26it/s] 28%|██▊       | 126171/450337 [00:20<00:51, 6236.84it/s] 28%|██▊       | 126812/450337 [00:20<00:51, 6286.70it/s] 28%|██▊       | 127442/450337 [00:21<00:53, 6035.52it/s] 28%|██▊       | 128113/450337 [00:21<00:51, 6226.17it/s] 29%|██▊       | 128739/450337 [00:21<00:53, 5955.96it/s] 29%|██▊       | 129368/450337 [00:21<00:53, 6049.63it/s] 29%|██▉       | 129977/450337 [00:21<00:54, 5853.00it/s] 29%|██▉       | 130566/450337 [00:21<00:54, 5843.30it/s] 29%|██▉       | 131207/450337 [00:21<00:53, 6005.44it/s] 29%|██▉       | 131810/450337 [00:21<00:59, 5342.79it/s] 29%|██▉       | 132567/450337 [00:21<00:53, 5920.09it/s] 30%|██▉       | 133188/450337 [00:22<00:52, 5992.97it/s] 30%|██▉       | 133799/450337 [00:22<00:54, 5838.78it/s] 30%|██▉       | 134528/450337 [00:22<00:50, 6245.24it/s] 30%|███       | 135241/450337 [00:22<00:48, 6495.70it/s] 30%|███       | 135898/450337 [00:22<00:49, 6365.38it/s] 30%|███       | 136601/450337 [00:22<00:47, 6556.47it/s] 30%|███       | 137340/450337 [00:22<00:46, 6796.34it/s] 31%|███       | 138024/450337 [00:22<00:49, 6311.44it/s] 31%|███       | 138665/450337 [00:22<00:49, 6256.29it/s] 31%|███       | 139297/450337 [00:23<00:50, 6121.67it/s] 31%|███       | 139930/450337 [00:23<00:50, 6178.89it/s] 31%|███       | 140552/450337 [00:23<00:51, 6027.25it/s] 31%|███▏      | 141188/450337 [00:23<00:50, 6121.91it/s] 31%|███▏      | 141803/450337 [00:23<00:52, 5916.36it/s] 32%|███▏      | 142410/450337 [00:23<00:51, 5955.14it/s] 32%|███▏      | 143008/450337 [00:23<00:52, 5804.63it/s] 32%|███▏      | 143748/450337 [00:23<00:49, 6240.84it/s] 32%|███▏      | 144375/450337 [00:23<00:49, 6198.40it/s] 32%|███▏      | 145126/450337 [00:23<00:46, 6576.63it/s] 32%|███▏      | 145787/450337 [00:24<00:47, 6447.36it/s] 33%|███▎      | 146434/450337 [00:24<00:48, 6264.65it/s] 33%|███▎      | 147063/450337 [00:24<00:50, 6006.83it/s] 33%|███▎      | 147667/450337 [00:24<00:52, 5760.36it/s] 33%|███▎      | 148249/450337 [00:24<00:52, 5769.41it/s] 33%|███▎      | 148881/450337 [00:24<00:50, 5922.33it/s] 33%|███▎      | 149476/450337 [00:24<00:50, 5918.16it/s] 33%|███▎      | 150070/450337 [00:24<00:51, 5803.00it/s] 33%|███▎      | 150652/450337 [00:24<00:51, 5774.15it/s] 34%|███▎      | 151231/450337 [00:25<00:53, 5593.12it/s] 34%|███▎      | 151843/450337 [00:25<00:51, 5741.61it/s] 34%|███▍      | 152419/450337 [00:25<00:52, 5714.48it/s] 34%|███▍      | 153080/450337 [00:25<00:49, 5976.13it/s] 34%|███▍      | 153680/450337 [00:25<00:49, 5962.51it/s] 34%|███▍      | 154278/450337 [00:25<00:50, 5906.99it/s] 34%|███▍      | 154870/450337 [00:25<00:50, 5899.49it/s] 35%|███▍      | 155506/450337 [00:25<00:48, 6030.20it/s] 35%|███▍      | 156151/450337 [00:25<00:47, 6154.08it/s] 35%|███▍      | 156767/450337 [00:25<00:48, 6081.97it/s] 35%|███▍      | 157449/450337 [00:26<00:46, 6298.81it/s] 35%|███▌      | 158114/450337 [00:26<00:45, 6400.96it/s] 35%|███▌      | 158762/450337 [00:26<00:45, 6409.24it/s] 35%|███▌      | 159404/450337 [00:26<00:47, 6094.86it/s] 36%|███▌      | 160089/450337 [00:26<00:46, 6306.37it/s] 36%|███▌      | 160723/450337 [00:26<00:47, 6039.94it/s] 36%|███▌      | 161379/450337 [00:26<00:46, 6176.63it/s] 36%|███▌      | 162001/450337 [00:26<00:47, 6031.00it/s] 36%|███▌      | 162656/450337 [00:26<00:46, 6175.74it/s] 36%|███▋      | 163300/450337 [00:26<00:45, 6251.98it/s] 36%|███▋      | 163928/450337 [00:27<00:48, 5906.27it/s] 37%|███▋      | 164529/450337 [00:27<00:48, 5932.99it/s] 37%|███▋      | 165230/450337 [00:27<00:45, 6241.11it/s] 37%|███▋      | 165896/450337 [00:27<00:44, 6358.22it/s] 37%|███▋      | 166535/450337 [00:27<00:44, 6316.19it/s] 37%|███▋      | 167215/450337 [00:27<00:43, 6457.81it/s] 37%|███▋      | 167863/450337 [00:27<00:45, 6151.58it/s] 37%|███▋      | 168483/450337 [00:27<00:46, 6007.77it/s] 38%|███▊      | 169140/450337 [00:27<00:45, 6167.81it/s] 38%|███▊      | 169760/450337 [00:28<00:46, 5995.12it/s] 38%|███▊      | 170363/450337 [00:28<00:49, 5671.97it/s] 38%|███▊      | 170973/450337 [00:28<00:48, 5782.98it/s] 38%|███▊      | 171592/450337 [00:28<00:47, 5898.37it/s] 38%|███▊      | 172186/450337 [00:28<00:49, 5655.57it/s] 38%|███▊      | 172829/450337 [00:28<00:47, 5871.27it/s] 39%|███▊      | 173421/450337 [00:28<00:47, 5871.41it/s] 39%|███▊      | 174043/450337 [00:28<00:46, 5958.24it/s] 39%|███▉      | 174724/450337 [00:28<00:44, 6197.17it/s] 39%|███▉      | 175346/450337 [00:28<00:46, 5937.96it/s] 39%|███▉      | 175944/450337 [00:29<00:46, 5855.73it/s] 39%|███▉      | 176532/450337 [00:29<00:48, 5686.14it/s] 39%|███▉      | 177127/450337 [00:29<00:47, 5749.05it/s] 39%|███▉      | 177704/450337 [00:29<00:51, 5283.78it/s] 40%|███▉      | 178345/450337 [00:29<00:48, 5591.11it/s] 40%|███▉      | 178971/450337 [00:29<00:46, 5779.43it/s] 40%|███▉      | 179608/450337 [00:29<00:45, 5940.95it/s] 40%|████      | 180208/450337 [00:29<00:46, 5748.64it/s] 40%|████      | 180831/450337 [00:29<00:45, 5880.19it/s] 40%|████      | 181423/450337 [00:30<00:46, 5767.86it/s] 40%|████      | 182003/450337 [00:30<00:48, 5544.83it/s] 41%|████      | 182586/450337 [00:30<00:47, 5624.75it/s] 41%|████      | 183180/450337 [00:30<00:46, 5714.27it/s] 41%|████      | 183754/450337 [00:30<00:49, 5348.85it/s] 41%|████      | 184295/450337 [00:30<00:49, 5364.41it/s] 41%|████      | 184921/450337 [00:30<00:47, 5616.41it/s] 41%|████      | 185487/450337 [00:30<00:48, 5480.53it/s] 41%|████▏     | 186047/450337 [00:30<00:47, 5512.38it/s] 41%|████▏     | 186623/450337 [00:30<00:47, 5583.78it/s] 42%|████▏     | 187240/450337 [00:31<00:45, 5742.79it/s] 42%|████▏     | 187844/450337 [00:31<00:45, 5830.17it/s] 42%|████▏     | 188480/450337 [00:31<00:43, 5987.05it/s] 42%|████▏     | 189112/450337 [00:31<00:42, 6084.43it/s] 42%|████▏     | 189728/450337 [00:31<00:42, 6105.20it/s] 42%|████▏     | 190340/450337 [00:31<00:42, 6051.14it/s] 42%|████▏     | 190968/450337 [00:31<00:42, 6116.97it/s] 43%|████▎     | 191581/450337 [00:31<00:43, 5931.28it/s] 43%|████▎     | 192229/450337 [00:31<00:42, 6091.20it/s] 43%|████▎     | 192842/450337 [00:32<00:42, 6102.52it/s] 43%|████▎     | 193461/450337 [00:32<00:41, 6123.19it/s] 43%|████▎     | 194094/450337 [00:32<00:41, 6182.43it/s] 43%|████▎     | 194713/450337 [00:32<00:42, 5949.41it/s] 43%|████▎     | 195357/450337 [00:32<00:41, 6086.07it/s] 44%|████▎     | 195984/450337 [00:32<00:41, 6132.45it/s] 44%|████▎     | 196599/450337 [00:32<00:43, 5867.38it/s] 44%|████▍     | 197477/450337 [00:32<00:37, 6703.41it/s] 44%|████▍     | 198154/450337 [00:32<00:39, 6412.50it/s] 44%|████▍     | 198802/450337 [00:32<00:40, 6197.38it/s] 44%|████▍     | 199465/450337 [00:33<00:39, 6317.67it/s] 44%|████▍     | 200102/450337 [00:33<00:41, 5995.06it/s] 45%|████▍     | 200707/450337 [00:33<00:43, 5712.27it/s] 45%|████▍     | 201323/450337 [00:33<00:42, 5824.01it/s] 45%|████▍     | 201938/450337 [00:33<00:42, 5911.99it/s] 45%|████▍     | 202533/450337 [00:33<00:42, 5851.94it/s] 45%|████▌     | 203174/450337 [00:33<00:41, 6011.24it/s] 45%|████▌     | 203886/450337 [00:33<00:38, 6334.39it/s] 45%|████▌     | 204523/450337 [00:33<00:40, 6002.14it/s] 46%|████▌     | 205129/450337 [00:34<00:41, 5858.73it/s] 46%|████▌     | 205719/450337 [00:34<00:42, 5816.16it/s] 46%|████▌     | 206389/450337 [00:34<00:40, 6069.04it/s] 46%|████▌     | 206999/450337 [00:34<00:41, 5820.73it/s] 46%|████▌     | 207585/450337 [00:34<00:41, 5818.62it/s] 46%|████▌     | 208170/450337 [00:34<00:42, 5765.01it/s] 46%|████▋     | 208749/450337 [00:34<00:43, 5554.27it/s] 46%|████▋     | 209309/450337 [00:34<00:43, 5566.10it/s] 47%|████▋     | 209959/450337 [00:34<00:41, 5830.58it/s] 47%|████▋     | 210592/450337 [00:34<00:40, 5967.58it/s] 47%|████▋     | 211240/450337 [00:35<00:39, 6112.28it/s] 47%|████▋     | 211853/450337 [00:35<00:39, 6023.80it/s] 47%|████▋     | 212457/450337 [00:35<00:39, 5983.58it/s] 47%|████▋     | 213166/450337 [00:35<00:37, 6307.99it/s] 47%|████▋     | 213799/450337 [00:35<00:40, 5795.82it/s] 48%|████▊     | 214388/450337 [00:35<00:40, 5809.96it/s] 48%|████▊     | 214976/450337 [00:35<00:43, 5430.72it/s] 48%|████▊     | 215528/450337 [00:35<00:43, 5443.56it/s] 48%|████▊     | 216142/450337 [00:35<00:41, 5638.58it/s] 48%|████▊     | 216712/450337 [00:36<00:41, 5604.67it/s] 48%|████▊     | 217302/450337 [00:36<00:40, 5684.72it/s] 48%|████▊     | 217958/450337 [00:36<00:39, 5937.60it/s] 49%|████▊     | 218575/450337 [00:36<00:38, 6005.30it/s] 49%|████▊     | 219230/450337 [00:36<00:37, 6163.54it/s] 49%|████▉     | 219848/450337 [00:36<00:39, 5879.89it/s] 49%|████▉     | 220440/450337 [00:36<00:39, 5844.20it/s] 49%|████▉     | 221093/450337 [00:36<00:37, 6038.92it/s] 49%|████▉     | 221700/450337 [00:36<00:38, 5882.48it/s] 49%|████▉     | 222291/450337 [00:36<00:40, 5588.10it/s] 50%|████▉     | 222918/450337 [00:37<00:39, 5777.39it/s] 50%|████▉     | 223500/450337 [00:37<00:41, 5460.08it/s] 50%|████▉     | 224131/450337 [00:37<00:39, 5696.41it/s] 50%|████▉     | 224741/450337 [00:37<00:38, 5811.11it/s] 50%|█████     | 225405/450337 [00:37<00:37, 6047.37it/s] 50%|█████     | 226168/450337 [00:37<00:34, 6509.30it/s] 50%|█████     | 226823/450337 [00:37<00:36, 6136.66it/s] 51%|█████     | 227444/450337 [00:37<00:36, 6077.20it/s] 51%|█████     | 228057/450337 [00:37<00:37, 5875.52it/s] 51%|█████     | 228649/450337 [00:38<00:38, 5762.82it/s] 51%|█████     | 229228/450337 [00:38<00:38, 5752.62it/s] 51%|█████     | 229806/450337 [00:38<00:40, 5388.08it/s] 51%|█████     | 230438/450337 [00:38<00:38, 5645.81it/s] 51%|█████▏    | 231045/450337 [00:38<00:38, 5759.64it/s] 51%|█████▏    | 231626/450337 [00:38<00:38, 5681.46it/s] 52%|█████▏    | 232198/450337 [00:38<00:38, 5625.39it/s] 52%|█████▏    | 232910/450337 [00:38<00:35, 6057.13it/s] 52%|█████▏    | 233636/450337 [00:38<00:33, 6398.88it/s] 52%|█████▏    | 234279/450337 [00:39<00:33, 6371.33it/s] 52%|█████▏    | 234919/450337 [00:39<00:34, 6293.30it/s] 52%|█████▏    | 235566/450337 [00:39<00:33, 6343.93it/s] 52%|█████▏    | 236202/450337 [00:39<00:34, 6131.71it/s] 53%|█████▎    | 236818/450337 [00:39<00:36, 5861.67it/s] 53%|█████▎    | 237426/450337 [00:39<00:35, 5921.95it/s] 53%|█████▎    | 238021/450337 [00:39<00:37, 5674.23it/s] 53%|█████▎    | 238830/450337 [00:39<00:33, 6350.96it/s] 53%|█████▎    | 239472/450337 [00:39<00:34, 6133.49it/s] 53%|█████▎    | 240091/450337 [00:39<00:35, 5847.65it/s] 53%|█████▎    | 240902/450337 [00:40<00:32, 6475.65it/s] 54%|█████▎    | 241559/450337 [00:40<00:32, 6344.71it/s] 54%|█████▍    | 242200/450337 [00:40<00:32, 6360.07it/s] 54%|█████▍    | 242841/450337 [00:40<00:33, 6232.11it/s] 54%|█████▍    | 243468/450337 [00:40<00:33, 6218.16it/s] 54%|█████▍    | 244093/450337 [00:40<00:33, 6090.96it/s] 54%|█████▍    | 244704/450337 [00:40<00:33, 6086.89it/s] 54%|█████▍    | 245314/450337 [00:40<00:33, 6088.14it/s] 55%|█████▍    | 245977/450337 [00:40<00:32, 6246.09it/s] 55%|█████▍    | 246701/450337 [00:41<00:31, 6539.36it/s] 55%|█████▍    | 247361/450337 [00:41<00:31, 6544.16it/s] 55%|█████▌    | 248017/450337 [00:41<00:31, 6343.65it/s] 55%|█████▌    | 248654/450337 [00:41<00:32, 6258.12it/s] 55%|█████▌    | 249346/450337 [00:41<00:31, 6449.54it/s] 56%|█████▌    | 250003/450337 [00:41<00:30, 6478.97it/s] 56%|█████▌    | 250652/450337 [00:41<00:31, 6313.62it/s] 56%|█████▌    | 251291/450337 [00:41<00:31, 6332.93it/s] 56%|█████▌    | 251926/450337 [00:41<00:31, 6306.10it/s] 56%|█████▌    | 252558/450337 [00:41<00:31, 6228.82it/s] 56%|█████▌    | 253182/450337 [00:42<00:32, 5983.49it/s] 56%|█████▋    | 253884/450337 [00:42<00:31, 6279.97it/s] 57%|█████▋    | 254515/450337 [00:42<00:32, 5996.27it/s] 57%|█████▋    | 255155/450337 [00:42<00:31, 6105.27it/s] 57%|█████▋    | 255826/450337 [00:42<00:31, 6273.92it/s] 57%|█████▋    | 256457/450337 [00:42<00:32, 6058.46it/s] 57%|█████▋    | 257132/450337 [00:42<00:30, 6256.17it/s] 57%|█████▋    | 257812/450337 [00:42<00:30, 6413.98it/s] 57%|█████▋    | 258457/450337 [00:42<00:33, 5685.03it/s] 58%|█████▊    | 259042/450337 [00:43<00:33, 5708.98it/s] 58%|█████▊    | 259625/450337 [00:43<00:33, 5692.20it/s] 58%|█████▊    | 260256/450337 [00:43<00:32, 5864.34it/s] 58%|█████▊    | 261002/450337 [00:43<00:29, 6321.82it/s] 58%|█████▊    | 261641/450337 [00:43<00:30, 6198.47it/s] 58%|█████▊    | 262266/450337 [00:43<00:31, 5909.15it/s] 58%|█████▊    | 262863/450337 [00:43<00:32, 5840.42it/s] 59%|█████▊    | 263451/450337 [00:43<00:32, 5670.98it/s] 59%|█████▊    | 264058/450337 [00:43<00:32, 5783.09it/s] 59%|█████▉    | 264644/450337 [00:43<00:32, 5799.91it/s] 59%|█████▉    | 265227/450337 [00:44<00:31, 5803.10it/s] 59%|█████▉    | 265809/450337 [00:44<00:32, 5732.68it/s] 59%|█████▉    | 266411/450337 [00:44<00:31, 5816.52it/s] 59%|█████▉    | 266994/450337 [00:44<00:32, 5668.00it/s] 59%|█████▉    | 267583/450337 [00:44<00:31, 5718.05it/s] 60%|█████▉    | 268157/450337 [00:44<00:31, 5723.69it/s] 60%|█████▉    | 268783/450337 [00:44<00:30, 5880.41it/s] 60%|█████▉    | 269444/450337 [00:44<00:29, 6091.13it/s] 60%|█████▉    | 270054/450337 [00:44<00:30, 5961.47it/s] 60%|██████    | 270681/450337 [00:45<00:29, 6046.15it/s] 60%|██████    | 271287/450337 [00:45<00:30, 5961.86it/s] 60%|██████    | 271944/450337 [00:45<00:29, 6134.27it/s] 61%|██████    | 272707/450337 [00:45<00:27, 6575.17it/s] 61%|██████    | 273366/450337 [00:45<00:27, 6537.84it/s] 61%|██████    | 274021/450337 [00:45<00:27, 6388.26it/s] 61%|██████    | 274662/450337 [00:45<00:28, 6188.61it/s] 61%|██████    | 275283/450337 [00:45<00:29, 5853.31it/s] 61%|██████▏   | 275900/450337 [00:45<00:29, 5941.70it/s] 61%|██████▏   | 276498/450337 [00:45<00:31, 5601.73it/s] 62%|██████▏   | 277117/450337 [00:46<00:30, 5764.48it/s] 62%|██████▏   | 277699/450337 [00:46<00:30, 5594.24it/s] 62%|██████▏   | 278385/450337 [00:46<00:28, 5950.67it/s] 62%|██████▏   | 279016/450337 [00:46<00:28, 6048.35it/s] 62%|██████▏   | 279625/450337 [00:46<00:28, 5921.43it/s] 62%|██████▏   | 280298/450337 [00:46<00:27, 6154.65it/s] 62%|██████▏   | 280917/450337 [00:46<00:29, 5677.66it/s] 63%|██████▎   | 281525/450337 [00:46<00:29, 5789.05it/s] 63%|██████▎   | 282149/450337 [00:46<00:28, 5909.70it/s] 63%|██████▎   | 282746/450337 [00:47<00:29, 5755.04it/s] 63%|██████▎   | 283426/450337 [00:47<00:27, 6049.26it/s] 63%|██████▎   | 284060/450337 [00:47<00:27, 6129.83it/s] 63%|██████▎   | 284677/450337 [00:47<00:27, 5999.34it/s] 63%|██████▎   | 285280/450337 [00:47<00:27, 5981.12it/s] 63%|██████▎   | 285895/450337 [00:47<00:27, 6028.30it/s] 64%|██████▎   | 286500/450337 [00:47<00:28, 5810.53it/s] 64%|██████▎   | 287084/450337 [00:47<00:28, 5794.47it/s] 64%|██████▍   | 287666/450337 [00:47<00:28, 5800.82it/s] 64%|██████▍   | 288353/450337 [00:47<00:26, 6113.60it/s] 64%|██████▍   | 288974/450337 [00:48<00:26, 6138.81it/s] 64%|██████▍   | 289589/450337 [00:48<00:27, 5825.23it/s] 64%|██████▍   | 290176/450337 [00:48<00:28, 5617.12it/s] 65%|██████▍   | 290815/450337 [00:48<00:27, 5829.54it/s] 65%|██████▍   | 291402/450337 [00:48<00:27, 5794.11it/s] 65%|██████▍   | 292002/450337 [00:48<00:27, 5850.43it/s] 65%|██████▍   | 292646/450337 [00:48<00:26, 6021.24it/s] 65%|██████▌   | 293250/450337 [00:48<00:26, 5924.78it/s] 65%|██████▌   | 293844/450337 [00:48<00:27, 5777.84it/s] 65%|██████▌   | 294465/450337 [00:49<00:26, 5902.80it/s] 66%|██████▌   | 295173/450337 [00:49<00:24, 6242.63it/s] 66%|██████▌   | 295800/450337 [00:49<00:24, 6183.66it/s] 66%|██████▌   | 296420/450337 [00:49<00:25, 5998.79it/s] 66%|██████▌   | 297053/450337 [00:49<00:25, 6093.54it/s] 66%|██████▌   | 297665/450337 [00:49<00:25, 6088.05it/s] 66%|██████▌   | 298275/450337 [00:49<00:25, 6075.91it/s] 66%|██████▋   | 298884/450337 [00:49<00:25, 6026.10it/s] 67%|██████▋   | 299542/450337 [00:49<00:24, 6183.47it/s] 67%|██████▋   | 300161/450337 [00:49<00:24, 6159.73it/s] 67%|██████▋   | 300778/450337 [00:50<00:24, 6034.37it/s] 67%|██████▋   | 301383/450337 [00:50<00:25, 5808.03it/s] 67%|██████▋   | 301966/450337 [00:50<00:26, 5576.28it/s] 67%|██████▋   | 302663/450337 [00:50<00:24, 5960.63it/s] 67%|██████▋   | 303315/450337 [00:50<00:24, 6118.74it/s] 67%|██████▋   | 303936/450337 [00:50<00:23, 6140.89it/s] 68%|██████▊   | 304553/450337 [00:50<00:24, 5999.27it/s] 68%|██████▊   | 305165/450337 [00:50<00:24, 6030.43it/s] 68%|██████▊   | 305770/450337 [00:50<00:24, 5866.53it/s] 68%|██████▊   | 306388/450337 [00:50<00:24, 5950.75it/s] 68%|██████▊   | 306985/450337 [00:51<00:24, 5762.21it/s] 68%|██████▊   | 307617/450337 [00:51<00:24, 5921.06it/s] 68%|██████▊   | 308212/450337 [00:51<00:24, 5721.86it/s] 69%|██████▊   | 308787/450337 [00:51<00:25, 5654.67it/s] 69%|██████▉   | 309716/450337 [00:51<00:20, 6698.08it/s] 69%|██████▉   | 310392/450337 [00:51<00:21, 6396.84it/s] 69%|██████▉   | 311038/450337 [00:51<00:22, 6208.11it/s] 69%|██████▉   | 311664/450337 [00:51<00:22, 6099.00it/s] 69%|██████▉   | 312314/450337 [00:51<00:22, 6209.04it/s] 69%|██████▉   | 312970/450337 [00:52<00:21, 6309.92it/s] 70%|██████▉   | 313604/450337 [00:52<00:21, 6315.89it/s] 70%|██████▉   | 314303/450337 [00:52<00:20, 6505.83it/s] 70%|██████▉   | 314956/450337 [00:52<00:22, 5945.63it/s] 70%|███████   | 315561/450337 [00:52<00:22, 5895.04it/s] 70%|███████   | 316193/450337 [00:52<00:22, 6012.52it/s] 70%|███████   | 316800/450337 [00:52<00:23, 5791.71it/s] 70%|███████   | 317411/450337 [00:52<00:22, 5877.71it/s] 71%|███████   | 318003/450337 [00:52<00:23, 5678.54it/s] 71%|███████   | 318575/450337 [00:53<00:23, 5639.08it/s] 71%|███████   | 319203/450337 [00:53<00:22, 5818.76it/s] 71%|███████   | 319822/450337 [00:53<00:22, 5920.97it/s] 71%|███████   | 320417/450337 [00:53<00:22, 5675.38it/s] 71%|███████▏  | 321139/450337 [00:53<00:21, 6114.57it/s] 71%|███████▏  | 321755/450337 [00:53<00:21, 5963.79it/s] 72%|███████▏  | 322355/450337 [00:53<00:22, 5712.74it/s] 72%|███████▏  | 322931/450337 [00:53<00:22, 5713.62it/s] 72%|███████▏  | 323557/450337 [00:53<00:21, 5866.84it/s] 72%|███████▏  | 324209/450337 [00:53<00:20, 6045.37it/s] 72%|███████▏  | 324816/450337 [00:54<00:20, 5981.57it/s] 72%|███████▏  | 325416/450337 [00:54<00:22, 5503.66it/s] 72%|███████▏  | 326022/450337 [00:54<00:22, 5647.96it/s] 73%|███████▎  | 326667/450337 [00:54<00:21, 5872.62it/s] 73%|███████▎  | 327346/450337 [00:54<00:20, 6134.12it/s] 73%|███████▎  | 328155/450337 [00:54<00:18, 6702.35it/s] 73%|███████▎  | 328831/450337 [00:54<00:18, 6648.22it/s] 73%|███████▎  | 329500/450337 [00:54<00:19, 6195.25it/s] 73%|███████▎  | 330128/450337 [00:54<00:20, 5775.27it/s] 73%|███████▎  | 330716/450337 [00:55<00:21, 5644.55it/s] 74%|███████▎  | 331308/450337 [00:55<00:20, 5715.48it/s] 74%|███████▎  | 331891/450337 [00:55<00:20, 5745.32it/s] 74%|███████▍  | 332558/450337 [00:55<00:19, 6009.12it/s] 74%|███████▍  | 333163/450337 [00:55<00:20, 5780.50it/s] 74%|███████▍  | 333806/450337 [00:55<00:19, 5962.75it/s] 74%|███████▍  | 334407/450337 [00:55<00:19, 5914.22it/s] 74%|███████▍  | 335027/450337 [00:55<00:19, 5996.90it/s] 75%|███████▍  | 335629/450337 [00:55<00:19, 5986.90it/s] 75%|███████▍  | 336256/450337 [00:55<00:18, 6067.90it/s] 75%|███████▍  | 336864/450337 [00:56<00:18, 5995.49it/s] 75%|███████▍  | 337556/450337 [00:56<00:18, 6258.60it/s] 75%|███████▌  | 338183/450337 [00:56<00:17, 6244.68it/s] 75%|███████▌  | 338841/450337 [00:56<00:17, 6341.37it/s] 75%|███████▌  | 339476/450337 [00:56<00:17, 6254.71it/s] 76%|███████▌  | 340103/450337 [00:56<00:19, 5599.01it/s] 76%|███████▌  | 340747/450337 [00:56<00:18, 5828.83it/s] 76%|███████▌  | 341360/450337 [00:56<00:18, 5911.70it/s] 76%|███████▌  | 341971/450337 [00:56<00:18, 5967.71it/s] 76%|███████▌  | 342596/450337 [00:57<00:17, 6047.84it/s] 76%|███████▌  | 343206/450337 [00:57<00:17, 6033.44it/s] 76%|███████▋  | 343914/450337 [00:57<00:16, 6340.92it/s] 77%|███████▋  | 344551/450337 [00:57<00:18, 5835.89it/s] 77%|███████▋  | 345233/450337 [00:57<00:17, 6103.40it/s] 77%|███████▋  | 345852/450337 [00:57<00:18, 5575.00it/s] 77%|███████▋  | 346571/450337 [00:57<00:17, 6010.16it/s] 77%|███████▋  | 347187/450337 [00:57<00:17, 5921.97it/s] 77%|███████▋  | 347789/450337 [00:57<00:17, 5887.31it/s] 77%|███████▋  | 348385/450337 [00:58<00:17, 5765.11it/s] 78%|███████▊  | 349021/450337 [00:58<00:17, 5931.64it/s] 78%|███████▊  | 349619/450337 [00:58<00:16, 5941.30it/s] 78%|███████▊  | 350454/450337 [00:58<00:15, 6641.04it/s] 78%|███████▊  | 351123/450337 [00:58<00:15, 6203.14it/s] 78%|███████▊  | 351752/450337 [00:58<00:15, 6165.86it/s] 78%|███████▊  | 352375/450337 [00:58<00:17, 5675.43it/s] 78%|███████▊  | 352954/450337 [00:58<00:17, 5705.44it/s] 79%|███████▊  | 353566/450337 [00:58<00:16, 5817.31it/s] 79%|███████▊  | 354154/450337 [00:58<00:16, 5660.28it/s] 79%|███████▉  | 354833/450337 [00:59<00:15, 5979.20it/s] 79%|███████▉  | 355436/450337 [00:59<00:16, 5734.43it/s] 79%|███████▉  | 356099/450337 [00:59<00:15, 5985.68it/s] 79%|███████▉  | 356703/450337 [00:59<00:15, 5902.95it/s] 79%|███████▉  | 357297/450337 [00:59<00:17, 5425.36it/s] 79%|███████▉  | 357949/450337 [00:59<00:16, 5722.00it/s] 80%|███████▉  | 358534/450337 [00:59<00:15, 5756.53it/s] 80%|███████▉  | 359125/450337 [00:59<00:15, 5797.45it/s] 80%|███████▉  | 359744/450337 [00:59<00:15, 5911.24it/s] 80%|████████  | 360390/450337 [01:00<00:14, 6060.38it/s] 80%|████████  | 361041/450337 [01:00<00:14, 6192.66it/s] 80%|████████  | 361701/450337 [01:00<00:14, 6311.56it/s] 80%|████████  | 362381/450337 [01:00<00:13, 6454.54it/s] 81%|████████  | 363028/450337 [01:00<00:13, 6431.82it/s] 81%|████████  | 363673/450337 [01:00<00:13, 6351.26it/s] 81%|████████  | 364370/450337 [01:00<00:13, 6525.04it/s] 81%|████████  | 365024/450337 [01:00<00:13, 6502.57it/s] 81%|████████  | 365675/450337 [01:00<00:13, 6493.66it/s] 81%|████████▏ | 366325/450337 [01:00<00:13, 6316.98it/s] 81%|████████▏ | 366958/450337 [01:01<00:13, 6052.39it/s] 82%|████████▏ | 367592/450337 [01:01<00:13, 6134.24it/s] 82%|████████▏ | 368208/450337 [01:01<00:14, 5654.48it/s] 82%|████████▏ | 368871/450337 [01:01<00:13, 5922.90it/s] 82%|████████▏ | 369496/450337 [01:01<00:13, 6007.45it/s] 82%|████████▏ | 370120/450337 [01:01<00:13, 6070.81it/s] 82%|████████▏ | 370732/450337 [01:01<00:13, 5846.15it/s] 82%|████████▏ | 371336/450337 [01:01<00:13, 5893.58it/s] 83%|████████▎ | 371929/450337 [01:01<00:13, 5743.11it/s] 83%|████████▎ | 372560/450337 [01:02<00:13, 5902.14it/s] 83%|████████▎ | 373153/450337 [01:02<00:13, 5883.77it/s] 83%|████████▎ | 373744/450337 [01:02<00:13, 5853.87it/s] 83%|████████▎ | 374424/450337 [01:02<00:12, 6113.77it/s] 83%|████████▎ | 375048/450337 [01:02<00:12, 6148.43it/s] 83%|████████▎ | 375664/450337 [01:02<00:12, 5749.51it/s] 84%|████████▎ | 376260/450337 [01:02<00:12, 5801.55it/s] 84%|████████▎ | 376845/450337 [01:02<00:13, 5506.54it/s] 84%|████████▍ | 377426/450337 [01:02<00:13, 5590.81it/s] 84%|████████▍ | 378092/450337 [01:02<00:12, 5893.50it/s] 84%|████████▍ | 378697/450337 [01:03<00:12, 5937.90it/s] 84%|████████▍ | 379340/450337 [01:03<00:11, 6081.55it/s] 84%|████████▍ | 380034/450337 [01:03<00:11, 6333.94it/s] 85%|████████▍ | 380670/450337 [01:03<00:11, 6275.95it/s] 85%|████████▍ | 381379/450337 [01:03<00:10, 6509.38it/s] 85%|████████▍ | 382032/450337 [01:03<00:11, 5998.30it/s] 85%|████████▍ | 382641/450337 [01:03<00:11, 5677.46it/s] 85%|████████▌ | 383217/450337 [01:03<00:11, 5685.45it/s] 85%|████████▌ | 383800/450337 [01:03<00:11, 5725.99it/s] 85%|████████▌ | 384512/450337 [01:04<00:10, 6121.18it/s] 86%|████████▌ | 385129/450337 [01:04<00:10, 6099.06it/s] 86%|████████▌ | 385757/450337 [01:04<00:10, 6143.18it/s] 86%|████████▌ | 386374/450337 [01:04<00:11, 5761.54it/s] 86%|████████▌ | 387052/450337 [01:04<00:10, 6045.91it/s] 86%|████████▌ | 387824/450337 [01:04<00:09, 6520.41it/s] 86%|████████▋ | 388483/450337 [01:04<00:10, 6088.08it/s] 86%|████████▋ | 389102/450337 [01:04<00:10, 6054.71it/s] 87%|████████▋ | 389744/450337 [01:04<00:09, 6155.46it/s] 87%|████████▋ | 390371/450337 [01:04<00:09, 6187.84it/s] 87%|████████▋ | 390994/450337 [01:05<00:09, 6155.81it/s] 87%|████████▋ | 391613/450337 [01:05<00:10, 5794.16it/s] 87%|████████▋ | 392198/450337 [01:05<00:10, 5635.41it/s] 87%|████████▋ | 392797/450337 [01:05<00:10, 5733.49it/s] 87%|████████▋ | 393374/450337 [01:05<00:10, 5600.52it/s] 87%|████████▋ | 394006/450337 [01:05<00:09, 5798.18it/s] 88%|████████▊ | 394622/450337 [01:05<00:09, 5902.55it/s] 88%|████████▊ | 395217/450337 [01:05<00:09, 5914.56it/s] 88%|████████▊ | 395815/450337 [01:05<00:09, 5925.78it/s] 88%|████████▊ | 396409/450337 [01:06<00:09, 5896.01it/s] 88%|████████▊ | 397000/450337 [01:06<00:09, 5779.27it/s] 88%|████████▊ | 397579/450337 [01:06<00:09, 5640.76it/s] 88%|████████▊ | 398266/450337 [01:06<00:08, 5996.67it/s] 89%|████████▊ | 398925/450337 [01:06<00:08, 6159.51it/s] 89%|████████▊ | 399616/450337 [01:06<00:07, 6373.92it/s] 89%|████████▉ | 400255/450337 [01:06<00:08, 6164.18it/s] 89%|████████▉ | 400900/450337 [01:06<00:07, 6242.79it/s] 89%|████████▉ | 401527/450337 [01:06<00:08, 5974.97it/s] 89%|████████▉ | 402128/450337 [01:07<00:08, 5616.10it/s] 89%|████████▉ | 402696/450337 [01:07<00:08, 5506.15it/s] 90%|████████▉ | 403296/450337 [01:07<00:08, 5643.56it/s] 90%|████████▉ | 403879/450337 [01:07<00:08, 5696.02it/s] 90%|████████▉ | 404507/450337 [01:07<00:07, 5863.08it/s] 90%|████████▉ | 405096/450337 [01:07<00:07, 5695.40it/s] 90%|█████████ | 405669/450337 [01:07<00:07, 5703.98it/s] 90%|█████████ | 406288/450337 [01:07<00:07, 5843.92it/s] 90%|█████████ | 406875/450337 [01:07<00:07, 5665.20it/s] 90%|█████████ | 407480/450337 [01:07<00:07, 5773.86it/s] 91%|█████████ | 408060/450337 [01:08<00:07, 5548.69it/s] 91%|█████████ | 408739/450337 [01:08<00:07, 5899.67it/s] 91%|█████████ | 409362/450337 [01:08<00:06, 5984.87it/s] 91%|█████████ | 409964/450337 [01:08<00:07, 5741.71it/s] 91%|█████████ | 410606/450337 [01:08<00:06, 5935.50it/s] 91%|█████████▏| 411204/450337 [01:08<00:06, 5791.92it/s] 91%|█████████▏| 411807/450337 [01:08<00:06, 5854.32it/s] 92%|█████████▏| 412395/450337 [01:08<00:07, 5371.81it/s] 92%|█████████▏| 412941/450337 [01:08<00:06, 5369.84it/s] 92%|█████████▏| 413546/450337 [01:09<00:06, 5557.61it/s] 92%|█████████▏| 414108/450337 [01:09<00:06, 5310.74it/s] 92%|█████████▏| 414645/450337 [01:09<00:06, 5287.80it/s] 92%|█████████▏| 415208/450337 [01:09<00:06, 5385.09it/s] 92%|█████████▏| 415807/450337 [01:09<00:06, 5553.82it/s] 92%|█████████▏| 416366/450337 [01:09<00:06, 5342.14it/s] 93%|█████████▎| 416987/450337 [01:09<00:05, 5582.71it/s] 93%|█████████▎| 417575/450337 [01:09<00:05, 5661.27it/s] 93%|█████████▎| 418236/450337 [01:09<00:05, 5936.66it/s] 93%|█████████▎| 418833/450337 [01:09<00:05, 5901.31it/s] 93%|█████████▎| 419427/450337 [01:10<00:05, 5905.31it/s] 93%|█████████▎| 420032/450337 [01:10<00:05, 5947.87it/s] 93%|█████████▎| 420628/450337 [01:10<00:04, 5948.24it/s] 94%|█████████▎| 421224/450337 [01:10<00:05, 5531.97it/s] 94%|█████████▎| 421806/450337 [01:10<00:05, 5611.05it/s] 94%|█████████▍| 422420/450337 [01:10<00:04, 5763.08it/s] 94%|█████████▍| 423083/450337 [01:10<00:04, 6008.53it/s] 94%|█████████▍| 423688/450337 [01:10<00:04, 5887.19it/s] 94%|█████████▍| 424280/450337 [01:10<00:04, 5893.09it/s] 94%|█████████▍| 424872/450337 [01:10<00:04, 5768.15it/s] 94%|█████████▍| 425451/450337 [01:11<00:04, 5677.66it/s] 95%|█████████▍| 426091/450337 [01:11<00:04, 5886.75it/s] 95%|█████████▍| 426818/450337 [01:11<00:03, 6279.76it/s] 95%|█████████▍| 427448/450337 [01:11<00:03, 6149.53it/s] 95%|█████████▌| 428065/450337 [01:11<00:03, 5960.94it/s] 95%|█████████▌| 428814/450337 [01:11<00:03, 6398.33it/s] 95%|█████████▌| 429458/450337 [01:11<00:03, 6283.36it/s] 96%|█████████▌| 430096/450337 [01:11<00:03, 6308.34it/s] 96%|█████████▌| 430729/450337 [01:11<00:03, 6121.00it/s] 96%|█████████▌| 431344/450337 [01:12<00:03, 5774.42it/s] 96%|█████████▌| 431927/450337 [01:12<00:03, 5570.72it/s] 96%|█████████▌| 432806/450337 [01:12<00:02, 6464.03it/s] 96%|█████████▋| 433463/450337 [01:12<00:02, 6190.66it/s] 96%|█████████▋| 434091/450337 [01:12<00:02, 5958.55it/s] 97%|█████████▋| 434694/450337 [01:12<00:02, 5683.01it/s] 97%|█████████▋| 435268/450337 [01:12<00:02, 5671.95it/s] 97%|█████████▋| 435922/450337 [01:12<00:02, 5911.55it/s] 97%|█████████▋| 436518/450337 [01:12<00:02, 5771.17it/s] 97%|█████████▋| 437123/450337 [01:13<00:02, 5846.54it/s] 97%|█████████▋| 437757/450337 [01:13<00:02, 5982.62it/s] 97%|█████████▋| 438358/450337 [01:13<00:02, 5811.92it/s] 97%|█████████▋| 438942/450337 [01:13<00:01, 5746.40it/s] 98%|█████████▊| 439519/450337 [01:13<00:01, 5707.85it/s] 98%|█████████▊| 440091/450337 [01:13<00:01, 5650.85it/s] 98%|█████████▊| 440657/450337 [01:13<00:01, 5563.51it/s] 98%|█████████▊| 441298/450337 [01:13<00:01, 5805.31it/s] 98%|█████████▊| 441921/450337 [01:13<00:01, 5919.91it/s] 98%|█████████▊| 442515/450337 [01:13<00:01, 5857.37it/s] 98%|█████████▊| 443152/450337 [01:14<00:01, 6001.27it/s] 99%|█████████▊| 443804/450337 [01:14<00:01, 6150.22it/s] 99%|█████████▊| 444420/450337 [01:14<00:01, 5844.97it/s] 99%|█████████▉| 445008/450337 [01:14<00:00, 5533.86it/s] 99%|█████████▉| 445579/450337 [01:14<00:00, 5581.62it/s] 99%|█████████▉| 446141/450337 [01:14<00:00, 5549.83it/s] 99%|█████████▉| 446699/450337 [01:14<00:00, 5531.54it/s] 99%|█████████▉| 447381/450337 [01:14<00:00, 5903.71it/s] 99%|█████████▉| 447974/450337 [01:14<00:00, 5755.51it/s]100%|█████████▉| 448613/450337 [01:15<00:00, 5938.67it/s]100%|█████████▉| 449210/450337 [01:15<00:00, 5947.57it/s]100%|█████████▉| 449807/450337 [01:15<00:00, 5667.41it/s]100%|██████████| 450337/450337 [01:15<00:00, 5979.64it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1922/450337 [00:00<00:23, 19219.06it/s]  1%|          | 3998/450337 [00:00<00:22, 20114.37it/s]  1%|▏         | 6243/450337 [00:00<00:20, 21179.69it/s]  2%|▏         | 8361/450337 [00:00<00:21, 20367.22it/s]  2%|▏         | 10403/450337 [00:00<00:21, 20199.73it/s]  3%|▎         | 12426/450337 [00:00<00:21, 20126.00it/s]  3%|▎         | 14472/450337 [00:00<00:21, 20229.89it/s]  4%|▎         | 16497/450337 [00:00<00:21, 20090.48it/s]  4%|▍         | 18507/450337 [00:00<00:21, 20036.66it/s]  5%|▍         | 20674/450337 [00:01<00:20, 20532.84it/s]  5%|▌         | 22729/450337 [00:01<00:21, 20305.17it/s]  6%|▌         | 24987/450337 [00:01<00:20, 20986.71it/s]  6%|▌         | 27088/450337 [00:01<00:20, 20466.86it/s]  6%|▋         | 29162/450337 [00:01<00:20, 20537.27it/s]  7%|▋         | 31219/450337 [00:01<00:21, 19908.86it/s]  7%|▋         | 33216/450337 [00:01<00:21, 19669.74it/s]  8%|▊         | 35278/450337 [00:01<00:20, 19942.40it/s]  8%|▊         | 37276/450337 [00:01<00:20, 19735.59it/s]  9%|▊         | 39252/450337 [00:01<00:20, 19697.72it/s]  9%|▉         | 41320/450337 [00:02<00:20, 19971.91it/s] 10%|▉         | 43319/450337 [00:02<00:21, 19329.27it/s] 10%|█         | 45400/450337 [00:02<00:20, 19754.78it/s] 11%|█         | 47825/450337 [00:02<00:19, 21061.74it/s] 11%|█         | 50014/450337 [00:02<00:18, 21305.69it/s] 12%|█▏        | 52150/450337 [00:02<00:18, 20972.59it/s] 12%|█▏        | 54252/450337 [00:02<00:18, 20885.60it/s] 13%|█▎        | 56344/450337 [00:02<00:18, 20799.81it/s] 13%|█▎        | 58426/450337 [00:02<00:19, 20423.69it/s] 13%|█▎        | 60471/450337 [00:02<00:19, 20246.14it/s] 14%|█▍        | 62602/450337 [00:03<00:18, 20555.20it/s] 14%|█▍        | 64660/450337 [00:03<00:18, 20340.73it/s] 15%|█▍        | 66793/450337 [00:03<00:18, 20627.61it/s] 15%|█▌        | 68858/450337 [00:03<00:18, 20321.70it/s] 16%|█▌        | 70892/450337 [00:03<00:18, 20240.66it/s] 16%|█▌        | 73095/450337 [00:03<00:18, 20762.75it/s] 17%|█▋        | 75329/450337 [00:03<00:17, 21225.83it/s] 17%|█▋        | 77454/450337 [00:03<00:17, 21222.00it/s] 18%|█▊        | 79578/450337 [00:03<00:18, 20251.53it/s] 18%|█▊        | 81634/450337 [00:04<00:18, 20336.15it/s] 19%|█▊        | 83740/450337 [00:04<00:17, 20536.51it/s] 19%|█▉        | 85800/450337 [00:04<00:17, 20415.02it/s] 20%|█▉        | 87950/450337 [00:04<00:17, 20732.54it/s] 20%|█▉        | 90027/450337 [00:04<00:17, 20022.88it/s] 20%|██        | 92038/450337 [00:04<00:17, 20043.30it/s] 21%|██        | 94048/450337 [00:04<00:17, 20056.25it/s] 21%|██▏       | 96119/450337 [00:04<00:17, 20241.46it/s] 22%|██▏       | 98338/450337 [00:04<00:16, 20816.43it/s] 22%|██▏       | 100423/450337 [00:04<00:17, 20073.96it/s] 23%|██▎       | 102438/450337 [00:05<00:17, 20066.02it/s] 23%|██▎       | 104450/450337 [00:05<00:17, 20045.86it/s] 24%|██▎       | 106458/450337 [00:05<00:17, 19982.33it/s] 24%|██▍       | 108459/450337 [00:05<00:17, 19884.57it/s] 25%|██▍       | 110450/450337 [00:05<00:17, 19675.05it/s] 25%|██▍       | 112563/450337 [00:05<00:16, 20102.91it/s] 25%|██▌       | 114576/450337 [00:05<00:16, 19857.96it/s] 26%|██▌       | 116564/450337 [00:05<00:17, 19538.29it/s] 26%|██▋       | 118563/450337 [00:05<00:16, 19667.60it/s] 27%|██▋       | 120633/450337 [00:05<00:16, 19961.08it/s] 27%|██▋       | 122631/450337 [00:06<00:16, 19729.98it/s] 28%|██▊       | 124606/450337 [00:06<00:16, 19602.23it/s] 28%|██▊       | 126722/450337 [00:06<00:16, 20056.16it/s] 29%|██▊       | 128730/450337 [00:06<00:16, 20002.46it/s] 29%|██▉       | 130732/450337 [00:06<00:16, 19888.37it/s] 29%|██▉       | 132722/450337 [00:06<00:16, 19439.61it/s] 30%|██▉       | 134943/450337 [00:06<00:15, 20249.69it/s] 30%|███       | 137205/450337 [00:06<00:14, 20948.20it/s] 31%|███       | 139304/450337 [00:06<00:15, 20683.82it/s] 31%|███▏      | 141376/450337 [00:06<00:15, 20470.31it/s] 32%|███▏      | 143426/450337 [00:07<00:15, 20349.35it/s] 32%|███▏      | 145674/450337 [00:07<00:14, 20971.85it/s] 33%|███▎      | 147774/450337 [00:07<00:14, 20187.41it/s] 33%|███▎      | 149800/450337 [00:07<00:14, 20194.18it/s] 34%|███▎      | 151825/450337 [00:07<00:15, 19857.92it/s] 34%|███▍      | 153834/450337 [00:07<00:14, 19925.21it/s] 35%|███▍      | 155859/450337 [00:07<00:14, 20019.02it/s] 35%|███▌      | 158025/450337 [00:07<00:14, 20495.92it/s] 36%|███▌      | 160107/450337 [00:07<00:14, 20586.65it/s] 36%|███▌      | 162168/450337 [00:08<00:14, 20447.48it/s] 36%|███▋      | 164215/450337 [00:08<00:14, 20308.85it/s] 37%|███▋      | 166378/450337 [00:08<00:13, 20699.70it/s] 37%|███▋      | 168450/450337 [00:08<00:13, 20495.49it/s] 38%|███▊      | 170501/450337 [00:08<00:13, 20162.95it/s] 38%|███▊      | 172519/450337 [00:08<00:14, 19824.30it/s] 39%|███▊      | 174504/450337 [00:08<00:13, 19828.94it/s] 39%|███▉      | 176489/450337 [00:08<00:13, 19608.39it/s] 40%|███▉      | 178451/450337 [00:08<00:14, 19268.78it/s] 40%|████      | 180517/450337 [00:08<00:13, 19671.92it/s] 41%|████      | 182487/450337 [00:09<00:13, 19295.53it/s] 41%|████      | 184419/450337 [00:09<00:13, 19019.14it/s] 41%|████▏     | 186323/450337 [00:09<00:13, 18891.81it/s] 42%|████▏     | 188372/450337 [00:09<00:13, 19358.39it/s] 42%|████▏     | 190441/450337 [00:09<00:13, 19747.86it/s] 43%|████▎     | 192483/450337 [00:09<00:12, 19939.00it/s] 43%|████▎     | 194521/450337 [00:09<00:12, 20069.71it/s] 44%|████▎     | 196530/450337 [00:09<00:12, 20054.66it/s] 44%|████▍     | 198748/450337 [00:09<00:12, 20685.36it/s] 45%|████▍     | 200818/450337 [00:09<00:12, 20100.79it/s] 45%|████▌     | 202833/450337 [00:10<00:12, 20027.47it/s] 45%|████▌     | 204862/450337 [00:10<00:12, 20101.01it/s] 46%|████▌     | 206875/450337 [00:10<00:12, 20008.87it/s] 46%|████▋     | 208878/450337 [00:10<00:12, 19531.89it/s] 47%|████▋     | 210955/450337 [00:10<00:12, 19892.04it/s] 47%|████▋     | 213077/450337 [00:10<00:11, 20277.26it/s] 48%|████▊     | 215108/450337 [00:10<00:11, 19630.82it/s] 48%|████▊     | 217077/450337 [00:10<00:12, 19397.42it/s] 49%|████▊     | 219215/450337 [00:10<00:11, 19967.15it/s] 49%|████▉     | 221217/450337 [00:10<00:11, 19901.33it/s] 50%|████▉     | 223211/450337 [00:11<00:11, 19413.76it/s] 50%|█████     | 225246/450337 [00:11<00:11, 19685.56it/s] 50%|█████     | 227359/450337 [00:11<00:11, 20109.18it/s] 51%|█████     | 229374/450337 [00:11<00:11, 19549.35it/s] 51%|█████▏    | 231335/450337 [00:11<00:11, 19559.06it/s] 52%|█████▏    | 233425/450337 [00:11<00:10, 19950.01it/s] 52%|█████▏    | 235582/450337 [00:11<00:10, 20426.26it/s] 53%|█████▎    | 237628/450337 [00:11<00:10, 20133.02it/s] 53%|█████▎    | 239686/450337 [00:11<00:10, 20264.48it/s] 54%|█████▎    | 241890/450337 [00:12<00:10, 20789.78it/s] 54%|█████▍    | 243972/450337 [00:12<00:10, 20504.76it/s] 55%|█████▍    | 246113/450337 [00:12<00:09, 20767.80it/s] 55%|█████▌    | 248230/450337 [00:12<00:09, 20886.35it/s] 56%|█████▌    | 250388/450337 [00:12<00:09, 21092.14it/s] 56%|█████▌    | 252499/450337 [00:12<00:09, 21050.50it/s] 57%|█████▋    | 254605/450337 [00:12<00:09, 20616.54it/s] 57%|█████▋    | 256760/450337 [00:12<00:09, 20882.69it/s] 57%|█████▋    | 258851/450337 [00:12<00:09, 20316.70it/s] 58%|█████▊    | 260968/450337 [00:12<00:09, 20559.44it/s] 58%|█████▊    | 263028/450337 [00:13<00:09, 19996.44it/s] 59%|█████▉    | 265033/450337 [00:13<00:09, 19962.90it/s] 59%|█████▉    | 267033/450337 [00:13<00:09, 19639.92it/s] 60%|█████▉    | 269128/450337 [00:13<00:09, 20017.10it/s] 60%|██████    | 271133/450337 [00:13<00:09, 19902.39it/s] 61%|██████    | 273380/450337 [00:13<00:08, 20656.45it/s] 61%|██████    | 275449/450337 [00:13<00:08, 20168.63it/s] 62%|██████▏   | 277470/450337 [00:13<00:08, 19738.66it/s] 62%|██████▏   | 279555/450337 [00:13<00:08, 20058.53it/s] 63%|██████▎   | 281565/450337 [00:13<00:08, 19934.23it/s] 63%|██████▎   | 283597/450337 [00:14<00:08, 20044.43it/s] 63%|██████▎   | 285604/450337 [00:14<00:08, 19974.42it/s] 64%|██████▍   | 287603/450337 [00:14<00:08, 19910.07it/s] 64%|██████▍   | 289626/450337 [00:14<00:08, 20000.04it/s] 65%|██████▍   | 291640/450337 [00:14<00:07, 20041.13it/s] 65%|██████▌   | 293645/450337 [00:14<00:07, 19869.20it/s] 66%|██████▌   | 295728/450337 [00:14<00:07, 20153.94it/s] 66%|██████▌   | 297767/450337 [00:14<00:07, 20223.88it/s] 67%|██████▋   | 299840/450337 [00:14<00:07, 20374.47it/s] 67%|██████▋   | 301878/450337 [00:14<00:07, 19640.18it/s] 68%|██████▊   | 304047/450337 [00:15<00:07, 20234.41it/s] 68%|██████▊   | 306077/450337 [00:15<00:07, 19858.02it/s] 68%|██████▊   | 308068/450337 [00:15<00:07, 19768.32it/s] 69%|██████▉   | 310206/450337 [00:15<00:06, 20234.95it/s] 69%|██████▉   | 312269/450337 [00:15<00:06, 20348.25it/s] 70%|██████▉   | 314429/450337 [00:15<00:06, 20713.99it/s] 70%|███████   | 316503/450337 [00:15<00:06, 19889.92it/s] 71%|███████   | 318501/450337 [00:15<00:06, 19637.70it/s] 71%|███████   | 320471/450337 [00:15<00:06, 19555.44it/s] 72%|███████▏  | 322431/450337 [00:16<00:06, 19515.33it/s] 72%|███████▏  | 324488/450337 [00:16<00:06, 19811.63it/s] 72%|███████▏  | 326472/450337 [00:16<00:06, 19479.52it/s] 73%|███████▎  | 328813/450337 [00:16<00:05, 20627.82it/s] 73%|███████▎  | 330881/450337 [00:16<00:06, 19868.38it/s] 74%|███████▍  | 332876/450337 [00:16<00:05, 19807.79it/s] 74%|███████▍  | 334869/450337 [00:16<00:05, 19835.13it/s] 75%|███████▍  | 336862/450337 [00:16<00:05, 19859.30it/s] 75%|███████▌  | 339013/450337 [00:16<00:05, 20345.90it/s] 76%|███████▌  | 341051/450337 [00:16<00:05, 19703.31it/s] 76%|███████▌  | 343113/450337 [00:17<00:05, 19968.18it/s] 77%|███████▋  | 345169/450337 [00:17<00:05, 20141.45it/s] 77%|███████▋  | 347187/450337 [00:17<00:05, 19791.84it/s] 78%|███████▊  | 349170/450337 [00:17<00:05, 19686.41it/s] 78%|███████▊  | 351338/450337 [00:17<00:04, 20269.10it/s] 78%|███████▊  | 353368/450337 [00:17<00:04, 19458.13it/s] 79%|███████▉  | 355322/450337 [00:17<00:04, 19452.68it/s] 79%|███████▉  | 357273/450337 [00:17<00:04, 19233.78it/s] 80%|███████▉  | 359296/450337 [00:17<00:04, 19521.13it/s] 80%|████████  | 361381/450337 [00:17<00:04, 19907.25it/s] 81%|████████  | 363553/450337 [00:18<00:04, 20442.84it/s] 81%|████████  | 365701/450337 [00:18<00:04, 20748.66it/s] 82%|████████▏ | 367779/450337 [00:18<00:04, 20247.82it/s] 82%|████████▏ | 369824/450337 [00:18<00:03, 20302.57it/s] 83%|████████▎ | 371858/450337 [00:18<00:03, 19912.63it/s] 83%|████████▎ | 373853/450337 [00:18<00:03, 19864.00it/s] 83%|████████▎ | 375842/450337 [00:18<00:03, 19862.31it/s] 84%|████████▍ | 377830/450337 [00:18<00:03, 19725.33it/s] 84%|████████▍ | 379933/450337 [00:18<00:03, 20109.28it/s] 85%|████████▍ | 381967/450337 [00:18<00:03, 20176.38it/s] 85%|████████▌ | 383986/450337 [00:19<00:03, 19657.47it/s] 86%|████████▌ | 386003/450337 [00:19<00:03, 19800.22it/s] 86%|████████▌ | 388142/450337 [00:19<00:03, 20267.84it/s] 87%|████████▋ | 390199/450337 [00:19<00:02, 20350.12it/s] 87%|████████▋ | 392236/450337 [00:19<00:02, 19674.28it/s] 88%|████████▊ | 394212/450337 [00:19<00:02, 19697.83it/s] 88%|████████▊ | 396186/450337 [00:19<00:02, 19703.49it/s] 88%|████████▊ | 398160/450337 [00:19<00:02, 19625.63it/s] 89%|████████▉ | 400272/450337 [00:19<00:02, 20063.51it/s] 89%|████████▉ | 402281/450337 [00:20<00:02, 19581.30it/s] 90%|████████▉ | 404243/450337 [00:20<00:02, 19451.39it/s] 90%|█████████ | 406194/450337 [00:20<00:02, 19465.50it/s] 91%|█████████ | 408143/450337 [00:20<00:02, 19089.03it/s] 91%|█████████ | 410130/450337 [00:20<00:02, 19314.90it/s] 92%|█████████▏| 412082/450337 [00:20<00:01, 19369.94it/s] 92%|█████████▏| 414021/450337 [00:20<00:01, 18617.97it/s] 92%|█████████▏| 415892/450337 [00:20<00:01, 18640.85it/s] 93%|█████████▎| 417890/450337 [00:20<00:01, 19026.13it/s] 93%|█████████▎| 419864/450337 [00:20<00:01, 19235.95it/s] 94%|█████████▎| 421792/450337 [00:21<00:01, 19074.25it/s] 94%|█████████▍| 423852/450337 [00:21<00:01, 19523.38it/s] 95%|█████████▍| 425807/450337 [00:21<00:01, 19456.83it/s] 95%|█████████▌| 427906/450337 [00:21<00:01, 19900.22it/s] 95%|█████████▌| 430043/450337 [00:21<00:00, 20333.84it/s] 96%|█████████▌| 432078/450337 [00:21<00:00, 19702.23it/s] 96%|█████████▋| 434232/450337 [00:21<00:00, 20237.83it/s] 97%|█████████▋| 436261/450337 [00:21<00:00, 19911.05it/s] 97%|█████████▋| 438281/450337 [00:21<00:00, 19987.33it/s] 98%|█████████▊| 440283/450337 [00:21<00:00, 19502.21it/s] 98%|█████████▊| 442341/450337 [00:22<00:00, 19814.56it/s] 99%|█████████▊| 444364/450337 [00:22<00:00, 19936.07it/s] 99%|█████████▉| 446361/450337 [00:22<00:00, 19398.39it/s]100%|█████████▉| 448430/450337 [00:22<00:00, 19768.73it/s]100%|██████████| 450337/450337 [00:22<00:00, 20029.58it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 21.56it/s]2022-03-02 10:39:08 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-02 10:39:08 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-02 10:39:08 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-02 10:39:08 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-02 10:39:08 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-02 10:39:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-02 10:39:08 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-02 10:39:08 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-02 10:39:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-02 10:39:08 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-02 10:39:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-02 10:39:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-02 10:39:08 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-02 10:39:08 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_last.pt
2022-03-02 10:39:08 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_last.pt
2022-03-02 10:39:08 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-02 10:39:09 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-02 10:39:09 | INFO | fairseq.trainer | begin training epoch 1
2022-03-02 10:39:09 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-02 10:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-02 10:39:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-02 10:39:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 10:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 10:39:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-02 10:47:13 | INFO | train_inner | epoch 001:    105 / 393 loss=17.076, ppl=138139, wps=14689.8, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.144, loss_scale=4, train_wall=479, gb_free=10.1, wall=484
2022-03-02 10:54:39 | INFO | train_inner | epoch 001:    205 / 393 loss=14.811, ppl=28753.3, wps=14693.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.42, loss_scale=4, train_wall=441, gb_free=10.1, wall=930
2022-03-02 11:02:05 | INFO | train_inner | epoch 001:    305 / 393 loss=12.896, ppl=7622.7, wps=14697.9, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.956, loss_scale=4, train_wall=441, gb_free=10.1, wall=1376
2022-03-02 11:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 11:08:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.215 | ppl 2377.72 | wps 33942.3 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-02 11:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-02 11:08:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 11:08:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 11:08:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.215) (writing took 4.9765169164165854 seconds)
2022-03-02 11:08:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-02 11:08:46 | INFO | train | epoch 001 | loss 14.176 | ppl 18509.1 | wps 14601.2 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.557 | loss_scale 4 | train_wall 1746 | gb_free 10.1 | wall 1778
2022-03-02 11:08:46 | INFO | fairseq.trainer | begin training epoch 2
2022-03-02 11:08:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 11:09:40 | INFO | train_inner | epoch 002:     12 / 393 loss=11.556, ppl=3010.22, wps=14339.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.576, loss_scale=4, train_wall=439, gb_free=10.1, wall=1831
2022-03-02 11:17:05 | INFO | train_inner | epoch 002:    112 / 393 loss=11.02, ppl=2076.65, wps=14709.5, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.475, loss_scale=4, train_wall=441, gb_free=10.1, wall=2277
2022-03-02 11:24:31 | INFO | train_inner | epoch 002:    212 / 393 loss=10.731, ppl=1699.75, wps=14701.2, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.517, loss_scale=8, train_wall=441, gb_free=10.1, wall=2722
2022-03-02 11:31:57 | INFO | train_inner | epoch 002:    312 / 393 loss=10.488, ppl=1436.22, wps=14703.3, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.597, loss_scale=8, train_wall=441, gb_free=10.1, wall=3168
2022-03-02 11:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 11:38:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.156 | ppl 1140.91 | wps 34051.4 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 10.156
2022-03-02 11:38:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-02 11:38:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 11:38:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 11:38:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 2 @ 781 updates, score 10.156) (writing took 4.9579371102154255 seconds)
2022-03-02 11:38:07 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-02 11:38:07 | INFO | train | epoch 002 | loss 10.669 | ppl 1628.67 | wps 14608.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.548 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 3539
2022-03-02 11:38:07 | INFO | fairseq.trainer | begin training epoch 3
2022-03-02 11:38:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 11:39:32 | INFO | train_inner | epoch 003:     19 / 393 loss=10.275, ppl=1239.17, wps=14331.8, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.658, loss_scale=8, train_wall=439, gb_free=10.1, wall=3623
2022-03-02 11:46:58 | INFO | train_inner | epoch 003:    119 / 393 loss=10.069, ppl=1073.92, wps=14704.3, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.73, loss_scale=8, train_wall=441, gb_free=10.1, wall=4069
2022-03-02 11:54:23 | INFO | train_inner | epoch 003:    219 / 393 loss=9.898, ppl=954.11, wps=14698.8, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.813, loss_scale=8, train_wall=441, gb_free=10.1, wall=4515
2022-03-02 12:01:49 | INFO | train_inner | epoch 003:    319 / 393 loss=9.743, ppl=857.01, wps=14701.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.805, loss_scale=16, train_wall=441, gb_free=10.1, wall=4961
2022-03-02 12:07:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 12:07:24 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.494 | ppl 721.27 | wps 34029 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 9.494
2022-03-02 12:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-02 12:07:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 12:07:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 12:07:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 3 @ 1174 updates, score 9.494) (writing took 5.2654485469684005 seconds)
2022-03-02 12:07:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-02 12:07:29 | INFO | train | epoch 003 | loss 9.863 | ppl 931.32 | wps 14600.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.788 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 5301
2022-03-02 12:07:29 | INFO | fairseq.trainer | begin training epoch 4
2022-03-02 12:07:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 12:09:25 | INFO | train_inner | epoch 004:     26 / 393 loss=9.592, ppl=771.86, wps=14306.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.82, loss_scale=16, train_wall=439, gb_free=10.1, wall=5417
2022-03-02 12:16:51 | INFO | train_inner | epoch 004:    126 / 393 loss=9.445, ppl=696.99, wps=14690.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.817, loss_scale=16, train_wall=441, gb_free=10.1, wall=5863
2022-03-02 12:24:17 | INFO | train_inner | epoch 004:    226 / 393 loss=9.333, ppl=644.93, wps=14695.9, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.828, loss_scale=16, train_wall=441, gb_free=10.1, wall=6309
2022-03-02 12:31:43 | INFO | train_inner | epoch 004:    326 / 393 loss=9.229, ppl=599.97, wps=14688.4, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.839, loss_scale=16, train_wall=441, gb_free=10.1, wall=6755
2022-03-02 12:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 12:36:47 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.034 | ppl 524.13 | wps 34049.3 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 9.034
2022-03-02 12:36:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-02 12:36:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 12:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 12:36:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 4 @ 1567 updates, score 9.034) (writing took 4.740082615055144 seconds)
2022-03-02 12:36:52 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-02 12:36:52 | INFO | train | epoch 004 | loss 9.312 | ppl 635.58 | wps 14596.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.826 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 7063
2022-03-02 12:36:52 | INFO | fairseq.trainer | begin training epoch 5
2022-03-02 12:36:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 12:39:19 | INFO | train_inner | epoch 005:     33 / 393 loss=9.096, ppl=547.14, wps=14326, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.83, loss_scale=32, train_wall=439, gb_free=10.1, wall=7210
2022-03-02 12:46:45 | INFO | train_inner | epoch 005:    133 / 393 loss=8.979, ppl=504.62, wps=14684.6, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.827, loss_scale=32, train_wall=441, gb_free=10.1, wall=7657
2022-03-02 12:54:11 | INFO | train_inner | epoch 005:    233 / 393 loss=8.897, ppl=476.88, wps=14684.4, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.831, loss_scale=32, train_wall=441, gb_free=10.1, wall=8103
2022-03-02 13:01:38 | INFO | train_inner | epoch 005:    333 / 393 loss=8.803, ppl=446.76, wps=14685.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.801, loss_scale=32, train_wall=441, gb_free=10.1, wall=8549
2022-03-02 13:06:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 13:06:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.678 | ppl 409.48 | wps 34023.3 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 8.678
2022-03-02 13:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-02 13:06:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 13:06:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 13:06:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 5 @ 1960 updates, score 8.678) (writing took 4.807067362591624 seconds)
2022-03-02 13:06:15 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-02 13:06:15 | INFO | train | epoch 005 | loss 8.881 | ppl 471.32 | wps 14591.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.821 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 8826
2022-03-02 13:06:15 | INFO | fairseq.trainer | begin training epoch 6
2022-03-02 13:06:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 13:09:13 | INFO | train_inner | epoch 006:     40 / 393 loss=8.693, ppl=413.89, wps=14322.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.799, loss_scale=32, train_wall=439, gb_free=10.1, wall=9005
2022-03-02 13:12:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-02 13:16:44 | INFO | train_inner | epoch 006:    141 / 393 loss=8.593, ppl=386.08, wps=14541.1, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.78, loss_scale=32, train_wall=446, gb_free=10.1, wall=9455
2022-03-02 13:24:10 | INFO | train_inner | epoch 006:    241 / 393 loss=8.529, ppl=369.43, wps=14689.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.749, loss_scale=32, train_wall=441, gb_free=10.1, wall=9902
2022-03-02 13:31:36 | INFO | train_inner | epoch 006:    341 / 393 loss=8.464, ppl=353.07, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.787, loss_scale=32, train_wall=441, gb_free=10.1, wall=10348
2022-03-02 13:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 13:35:33 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.4 | ppl 337.89 | wps 34006.8 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 8.4
2022-03-02 13:35:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-02 13:35:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 13:35:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 13:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 6 @ 2352 updates, score 8.4) (writing took 4.918489333242178 seconds)
2022-03-02 13:35:38 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-02 13:35:38 | INFO | train | epoch 006 | loss 8.523 | ppl 367.84 | wps 14555 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.771 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 10589
2022-03-02 13:35:38 | INFO | fairseq.trainer | begin training epoch 7
2022-03-02 13:35:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 13:39:12 | INFO | train_inner | epoch 007:     48 / 393 loss=8.352, ppl=326.72, wps=14323.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.759, loss_scale=32, train_wall=439, gb_free=10.1, wall=10803
2022-03-02 13:46:38 | INFO | train_inner | epoch 007:    148 / 393 loss=8.263, ppl=307.1, wps=14685.6, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.744, loss_scale=32, train_wall=441, gb_free=10.1, wall=11250
2022-03-02 13:46:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 13:54:09 | INFO | train_inner | epoch 007:    249 / 393 loss=8.228, ppl=299.75, wps=14548.8, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.729, loss_scale=16, train_wall=445, gb_free=10.1, wall=11700
2022-03-02 14:01:35 | INFO | train_inner | epoch 007:    349 / 393 loss=8.172, ppl=288.44, wps=14693, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.72, loss_scale=16, train_wall=441, gb_free=10.1, wall=12146
2022-03-02 14:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 14:04:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.191 | ppl 292.2 | wps 33968.6 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 8.191
2022-03-02 14:04:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-02 14:04:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 14:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 14:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 7 @ 2744 updates, score 8.191) (writing took 4.777038965374231 seconds)
2022-03-02 14:05:00 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-02 14:05:00 | INFO | train | epoch 007 | loss 8.222 | ppl 298.61 | wps 14560.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.74 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 12352
2022-03-02 14:05:00 | INFO | fairseq.trainer | begin training epoch 8
2022-03-02 14:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 14:09:10 | INFO | train_inner | epoch 008:     56 / 393 loss=8.074, ppl=269.45, wps=14327.5, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.731, loss_scale=16, train_wall=439, gb_free=10.1, wall=12602
2022-03-02 14:16:36 | INFO | train_inner | epoch 008:    156 / 393 loss=7.987, ppl=253.75, wps=14687, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.7, loss_scale=16, train_wall=441, gb_free=10.1, wall=13048
2022-03-02 14:24:02 | INFO | train_inner | epoch 008:    256 / 393 loss=7.968, ppl=250.42, wps=14690.9, ups=0.22, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.696, loss_scale=16, train_wall=441, gb_free=10.1, wall=13494
2022-03-02 14:31:28 | INFO | train_inner | epoch 008:    356 / 393 loss=7.939, ppl=245.37, wps=14687.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.681, loss_scale=32, train_wall=441, gb_free=10.1, wall=13940
2022-03-02 14:32:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 14:34:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 14:34:18 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.014 | ppl 258.48 | wps 34037.9 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 8.014
2022-03-02 14:34:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-02 14:34:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 14:34:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 14:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 8 @ 3136 updates, score 8.014) (writing took 4.701561815105379 seconds)
2022-03-02 14:34:23 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-02 14:34:23 | INFO | train | epoch 008 | loss 7.969 | ppl 250.61 | wps 14558.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.7 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 14114
2022-03-02 14:34:23 | INFO | fairseq.trainer | begin training epoch 9
2022-03-02 14:34:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 14:39:08 | INFO | train_inner | epoch 009:     64 / 393 loss=7.832, ppl=227.81, wps=14190.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.702, loss_scale=16, train_wall=444, gb_free=10.1, wall=14400
2022-03-02 14:46:34 | INFO | train_inner | epoch 009:    164 / 393 loss=7.777, ppl=219.27, wps=14694.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.67, loss_scale=16, train_wall=441, gb_free=10.1, wall=14846
2022-03-02 14:54:00 | INFO | train_inner | epoch 009:    264 / 393 loss=7.763, ppl=217.25, wps=14695.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.665, loss_scale=16, train_wall=441, gb_free=10.1, wall=15292
2022-03-02 15:01:26 | INFO | train_inner | epoch 009:    364 / 393 loss=7.736, ppl=213.13, wps=14686.8, ups=0.22, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.646, loss_scale=16, train_wall=441, gb_free=10.1, wall=15738
2022-03-02 15:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 15:03:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.881 | ppl 235.73 | wps 34053.3 | wpb 2034.1 | bsz 4 | num_updates 3529 | best_loss 7.881
2022-03-02 15:03:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3529 updates
2022-03-02 15:03:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 15:03:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 15:03:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 9 @ 3529 updates, score 7.881) (writing took 4.800887945108116 seconds)
2022-03-02 15:03:45 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-02 15:03:45 | INFO | train | epoch 009 | loss 7.76 | ppl 216.74 | wps 14598.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3529 | lr 0.000441137 | gnorm 0.661 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 15877
2022-03-02 15:03:45 | INFO | fairseq.trainer | begin training epoch 10
2022-03-02 15:03:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 15:09:02 | INFO | train_inner | epoch 010:     71 / 393 loss=7.623, ppl=197.17, wps=14330.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.64, loss_scale=16, train_wall=439, gb_free=10.1, wall=16193
2022-03-02 15:16:28 | INFO | train_inner | epoch 010:    171 / 393 loss=7.583, ppl=191.69, wps=14689.6, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.647, loss_scale=32, train_wall=441, gb_free=10.1, wall=16639
2022-03-02 15:16:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 15:23:58 | INFO | train_inner | epoch 010:    272 / 393 loss=7.59, ppl=192.71, wps=14548.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.649, loss_scale=16, train_wall=445, gb_free=10.1, wall=17090
2022-03-02 15:31:24 | INFO | train_inner | epoch 010:    372 / 393 loss=7.579, ppl=191.21, wps=14691.7, ups=0.22, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.618, loss_scale=16, train_wall=441, gb_free=10.1, wall=17536
2022-03-02 15:32:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 15:33:02 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.794 | ppl 221.92 | wps 34174.1 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.794
2022-03-02 15:33:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-02 15:33:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 15:33:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 15:33:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.794) (writing took 4.653554622083902 seconds)
2022-03-02 15:33:07 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-02 15:33:07 | INFO | train | epoch 010 | loss 7.582 | ppl 191.65 | wps 14562.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.641 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 17639
2022-03-02 15:33:07 | INFO | fairseq.trainer | begin training epoch 11
2022-03-02 15:33:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 15:39:00 | INFO | train_inner | epoch 011:     79 / 393 loss=7.442, ppl=173.94, wps=14333.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.634, loss_scale=16, train_wall=439, gb_free=10.1, wall=17991
2022-03-02 15:46:26 | INFO | train_inner | epoch 011:    179 / 393 loss=7.426, ppl=171.91, wps=14694.2, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.613, loss_scale=16, train_wall=441, gb_free=10.1, wall=18437
2022-03-02 15:53:52 | INFO | train_inner | epoch 011:    279 / 393 loss=7.426, ppl=172.01, wps=14693.4, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.594, loss_scale=16, train_wall=441, gb_free=10.1, wall=18883
2022-03-02 15:55:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 16:01:22 | INFO | train_inner | epoch 011:    380 / 393 loss=7.434, ppl=172.97, wps=14547.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.564, loss_scale=16, train_wall=446, gb_free=10.1, wall=19334
2022-03-02 16:02:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 16:02:25 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.697 | ppl 207.57 | wps 33960.1 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 7.697
2022-03-02 16:02:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-02 16:02:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 16:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 16:02:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 11 @ 4313 updates, score 7.697) (writing took 4.679830878041685 seconds)
2022-03-02 16:02:29 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-02 16:02:29 | INFO | train | epoch 011 | loss 7.424 | ppl 171.76 | wps 14562.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.597 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 19401
2022-03-02 16:02:29 | INFO | fairseq.trainer | begin training epoch 12
2022-03-02 16:02:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 16:08:57 | INFO | train_inner | epoch 012:     87 / 393 loss=7.279, ppl=155.27, wps=14330.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.575, loss_scale=16, train_wall=439, gb_free=10.1, wall=19789
2022-03-02 16:16:23 | INFO | train_inner | epoch 012:    187 / 393 loss=7.275, ppl=154.86, wps=14695.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.563, loss_scale=16, train_wall=441, gb_free=10.1, wall=20235
2022-03-02 16:23:50 | INFO | train_inner | epoch 012:    287 / 393 loss=7.288, ppl=156.26, wps=14674.4, ups=0.22, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.564, loss_scale=16, train_wall=442, gb_free=10.1, wall=20681
2022-03-02 16:31:16 | INFO | train_inner | epoch 012:    387 / 393 loss=7.28, ppl=155.39, wps=14690.5, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.54, loss_scale=16, train_wall=441, gb_free=10.1, wall=21128
2022-03-02 16:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 16:31:47 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.63 | ppl 198.08 | wps 34133.4 | wpb 2034.1 | bsz 4 | num_updates 4706 | best_loss 7.63
2022-03-02 16:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4706 updates
2022-03-02 16:31:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 16:31:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 16:31:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 12 @ 4706 updates, score 7.63) (writing took 4.580160555429757 seconds)
2022-03-02 16:31:52 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-02 16:31:52 | INFO | train | epoch 012 | loss 7.276 | ppl 155 | wps 14595.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4706 | lr 0.000460971 | gnorm 0.558 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 21163
2022-03-02 16:31:52 | INFO | fairseq.trainer | begin training epoch 13
2022-03-02 16:31:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 16:34:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 16:38:56 | INFO | train_inner | epoch 013:     95 / 393 loss=7.133, ppl=140.32, wps=14196.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.554, loss_scale=16, train_wall=444, gb_free=10.1, wall=21587
2022-03-02 16:46:22 | INFO | train_inner | epoch 013:    195 / 393 loss=7.147, ppl=141.69, wps=14696.4, ups=0.22, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.548, loss_scale=16, train_wall=441, gb_free=10.1, wall=22033
2022-03-02 16:53:47 | INFO | train_inner | epoch 013:    295 / 393 loss=7.16, ppl=142.99, wps=14696.5, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.53, loss_scale=16, train_wall=441, gb_free=10.1, wall=22479
2022-03-02 17:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:01:09 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.595 | ppl 193.28 | wps 34027.7 | wpb 2034.1 | bsz 4 | num_updates 5098 | best_loss 7.595
2022-03-02 17:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5098 updates
2022-03-02 17:01:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 17:01:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 17:01:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 13 @ 5098 updates, score 7.595) (writing took 4.684423151426017 seconds)
2022-03-02 17:01:14 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-02 17:01:14 | INFO | train | epoch 013 | loss 7.151 | ppl 142.16 | wps 14564.8 | ups 0.22 | wpb 65462.7 | bsz 127.9 | num_updates 5098 | lr 0.000442894 | gnorm 0.542 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 22925
2022-03-02 17:01:14 | INFO | fairseq.trainer | begin training epoch 14
2022-03-02 17:01:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 17:01:23 | INFO | train_inner | epoch 014:      2 / 393 loss=7.172, ppl=144.19, wps=14332.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.535, loss_scale=16, train_wall=439, gb_free=10.1, wall=22934
2022-03-02 17:08:49 | INFO | train_inner | epoch 014:    102 / 393 loss=7, ppl=128, wps=14694.6, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.535, loss_scale=16, train_wall=441, gb_free=10.1, wall=23380
2022-03-02 17:13:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 17:16:19 | INFO | train_inner | epoch 014:    203 / 393 loss=7.047, ppl=132.22, wps=14551.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.549, loss_scale=16, train_wall=445, gb_free=10.1, wall=23831
2022-03-02 17:23:45 | INFO | train_inner | epoch 014:    303 / 393 loss=7.069, ppl=134.28, wps=14697.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.527, loss_scale=16, train_wall=441, gb_free=10.1, wall=24277
2022-03-02 17:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:30:31 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.564 | ppl 189.26 | wps 34045.7 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 7.564
2022-03-02 17:30:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-02 17:30:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 17:30:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 17:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 14 @ 5490 updates, score 7.564) (writing took 4.699241636320949 seconds)
2022-03-02 17:30:35 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-02 17:30:35 | INFO | train | epoch 014 | loss 7.046 | ppl 132.14 | wps 14565.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.535 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 24687
2022-03-02 17:30:35 | INFO | fairseq.trainer | begin training epoch 15
2022-03-02 17:30:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 17:31:20 | INFO | train_inner | epoch 015:     10 / 393 loss=7.053, ppl=132.83, wps=14333.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.529, loss_scale=16, train_wall=439, gb_free=10.1, wall=24732
2022-03-02 17:38:46 | INFO | train_inner | epoch 015:    110 / 393 loss=6.912, ppl=120.41, wps=14696.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.53, loss_scale=16, train_wall=441, gb_free=10.1, wall=25178
2022-03-02 17:46:12 | INFO | train_inner | epoch 015:    210 / 393 loss=6.948, ppl=123.5, wps=14697.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.524, loss_scale=16, train_wall=441, gb_free=10.1, wall=25624
2022-03-02 17:52:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 17:53:42 | INFO | train_inner | epoch 015:    311 / 393 loss=6.977, ppl=125.96, wps=14547.9, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.522, loss_scale=16, train_wall=446, gb_free=10.1, wall=26074
2022-03-02 17:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:59:53 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.544 | ppl 186.63 | wps 34101.2 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 7.544
2022-03-02 17:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-02 17:59:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 17:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 17:59:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 15 @ 5882 updates, score 7.544) (writing took 4.410026288591325 seconds)
2022-03-02 17:59:57 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-02 17:59:57 | INFO | train | epoch 015 | loss 6.953 | ppl 123.94 | wps 14566.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.522 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 26449
2022-03-02 17:59:57 | INFO | fairseq.trainer | begin training epoch 16
2022-03-02 17:59:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 18:01:17 | INFO | train_inner | epoch 016:     18 / 393 loss=6.959, ppl=124.39, wps=14341.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.516, loss_scale=16, train_wall=439, gb_free=10.1, wall=26529
2022-03-02 18:08:44 | INFO | train_inner | epoch 016:    118 / 393 loss=6.827, ppl=113.52, wps=14692, ups=0.22, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.541, loss_scale=16, train_wall=441, gb_free=10.1, wall=26975
2022-03-02 18:16:09 | INFO | train_inner | epoch 016:    218 / 393 loss=6.867, ppl=116.76, wps=14694.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.528, loss_scale=16, train_wall=441, gb_free=10.1, wall=27421
2022-03-02 18:23:35 | INFO | train_inner | epoch 016:    318 / 393 loss=6.901, ppl=119.48, wps=14692.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.523, loss_scale=16, train_wall=441, gb_free=10.1, wall=27867
2022-03-02 18:29:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 18:29:14 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.531 | ppl 184.9 | wps 34055.3 | wpb 2034.1 | bsz 4 | num_updates 6275 | best_loss 7.531
2022-03-02 18:29:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6275 updates
2022-03-02 18:29:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 18:29:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 18:29:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 16 @ 6275 updates, score 7.531) (writing took 4.6935910964384675 seconds)
2022-03-02 18:29:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-02 18:29:19 | INFO | train | epoch 016 | loss 6.873 | ppl 117.19 | wps 14600.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 6275 | lr 0.000399202 | gnorm 0.528 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 28211
2022-03-02 18:29:19 | INFO | fairseq.trainer | begin training epoch 17
2022-03-02 18:29:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 18:31:11 | INFO | train_inner | epoch 017:     25 / 393 loss=6.868, ppl=116.81, wps=14336.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.508, loss_scale=16, train_wall=439, gb_free=10.1, wall=28322
2022-03-02 18:31:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 18:38:41 | INFO | train_inner | epoch 017:    126 / 393 loss=6.762, ppl=108.57, wps=14547.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.544, loss_scale=16, train_wall=445, gb_free=10.1, wall=28773
2022-03-02 18:46:07 | INFO | train_inner | epoch 017:    226 / 393 loss=6.802, ppl=111.55, wps=14696.5, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.527, loss_scale=16, train_wall=441, gb_free=10.1, wall=29219
2022-03-02 18:53:33 | INFO | train_inner | epoch 017:    326 / 393 loss=6.822, ppl=113.11, wps=14695.4, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.525, loss_scale=16, train_wall=441, gb_free=10.1, wall=29665
2022-03-02 18:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 18:58:36 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.519 | ppl 183.44 | wps 33977.7 | wpb 2034.1 | bsz 4 | num_updates 6667 | best_loss 7.519
2022-03-02 18:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6667 updates
2022-03-02 18:58:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 18:58:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt
2022-03-02 18:58:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.08_0.02_0.9_#1/checkpoint_best.pt (epoch 17 @ 6667 updates, score 7.519) (writing took 4.475446464493871 seconds)
2022-03-02 18:58:41 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-02 18:58:41 | INFO | train | epoch 017 | loss 6.8 | ppl 111.41 | wps 14566.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6667 | lr 0.000387289 | gnorm 0.526 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 29972
2022-03-02 18:58:41 | INFO | fairseq.trainer | begin training epoch 18
2022-03-02 18:58:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 19:01:08 | INFO | train_inner | epoch 018:     33 / 393 loss=6.792, ppl=110.85, wps=14336.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.524, loss_scale=16, train_wall=439, gb_free=10.1, wall=30120
2022-03-02 19:08:34 | INFO | train_inner | epoch 018:    133 / 393 loss=6.685, ppl=102.92, wps=14688.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.528, loss_scale=16, train_wall=441, gb_free=10.1, wall=30566
2022-03-02 19:10:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 19:16:05 | INFO | train_inner | epoch 018:    234 / 393 loss=6.731, ppl=106.22, wps=14551, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.527, loss_scale=16, train_wall=445, gb_free=10.1, wall=31016
2022-03-02 19:23:31 | INFO | train_inner | epoch 018:    334 / 393 loss=6.773, ppl=109.37, wps=14691.7, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.529, loss_scale=16, train_wall=441, gb_free=10.1, wall=31462
2022-03-02 19:27:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 19:27:58 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.521 | ppl 183.63 | wps 33979.6 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 7.519
2022-03-02 19:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-02 19:27:58 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-02 19:27:58 | INFO | train | epoch 018 | loss 6.734 | ppl 106.48 | wps 14600.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.532 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 31730
2022-03-02 19:27:58 | INFO | fairseq.trainer | begin training epoch 19
2022-03-02 19:27:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 19:31:01 | INFO | train_inner | epoch 019:     41 / 393 loss=6.715, ppl=105.06, wps=14483.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.53, loss_scale=16, train_wall=439, gb_free=10.1, wall=31913
2022-03-02 19:38:27 | INFO | train_inner | epoch 019:    141 / 393 loss=6.64, ppl=99.75, wps=14695.5, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.534, loss_scale=16, train_wall=441, gb_free=10.1, wall=32359
2022-03-02 19:45:53 | INFO | train_inner | epoch 019:    241 / 393 loss=6.674, ppl=102.11, wps=14692.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.534, loss_scale=16, train_wall=441, gb_free=10.1, wall=32805
2022-03-02 19:48:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 19:53:24 | INFO | train_inner | epoch 019:    342 / 393 loss=6.706, ppl=104.41, wps=14549.6, ups=0.22, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.554, loss_scale=16, train_wall=445, gb_free=10.1, wall=33255
2022-03-02 19:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 19:57:15 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.522 | ppl 183.81 | wps 34099.7 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 7.519
2022-03-02 19:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-02 19:57:15 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-02 19:57:15 | INFO | train | epoch 019 | loss 6.674 | ppl 102.1 | wps 14602.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.538 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 33487
2022-03-02 19:57:16 | INFO | fairseq.trainer | begin training epoch 20
2022-03-02 19:57:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 20:00:54 | INFO | train_inner | epoch 020:     49 / 393 loss=6.639, ppl=99.69, wps=14485.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.524, loss_scale=16, train_wall=439, gb_free=10.1, wall=33706
2022-03-02 20:08:20 | INFO | train_inner | epoch 020:    149 / 393 loss=6.583, ppl=95.9, wps=14695.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.546, loss_scale=16, train_wall=441, gb_free=10.1, wall=34152
2022-03-02 20:15:46 | INFO | train_inner | epoch 020:    249 / 393 loss=6.623, ppl=98.58, wps=14692.9, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.536, loss_scale=16, train_wall=441, gb_free=10.1, wall=34598
2022-03-02 20:23:12 | INFO | train_inner | epoch 020:    349 / 393 loss=6.661, ppl=101.22, wps=14690.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.53, loss_scale=16, train_wall=441, gb_free=10.1, wall=35044
2022-03-02 20:26:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 20:26:33 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.53 | ppl 184.79 | wps 34014 | wpb 2034.1 | bsz 4 | num_updates 7844 | best_loss 7.519
2022-03-02 20:26:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7844 updates
2022-03-02 20:26:33 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-02 20:26:33 | INFO | train | epoch 020 | loss 6.62 | ppl 98.33 | wps 14638.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7844 | lr 0.000357052 | gnorm 0.536 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 35245
2022-03-02 20:26:33 | INFO | fairseq.trainer | begin training epoch 21
2022-03-02 20:26:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 20:27:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 20:30:47 | INFO | train_inner | epoch 021:     57 / 393 loss=6.58, ppl=95.7, wps=14338.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.545, loss_scale=16, train_wall=444, gb_free=10.1, wall=35499
2022-03-02 20:38:13 | INFO | train_inner | epoch 021:    157 / 393 loss=6.537, ppl=92.86, wps=14696.2, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.534, loss_scale=16, train_wall=441, gb_free=10.1, wall=35945
2022-03-02 20:45:39 | INFO | train_inner | epoch 021:    257 / 393 loss=6.58, ppl=95.67, wps=14692.2, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.548, loss_scale=16, train_wall=441, gb_free=10.1, wall=36391
2022-03-02 20:53:05 | INFO | train_inner | epoch 021:    357 / 393 loss=6.594, ppl=96.59, wps=14699.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.536, loss_scale=16, train_wall=441, gb_free=10.1, wall=36837
2022-03-02 20:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 20:55:50 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.535 | ppl 185.47 | wps 34045.9 | wpb 2034.1 | bsz 4 | num_updates 8236 | best_loss 7.519
2022-03-02 20:55:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8236 updates
2022-03-02 20:55:50 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-02 20:55:50 | INFO | train | epoch 021 | loss 6.568 | ppl 94.88 | wps 14604.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8236 | lr 0.000348451 | gnorm 0.542 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 37002
2022-03-02 20:55:50 | INFO | fairseq.trainer | begin training epoch 22
2022-03-02 20:55:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 21:00:36 | INFO | train_inner | epoch 022:     64 / 393 loss=6.523, ppl=91.98, wps=14482.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.555, loss_scale=16, train_wall=439, gb_free=10.1, wall=37287
2022-03-02 21:06:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 21:08:06 | INFO | train_inner | epoch 022:    165 / 393 loss=6.491, ppl=89.95, wps=14547.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.557, loss_scale=16, train_wall=446, gb_free=10.1, wall=37738
2022-03-02 21:15:32 | INFO | train_inner | epoch 022:    265 / 393 loss=6.53, ppl=92.41, wps=14689.5, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.54, loss_scale=16, train_wall=441, gb_free=10.1, wall=38184
2022-03-02 21:22:58 | INFO | train_inner | epoch 022:    365 / 393 loss=6.568, ppl=94.89, wps=14694.2, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.546, loss_scale=16, train_wall=441, gb_free=10.1, wall=38630
2022-03-02 21:25:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 21:25:07 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.546 | ppl 186.91 | wps 34077.9 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 7.519
2022-03-02 21:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-02 21:25:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-02 21:25:08 | INFO | train | epoch 022 | loss 6.521 | ppl 91.81 | wps 14601.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.549 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 38759
2022-03-02 21:25:08 | INFO | fairseq.trainer | begin training epoch 23
2022-03-02 21:25:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 21:30:29 | INFO | train_inner | epoch 023:     72 / 393 loss=6.454, ppl=87.69, wps=14480.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.549, loss_scale=16, train_wall=439, gb_free=10.1, wall=39080
2022-03-02 21:37:55 | INFO | train_inner | epoch 023:    172 / 393 loss=6.447, ppl=87.22, wps=14693.8, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.555, loss_scale=16, train_wall=441, gb_free=10.1, wall=39526
2022-03-02 21:45:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 21:45:25 | INFO | train_inner | epoch 023:    273 / 393 loss=6.495, ppl=90.18, wps=14544.6, ups=0.22, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.554, loss_scale=16, train_wall=446, gb_free=10.1, wall=39977
2022-03-02 21:52:51 | INFO | train_inner | epoch 023:    373 / 393 loss=6.528, ppl=92.26, wps=14693.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.574, loss_scale=16, train_wall=441, gb_free=10.1, wall=40423
2022-03-02 21:54:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 21:54:25 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.552 | ppl 187.61 | wps 34023.5 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 7.519
2022-03-02 21:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-02 21:54:25 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-02 21:54:25 | INFO | train | epoch 023 | loss 6.476 | ppl 89.05 | wps 14600.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.556 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 40517
2022-03-02 21:54:25 | INFO | fairseq.trainer | begin training epoch 24
2022-03-02 21:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 22:00:22 | INFO | train_inner | epoch 024:     80 / 393 loss=6.401, ppl=84.5, wps=14483.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.551, loss_scale=16, train_wall=439, gb_free=10.1, wall=40873
2022-03-02 22:07:48 | INFO | train_inner | epoch 024:    180 / 393 loss=6.408, ppl=84.93, wps=14689.7, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.564, loss_scale=16, train_wall=441, gb_free=10.1, wall=41320
2022-03-02 22:15:14 | INFO | train_inner | epoch 024:    280 / 393 loss=6.456, ppl=87.78, wps=14693.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.546, loss_scale=16, train_wall=441, gb_free=10.1, wall=41766
2022-03-02 22:22:40 | INFO | train_inner | epoch 024:    380 / 393 loss=6.487, ppl=89.67, wps=14691.7, ups=0.22, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.557, loss_scale=16, train_wall=441, gb_free=10.1, wall=42212
2022-03-02 22:23:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 22:23:43 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.564 | ppl 189.25 | wps 34096.7 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 7.519
2022-03-02 22:23:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-02 22:23:43 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-02 22:23:43 | INFO | train | epoch 024 | loss 6.435 | ppl 86.55 | wps 14638 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.556 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 42274
2022-03-02 22:23:43 | INFO | fairseq.trainer | begin training epoch 25
2022-03-02 22:23:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 22:24:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 22:30:15 | INFO | train_inner | epoch 025:     88 / 393 loss=6.352, ppl=81.67, wps=14338.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.557, loss_scale=16, train_wall=444, gb_free=10.1, wall=42667
2022-03-02 22:37:41 | INFO | train_inner | epoch 025:    188 / 393 loss=6.373, ppl=82.88, wps=14693.6, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.56, loss_scale=16, train_wall=441, gb_free=10.1, wall=43113
2022-03-02 22:45:07 | INFO | train_inner | epoch 025:    288 / 393 loss=6.414, ppl=85.29, wps=14691.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.561, loss_scale=16, train_wall=441, gb_free=10.1, wall=43559
2022-03-02 22:47:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 22:52:37 | INFO | train_inner | epoch 025:    389 / 393 loss=6.46, ppl=88.01, wps=14554.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.553, loss_scale=8, train_wall=445, gb_free=10.1, wall=44009
2022-03-02 22:52:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 22:53:00 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.579 | ppl 191.22 | wps 33984.9 | wpb 2034.1 | bsz 4 | num_updates 9804 | best_loss 7.519
2022-03-02 22:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9804 updates
2022-03-02 22:53:00 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-02 22:53:00 | INFO | train | epoch 025 | loss 6.396 | ppl 84.24 | wps 14565.5 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 9804 | lr 0.000319373 | gnorm 0.558 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 44031
2022-03-02 22:53:00 | INFO | fairseq.trainer | begin training epoch 26
2022-03-02 22:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 23:00:08 | INFO | train_inner | epoch 026:     96 / 393 loss=6.301, ppl=78.87, wps=14484.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.558, loss_scale=8, train_wall=439, gb_free=10.1, wall=44459
2022-03-02 23:07:34 | INFO | train_inner | epoch 026:    196 / 393 loss=6.345, ppl=81.31, wps=14698.8, ups=0.22, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.581, loss_scale=8, train_wall=441, gb_free=10.1, wall=44905
2022-03-02 23:15:00 | INFO | train_inner | epoch 026:    296 / 393 loss=6.378, ppl=83.15, wps=14696.6, ups=0.22, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.574, loss_scale=8, train_wall=441, gb_free=10.1, wall=45351
2022-03-02 23:22:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 23:22:17 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.596 | ppl 193.44 | wps 34031.6 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 7.519
2022-03-02 23:22:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-02 23:22:17 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-02 23:22:17 | INFO | train | epoch 026 | loss 6.36 | ppl 82.16 | wps 14643.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.568 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 45788
2022-03-02 23:22:17 | INFO | fairseq.trainer | begin training epoch 27
2022-03-02 23:22:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 23:22:30 | INFO | train_inner | epoch 027:      3 / 393 loss=6.416, ppl=85.4, wps=14485.2, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.559, loss_scale=8, train_wall=439, gb_free=10.1, wall=45802
2022-03-02 23:29:56 | INFO | train_inner | epoch 027:    103 / 393 loss=6.263, ppl=76.79, wps=14690, ups=0.22, wpb=65530.9, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.569, loss_scale=16, train_wall=441, gb_free=10.1, wall=46248
2022-03-02 23:30:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 23:37:27 | INFO | train_inner | epoch 027:    204 / 393 loss=6.311, ppl=79.37, wps=14551.8, ups=0.22, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.573, loss_scale=8, train_wall=445, gb_free=10.1, wall=46698
2022-03-02 23:44:53 | INFO | train_inner | epoch 027:    304 / 393 loss=6.355, ppl=81.84, wps=14691.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.574, loss_scale=8, train_wall=441, gb_free=10.1, wall=47144
2022-03-02 23:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 23:51:34 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.598 | ppl 193.76 | wps 34058.9 | wpb 2034.1 | bsz 4 | num_updates 10589 | best_loss 7.519
2022-03-02 23:51:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10589 updates
2022-03-02 23:51:34 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-02 23:51:34 | INFO | train | epoch 027 | loss 6.327 | ppl 80.27 | wps 14603 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10589 | lr 0.000307307 | gnorm 0.572 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 47545
2022-03-02 23:51:34 | INFO | fairseq.trainer | begin training epoch 28
2022-03-02 23:51:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 23:52:23 | INFO | train_inner | epoch 028:     11 / 393 loss=6.369, ppl=82.63, wps=14487.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.572, loss_scale=8, train_wall=439, gb_free=10.1, wall=47595
2022-03-02 23:59:49 | INFO | train_inner | epoch 028:    111 / 393 loss=6.228, ppl=74.96, wps=14697.7, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=48040
2022-03-03 00:07:15 | INFO | train_inner | epoch 028:    211 / 393 loss=6.278, ppl=77.58, wps=14697.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.581, loss_scale=8, train_wall=441, gb_free=10.1, wall=48486
2022-03-03 00:12:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 00:14:45 | INFO | train_inner | epoch 028:    312 / 393 loss=6.325, ppl=80.17, wps=14547.9, ups=0.22, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.612, loss_scale=8, train_wall=446, gb_free=10.1, wall=48937
2022-03-03 00:20:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 00:20:51 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.622 | ppl 197 | wps 33981.8 | wpb 2034.1 | bsz 4 | num_updates 10981 | best_loss 7.519
2022-03-03 00:20:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10981 updates
2022-03-03 00:20:51 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-03 00:20:51 | INFO | train | epoch 028 | loss 6.294 | ppl 78.47 | wps 14604.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10981 | lr 0.000301772 | gnorm 0.582 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 49303
2022-03-03 00:20:51 | INFO | fairseq.trainer | begin training epoch 29
2022-03-03 00:20:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 00:22:16 | INFO | train_inner | epoch 029:     19 / 393 loss=6.335, ppl=80.72, wps=14482, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.569, loss_scale=8, train_wall=439, gb_free=10.1, wall=49387
2022-03-03 00:29:42 | INFO | train_inner | epoch 029:    119 / 393 loss=6.209, ppl=73.97, wps=14687.4, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.582, loss_scale=8, train_wall=441, gb_free=10.1, wall=49834
2022-03-03 00:37:08 | INFO | train_inner | epoch 029:    219 / 393 loss=6.254, ppl=76.3, wps=14699, ups=0.22, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.592, loss_scale=8, train_wall=441, gb_free=10.1, wall=50279
2022-03-03 00:44:34 | INFO | train_inner | epoch 029:    319 / 393 loss=6.3, ppl=78.77, wps=14701.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.584, loss_scale=8, train_wall=441, gb_free=10.1, wall=50725
2022-03-03 00:50:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 00:50:08 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.635 | ppl 198.78 | wps 34078.8 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 7.519
2022-03-03 00:50:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-03 00:50:08 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-03 00:50:08 | INFO | train | epoch 029 | loss 6.263 | ppl 76.8 | wps 14641.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.584 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 51060
2022-03-03 00:50:08 | INFO | fairseq.trainer | begin training epoch 30
2022-03-03 00:50:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 00:52:04 | INFO | train_inner | epoch 030:     26 / 393 loss=6.279, ppl=77.65, wps=14485.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.575, loss_scale=16, train_wall=439, gb_free=10.1, wall=51176
2022-03-03 00:56:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 00:59:34 | INFO | train_inner | epoch 030:    127 / 393 loss=6.188, ppl=72.89, wps=14551.2, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.614, loss_scale=8, train_wall=445, gb_free=10.1, wall=51626
2022-03-03 01:07:00 | INFO | train_inner | epoch 030:    227 / 393 loss=6.219, ppl=74.5, wps=14700.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.581, loss_scale=8, train_wall=441, gb_free=10.1, wall=52072
2022-03-03 01:14:26 | INFO | train_inner | epoch 030:    327 / 393 loss=6.271, ppl=77.22, wps=14697.4, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.583, loss_scale=8, train_wall=441, gb_free=10.1, wall=52518
2022-03-03 01:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 01:19:25 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.65 | ppl 200.79 | wps 34072.3 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 7.519
2022-03-03 01:19:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-03 01:19:25 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-03 01:19:25 | INFO | train | epoch 030 | loss 6.234 | ppl 75.25 | wps 14606.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.592 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 52816
2022-03-03 01:19:25 | INFO | fairseq.trainer | begin training epoch 31
2022-03-03 01:19:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 01:21:56 | INFO | train_inner | epoch 031:     34 / 393 loss=6.237, ppl=75.43, wps=14487.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.593, loss_scale=8, train_wall=439, gb_free=10.1, wall=52968
2022-03-03 01:29:22 | INFO | train_inner | epoch 031:    134 / 393 loss=6.157, ppl=71.38, wps=14697.6, ups=0.22, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.584, loss_scale=8, train_wall=441, gb_free=10.1, wall=53414
2022-03-03 01:36:48 | INFO | train_inner | epoch 031:    234 / 393 loss=6.209, ppl=73.98, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.604, loss_scale=16, train_wall=441, gb_free=10.1, wall=53860
2022-03-03 01:37:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 01:44:19 | INFO | train_inner | epoch 031:    335 / 393 loss=6.243, ppl=75.76, wps=14552.6, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.59, loss_scale=8, train_wall=445, gb_free=10.1, wall=54310
2022-03-03 01:48:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 01:48:42 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.668 | ppl 203.36 | wps 34154.4 | wpb 2034.1 | bsz 4 | num_updates 12158 | best_loss 7.519
2022-03-03 01:48:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12158 updates
2022-03-03 01:48:42 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-03 01:48:42 | INFO | train | epoch 031 | loss 6.206 | ppl 73.83 | wps 14606.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12158 | lr 0.000286793 | gnorm 0.594 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 54573
2022-03-03 01:48:42 | INFO | fairseq.trainer | begin training epoch 32
2022-03-03 01:48:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 01:51:49 | INFO | train_inner | epoch 032:     42 / 393 loss=6.193, ppl=73.19, wps=14480.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.607, loss_scale=8, train_wall=439, gb_free=10.1, wall=54761
2022-03-03 01:59:15 | INFO | train_inner | epoch 032:    142 / 393 loss=6.13, ppl=70.05, wps=14697.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.583, loss_scale=8, train_wall=441, gb_free=10.1, wall=55207
2022-03-03 02:06:41 | INFO | train_inner | epoch 032:    242 / 393 loss=6.188, ppl=72.89, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.604, loss_scale=8, train_wall=441, gb_free=10.1, wall=55653
2022-03-03 02:14:07 | INFO | train_inner | epoch 032:    342 / 393 loss=6.219, ppl=74.5, wps=14700.9, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.6, loss_scale=8, train_wall=441, gb_free=10.1, wall=56098
2022-03-03 02:17:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 02:17:59 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.667 | ppl 203.24 | wps 34147.7 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 7.519
2022-03-03 02:17:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-03 02:17:59 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-03 02:17:59 | INFO | train | epoch 032 | loss 6.18 | ppl 72.53 | wps 14643.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.596 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 56330
2022-03-03 02:17:59 | INFO | fairseq.trainer | begin training epoch 33
2022-03-03 02:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 02:20:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 02:21:42 | INFO | train_inner | epoch 033:     50 / 393 loss=6.173, ppl=72.17, wps=14343.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.599, loss_scale=8, train_wall=444, gb_free=10.1, wall=56553
2022-03-03 02:29:08 | INFO | train_inner | epoch 033:    150 / 393 loss=6.116, ppl=69.38, wps=14696.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.591, loss_scale=8, train_wall=441, gb_free=10.1, wall=56999
2022-03-03 02:36:34 | INFO | train_inner | epoch 033:    250 / 393 loss=6.153, ppl=71.15, wps=14691.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.611, loss_scale=8, train_wall=441, gb_free=10.1, wall=57445
2022-03-03 02:44:00 | INFO | train_inner | epoch 033:    350 / 393 loss=6.199, ppl=73.46, wps=14696.4, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.616, loss_scale=8, train_wall=441, gb_free=10.1, wall=57891
2022-03-03 02:47:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 02:47:16 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.693 | ppl 207 | wps 34016.3 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.519
2022-03-03 02:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-03 02:47:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-03 02:47:16 | INFO | train | epoch 033 | loss 6.155 | ppl 71.28 | wps 14604.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.607 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 58087
2022-03-03 02:47:16 | INFO | fairseq.trainer | begin training epoch 34
2022-03-03 02:47:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 02:51:30 | INFO | train_inner | epoch 034:     57 / 393 loss=6.132, ppl=70.11, wps=14488.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.623, loss_scale=8, train_wall=439, gb_free=10.1, wall=58341
2022-03-03 02:58:56 | INFO | train_inner | epoch 034:    157 / 393 loss=6.086, ppl=67.94, wps=14700.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.599, loss_scale=16, train_wall=441, gb_free=10.1, wall=58787
2022-03-03 03:06:22 | INFO | train_inner | epoch 034:    257 / 393 loss=6.141, ppl=70.58, wps=14694.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.601, loss_scale=16, train_wall=441, gb_free=10.1, wall=59233
2022-03-03 03:08:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 03:13:52 | INFO | train_inner | epoch 034:    358 / 393 loss=6.181, ppl=72.55, wps=14550, ups=0.22, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.634, loss_scale=8, train_wall=446, gb_free=10.1, wall=59684
2022-03-03 03:16:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 03:16:33 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.714 | ppl 209.98 | wps 34076.3 | wpb 2034.1 | bsz 4 | num_updates 13335 | best_loss 7.519
2022-03-03 03:16:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13335 updates
2022-03-03 03:16:33 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-03 03:16:33 | INFO | train | epoch 034 | loss 6.131 | ppl 70.1 | wps 14605.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13335 | lr 0.000273844 | gnorm 0.614 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 59844
2022-03-03 03:16:33 | INFO | fairseq.trainer | begin training epoch 35
2022-03-03 03:16:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 03:21:23 | INFO | train_inner | epoch 035:     65 / 393 loss=6.1, ppl=68.6, wps=14483.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.615, loss_scale=8, train_wall=439, gb_free=10.1, wall=60134
2022-03-03 03:28:48 | INFO | train_inner | epoch 035:    165 / 393 loss=6.071, ppl=67.21, wps=14698.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.605, loss_scale=8, train_wall=441, gb_free=10.1, wall=60580
2022-03-03 03:36:14 | INFO | train_inner | epoch 035:    265 / 393 loss=6.124, ppl=69.74, wps=14697.8, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.623, loss_scale=8, train_wall=441, gb_free=10.1, wall=61026
2022-03-03 03:43:40 | INFO | train_inner | epoch 035:    365 / 393 loss=6.157, ppl=71.37, wps=14692.6, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.624, loss_scale=8, train_wall=441, gb_free=10.1, wall=61472
2022-03-03 03:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 03:45:50 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.708 | ppl 209.16 | wps 34000.7 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 7.519
2022-03-03 03:45:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-03 03:45:50 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-03 03:45:50 | INFO | train | epoch 035 | loss 6.109 | ppl 69.01 | wps 14641.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.619 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 61601
2022-03-03 03:45:50 | INFO | fairseq.trainer | begin training epoch 36
2022-03-03 03:45:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 03:47:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 03:51:15 | INFO | train_inner | epoch 036:     73 / 393 loss=6.065, ppl=66.95, wps=14338.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.615, loss_scale=8, train_wall=444, gb_free=10.1, wall=61927
2022-03-03 03:58:41 | INFO | train_inner | epoch 036:    173 / 393 loss=6.061, ppl=66.75, wps=14702.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.622, loss_scale=8, train_wall=441, gb_free=10.1, wall=62373
2022-03-03 04:06:07 | INFO | train_inner | epoch 036:    273 / 393 loss=6.1, ppl=68.57, wps=14701.4, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.635, loss_scale=8, train_wall=441, gb_free=10.1, wall=62818
2022-03-03 04:13:33 | INFO | train_inner | epoch 036:    373 / 393 loss=6.136, ppl=70.32, wps=14699.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.624, loss_scale=8, train_wall=441, gb_free=10.1, wall=63264
2022-03-03 04:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 04:15:06 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.749 | ppl 215.06 | wps 34086.4 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.519
2022-03-03 04:15:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-03 04:15:06 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-03 04:15:06 | INFO | train | epoch 036 | loss 6.087 | ppl 67.98 | wps 14607.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.624 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 63358
2022-03-03 04:15:06 | INFO | fairseq.trainer | begin training epoch 37
2022-03-03 04:15:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 04:21:03 | INFO | train_inner | epoch 037:     80 / 393 loss=6.02, ppl=64.87, wps=14484.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.624, loss_scale=8, train_wall=439, gb_free=10.1, wall=63715
2022-03-03 04:27:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 04:28:33 | INFO | train_inner | epoch 037:    181 / 393 loss=6.04, ppl=65.81, wps=14555.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.635, loss_scale=8, train_wall=445, gb_free=10.1, wall=64165
2022-03-03 04:35:59 | INFO | train_inner | epoch 037:    281 / 393 loss=6.085, ppl=67.9, wps=14699.1, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.63, loss_scale=8, train_wall=441, gb_free=10.1, wall=64611
2022-03-03 04:43:25 | INFO | train_inner | epoch 037:    381 / 393 loss=6.132, ppl=70.14, wps=14698.5, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.633, loss_scale=8, train_wall=441, gb_free=10.1, wall=65057
2022-03-03 04:44:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 04:44:23 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.741 | ppl 213.87 | wps 34030.6 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 7.519
2022-03-03 04:44:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-03 04:44:23 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-03 04:44:23 | INFO | train | epoch 037 | loss 6.066 | ppl 67.01 | wps 14607.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.63 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 65115
2022-03-03 04:44:23 | INFO | fairseq.trainer | begin training epoch 38
2022-03-03 04:44:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 04:50:56 | INFO | train_inner | epoch 038:     88 / 393 loss=5.998, ppl=63.9, wps=14483.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.617, loss_scale=8, train_wall=439, gb_free=10.1, wall=65507
2022-03-03 04:58:21 | INFO | train_inner | epoch 038:    188 / 393 loss=6.019, ppl=64.86, wps=14699.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.63, loss_scale=8, train_wall=441, gb_free=10.1, wall=65953
2022-03-03 05:05:47 | INFO | train_inner | epoch 038:    288 / 393 loss=6.067, ppl=67.04, wps=14694.5, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.629, loss_scale=16, train_wall=441, gb_free=10.1, wall=66399
2022-03-03 05:06:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 05:13:18 | INFO | train_inner | epoch 038:    389 / 393 loss=6.112, ppl=69.15, wps=14550.8, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.639, loss_scale=8, train_wall=445, gb_free=10.1, wall=66849
2022-03-03 05:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 05:13:40 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.77 | ppl 218.28 | wps 34037.8 | wpb 2034.1 | bsz 4 | num_updates 14904 | best_loss 7.519
2022-03-03 05:13:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14904 updates
2022-03-03 05:13:40 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-03 05:13:40 | INFO | train | epoch 038 | loss 6.047 | ppl 66.14 | wps 14604.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14904 | lr 0.000259029 | gnorm 0.629 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 66872
2022-03-03 05:13:40 | INFO | fairseq.trainer | begin training epoch 39
2022-03-03 05:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 05:20:48 | INFO | train_inner | epoch 039:     96 / 393 loss=5.96, ppl=62.24, wps=14483.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.628, loss_scale=8, train_wall=439, gb_free=10.1, wall=67300
2022-03-03 05:28:14 | INFO | train_inner | epoch 039:    196 / 393 loss=6.013, ppl=64.58, wps=14700.6, ups=0.22, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.632, loss_scale=8, train_wall=441, gb_free=10.1, wall=67746
2022-03-03 05:35:40 | INFO | train_inner | epoch 039:    296 / 393 loss=6.059, ppl=66.66, wps=14700.7, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.646, loss_scale=8, train_wall=441, gb_free=10.1, wall=68191
2022-03-03 05:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 05:42:57 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.773 | ppl 218.73 | wps 34111 | wpb 2034.1 | bsz 4 | num_updates 15297 | best_loss 7.519
2022-03-03 05:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15297 updates
2022-03-03 05:42:57 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-03 05:42:57 | INFO | train | epoch 039 | loss 6.029 | ppl 65.28 | wps 14645 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15297 | lr 0.00025568 | gnorm 0.639 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 68628
2022-03-03 05:42:57 | INFO | fairseq.trainer | begin training epoch 40
2022-03-03 05:42:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 05:43:10 | INFO | train_inner | epoch 040:      3 / 393 loss=6.084, ppl=67.83, wps=14487.5, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.652, loss_scale=8, train_wall=439, gb_free=10.1, wall=68642
2022-03-03 05:49:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 05:50:41 | INFO | train_inner | epoch 040:    104 / 393 loss=5.941, ppl=61.45, wps=14550.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.646, loss_scale=8, train_wall=445, gb_free=10.1, wall=69092
2022-03-03 05:58:07 | INFO | train_inner | epoch 040:    204 / 393 loss=5.989, ppl=63.51, wps=14695.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.642, loss_scale=8, train_wall=441, gb_free=10.1, wall=69538
2022-03-03 06:05:33 | INFO | train_inner | epoch 040:    304 / 393 loss=6.038, ppl=65.71, wps=14698.6, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.645, loss_scale=8, train_wall=441, gb_free=10.1, wall=69984
2022-03-03 06:12:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 06:12:14 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.787 | ppl 220.91 | wps 34066.7 | wpb 2034.1 | bsz 4 | num_updates 15689 | best_loss 7.519
2022-03-03 06:12:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15689 updates
2022-03-03 06:12:14 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-03 06:12:14 | INFO | train | epoch 040 | loss 6.01 | ppl 64.47 | wps 14604.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15689 | lr 0.000252466 | gnorm 0.647 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 70385
2022-03-03 06:12:14 | INFO | fairseq.trainer | begin training epoch 41
2022-03-03 06:12:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 06:13:03 | INFO | train_inner | epoch 041:     11 / 393 loss=6.062, ppl=66.83, wps=14483.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.649, loss_scale=8, train_wall=439, gb_free=10.1, wall=70435
2022-03-03 06:20:29 | INFO | train_inner | epoch 041:    111 / 393 loss=5.926, ppl=60.8, wps=14700.3, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.644, loss_scale=8, train_wall=441, gb_free=10.1, wall=70880
2022-03-03 06:27:55 | INFO | train_inner | epoch 041:    211 / 393 loss=5.984, ppl=63.29, wps=14695.4, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.647, loss_scale=16, train_wall=441, gb_free=10.1, wall=71326
2022-03-03 06:30:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 06:35:25 | INFO | train_inner | epoch 041:    312 / 393 loss=6.02, ppl=64.9, wps=14553.4, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.661, loss_scale=8, train_wall=445, gb_free=10.1, wall=71777
2022-03-03 06:41:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 06:41:31 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.782 | ppl 220.15 | wps 34046 | wpb 2034.1 | bsz 4 | num_updates 16081 | best_loss 7.519
2022-03-03 06:41:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16081 updates
2022-03-03 06:41:31 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-03 06:41:31 | INFO | train | epoch 041 | loss 5.993 | ppl 63.7 | wps 14605.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16081 | lr 0.00024937 | gnorm 0.652 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 72142
2022-03-03 06:41:31 | INFO | fairseq.trainer | begin training epoch 42
2022-03-03 06:41:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 06:42:56 | INFO | train_inner | epoch 042:     19 / 393 loss=6.036, ppl=65.63, wps=14481.2, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.656, loss_scale=8, train_wall=439, gb_free=10.1, wall=72227
2022-03-03 06:50:22 | INFO | train_inner | epoch 042:    119 / 393 loss=5.922, ppl=60.64, wps=14695.2, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.637, loss_scale=8, train_wall=441, gb_free=10.1, wall=72673
2022-03-03 06:57:48 | INFO | train_inner | epoch 042:    219 / 393 loss=5.968, ppl=62.61, wps=14696.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.648, loss_scale=8, train_wall=441, gb_free=10.1, wall=73119
2022-03-03 07:05:13 | INFO | train_inner | epoch 042:    319 / 393 loss=6.009, ppl=64.41, wps=14699.9, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.662, loss_scale=8, train_wall=441, gb_free=10.1, wall=73565
2022-03-03 07:10:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 07:10:48 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.796 | ppl 222.29 | wps 34052.6 | wpb 2034.1 | bsz 4 | num_updates 16474 | best_loss 7.519
2022-03-03 07:10:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16474 updates
2022-03-03 07:10:48 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-03 07:10:48 | INFO | train | epoch 042 | loss 5.977 | ppl 63 | wps 14641.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.648 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 73899
2022-03-03 07:10:48 | INFO | fairseq.trainer | begin training epoch 43
2022-03-03 07:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 07:12:44 | INFO | train_inner | epoch 043:     26 / 393 loss=5.995, ppl=63.79, wps=14480, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.646, loss_scale=16, train_wall=439, gb_free=10.1, wall=74016
2022-03-03 07:20:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 07:20:14 | INFO | train_inner | epoch 043:    127 / 393 loss=5.907, ppl=59.99, wps=14549.7, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.649, loss_scale=8, train_wall=445, gb_free=10.1, wall=74466
2022-03-03 07:27:41 | INFO | train_inner | epoch 043:    227 / 393 loss=5.953, ppl=61.95, wps=14688.2, ups=0.22, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.666, loss_scale=8, train_wall=441, gb_free=10.1, wall=74912
2022-03-03 07:35:07 | INFO | train_inner | epoch 043:    327 / 393 loss=6.003, ppl=64.15, wps=14695, ups=0.22, wpb=65530.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.662, loss_scale=8, train_wall=441, gb_free=10.1, wall=75358
2022-03-03 07:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 07:40:05 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.837 | ppl 228.67 | wps 34123.1 | wpb 2034.1 | bsz 4 | num_updates 16866 | best_loss 7.519
2022-03-03 07:40:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16866 updates
2022-03-03 07:40:05 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-03 07:40:05 | INFO | train | epoch 043 | loss 5.961 | ppl 62.29 | wps 14601.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 0.66 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 75657
2022-03-03 07:40:05 | INFO | fairseq.trainer | begin training epoch 44
2022-03-03 07:40:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 07:42:37 | INFO | train_inner | epoch 044:     34 / 393 loss=5.969, ppl=62.63, wps=14482.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.665, loss_scale=8, train_wall=439, gb_free=10.1, wall=75809
2022-03-03 07:50:03 | INFO | train_inner | epoch 044:    134 / 393 loss=5.889, ppl=59.25, wps=14696, ups=0.22, wpb=65535.4, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.662, loss_scale=8, train_wall=441, gb_free=10.1, wall=76255
2022-03-03 07:57:29 | INFO | train_inner | epoch 044:    234 / 393 loss=5.944, ppl=61.56, wps=14699.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.655, loss_scale=8, train_wall=441, gb_free=10.1, wall=76700
2022-03-03 07:59:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 08:04:59 | INFO | train_inner | epoch 044:    335 / 393 loss=5.983, ppl=63.27, wps=14544.9, ups=0.22, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.665, loss_scale=8, train_wall=446, gb_free=10.1, wall=77151
2022-03-03 08:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 08:09:23 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.829 | ppl 227.34 | wps 34058.7 | wpb 2034.1 | bsz 4 | num_updates 17258 | best_loss 7.519
2022-03-03 08:09:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17258 updates
2022-03-03 08:09:23 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-03 08:09:23 | INFO | train | epoch 044 | loss 5.946 | ppl 61.63 | wps 14602.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17258 | lr 0.000240716 | gnorm 0.66 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 77414
2022-03-03 08:09:23 | INFO | fairseq.trainer | begin training epoch 45
2022-03-03 08:09:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 08:12:30 | INFO | train_inner | epoch 045:     42 / 393 loss=5.956, ppl=62.06, wps=14480.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.664, loss_scale=8, train_wall=439, gb_free=10.1, wall=77602
2022-03-03 08:19:56 | INFO | train_inner | epoch 045:    142 / 393 loss=5.889, ppl=59.25, wps=14693.6, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.656, loss_scale=8, train_wall=441, gb_free=10.1, wall=78048
2022-03-03 08:27:22 | INFO | train_inner | epoch 045:    242 / 393 loss=5.928, ppl=60.88, wps=14694.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.666, loss_scale=8, train_wall=441, gb_free=10.1, wall=78494
2022-03-03 08:34:48 | INFO | train_inner | epoch 045:    342 / 393 loss=5.977, ppl=63.01, wps=14692.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.658, loss_scale=8, train_wall=441, gb_free=10.1, wall=78940
2022-03-03 08:38:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 08:38:40 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.847 | ppl 230.26 | wps 34152.1 | wpb 2034.1 | bsz 4 | num_updates 17651 | best_loss 7.519
2022-03-03 08:38:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17651 updates
2022-03-03 08:38:40 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-03 08:38:40 | INFO | train | epoch 045 | loss 5.932 | ppl 61.04 | wps 14640.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17651 | lr 0.000238021 | gnorm 0.663 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 79171
2022-03-03 08:38:40 | INFO | fairseq.trainer | begin training epoch 46
2022-03-03 08:38:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 08:42:19 | INFO | train_inner | epoch 046:     49 / 393 loss=5.926, ppl=60.8, wps=14484.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.669, loss_scale=16, train_wall=439, gb_free=10.1, wall=79390
2022-03-03 08:43:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 08:49:49 | INFO | train_inner | epoch 046:    150 / 393 loss=5.87, ppl=58.49, wps=14551.5, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.687, loss_scale=8, train_wall=445, gb_free=10.1, wall=79840
2022-03-03 08:57:15 | INFO | train_inner | epoch 046:    250 / 393 loss=5.918, ppl=60.46, wps=14698.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.669, loss_scale=8, train_wall=441, gb_free=10.1, wall=80286
2022-03-03 09:04:41 | INFO | train_inner | epoch 046:    350 / 393 loss=5.968, ppl=62.61, wps=14698.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.671, loss_scale=8, train_wall=441, gb_free=10.1, wall=80732
2022-03-03 09:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 09:07:57 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.864 | ppl 233 | wps 34016.8 | wpb 2034.1 | bsz 4 | num_updates 18043 | best_loss 7.519
2022-03-03 09:07:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18043 updates
2022-03-03 09:07:57 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-03 09:07:57 | INFO | train | epoch 046 | loss 5.917 | ppl 60.44 | wps 14605.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18043 | lr 0.000235421 | gnorm 0.676 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 80928
2022-03-03 09:07:57 | INFO | fairseq.trainer | begin training epoch 47
2022-03-03 09:07:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 09:12:11 | INFO | train_inner | epoch 047:     57 / 393 loss=5.897, ppl=59.6, wps=14484.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.687, loss_scale=8, train_wall=439, gb_free=10.1, wall=81183
2022-03-03 09:19:37 | INFO | train_inner | epoch 047:    157 / 393 loss=5.859, ppl=58.05, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.674, loss_scale=8, train_wall=441, gb_free=10.1, wall=81629
2022-03-03 09:25:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 09:27:08 | INFO | train_inner | epoch 047:    258 / 393 loss=5.913, ppl=60.27, wps=14554.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.671, loss_scale=8, train_wall=445, gb_free=10.1, wall=82079
2022-03-03 09:34:33 | INFO | train_inner | epoch 047:    358 / 393 loss=5.959, ppl=62.22, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.687, loss_scale=8, train_wall=441, gb_free=10.1, wall=82525
2022-03-03 09:37:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 09:37:14 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.849 | ppl 230.62 | wps 34038 | wpb 2034.1 | bsz 4 | num_updates 18435 | best_loss 7.519
2022-03-03 09:37:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18435 updates
2022-03-03 09:37:14 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-03 09:37:14 | INFO | train | epoch 047 | loss 5.905 | ppl 59.91 | wps 14603.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18435 | lr 0.000232905 | gnorm 0.678 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 82686
2022-03-03 09:37:14 | INFO | fairseq.trainer | begin training epoch 48
2022-03-03 09:37:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 09:42:04 | INFO | train_inner | epoch 048:     65 / 393 loss=5.871, ppl=58.54, wps=14487.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.686, loss_scale=8, train_wall=439, gb_free=10.1, wall=82975
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
