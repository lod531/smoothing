Sender: LSF System <lsfadmin@eu-g3-002>
Subject: Job 206346947: <w103_size_0.5_fp16_label_smoothing_0.04_#2> in cluster <euler> Exited

Job <w103_size_0.5_fp16_label_smoothing_0.04_#2> was submitted from host <eu-login-46> by user <andriusb> in cluster <euler> at Thu Feb 24 11:14:48 2022
Job was executed on host(s) <eu-g3-002>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Thu Feb 24 11:16:02 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Feb 24 11:16:02 2022
Terminated at Sat Feb 26 11:16:19 2022
Results reported at Sat Feb 26 11:16:19 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.04 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 1321672 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   172714.00 sec.
    Max Memory :                                 12598 MB
    Average Memory :                             4943.93 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               7402.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   172816 sec.
    Turnaround time :                            172891 sec.

The output (if any) follows:

2022-02-24 11:16:17 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321672, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321672, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.04, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-24 11:16:18 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-24 11:16:23 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-24 11:16:23 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-24 11:16:23 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-24 11:16:23 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-24 11:16:23 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-24 11:16:23 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-24 11:16:23 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-24 11:16:33 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-24 11:16:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-24 11:16:33 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-02-24 11:16:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-24 11:16:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-24 11:16:33 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-02-24 11:16:33 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-24 11:16:33 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-24 11:16:33 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-24 11:16:33 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-24 11:16:33 | INFO | fairseq.trainer | begin training epoch 1
2022-02-24 11:16:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 11:16:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-24 11:16:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 11:17:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 11:17:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-24 11:17:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-24 11:27:09 | INFO | train_inner | epoch 001:    105 / 788 loss=17.599, nll_loss=17.522, ppl=188174, wps=11134.7, ups=0.17, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.293, loss_scale=4, train_wall=612, gb_free=3.3, wall=636
2022-02-24 11:36:52 | INFO | train_inner | epoch 001:    205 / 788 loss=15.166, nll_loss=14.987, ppl=32480.3, wps=11248.5, ups=0.17, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.649, loss_scale=8, train_wall=560, gb_free=3.3, wall=1219
2022-02-24 11:46:35 | INFO | train_inner | epoch 001:    305 / 788 loss=12.948, nll_loss=12.665, ppl=6496.57, wps=11238.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.156, loss_scale=16, train_wall=561, gb_free=3.3, wall=1802
2022-02-24 11:56:18 | INFO | train_inner | epoch 001:    405 / 788 loss=11.316, nll_loss=10.929, ppl=1949.88, wps=11233, ups=0.17, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.668, loss_scale=32, train_wall=561, gb_free=3.3, wall=2385
2022-02-24 12:06:01 | INFO | train_inner | epoch 001:    505 / 788 loss=10.651, nll_loss=10.195, ppl=1172.25, wps=11249.2, ups=0.17, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.487, loss_scale=32, train_wall=560, gb_free=3.3, wall=2968
2022-02-24 12:07:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 12:15:49 | INFO | train_inner | epoch 001:    606 / 788 loss=10.313, nll_loss=9.823, ppl=905.5, wps=11137, ups=0.17, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.565, loss_scale=32, train_wall=566, gb_free=3.3, wall=3556
2022-02-24 12:20:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 12:25:38 | INFO | train_inner | epoch 001:    707 / 788 loss=10.042, nll_loss=9.531, ppl=739.92, wps=11139.6, ups=0.17, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.635, loss_scale=32, train_wall=566, gb_free=3.3, wall=4145
2022-02-24 12:33:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 12:33:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 12:33:35 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.647 | nll_loss 9.115 | ppl 554.49 | wps 28233.6 | wpb 510.9 | bsz 1 | num_updates 780
2022-02-24 12:33:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 780 updates
2022-02-24 12:33:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 12:33:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 12:33:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 1 @ 780 updates, score 9.647) (writing took 12.749063925817609 seconds)
2022-02-24 12:33:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-24 12:33:48 | INFO | train | epoch 001 | loss 12.296 | nll_loss 11.937 | ppl 3920.9 | wps 11137.2 | ups 0.17 | wpb 65497.1 | bsz 127.9 | num_updates 780 | lr 9.75805e-05 | gnorm 1.155 | loss_scale 32 | train_wall 4438 | gb_free 3.3 | wall 4635
2022-02-24 12:33:48 | INFO | fairseq.trainer | begin training epoch 2
2022-02-24 12:33:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 12:35:44 | INFO | train_inner | epoch 002:     20 / 788 loss=9.802, nll_loss=9.276, ppl=619.74, wps=10752.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.717, loss_scale=32, train_wall=564, gb_free=3.3, wall=4751
2022-02-24 12:45:28 | INFO | train_inner | epoch 002:    120 / 788 loss=9.582, nll_loss=9.042, ppl=527.15, wps=11234.8, ups=0.17, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.806, loss_scale=32, train_wall=561, gb_free=3.3, wall=5335
2022-02-24 12:46:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 12:55:16 | INFO | train_inner | epoch 002:    221 / 788 loss=9.386, nll_loss=8.835, ppl=456.58, wps=11132.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.875, loss_scale=32, train_wall=566, gb_free=3.3, wall=5923
2022-02-24 12:58:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 13:05:05 | INFO | train_inner | epoch 002:    322 / 788 loss=9.212, nll_loss=8.651, ppl=402.09, wps=11129.7, ups=0.17, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.831, loss_scale=32, train_wall=566, gb_free=3.3, wall=6512
2022-02-24 13:12:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 13:14:54 | INFO | train_inner | epoch 002:    423 / 788 loss=9.059, nll_loss=8.49, ppl=359.58, wps=11132, ups=0.17, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.931, loss_scale=32, train_wall=566, gb_free=3.3, wall=7101
2022-02-24 13:24:37 | INFO | train_inner | epoch 002:    523 / 788 loss=8.914, nll_loss=8.338, ppl=323.51, wps=11241.2, ups=0.17, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.88, loss_scale=32, train_wall=561, gb_free=3.3, wall=7684
2022-02-24 13:25:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 13:29:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 13:34:31 | INFO | train_inner | epoch 002:    625 / 788 loss=8.78, nll_loss=8.196, ppl=293.27, wps=11035.1, ups=0.17, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.915, loss_scale=16, train_wall=571, gb_free=3.3, wall=8278
2022-02-24 13:44:13 | INFO | train_inner | epoch 002:    725 / 788 loss=8.649, nll_loss=8.058, ppl=266.58, wps=11250.8, ups=0.17, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.927, loss_scale=32, train_wall=560, gb_free=3.3, wall=8860
2022-02-24 13:50:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 13:50:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.386 | nll_loss 7.775 | ppl 218.96 | wps 28352.8 | wpb 510.9 | bsz 1 | num_updates 1563 | best_loss 8.386
2022-02-24 13:50:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1563 updates
2022-02-24 13:50:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 13:50:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 13:50:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 2 @ 1563 updates, score 8.386) (writing took 12.789088828489184 seconds)
2022-02-24 13:50:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-24 13:50:38 | INFO | train | epoch 002 | loss 9.057 | nll_loss 8.488 | ppl 359.14 | wps 11123.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 1563 | lr 0.000195436 | gnorm 0.88 | loss_scale 32 | train_wall 4414 | gb_free 3.3 | wall 9245
2022-02-24 13:50:38 | INFO | fairseq.trainer | begin training epoch 3
2022-02-24 13:50:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 13:54:14 | INFO | train_inner | epoch 003:     37 / 788 loss=8.521, nll_loss=7.923, ppl=242.73, wps=10863, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.91, loss_scale=32, train_wall=558, gb_free=3.3, wall=9461
2022-02-24 13:55:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 14:04:02 | INFO | train_inner | epoch 003:    138 / 788 loss=8.383, nll_loss=7.778, ppl=219.44, wps=11133.2, ups=0.17, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.921, loss_scale=32, train_wall=566, gb_free=3.3, wall=10049
2022-02-24 14:08:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 14:13:51 | INFO | train_inner | epoch 003:    239 / 788 loss=8.282, nll_loss=7.671, ppl=203.84, wps=11138.2, ups=0.17, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.872, loss_scale=32, train_wall=566, gb_free=3.3, wall=10638
2022-02-24 14:21:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 14:23:39 | INFO | train_inner | epoch 003:    340 / 788 loss=8.203, nll_loss=7.588, ppl=192.42, wps=11136.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.88, loss_scale=32, train_wall=566, gb_free=3.3, wall=11226
2022-02-24 14:33:22 | INFO | train_inner | epoch 003:    440 / 788 loss=8.106, nll_loss=7.485, ppl=179.18, wps=11244.6, ups=0.17, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.873, loss_scale=32, train_wall=560, gb_free=3.3, wall=11809
2022-02-24 14:34:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 14:43:10 | INFO | train_inner | epoch 003:    541 / 788 loss=8.023, nll_loss=7.398, ppl=168.68, wps=11139.1, ups=0.17, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.859, loss_scale=32, train_wall=566, gb_free=3.3, wall=12397
2022-02-24 14:47:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 14:52:59 | INFO | train_inner | epoch 003:    642 / 788 loss=7.95, nll_loss=7.321, ppl=159.88, wps=11140.4, ups=0.17, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.85, loss_scale=32, train_wall=566, gb_free=3.3, wall=12986
2022-02-24 15:00:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 15:02:47 | INFO | train_inner | epoch 003:    743 / 788 loss=7.866, nll_loss=7.232, ppl=150.37, wps=11133.9, ups=0.17, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.816, loss_scale=32, train_wall=566, gb_free=3.3, wall=13574
2022-02-24 15:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 15:07:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.658 | nll_loss 7 | ppl 127.98 | wps 28190 | wpb 510.9 | bsz 1 | num_updates 2345 | best_loss 7.658
2022-02-24 15:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2345 updates
2022-02-24 15:07:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 15:07:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 15:07:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 3 @ 2345 updates, score 7.658) (writing took 12.708404483273625 seconds)
2022-02-24 15:07:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-24 15:07:27 | INFO | train | epoch 003 | loss 8.114 | nll_loss 7.494 | ppl 180.31 | wps 11112.9 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 2345 | lr 0.000293166 | gnorm 0.868 | loss_scale 32 | train_wall 4413 | gb_free 3.3 | wall 13854
2022-02-24 15:07:27 | INFO | fairseq.trainer | begin training epoch 4
2022-02-24 15:07:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 15:12:48 | INFO | train_inner | epoch 004:     55 / 788 loss=7.745, nll_loss=7.105, ppl=137.66, wps=10866, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.818, loss_scale=32, train_wall=557, gb_free=3.3, wall=14175
2022-02-24 15:13:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 15:22:37 | INFO | train_inner | epoch 004:    156 / 788 loss=7.657, nll_loss=7.012, ppl=129.07, wps=11128.8, ups=0.17, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.793, loss_scale=32, train_wall=566, gb_free=3.3, wall=14764
2022-02-24 15:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 15:32:25 | INFO | train_inner | epoch 004:    257 / 788 loss=7.604, nll_loss=6.957, ppl=124.2, wps=11137.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.774, loss_scale=32, train_wall=566, gb_free=3.3, wall=15352
2022-02-24 15:39:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 15:40:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 15:42:19 | INFO | train_inner | epoch 004:    359 / 788 loss=7.541, nll_loss=6.889, ppl=118.55, wps=11034, ups=0.17, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.791, loss_scale=16, train_wall=571, gb_free=3.3, wall=15946
2022-02-24 15:52:01 | INFO | train_inner | epoch 004:    459 / 788 loss=7.501, nll_loss=6.848, ppl=115.17, wps=11260.1, ups=0.17, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.774, loss_scale=16, train_wall=560, gb_free=3.3, wall=16528
2022-02-24 16:01:43 | INFO | train_inner | epoch 004:    559 / 788 loss=7.443, nll_loss=6.787, ppl=110.44, wps=11251.3, ups=0.17, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.762, loss_scale=32, train_wall=560, gb_free=3.3, wall=17110
2022-02-24 16:07:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 16:07:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 16:11:37 | INFO | train_inner | epoch 004:    661 / 788 loss=7.39, nll_loss=6.731, ppl=106.2, wps=11032.9, ups=0.17, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.722, loss_scale=16, train_wall=571, gb_free=3.3, wall=17704
2022-02-24 16:21:19 | INFO | train_inner | epoch 004:    761 / 788 loss=7.345, nll_loss=6.683, ppl=102.78, wps=11262.3, ups=0.17, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.756, loss_scale=32, train_wall=560, gb_free=3.3, wall=18286
2022-02-24 16:23:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 16:24:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.202 | nll_loss 6.511 | ppl 91.21 | wps 28037.3 | wpb 510.9 | bsz 1 | num_updates 3127 | best_loss 7.202
2022-02-24 16:24:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3127 updates
2022-02-24 16:24:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 16:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 16:24:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 4 @ 3127 updates, score 7.202) (writing took 12.765649089589715 seconds)
2022-02-24 16:24:14 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-24 16:24:14 | INFO | train | epoch 004 | loss 7.504 | nll_loss 6.851 | ppl 115.46 | wps 11116.8 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 3127 | lr 0.000390897 | gnorm 0.769 | loss_scale 32 | train_wall 4411 | gb_free 3.3 | wall 18462
2022-02-24 16:24:15 | INFO | fairseq.trainer | begin training epoch 5
2022-02-24 16:24:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 16:28:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 16:31:25 | INFO | train_inner | epoch 005:     74 / 788 loss=7.216, nll_loss=6.547, ppl=93.53, wps=10766, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.714, loss_scale=16, train_wall=563, gb_free=3.3, wall=18892
2022-02-24 16:41:07 | INFO | train_inner | epoch 005:    174 / 788 loss=7.16, nll_loss=6.488, ppl=89.75, wps=11263.7, ups=0.17, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.733, loss_scale=32, train_wall=559, gb_free=3.3, wall=19474
2022-02-24 16:48:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 16:50:55 | INFO | train_inner | epoch 005:    275 / 788 loss=7.136, nll_loss=6.463, ppl=88.21, wps=11149, ups=0.17, wpb=65534.7, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.715, loss_scale=16, train_wall=565, gb_free=3.3, wall=20062
2022-02-24 17:00:37 | INFO | train_inner | epoch 005:    375 / 788 loss=7.099, nll_loss=6.424, ppl=85.85, wps=11267.3, ups=0.17, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.703, loss_scale=16, train_wall=559, gb_free=3.3, wall=20644
2022-02-24 17:10:19 | INFO | train_inner | epoch 005:    475 / 788 loss=7.071, nll_loss=6.395, ppl=84.14, wps=11260.4, ups=0.17, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.693, loss_scale=32, train_wall=560, gb_free=3.3, wall=21226
2022-02-24 17:13:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-24 17:15:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 17:20:13 | INFO | train_inner | epoch 005:    577 / 788 loss=7.044, nll_loss=6.366, ppl=82.48, wps=11026.1, ups=0.17, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.696, loss_scale=16, train_wall=572, gb_free=3.3, wall=21820
2022-02-24 17:30:02 | INFO | train_inner | epoch 005:    677 / 788 loss=7.01, nll_loss=6.331, ppl=80.49, wps=11117.7, ups=0.17, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.671, loss_scale=32, train_wall=566, gb_free=3.3, wall=22409
2022-02-24 17:33:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 17:40:02 | INFO | train_inner | epoch 005:    778 / 788 loss=6.983, nll_loss=6.303, ppl=78.93, wps=10933.9, ups=0.17, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.663, loss_scale=16, train_wall=575, gb_free=3.3, wall=23009
2022-02-24 17:40:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 17:41:07 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.889 | nll_loss 6.192 | ppl 73.09 | wps 26318.1 | wpb 510.9 | bsz 1 | num_updates 3910 | best_loss 6.889
2022-02-24 17:41:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3910 updates
2022-02-24 17:41:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 17:41:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 17:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 5 @ 3910 updates, score 6.889) (writing took 13.39940502308309 seconds)
2022-02-24 17:41:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-24 17:41:20 | INFO | train | epoch 005 | loss 7.081 | nll_loss 6.405 | ppl 84.77 | wps 11086.8 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 3910 | lr 0.000488752 | gnorm 0.699 | loss_scale 16 | train_wall 4425 | gb_free 3.3 | wall 23087
2022-02-24 17:41:20 | INFO | fairseq.trainer | begin training epoch 6
2022-02-24 17:41:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 17:46:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 17:50:20 | INFO | train_inner | epoch 006:     91 / 788 loss=6.848, nll_loss=6.16, ppl=71.49, wps=10545.5, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.665, loss_scale=16, train_wall=572, gb_free=3.3, wall=23627
2022-02-24 18:00:09 | INFO | train_inner | epoch 006:    191 / 788 loss=6.829, nll_loss=6.14, ppl=70.52, wps=11125.4, ups=0.17, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.654, loss_scale=32, train_wall=565, gb_free=3.3, wall=24216
2022-02-24 18:02:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 18:10:02 | INFO | train_inner | epoch 006:    292 / 788 loss=6.804, nll_loss=6.113, ppl=69.21, wps=11056.6, ups=0.17, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.638, loss_scale=16, train_wall=570, gb_free=3.3, wall=24809
2022-02-24 18:16:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 18:19:56 | INFO | train_inner | epoch 006:    393 / 788 loss=6.774, nll_loss=6.082, ppl=67.75, wps=11033.1, ups=0.17, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.6, loss_scale=16, train_wall=571, gb_free=3.3, wall=25403
2022-02-24 18:29:44 | INFO | train_inner | epoch 006:    493 / 788 loss=6.762, nll_loss=6.07, ppl=67.16, wps=11142.5, ups=0.17, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.604, loss_scale=32, train_wall=566, gb_free=3.3, wall=25991
2022-02-24 18:32:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 18:39:38 | INFO | train_inner | epoch 006:    594 / 788 loss=6.74, nll_loss=6.047, ppl=66.1, wps=11035.1, ups=0.17, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.607, loss_scale=16, train_wall=571, gb_free=3.3, wall=26585
2022-02-24 18:45:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 18:49:32 | INFO | train_inner | epoch 006:    695 / 788 loss=6.73, nll_loss=6.036, ppl=65.61, wps=11035.7, ups=0.17, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.585, loss_scale=16, train_wall=571, gb_free=3.3, wall=27179
2022-02-24 18:58:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 18:58:44 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.689 | nll_loss 5.997 | ppl 63.86 | wps 27690 | wpb 510.9 | bsz 1 | num_updates 4693 | best_loss 6.689
2022-02-24 18:58:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4693 updates
2022-02-24 18:58:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 18:58:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 18:58:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 6 @ 4693 updates, score 6.689) (writing took 13.367507902905345 seconds)
2022-02-24 18:58:57 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-24 18:58:57 | INFO | train | epoch 006 | loss 6.774 | nll_loss 6.082 | ppl 67.73 | wps 11012 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 4693 | lr 0.000461609 | gnorm 0.615 | loss_scale 32 | train_wall 4456 | gb_free 3.3 | wall 27744
2022-02-24 18:58:57 | INFO | fairseq.trainer | begin training epoch 7
2022-02-24 18:58:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 18:59:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 18:59:44 | INFO | train_inner | epoch 007:      8 / 788 loss=6.706, nll_loss=6.011, ppl=64.51, wps=10652.4, ups=0.16, wpb=65232.6, bsz=127.4, num_updates=4700, lr=0.000461266, gnorm=0.574, loss_scale=16, train_wall=568, gb_free=3.3, wall=27792
2022-02-24 19:09:28 | INFO | train_inner | epoch 007:    108 / 788 loss=6.552, nll_loss=5.849, ppl=57.65, wps=11231.1, ups=0.17, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.582, loss_scale=16, train_wall=561, gb_free=3.3, wall=28375
2022-02-24 19:13:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 19:19:16 | INFO | train_inner | epoch 007:    209 / 788 loss=6.542, nll_loss=5.838, ppl=57.2, wps=11150, ups=0.17, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.559, loss_scale=16, train_wall=565, gb_free=3.3, wall=28963
2022-02-24 19:26:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 19:29:03 | INFO | train_inner | epoch 007:    310 / 788 loss=6.549, nll_loss=5.846, ppl=57.52, wps=11155.8, ups=0.17, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.56, loss_scale=16, train_wall=565, gb_free=3.3, wall=29550
2022-02-24 19:38:45 | INFO | train_inner | epoch 007:    410 / 788 loss=6.54, nll_loss=5.836, ppl=57.14, wps=11269.9, ups=0.17, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.562, loss_scale=16, train_wall=559, gb_free=3.3, wall=30132
2022-02-24 19:40:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 19:48:32 | INFO | train_inner | epoch 007:    511 / 788 loss=6.534, nll_loss=5.83, ppl=56.89, wps=11157.9, ups=0.17, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.563, loss_scale=16, train_wall=565, gb_free=3.3, wall=30719
2022-02-24 19:55:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 19:58:20 | INFO | train_inner | epoch 007:    612 / 788 loss=6.547, nll_loss=5.844, ppl=57.43, wps=11150.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.548, loss_scale=16, train_wall=565, gb_free=3.3, wall=31307
2022-02-24 20:07:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 20:08:07 | INFO | train_inner | epoch 007:    713 / 788 loss=6.536, nll_loss=5.834, ppl=57.03, wps=11156.3, ups=0.17, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.541, loss_scale=16, train_wall=565, gb_free=3.3, wall=31894
2022-02-24 20:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 20:15:29 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.541 | nll_loss 5.832 | ppl 56.98 | wps 28327 | wpb 510.9 | bsz 1 | num_updates 5475 | best_loss 6.541
2022-02-24 20:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5475 updates
2022-02-24 20:15:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 20:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 20:15:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 7 @ 5475 updates, score 6.541) (writing took 12.911967119202018 seconds)
2022-02-24 20:15:42 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-24 20:15:42 | INFO | train | epoch 007 | loss 6.541 | nll_loss 5.837 | ppl 57.18 | wps 11124.3 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 5475 | lr 0.000427374 | gnorm 0.561 | loss_scale 16 | train_wall 4407 | gb_free 3.3 | wall 32349
2022-02-24 20:15:42 | INFO | fairseq.trainer | begin training epoch 8
2022-02-24 20:15:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 20:18:07 | INFO | train_inner | epoch 008:     25 / 788 loss=6.474, nll_loss=5.767, ppl=54.46, wps=10876.6, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.552, loss_scale=16, train_wall=557, gb_free=3.3, wall=32494
2022-02-24 20:27:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 20:27:55 | INFO | train_inner | epoch 008:    126 / 788 loss=6.369, nll_loss=5.657, ppl=50.45, wps=11148, ups=0.17, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.551, loss_scale=16, train_wall=565, gb_free=3.3, wall=33082
2022-02-24 20:37:37 | INFO | train_inner | epoch 008:    226 / 788 loss=6.356, nll_loss=5.643, ppl=49.98, wps=11267.3, ups=0.17, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.53, loss_scale=16, train_wall=559, gb_free=3.3, wall=33664
2022-02-24 20:42:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 20:47:24 | INFO | train_inner | epoch 008:    327 / 788 loss=6.384, nll_loss=5.673, ppl=51.01, wps=11156.2, ups=0.17, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.551, loss_scale=16, train_wall=565, gb_free=3.3, wall=34251
2022-02-24 20:55:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 20:57:11 | INFO | train_inner | epoch 008:    428 / 788 loss=6.38, nll_loss=5.668, ppl=50.86, wps=11155.4, ups=0.17, wpb=65534.7, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.524, loss_scale=16, train_wall=565, gb_free=3.3, wall=34839
2022-02-24 21:06:53 | INFO | train_inner | epoch 008:    528 / 788 loss=6.379, nll_loss=5.667, ppl=50.82, wps=11263.2, ups=0.17, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.541, loss_scale=16, train_wall=559, gb_free=3.3, wall=35420
2022-02-24 21:07:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 21:16:41 | INFO | train_inner | epoch 008:    629 / 788 loss=6.391, nll_loss=5.68, ppl=51.26, wps=11155.2, ups=0.17, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.528, loss_scale=16, train_wall=565, gb_free=3.3, wall=36008
2022-02-24 21:24:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 21:26:28 | INFO | train_inner | epoch 008:    730 / 788 loss=6.382, nll_loss=5.671, ppl=50.94, wps=11154.4, ups=0.17, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.521, loss_scale=16, train_wall=565, gb_free=3.3, wall=36595
2022-02-24 21:32:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 21:32:11 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.447 | nll_loss 5.731 | ppl 53.1 | wps 28156.1 | wpb 510.9 | bsz 1 | num_updates 6258 | best_loss 6.447
2022-02-24 21:32:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6258 updates
2022-02-24 21:32:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 21:32:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 21:32:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 8 @ 6258 updates, score 6.447) (writing took 12.948372322134674 seconds)
2022-02-24 21:32:24 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-24 21:32:24 | INFO | train | epoch 008 | loss 6.376 | nll_loss 5.664 | ppl 50.7 | wps 11143.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 6258 | lr 0.000399744 | gnorm 0.534 | loss_scale 16 | train_wall 4405 | gb_free 3.3 | wall 36951
2022-02-24 21:32:24 | INFO | fairseq.trainer | begin training epoch 9
2022-02-24 21:32:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 21:36:28 | INFO | train_inner | epoch 009:     42 / 788 loss=6.307, nll_loss=5.592, ppl=48.23, wps=10873.3, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.528, loss_scale=16, train_wall=557, gb_free=3.3, wall=37195
2022-02-24 21:38:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 21:46:16 | INFO | train_inner | epoch 009:    143 / 788 loss=6.229, nll_loss=5.51, ppl=45.57, wps=11152, ups=0.17, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.53, loss_scale=16, train_wall=565, gb_free=3.3, wall=37783
2022-02-24 21:54:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 21:56:04 | INFO | train_inner | epoch 009:    244 / 788 loss=6.235, nll_loss=5.516, ppl=45.76, wps=11151.5, ups=0.17, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.543, loss_scale=16, train_wall=565, gb_free=3.3, wall=38371
2022-02-24 22:05:45 | INFO | train_inner | epoch 009:    344 / 788 loss=6.257, nll_loss=5.539, ppl=46.5, wps=11265.1, ups=0.17, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.542, loss_scale=16, train_wall=559, gb_free=3.3, wall=38952
2022-02-24 22:07:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 22:15:33 | INFO | train_inner | epoch 009:    445 / 788 loss=6.269, nll_loss=5.552, ppl=46.91, wps=11150.6, ups=0.17, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.519, loss_scale=16, train_wall=565, gb_free=3.3, wall=39540
2022-02-24 22:22:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 22:25:21 | INFO | train_inner | epoch 009:    546 / 788 loss=6.269, nll_loss=5.552, ppl=46.92, wps=11150.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.529, loss_scale=16, train_wall=565, gb_free=3.3, wall=40128
2022-02-24 22:34:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 22:35:08 | INFO | train_inner | epoch 009:    647 / 788 loss=6.26, nll_loss=5.542, ppl=46.6, wps=11153.2, ups=0.17, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.525, loss_scale=16, train_wall=565, gb_free=3.3, wall=40716
2022-02-24 22:44:50 | INFO | train_inner | epoch 009:    747 / 788 loss=6.254, nll_loss=5.537, ppl=46.42, wps=11269.2, ups=0.17, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.511, loss_scale=16, train_wall=559, gb_free=3.3, wall=41297
2022-02-24 22:48:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-24 22:48:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.385 | nll_loss 5.663 | ppl 50.65 | wps 28229.8 | wpb 510.9 | bsz 1 | num_updates 7041 | best_loss 6.385
2022-02-24 22:48:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7041 updates
2022-02-24 22:48:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 22:49:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-24 22:49:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 9 @ 7041 updates, score 6.385) (writing took 14.756532838568091 seconds)
2022-02-24 22:49:08 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-24 22:49:08 | INFO | train | epoch 009 | loss 6.252 | nll_loss 5.534 | ppl 46.33 | wps 11137.7 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 7041 | lr 0.000376862 | gnorm 0.528 | loss_scale 32 | train_wall 4406 | gb_free 3.3 | wall 41555
2022-02-24 22:49:09 | INFO | fairseq.trainer | begin training epoch 10
2022-02-24 22:49:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-24 22:50:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 22:54:58 | INFO | train_inner | epoch 010:     60 / 788 loss=6.163, nll_loss=5.44, ppl=43.42, wps=10735.9, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.528, loss_scale=16, train_wall=562, gb_free=3.3, wall=41905
2022-02-24 23:03:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 23:04:45 | INFO | train_inner | epoch 010:    161 / 788 loss=6.13, nll_loss=5.405, ppl=42.37, wps=11154.2, ups=0.17, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.524, loss_scale=16, train_wall=565, gb_free=3.3, wall=42492
2022-02-24 23:14:27 | INFO | train_inner | epoch 010:    261 / 788 loss=6.136, nll_loss=5.412, ppl=42.58, wps=11264.4, ups=0.17, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.557, loss_scale=16, train_wall=559, gb_free=3.3, wall=43074
2022-02-24 23:18:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 23:24:14 | INFO | train_inner | epoch 010:    362 / 788 loss=6.166, nll_loss=5.444, ppl=43.52, wps=11157.6, ups=0.17, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.522, loss_scale=16, train_wall=565, gb_free=3.3, wall=43661
2022-02-24 23:32:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 23:34:02 | INFO | train_inner | epoch 010:    463 / 788 loss=6.165, nll_loss=5.442, ppl=43.47, wps=11152.9, ups=0.17, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.534, loss_scale=16, train_wall=565, gb_free=3.3, wall=44249
2022-02-24 23:43:44 | INFO | train_inner | epoch 010:    563 / 788 loss=6.166, nll_loss=5.444, ppl=43.53, wps=11265.8, ups=0.17, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.526, loss_scale=16, train_wall=559, gb_free=3.3, wall=44831
2022-02-24 23:45:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-24 23:53:31 | INFO | train_inner | epoch 010:    664 / 788 loss=6.179, nll_loss=5.457, ppl=43.93, wps=11154.3, ups=0.17, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.528, loss_scale=16, train_wall=565, gb_free=3.3, wall=45418
2022-02-24 23:58:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 00:03:19 | INFO | train_inner | epoch 010:    765 / 788 loss=6.165, nll_loss=5.442, ppl=43.48, wps=11153.2, ups=0.17, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.517, loss_scale=16, train_wall=565, gb_free=3.3, wall=46006
2022-02-25 00:05:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 00:05:38 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.338 | nll_loss 5.619 | ppl 49.14 | wps 28236 | wpb 510.9 | bsz 1 | num_updates 7823 | best_loss 6.338
2022-02-25 00:05:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7823 updates
2022-02-25 00:05:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 00:05:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 00:05:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 10 @ 7823 updates, score 6.338) (writing took 12.69607121590525 seconds)
2022-02-25 00:05:50 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-25 00:05:50 | INFO | train | epoch 010 | loss 6.154 | nll_loss 5.431 | ppl 43.13 | wps 11129.8 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 7823 | lr 0.000357531 | gnorm 0.531 | loss_scale 16 | train_wall 4405 | gb_free 3.3 | wall 46157
2022-02-25 00:05:50 | INFO | fairseq.trainer | begin training epoch 11
2022-02-25 00:05:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 00:13:19 | INFO | train_inner | epoch 011:     77 / 788 loss=6.061, nll_loss=5.333, ppl=40.32, wps=10877.7, ups=0.17, wpb=65232.6, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.517, loss_scale=32, train_wall=557, gb_free=3.3, wall=46606
2022-02-25 00:14:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 00:23:06 | INFO | train_inner | epoch 011:    178 / 788 loss=6.039, nll_loss=5.309, ppl=39.65, wps=11151.6, ups=0.17, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.529, loss_scale=16, train_wall=565, gb_free=3.3, wall=47193
2022-02-25 00:27:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 00:32:54 | INFO | train_inner | epoch 011:    279 / 788 loss=6.063, nll_loss=5.335, ppl=40.36, wps=11150.7, ups=0.17, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.526, loss_scale=16, train_wall=565, gb_free=3.3, wall=47781
2022-02-25 00:40:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 00:42:42 | INFO | train_inner | epoch 011:    380 / 788 loss=6.067, nll_loss=5.339, ppl=40.48, wps=11150.8, ups=0.17, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.512, loss_scale=16, train_wall=565, gb_free=3.3, wall=48369
2022-02-25 00:52:23 | INFO | train_inner | epoch 011:    480 / 788 loss=6.078, nll_loss=5.351, ppl=40.81, wps=11265.3, ups=0.17, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.531, loss_scale=16, train_wall=559, gb_free=3.3, wall=48950
2022-02-25 00:53:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 01:02:11 | INFO | train_inner | epoch 011:    581 / 788 loss=6.101, nll_loss=5.376, ppl=41.52, wps=11156.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.535, loss_scale=16, train_wall=565, gb_free=3.3, wall=49538
2022-02-25 01:06:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 01:11:58 | INFO | train_inner | epoch 011:    682 / 788 loss=6.102, nll_loss=5.377, ppl=41.55, wps=11156.1, ups=0.17, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.527, loss_scale=16, train_wall=565, gb_free=3.3, wall=50125
2022-02-25 01:19:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 01:21:46 | INFO | train_inner | epoch 011:    783 / 788 loss=6.1, nll_loss=5.374, ppl=41.47, wps=11149.4, ups=0.17, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.526, loss_scale=16, train_wall=565, gb_free=3.3, wall=50713
2022-02-25 01:22:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 01:22:20 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.302 | nll_loss 5.567 | ppl 47.4 | wps 28128.3 | wpb 510.9 | bsz 1 | num_updates 8605 | best_loss 6.302
2022-02-25 01:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8605 updates
2022-02-25 01:22:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 01:22:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 01:22:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 11 @ 8605 updates, score 6.302) (writing took 12.985677083954215 seconds)
2022-02-25 01:22:33 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-25 01:22:33 | INFO | train | epoch 011 | loss 6.074 | nll_loss 5.346 | ppl 40.68 | wps 11127.4 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 8605 | lr 0.000340898 | gnorm 0.525 | loss_scale 16 | train_wall 4406 | gb_free 3.3 | wall 50760
2022-02-25 01:22:33 | INFO | fairseq.trainer | begin training epoch 12
2022-02-25 01:22:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 01:31:46 | INFO | train_inner | epoch 012:     95 / 788 loss=5.961, nll_loss=5.228, ppl=37.49, wps=10867.3, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.514, loss_scale=16, train_wall=557, gb_free=3.3, wall=51313
2022-02-25 01:32:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 01:41:34 | INFO | train_inner | epoch 012:    196 / 788 loss=5.974, nll_loss=5.241, ppl=37.83, wps=11149.9, ups=0.17, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.56, loss_scale=16, train_wall=565, gb_free=3.3, wall=51901
2022-02-25 01:45:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 01:51:22 | INFO | train_inner | epoch 012:    297 / 788 loss=5.995, nll_loss=5.264, ppl=38.42, wps=11155, ups=0.17, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.523, loss_scale=16, train_wall=565, gb_free=3.3, wall=52489
2022-02-25 01:57:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 02:01:09 | INFO | train_inner | epoch 012:    398 / 788 loss=6.005, nll_loss=5.274, ppl=38.69, wps=11150.1, ups=0.17, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.538, loss_scale=16, train_wall=565, gb_free=3.3, wall=53076
2022-02-25 02:10:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 02:10:57 | INFO | train_inner | epoch 012:    499 / 788 loss=6.014, nll_loss=5.283, ppl=38.95, wps=11151, ups=0.17, wpb=65534.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.534, loss_scale=16, train_wall=565, gb_free=3.3, wall=53664
2022-02-25 02:20:39 | INFO | train_inner | epoch 012:    599 / 788 loss=6.023, nll_loss=5.293, ppl=39.19, wps=11266.4, ups=0.17, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.523, loss_scale=16, train_wall=559, gb_free=3.3, wall=54246
2022-02-25 02:23:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 02:23:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 02:30:32 | INFO | train_inner | epoch 012:    701 / 788 loss=6.036, nll_loss=5.307, ppl=39.58, wps=11042.5, ups=0.17, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.537, loss_scale=8, train_wall=571, gb_free=3.3, wall=54839
2022-02-25 02:38:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 02:39:03 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.276 | nll_loss 5.537 | ppl 46.42 | wps 28180.2 | wpb 510.9 | bsz 1 | num_updates 9387 | best_loss 6.276
2022-02-25 02:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9387 updates
2022-02-25 02:39:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 02:39:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 02:39:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 12 @ 9387 updates, score 6.276) (writing took 12.91491534654051 seconds)
2022-02-25 02:39:16 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-25 02:39:16 | INFO | train | epoch 012 | loss 6.006 | nll_loss 5.275 | ppl 38.73 | wps 11127.2 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 9387 | lr 0.00032639 | gnorm 0.533 | loss_scale 16 | train_wall 4406 | gb_free 3.3 | wall 55363
2022-02-25 02:39:16 | INFO | fairseq.trainer | begin training epoch 13
2022-02-25 02:39:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 02:40:32 | INFO | train_inner | epoch 013:     13 / 788 loss=6.027, nll_loss=5.298, ppl=39.34, wps=10876.3, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=9400, lr=0.000326164, gnorm=0.532, loss_scale=16, train_wall=557, gb_free=3.3, wall=55439
2022-02-25 02:50:14 | INFO | train_inner | epoch 013:    113 / 788 loss=5.887, nll_loss=5.15, ppl=35.5, wps=11261.2, ups=0.17, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.522, loss_scale=32, train_wall=559, gb_free=3.3, wall=56021
2022-02-25 02:50:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 03:00:02 | INFO | train_inner | epoch 013:    214 / 788 loss=5.931, nll_loss=5.195, ppl=36.64, wps=11153.1, ups=0.17, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.537, loss_scale=16, train_wall=565, gb_free=3.3, wall=56609
2022-02-25 03:03:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 03:09:49 | INFO | train_inner | epoch 013:    315 / 788 loss=5.931, nll_loss=5.196, ppl=36.65, wps=11153.2, ups=0.17, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.539, loss_scale=16, train_wall=565, gb_free=3.3, wall=57196
2022-02-25 03:15:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 03:19:37 | INFO | train_inner | epoch 013:    416 / 788 loss=5.961, nll_loss=5.228, ppl=37.47, wps=11154.7, ups=0.17, wpb=65534.7, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.533, loss_scale=16, train_wall=565, gb_free=3.3, wall=57784
2022-02-25 03:24:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 03:29:24 | INFO | train_inner | epoch 013:    517 / 788 loss=5.964, nll_loss=5.231, ppl=37.56, wps=11159.5, ups=0.17, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.538, loss_scale=8, train_wall=565, gb_free=3.3, wall=58371
2022-02-25 03:37:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 03:39:11 | INFO | train_inner | epoch 013:    618 / 788 loss=5.975, nll_loss=5.242, ppl=37.84, wps=11157.1, ups=0.17, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.539, loss_scale=8, train_wall=565, gb_free=3.3, wall=58958
2022-02-25 03:48:53 | INFO | train_inner | epoch 013:    718 / 788 loss=5.978, nll_loss=5.246, ppl=37.95, wps=11267.5, ups=0.17, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.522, loss_scale=8, train_wall=559, gb_free=3.3, wall=59540
2022-02-25 03:55:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 03:55:45 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.261 | nll_loss 5.539 | ppl 46.49 | wps 28203.3 | wpb 510.9 | bsz 1 | num_updates 10170 | best_loss 6.261
2022-02-25 03:55:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10170 updates
2022-02-25 03:55:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 03:55:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 03:55:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 13 @ 10170 updates, score 6.261) (writing took 12.869535699486732 seconds)
2022-02-25 03:55:58 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-25 03:55:58 | INFO | train | epoch 013 | loss 5.949 | nll_loss 5.215 | ppl 37.14 | wps 11144.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 10170 | lr 0.000313574 | gnorm 0.531 | loss_scale 16 | train_wall 4405 | gb_free 3.3 | wall 59965
2022-02-25 03:55:58 | INFO | fairseq.trainer | begin training epoch 14
2022-02-25 03:55:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 03:58:53 | INFO | train_inner | epoch 014:     30 / 788 loss=5.941, nll_loss=5.207, ppl=36.95, wps=10877.5, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.55, loss_scale=16, train_wall=557, gb_free=3.3, wall=60140
2022-02-25 04:04:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 04:08:41 | INFO | train_inner | epoch 014:    131 / 788 loss=5.848, nll_loss=5.108, ppl=34.5, wps=11150.3, ups=0.17, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.51, loss_scale=16, train_wall=565, gb_free=3.3, wall=60728
2022-02-25 04:11:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 04:18:28 | INFO | train_inner | epoch 014:    232 / 788 loss=5.876, nll_loss=5.137, ppl=35.2, wps=11153.7, ups=0.17, wpb=65534.7, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.562, loss_scale=8, train_wall=565, gb_free=3.3, wall=61315
2022-02-25 04:28:10 | INFO | train_inner | epoch 014:    332 / 788 loss=5.88, nll_loss=5.142, ppl=35.31, wps=11267.3, ups=0.17, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.549, loss_scale=16, train_wall=559, gb_free=3.3, wall=61897
2022-02-25 04:36:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 04:37:57 | INFO | train_inner | epoch 014:    433 / 788 loss=5.901, nll_loss=5.164, ppl=35.84, wps=11152.2, ups=0.17, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.508, loss_scale=16, train_wall=565, gb_free=3.3, wall=62484
2022-02-25 04:39:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 04:47:45 | INFO | train_inner | epoch 014:    534 / 788 loss=5.923, nll_loss=5.188, ppl=36.45, wps=11159.6, ups=0.17, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.536, loss_scale=8, train_wall=565, gb_free=3.3, wall=63072
2022-02-25 04:57:26 | INFO | train_inner | epoch 014:    634 / 788 loss=5.927, nll_loss=5.192, ppl=36.55, wps=11267.2, ups=0.17, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.538, loss_scale=16, train_wall=559, gb_free=3.3, wall=63653
2022-02-25 05:04:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 05:07:14 | INFO | train_inner | epoch 014:    735 / 788 loss=5.941, nll_loss=5.206, ppl=36.92, wps=11158, ups=0.17, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.535, loss_scale=16, train_wall=565, gb_free=3.3, wall=64241
2022-02-25 05:12:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 05:12:27 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.241 | nll_loss 5.513 | ppl 45.66 | wps 28196.7 | wpb 510.9 | bsz 1 | num_updates 10953 | best_loss 6.241
2022-02-25 05:12:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 10953 updates
2022-02-25 05:12:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 05:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 05:12:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 14 @ 10953 updates, score 6.241) (writing took 12.889142138883471 seconds)
2022-02-25 05:12:40 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-25 05:12:40 | INFO | train | epoch 014 | loss 5.899 | nll_loss 5.162 | ppl 35.81 | wps 11144.5 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 10953 | lr 0.000302158 | gnorm 0.537 | loss_scale 16 | train_wall 4405 | gb_free 3.3 | wall 64567
2022-02-25 05:12:40 | INFO | fairseq.trainer | begin training epoch 15
2022-02-25 05:12:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 05:17:14 | INFO | train_inner | epoch 015:     47 / 788 loss=5.872, nll_loss=5.133, ppl=35.1, wps=10873.1, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.55, loss_scale=16, train_wall=557, gb_free=3.3, wall=64841
2022-02-25 05:17:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 05:27:01 | INFO | train_inner | epoch 015:    148 / 788 loss=5.802, nll_loss=5.06, ppl=33.36, wps=11152.6, ups=0.17, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.551, loss_scale=16, train_wall=565, gb_free=3.3, wall=65428
2022-02-25 05:27:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 05:36:49 | INFO | train_inner | epoch 015:    249 / 788 loss=5.822, nll_loss=5.08, ppl=33.83, wps=11153, ups=0.17, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.566, loss_scale=8, train_wall=565, gb_free=3.3, wall=66016
2022-02-25 05:46:30 | INFO | train_inner | epoch 015:    349 / 788 loss=5.846, nll_loss=5.106, ppl=34.45, wps=11268.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.527, loss_scale=16, train_wall=559, gb_free=3.3, wall=66597
2022-02-25 05:50:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 05:56:18 | INFO | train_inner | epoch 015:    450 / 788 loss=5.869, nll_loss=5.13, ppl=35.02, wps=11157.7, ups=0.17, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.564, loss_scale=8, train_wall=565, gb_free=3.3, wall=67185
2022-02-25 06:03:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 06:06:05 | INFO | train_inner | epoch 015:    551 / 788 loss=5.876, nll_loss=5.138, ppl=35.2, wps=11158.7, ups=0.17, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.555, loss_scale=8, train_wall=565, gb_free=3.3, wall=67772
2022-02-25 06:15:47 | INFO | train_inner | epoch 015:    651 / 788 loss=5.885, nll_loss=5.147, ppl=35.44, wps=11267.7, ups=0.17, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.522, loss_scale=8, train_wall=559, gb_free=3.3, wall=68354
2022-02-25 06:25:28 | INFO | train_inner | epoch 015:    751 / 788 loss=5.896, nll_loss=5.159, ppl=35.74, wps=11267.9, ups=0.17, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.507, loss_scale=16, train_wall=559, gb_free=3.3, wall=68935
2022-02-25 06:26:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 06:29:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 06:29:09 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.224 | nll_loss 5.491 | ppl 44.96 | wps 28248.6 | wpb 510.9 | bsz 1 | num_updates 11736 | best_loss 6.224
2022-02-25 06:29:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11736 updates
2022-02-25 06:29:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 06:29:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 06:29:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 15 @ 11736 updates, score 6.224) (writing took 12.951341988518834 seconds)
2022-02-25 06:29:22 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-25 06:29:22 | INFO | train | epoch 015 | loss 5.855 | nll_loss 5.115 | ppl 34.67 | wps 11144.7 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 11736 | lr 0.000291904 | gnorm 0.544 | loss_scale 8 | train_wall 4404 | gb_free 3.3 | wall 69169
2022-02-25 06:29:22 | INFO | fairseq.trainer | begin training epoch 16
2022-02-25 06:29:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 06:35:34 | INFO | train_inner | epoch 016:     64 / 788 loss=5.796, nll_loss=5.054, ppl=33.22, wps=10772.2, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.55, loss_scale=8, train_wall=562, gb_free=3.3, wall=69541
2022-02-25 06:41:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 06:45:21 | INFO | train_inner | epoch 016:    165 / 788 loss=5.771, nll_loss=5.027, ppl=32.6, wps=11157.9, ups=0.17, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.538, loss_scale=8, train_wall=565, gb_free=3.3, wall=70128
2022-02-25 06:55:03 | INFO | train_inner | epoch 016:    265 / 788 loss=5.784, nll_loss=5.041, ppl=32.92, wps=11264.1, ups=0.17, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.525, loss_scale=16, train_wall=559, gb_free=3.3, wall=70710
2022-02-25 06:57:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 07:04:51 | INFO | train_inner | epoch 016:    366 / 788 loss=5.813, nll_loss=5.071, ppl=33.62, wps=11155.4, ups=0.17, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.54, loss_scale=8, train_wall=565, gb_free=3.3, wall=71298
2022-02-25 07:14:32 | INFO | train_inner | epoch 016:    466 / 788 loss=5.829, nll_loss=5.088, ppl=34.02, wps=11262.5, ups=0.17, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.537, loss_scale=16, train_wall=560, gb_free=3.3, wall=71879
2022-02-25 07:14:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 07:24:20 | INFO | train_inner | epoch 016:    567 / 788 loss=5.846, nll_loss=5.106, ppl=34.43, wps=11157.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.549, loss_scale=8, train_wall=565, gb_free=3.3, wall=72467
2022-02-25 07:34:01 | INFO | train_inner | epoch 016:    667 / 788 loss=5.856, nll_loss=5.116, ppl=34.68, wps=11268.2, ups=0.17, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.541, loss_scale=16, train_wall=559, gb_free=3.3, wall=73048
2022-02-25 07:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 07:43:49 | INFO | train_inner | epoch 016:    768 / 788 loss=5.855, nll_loss=5.116, ppl=34.67, wps=11160.1, ups=0.17, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.547, loss_scale=8, train_wall=565, gb_free=3.3, wall=73636
2022-02-25 07:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 07:45:50 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.22 | nll_loss 5.486 | ppl 44.82 | wps 28136.8 | wpb 510.9 | bsz 1 | num_updates 12520 | best_loss 6.22
2022-02-25 07:45:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12520 updates
2022-02-25 07:45:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 07:45:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 07:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 16 @ 12520 updates, score 6.22) (writing took 12.810807610861957 seconds)
2022-02-25 07:46:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-25 07:46:03 | INFO | train | epoch 016 | loss 5.816 | nll_loss 5.075 | ppl 33.7 | wps 11159.5 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 12520 | lr 0.000282617 | gnorm 0.54 | loss_scale 8 | train_wall 4405 | gb_free 3.3 | wall 73770
2022-02-25 07:46:03 | INFO | fairseq.trainer | begin training epoch 17
2022-02-25 07:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 07:53:49 | INFO | train_inner | epoch 017:     80 / 788 loss=5.747, nll_loss=5.001, ppl=32.03, wps=10872.5, ups=0.17, wpb=65232.6, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.555, loss_scale=16, train_wall=557, gb_free=3.3, wall=74236
2022-02-25 07:59:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 08:03:36 | INFO | train_inner | epoch 017:    181 / 788 loss=5.74, nll_loss=4.993, ppl=31.85, wps=11152.8, ups=0.17, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.555, loss_scale=8, train_wall=565, gb_free=3.3, wall=74823
2022-02-25 08:13:18 | INFO | train_inner | epoch 017:    281 / 788 loss=5.759, nll_loss=5.014, ppl=32.32, wps=11264.7, ups=0.17, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.533, loss_scale=16, train_wall=559, gb_free=3.3, wall=75405
2022-02-25 08:23:00 | INFO | train_inner | epoch 017:    381 / 788 loss=5.772, nll_loss=5.027, ppl=32.61, wps=11264.4, ups=0.17, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.545, loss_scale=16, train_wall=559, gb_free=3.3, wall=75987
2022-02-25 08:25:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 08:32:47 | INFO | train_inner | epoch 017:    482 / 788 loss=5.784, nll_loss=5.04, ppl=32.9, wps=11156.9, ups=0.17, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.531, loss_scale=16, train_wall=565, gb_free=3.3, wall=76574
2022-02-25 08:38:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 08:38:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 08:42:40 | INFO | train_inner | epoch 017:    584 / 788 loss=5.801, nll_loss=5.058, ppl=33.32, wps=11047.9, ups=0.17, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.529, loss_scale=8, train_wall=570, gb_free=3.3, wall=77167
2022-02-25 08:52:22 | INFO | train_inner | epoch 017:    684 / 788 loss=5.826, nll_loss=5.085, ppl=33.93, wps=11268.6, ups=0.17, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.551, loss_scale=16, train_wall=559, gb_free=3.3, wall=77749
2022-02-25 08:54:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 09:02:10 | INFO | train_inner | epoch 017:    785 / 788 loss=5.832, nll_loss=5.092, ppl=34.1, wps=11153.3, ups=0.17, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.57, loss_scale=8, train_wall=565, gb_free=3.3, wall=78337
2022-02-25 09:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 09:02:32 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.203 | nll_loss 5.466 | ppl 44.19 | wps 28158 | wpb 510.9 | bsz 1 | num_updates 13303 | best_loss 6.203
2022-02-25 09:02:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13303 updates
2022-02-25 09:02:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 09:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 09:02:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 17 @ 13303 updates, score 6.203) (writing took 12.707363724708557 seconds)
2022-02-25 09:02:45 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-25 09:02:45 | INFO | train | epoch 017 | loss 5.781 | nll_loss 5.037 | ppl 32.83 | wps 11144.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 13303 | lr 0.000274173 | gnorm 0.547 | loss_scale 8 | train_wall 4405 | gb_free 3.3 | wall 78372
2022-02-25 09:02:45 | INFO | fairseq.trainer | begin training epoch 18
2022-02-25 09:02:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 09:12:09 | INFO | train_inner | epoch 018:     97 / 788 loss=5.685, nll_loss=4.936, ppl=30.61, wps=10875.1, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.563, loss_scale=16, train_wall=557, gb_free=3.3, wall=78936
2022-02-25 09:20:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 09:21:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 09:22:03 | INFO | train_inner | epoch 018:    199 / 788 loss=5.697, nll_loss=4.948, ppl=30.87, wps=11042.9, ups=0.17, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.536, loss_scale=8, train_wall=571, gb_free=3.3, wall=79530
2022-02-25 09:31:45 | INFO | train_inner | epoch 018:    299 / 788 loss=5.731, nll_loss=4.985, ppl=31.66, wps=11262.6, ups=0.17, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.561, loss_scale=8, train_wall=560, gb_free=3.3, wall=80112
2022-02-25 09:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 09:41:32 | INFO | train_inner | epoch 018:    400 / 788 loss=5.745, nll_loss=4.999, ppl=31.99, wps=11155.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.542, loss_scale=8, train_wall=565, gb_free=3.3, wall=80699
2022-02-25 09:51:14 | INFO | train_inner | epoch 018:    500 / 788 loss=5.773, nll_loss=5.029, ppl=32.65, wps=11272.9, ups=0.17, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.546, loss_scale=8, train_wall=559, gb_free=3.3, wall=81281
2022-02-25 09:53:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 10:01:01 | INFO | train_inner | epoch 018:    601 / 788 loss=5.777, nll_loss=5.033, ppl=32.74, wps=11153, ups=0.17, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.554, loss_scale=8, train_wall=565, gb_free=3.3, wall=81868
2022-02-25 10:07:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 10:10:49 | INFO | train_inner | epoch 018:    702 / 788 loss=5.787, nll_loss=5.044, ppl=32.98, wps=11155.3, ups=0.17, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.552, loss_scale=8, train_wall=565, gb_free=3.3, wall=82456
2022-02-25 10:19:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 10:19:14 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.2 | nll_loss 5.467 | ppl 44.22 | wps 28265.4 | wpb 510.9 | bsz 1 | num_updates 14086 | best_loss 6.2
2022-02-25 10:19:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14086 updates
2022-02-25 10:19:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 10:19:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 10:19:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 18 @ 14086 updates, score 6.2) (writing took 12.703336148522794 seconds)
2022-02-25 10:19:27 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-25 10:19:27 | INFO | train | epoch 018 | loss 5.749 | nll_loss 5.003 | ppl 32.07 | wps 11143.7 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 14086 | lr 0.000266444 | gnorm 0.549 | loss_scale 8 | train_wall 4406 | gb_free 3.3 | wall 82974
2022-02-25 10:19:27 | INFO | fairseq.trainer | begin training epoch 19
2022-02-25 10:19:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 10:20:49 | INFO | train_inner | epoch 019:     14 / 788 loss=5.782, nll_loss=5.038, ppl=32.86, wps=10875.6, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=14100, lr=0.000266312, gnorm=0.535, loss_scale=16, train_wall=557, gb_free=3.3, wall=83056
2022-02-25 10:23:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 10:30:36 | INFO | train_inner | epoch 019:    115 / 788 loss=5.66, nll_loss=4.91, ppl=30.06, wps=11155.1, ups=0.17, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.567, loss_scale=8, train_wall=565, gb_free=3.3, wall=83643
2022-02-25 10:40:18 | INFO | train_inner | epoch 019:    215 / 788 loss=5.672, nll_loss=4.923, ppl=30.33, wps=11266.2, ups=0.17, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.554, loss_scale=16, train_wall=559, gb_free=3.3, wall=84225
2022-02-25 10:44:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 10:50:05 | INFO | train_inner | epoch 019:    316 / 788 loss=5.703, nll_loss=4.955, ppl=31.01, wps=11155.9, ups=0.17, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.576, loss_scale=8, train_wall=565, gb_free=3.3, wall=84812
2022-02-25 10:59:47 | INFO | train_inner | epoch 019:    416 / 788 loss=5.72, nll_loss=4.973, ppl=31.4, wps=11265.1, ups=0.17, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.54, loss_scale=16, train_wall=559, gb_free=3.3, wall=85394
2022-02-25 11:02:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 11:09:34 | INFO | train_inner | epoch 019:    517 / 788 loss=5.744, nll_loss=4.998, ppl=31.96, wps=11158.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.568, loss_scale=8, train_wall=565, gb_free=3.3, wall=85981
2022-02-25 11:16:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 11:19:24 | INFO | train_inner | epoch 019:    618 / 788 loss=5.759, nll_loss=5.014, ppl=32.32, wps=11113, ups=0.17, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.569, loss_scale=8, train_wall=567, gb_free=3.3, wall=86571
2022-02-25 11:29:13 | INFO | train_inner | epoch 019:    718 / 788 loss=5.759, nll_loss=5.015, ppl=32.33, wps=11131.4, ups=0.17, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.536, loss_scale=16, train_wall=566, gb_free=3.3, wall=87160
2022-02-25 11:36:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 11:36:10 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.194 | nll_loss 5.452 | ppl 43.78 | wps 27194.4 | wpb 510.9 | bsz 1 | num_updates 14870 | best_loss 6.194
2022-02-25 11:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14870 updates
2022-02-25 11:36:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 11:36:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 11:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 19 @ 14870 updates, score 6.194) (writing took 12.848533403128386 seconds)
2022-02-25 11:36:23 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-25 11:36:23 | INFO | train | epoch 019 | loss 5.721 | nll_loss 4.974 | ppl 31.42 | wps 11123.7 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 14870 | lr 0.000259325 | gnorm 0.556 | loss_scale 16 | train_wall 4417 | gb_free 3.3 | wall 87590
2022-02-25 11:36:23 | INFO | fairseq.trainer | begin training epoch 20
2022-02-25 11:36:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 11:39:20 | INFO | train_inner | epoch 020:     30 / 788 loss=5.723, nll_loss=4.976, ppl=31.46, wps=10742.5, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=14900, lr=0.000259064, gnorm=0.54, loss_scale=16, train_wall=563, gb_free=3.3, wall=87767
2022-02-25 11:39:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 11:49:15 | INFO | train_inner | epoch 020:    131 / 788 loss=5.629, nll_loss=4.876, ppl=29.37, wps=11020.4, ups=0.17, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.564, loss_scale=8, train_wall=571, gb_free=3.3, wall=88362
2022-02-25 11:54:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 11:59:10 | INFO | train_inner | epoch 020:    232 / 788 loss=5.652, nll_loss=4.901, ppl=29.87, wps=11018.5, ups=0.17, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.539, loss_scale=8, train_wall=572, gb_free=3.3, wall=88957
2022-02-25 12:08:58 | INFO | train_inner | epoch 020:    332 / 788 loss=5.69, nll_loss=4.941, ppl=30.71, wps=11129.7, ups=0.17, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.569, loss_scale=16, train_wall=566, gb_free=3.3, wall=89545
2022-02-25 12:18:47 | INFO | train_inner | epoch 020:    432 / 788 loss=5.696, nll_loss=4.948, ppl=30.86, wps=11133.7, ups=0.17, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.562, loss_scale=16, train_wall=566, gb_free=3.3, wall=90134
2022-02-25 12:19:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 12:21:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 12:28:47 | INFO | train_inner | epoch 020:    534 / 788 loss=5.715, nll_loss=4.967, ppl=31.28, wps=10914.2, ups=0.17, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.565, loss_scale=8, train_wall=577, gb_free=3.3, wall=90734
2022-02-25 12:38:36 | INFO | train_inner | epoch 020:    634 / 788 loss=5.729, nll_loss=4.982, ppl=31.6, wps=11129.4, ups=0.17, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.552, loss_scale=16, train_wall=566, gb_free=3.3, wall=91323
2022-02-25 12:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 12:48:31 | INFO | train_inner | epoch 020:    735 / 788 loss=5.747, nll_loss=5.001, ppl=32.02, wps=11016.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.549, loss_scale=8, train_wall=572, gb_free=3.3, wall=91918
2022-02-25 12:53:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 12:53:49 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.187 | nll_loss 5.444 | ppl 43.52 | wps 27227 | wpb 510.9 | bsz 1 | num_updates 15653 | best_loss 6.187
2022-02-25 12:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15653 updates
2022-02-25 12:53:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 12:53:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 12:54:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 20 @ 15653 updates, score 6.187) (writing took 12.697374857030809 seconds)
2022-02-25 12:54:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-25 12:54:01 | INFO | train | epoch 020 | loss 5.694 | nll_loss 4.945 | ppl 30.81 | wps 11009.9 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 15653 | lr 0.000252756 | gnorm 0.559 | loss_scale 8 | train_wall 4457 | gb_free 3.3 | wall 92248
2022-02-25 12:54:01 | INFO | fairseq.trainer | begin training epoch 21
2022-02-25 12:54:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 12:58:38 | INFO | train_inner | epoch 021:     47 / 788 loss=5.676, nll_loss=4.926, ppl=30.4, wps=10746, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.577, loss_scale=16, train_wall=563, gb_free=3.3, wall=92525
2022-02-25 13:04:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 13:08:33 | INFO | train_inner | epoch 021:    148 / 788 loss=5.612, nll_loss=4.859, ppl=29.01, wps=11016.9, ups=0.17, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.556, loss_scale=8, train_wall=572, gb_free=3.3, wall=93120
2022-02-25 13:18:22 | INFO | train_inner | epoch 021:    248 / 788 loss=5.637, nll_loss=4.884, ppl=29.54, wps=11126.7, ups=0.17, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.562, loss_scale=16, train_wall=566, gb_free=3.3, wall=93709
2022-02-25 13:25:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 13:28:17 | INFO | train_inner | epoch 021:    349 / 788 loss=5.656, nll_loss=4.905, ppl=29.96, wps=11016.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.559, loss_scale=8, train_wall=572, gb_free=3.3, wall=94304
2022-02-25 13:34:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-25 13:38:11 | INFO | train_inner | epoch 021:    450 / 788 loss=5.677, nll_loss=4.927, ppl=30.42, wps=11025.3, ups=0.17, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.583, loss_scale=4, train_wall=571, gb_free=3.3, wall=94898
2022-02-25 13:48:00 | INFO | train_inner | epoch 021:    550 / 788 loss=5.694, nll_loss=4.945, ppl=30.81, wps=11138.2, ups=0.17, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.564, loss_scale=8, train_wall=566, gb_free=3.3, wall=95487
2022-02-25 13:57:49 | INFO | train_inner | epoch 021:    650 / 788 loss=5.71, nll_loss=4.962, ppl=31.16, wps=11132.4, ups=0.17, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.552, loss_scale=8, train_wall=566, gb_free=3.3, wall=96076
2022-02-25 14:07:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 14:07:43 | INFO | train_inner | epoch 021:    751 / 788 loss=5.724, nll_loss=4.977, ppl=31.49, wps=11019.7, ups=0.17, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.551, loss_scale=8, train_wall=572, gb_free=3.3, wall=96670
2022-02-25 14:11:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 14:11:26 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.186 | nll_loss 5.456 | ppl 43.91 | wps 27256.3 | wpb 510.9 | bsz 1 | num_updates 16437 | best_loss 6.186
2022-02-25 14:11:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16437 updates
2022-02-25 14:11:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 14:11:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 14:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 21 @ 16437 updates, score 6.186) (writing took 12.90197807084769 seconds)
2022-02-25 14:11:39 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-25 14:11:39 | INFO | train | epoch 021 | loss 5.67 | nll_loss 4.92 | ppl 30.27 | wps 11024.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 16437 | lr 0.000246654 | gnorm 0.562 | loss_scale 8 | train_wall 4456 | gb_free 3.3 | wall 96906
2022-02-25 14:11:39 | INFO | fairseq.trainer | begin training epoch 22
2022-02-25 14:11:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 14:17:50 | INFO | train_inner | epoch 022:     63 / 788 loss=5.628, nll_loss=4.876, ppl=29.36, wps=10747.1, ups=0.16, wpb=65233.9, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.564, loss_scale=8, train_wall=563, gb_free=3.3, wall=97277
2022-02-25 14:23:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 14:27:45 | INFO | train_inner | epoch 022:    164 / 788 loss=5.599, nll_loss=4.845, ppl=28.74, wps=11022, ups=0.17, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.587, loss_scale=8, train_wall=571, gb_free=3.3, wall=97872
2022-02-25 14:36:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 14:37:39 | INFO | train_inner | epoch 022:    265 / 788 loss=5.627, nll_loss=4.874, ppl=29.33, wps=11024, ups=0.17, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.573, loss_scale=8, train_wall=571, gb_free=3.3, wall=98466
2022-02-25 14:47:28 | INFO | train_inner | epoch 022:    365 / 788 loss=5.634, nll_loss=4.882, ppl=29.48, wps=11130.6, ups=0.17, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.575, loss_scale=8, train_wall=566, gb_free=3.3, wall=99055
2022-02-25 14:49:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 14:57:22 | INFO | train_inner | epoch 022:    466 / 788 loss=5.65, nll_loss=4.899, ppl=29.83, wps=11027.3, ups=0.17, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.58, loss_scale=8, train_wall=571, gb_free=3.3, wall=99649
2022-02-25 15:06:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 15:07:17 | INFO | train_inner | epoch 022:    567 / 788 loss=5.669, nll_loss=4.919, ppl=30.25, wps=11020.7, ups=0.17, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.572, loss_scale=8, train_wall=572, gb_free=3.3, wall=100244
2022-02-25 15:17:05 | INFO | train_inner | epoch 022:    667 / 788 loss=5.688, nll_loss=4.939, ppl=30.67, wps=11137.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.589, loss_scale=8, train_wall=566, gb_free=3.3, wall=100833
2022-02-25 15:20:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 15:26:53 | INFO | train_inner | epoch 022:    768 / 788 loss=5.702, nll_loss=4.954, ppl=30.99, wps=11155.1, ups=0.17, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.544, loss_scale=8, train_wall=565, gb_free=3.3, wall=101420
2022-02-25 15:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 15:28:54 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.182 | nll_loss 5.442 | ppl 43.48 | wps 28252.8 | wpb 510.9 | bsz 1 | num_updates 17220 | best_loss 6.182
2022-02-25 15:28:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17220 updates
2022-02-25 15:28:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 15:29:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 15:29:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 22 @ 17220 updates, score 6.182) (writing took 12.831614355556667 seconds)
2022-02-25 15:29:07 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-25 15:29:07 | INFO | train | epoch 022 | loss 5.648 | nll_loss 4.896 | ppl 29.78 | wps 11033.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 17220 | lr 0.000240981 | gnorm 0.572 | loss_scale 8 | train_wall 4448 | gb_free 3.3 | wall 101554
2022-02-25 15:29:07 | INFO | fairseq.trainer | begin training epoch 23
2022-02-25 15:29:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 15:36:52 | INFO | train_inner | epoch 023:     80 / 788 loss=5.588, nll_loss=4.833, ppl=28.5, wps=10882, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.561, loss_scale=16, train_wall=556, gb_free=3.3, wall=102020
2022-02-25 15:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 15:46:40 | INFO | train_inner | epoch 023:    181 / 788 loss=5.583, nll_loss=4.827, ppl=28.39, wps=11156.3, ups=0.17, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.589, loss_scale=8, train_wall=565, gb_free=3.3, wall=102607
2022-02-25 15:56:21 | INFO | train_inner | epoch 023:    281 / 788 loss=5.598, nll_loss=4.843, ppl=28.7, wps=11270.2, ups=0.17, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.566, loss_scale=16, train_wall=559, gb_free=3.3, wall=103188
2022-02-25 15:58:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 16:06:09 | INFO | train_inner | epoch 023:    382 / 788 loss=5.629, nll_loss=4.876, ppl=29.37, wps=11161.8, ups=0.17, wpb=65534.7, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.578, loss_scale=8, train_wall=565, gb_free=3.3, wall=103776
2022-02-25 16:14:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 16:15:56 | INFO | train_inner | epoch 023:    483 / 788 loss=5.633, nll_loss=4.88, ppl=29.45, wps=11161, ups=0.17, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.559, loss_scale=8, train_wall=565, gb_free=3.3, wall=104363
2022-02-25 16:25:37 | INFO | train_inner | epoch 023:    583 / 788 loss=5.654, nll_loss=4.903, ppl=29.92, wps=11271.9, ups=0.17, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.568, loss_scale=8, train_wall=559, gb_free=3.3, wall=104944
2022-02-25 16:29:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 16:35:24 | INFO | train_inner | epoch 023:    684 / 788 loss=5.676, nll_loss=4.925, ppl=30.39, wps=11159.5, ups=0.17, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.565, loss_scale=8, train_wall=565, gb_free=3.3, wall=105531
2022-02-25 16:45:06 | INFO | train_inner | epoch 023:    784 / 788 loss=5.677, nll_loss=4.927, ppl=30.42, wps=11267.3, ups=0.17, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.563, loss_scale=16, train_wall=559, gb_free=3.3, wall=106113
2022-02-25 16:45:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 16:45:34 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.182 | nll_loss 5.438 | ppl 43.35 | wps 28153.3 | wpb 510.9 | bsz 1 | num_updates 18004 | best_loss 6.182
2022-02-25 16:45:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18004 updates
2022-02-25 16:45:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 16:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 16:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 23 @ 18004 updates, score 6.182) (writing took 12.799594948999584 seconds)
2022-02-25 16:45:47 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-25 16:45:47 | INFO | train | epoch 023 | loss 5.628 | nll_loss 4.875 | ppl 29.35 | wps 11163 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 18004 | lr 0.000235676 | gnorm 0.569 | loss_scale 16 | train_wall 4403 | gb_free 3.3 | wall 106154
2022-02-25 16:45:47 | INFO | fairseq.trainer | begin training epoch 24
2022-02-25 16:45:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 16:46:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 16:55:12 | INFO | train_inner | epoch 024:     97 / 788 loss=5.535, nll_loss=4.777, ppl=27.42, wps=10771.8, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.574, loss_scale=8, train_wall=562, gb_free=3.3, wall=106719
2022-02-25 17:00:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 17:04:59 | INFO | train_inner | epoch 024:    198 / 788 loss=5.569, nll_loss=4.813, ppl=28.1, wps=11159.5, ups=0.17, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.613, loss_scale=8, train_wall=565, gb_free=3.3, wall=107306
2022-02-25 17:13:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 17:14:46 | INFO | train_inner | epoch 024:    299 / 788 loss=5.588, nll_loss=4.833, ppl=28.49, wps=11158.7, ups=0.17, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.58, loss_scale=8, train_wall=565, gb_free=3.3, wall=107893
2022-02-25 17:24:28 | INFO | train_inner | epoch 024:    399 / 788 loss=5.613, nll_loss=4.86, ppl=29.03, wps=11265.7, ups=0.17, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.567, loss_scale=8, train_wall=559, gb_free=3.3, wall=108475
2022-02-25 17:26:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 17:34:15 | INFO | train_inner | epoch 024:    500 / 788 loss=5.619, nll_loss=4.866, ppl=29.15, wps=11158, ups=0.17, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.574, loss_scale=8, train_wall=565, gb_free=3.3, wall=109062
2022-02-25 17:39:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 17:44:03 | INFO | train_inner | epoch 024:    601 / 788 loss=5.637, nll_loss=4.885, ppl=29.54, wps=11155.6, ups=0.17, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.581, loss_scale=8, train_wall=565, gb_free=3.3, wall=109650
2022-02-25 17:52:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 17:53:50 | INFO | train_inner | epoch 024:    702 / 788 loss=5.654, nll_loss=4.902, ppl=29.9, wps=11158.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.586, loss_scale=8, train_wall=565, gb_free=3.3, wall=110237
2022-02-25 18:02:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 18:02:15 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.184 | nll_loss 5.449 | ppl 43.7 | wps 28161.6 | wpb 510.9 | bsz 1 | num_updates 18786 | best_loss 6.182
2022-02-25 18:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18786 updates
2022-02-25 18:02:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-25 18:02:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-25 18:02:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 24 @ 18786 updates, score 6.184) (writing took 5.958702540025115 seconds)
2022-02-25 18:02:21 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-25 18:02:21 | INFO | train | epoch 024 | loss 5.609 | nll_loss 4.854 | ppl 28.93 | wps 11149.1 | ups 0.17 | wpb 65497.2 | bsz 127.9 | num_updates 18786 | lr 0.000230719 | gnorm 0.581 | loss_scale 8 | train_wall 4404 | gb_free 3.3 | wall 110748
2022-02-25 18:02:21 | INFO | fairseq.trainer | begin training epoch 25
2022-02-25 18:02:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 18:03:43 | INFO | train_inner | epoch 025:     14 / 788 loss=5.642, nll_loss=4.889, ppl=29.64, wps=11004.1, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=18800, lr=0.000230633, gnorm=0.568, loss_scale=8, train_wall=557, gb_free=3.3, wall=110830
2022-02-25 18:07:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 18:13:31 | INFO | train_inner | epoch 025:    115 / 788 loss=5.531, nll_loss=4.773, ppl=27.33, wps=11148.8, ups=0.17, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.572, loss_scale=8, train_wall=565, gb_free=3.3, wall=111418
2022-02-25 18:19:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 18:23:18 | INFO | train_inner | epoch 025:    216 / 788 loss=5.554, nll_loss=4.796, ppl=27.79, wps=11150.5, ups=0.17, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.596, loss_scale=8, train_wall=565, gb_free=3.3, wall=112005
2022-02-25 18:33:05 | INFO | train_inner | epoch 025:    316 / 788 loss=5.572, nll_loss=4.816, ppl=28.16, wps=11175.5, ups=0.17, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.577, loss_scale=16, train_wall=564, gb_free=3.3, wall=112592
2022-02-25 18:37:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 18:42:53 | INFO | train_inner | epoch 025:    417 / 788 loss=5.585, nll_loss=4.829, ppl=28.43, wps=11152.4, ups=0.17, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.584, loss_scale=8, train_wall=565, gb_free=3.3, wall=113180
2022-02-25 18:52:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 18:52:40 | INFO | train_inner | epoch 025:    518 / 788 loss=5.616, nll_loss=4.862, ppl=29.09, wps=11150.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.583, loss_scale=8, train_wall=565, gb_free=3.3, wall=113767
2022-02-25 19:02:22 | INFO | train_inner | epoch 025:    618 / 788 loss=5.619, nll_loss=4.865, ppl=29.14, wps=11260.6, ups=0.17, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.578, loss_scale=8, train_wall=560, gb_free=3.3, wall=114349
2022-02-25 19:05:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 19:12:10 | INFO | train_inner | epoch 025:    719 / 788 loss=5.636, nll_loss=4.883, ppl=29.51, wps=11147.9, ups=0.17, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.584, loss_scale=8, train_wall=565, gb_free=3.3, wall=114937
2022-02-25 19:18:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 19:18:57 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.176 | nll_loss 5.442 | ppl 43.48 | wps 28202.1 | wpb 510.9 | bsz 1 | num_updates 19569 | best_loss 6.176
2022-02-25 19:18:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19569 updates
2022-02-25 19:18:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 19:19:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 19:19:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 25 @ 19569 updates, score 6.176) (writing took 12.727847430855036 seconds)
2022-02-25 19:19:09 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-25 19:19:09 | INFO | train | epoch 025 | loss 5.591 | nll_loss 4.836 | ppl 28.57 | wps 11129.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 19569 | lr 0.000226056 | gnorm 0.58 | loss_scale 16 | train_wall 4410 | gb_free 3.3 | wall 115356
2022-02-25 19:19:09 | INFO | fairseq.trainer | begin training epoch 26
2022-02-25 19:19:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 19:22:10 | INFO | train_inner | epoch 026:     31 / 788 loss=5.603, nll_loss=4.848, ppl=28.81, wps=10877.4, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=19600, lr=0.000225877, gnorm=0.556, loss_scale=16, train_wall=557, gb_free=3.3, wall=115537
2022-02-25 19:22:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 19:31:57 | INFO | train_inner | epoch 026:    132 / 788 loss=5.515, nll_loss=4.756, ppl=27.02, wps=11158.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.6, loss_scale=8, train_wall=565, gb_free=3.3, wall=116124
2022-02-25 19:35:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 19:41:44 | INFO | train_inner | epoch 026:    233 / 788 loss=5.537, nll_loss=4.779, ppl=27.45, wps=11158.6, ups=0.17, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.593, loss_scale=8, train_wall=565, gb_free=3.3, wall=116712
2022-02-25 19:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 19:51:32 | INFO | train_inner | epoch 026:    334 / 788 loss=5.554, nll_loss=4.796, ppl=27.78, wps=11155.2, ups=0.17, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.577, loss_scale=8, train_wall=565, gb_free=3.3, wall=117299
2022-02-25 20:01:14 | INFO | train_inner | epoch 026:    434 / 788 loss=5.578, nll_loss=4.822, ppl=28.28, wps=11269.6, ups=0.17, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.58, loss_scale=8, train_wall=559, gb_free=3.3, wall=117881
2022-02-25 20:10:55 | INFO | train_inner | epoch 026:    534 / 788 loss=5.595, nll_loss=4.84, ppl=28.63, wps=11269.7, ups=0.17, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.568, loss_scale=16, train_wall=559, gb_free=3.3, wall=118462
2022-02-25 20:11:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 20:20:42 | INFO | train_inner | epoch 026:    635 / 788 loss=5.608, nll_loss=4.854, ppl=28.91, wps=11158.4, ups=0.17, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.579, loss_scale=8, train_wall=565, gb_free=3.3, wall=119049
2022-02-25 20:27:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 20:30:30 | INFO | train_inner | epoch 026:    736 / 788 loss=5.626, nll_loss=4.873, ppl=29.3, wps=11156.8, ups=0.17, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.597, loss_scale=8, train_wall=565, gb_free=3.3, wall=119637
2022-02-25 20:35:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 20:35:37 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.175 | nll_loss 5.435 | ppl 43.26 | wps 28153 | wpb 510.9 | bsz 1 | num_updates 20352 | best_loss 6.175
2022-02-25 20:35:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20352 updates
2022-02-25 20:35:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 20:35:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 20:35:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 26 @ 20352 updates, score 6.175) (writing took 12.560794161632657 seconds)
2022-02-25 20:35:50 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-25 20:35:50 | INFO | train | epoch 026 | loss 5.575 | nll_loss 4.819 | ppl 28.22 | wps 11147.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 20352 | lr 0.000221665 | gnorm 0.584 | loss_scale 8 | train_wall 4404 | gb_free 3.3 | wall 119957
2022-02-25 20:35:50 | INFO | fairseq.trainer | begin training epoch 27
2022-02-25 20:35:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 20:40:29 | INFO | train_inner | epoch 027:     48 / 788 loss=5.568, nll_loss=4.811, ppl=28.08, wps=10881.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.584, loss_scale=16, train_wall=557, gb_free=3.3, wall=120236
2022-02-25 20:43:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 20:50:17 | INFO | train_inner | epoch 027:    149 / 788 loss=5.5, nll_loss=4.739, ppl=26.71, wps=11158.5, ups=0.17, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.593, loss_scale=8, train_wall=565, gb_free=3.3, wall=120824
2022-02-25 20:59:58 | INFO | train_inner | epoch 027:    249 / 788 loss=5.526, nll_loss=4.767, ppl=27.23, wps=11264.9, ups=0.17, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.569, loss_scale=16, train_wall=559, gb_free=3.3, wall=121405
2022-02-25 21:03:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 21:09:46 | INFO | train_inner | epoch 027:    350 / 788 loss=5.545, nll_loss=4.787, ppl=27.6, wps=11157.9, ups=0.17, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.585, loss_scale=8, train_wall=565, gb_free=3.3, wall=121993
2022-02-25 21:16:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 21:19:33 | INFO | train_inner | epoch 027:    451 / 788 loss=5.57, nll_loss=4.813, ppl=28.12, wps=11159, ups=0.17, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.581, loss_scale=8, train_wall=565, gb_free=3.3, wall=122580
2022-02-25 21:29:15 | INFO | train_inner | epoch 027:    551 / 788 loss=5.579, nll_loss=4.823, ppl=28.31, wps=11268.9, ups=0.17, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.593, loss_scale=16, train_wall=559, gb_free=3.3, wall=123162
2022-02-25 21:29:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 21:39:02 | INFO | train_inner | epoch 027:    652 / 788 loss=5.605, nll_loss=4.851, ppl=28.85, wps=11160.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.606, loss_scale=8, train_wall=565, gb_free=3.3, wall=123749
2022-02-25 21:43:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 21:48:49 | INFO | train_inner | epoch 027:    753 / 788 loss=5.608, nll_loss=4.854, ppl=28.92, wps=11159.5, ups=0.17, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.578, loss_scale=8, train_wall=565, gb_free=3.3, wall=124336
2022-02-25 21:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 21:52:18 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.171 | nll_loss 5.43 | ppl 43.11 | wps 28234.6 | wpb 510.9 | bsz 1 | num_updates 21135 | best_loss 6.171
2022-02-25 21:52:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21135 updates
2022-02-25 21:52:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 21:52:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-25 21:52:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 27 @ 21135 updates, score 6.171) (writing took 12.664404939860106 seconds)
2022-02-25 21:52:30 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-25 21:52:30 | INFO | train | epoch 027 | loss 5.559 | nll_loss 4.802 | ppl 27.9 | wps 11147.6 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 21135 | lr 0.00021752 | gnorm 0.585 | loss_scale 8 | train_wall 4404 | gb_free 3.3 | wall 124557
2022-02-25 21:52:30 | INFO | fairseq.trainer | begin training epoch 28
2022-02-25 21:52:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 21:58:49 | INFO | train_inner | epoch 028:     65 / 788 loss=5.52, nll_loss=4.761, ppl=27.12, wps=10880.2, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.572, loss_scale=16, train_wall=557, gb_free=3.3, wall=124936
2022-02-25 22:02:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 22:08:36 | INFO | train_inner | epoch 028:    166 / 788 loss=5.49, nll_loss=4.729, ppl=26.51, wps=11161.5, ups=0.17, wpb=65534.7, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.614, loss_scale=8, train_wall=565, gb_free=3.3, wall=125523
2022-02-25 22:18:17 | INFO | train_inner | epoch 028:    266 / 788 loss=5.517, nll_loss=4.757, ppl=27.04, wps=11266.9, ups=0.17, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.571, loss_scale=16, train_wall=559, gb_free=3.3, wall=126104
2022-02-25 22:20:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 22:28:05 | INFO | train_inner | epoch 028:    367 / 788 loss=5.527, nll_loss=4.767, ppl=27.23, wps=11157.7, ups=0.17, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.626, loss_scale=8, train_wall=565, gb_free=3.3, wall=126692
2022-02-25 22:34:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 22:37:52 | INFO | train_inner | epoch 028:    468 / 788 loss=5.546, nll_loss=4.788, ppl=27.63, wps=11155.8, ups=0.17, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.576, loss_scale=8, train_wall=565, gb_free=3.3, wall=127279
2022-02-25 22:47:34 | INFO | train_inner | epoch 028:    568 / 788 loss=5.577, nll_loss=4.821, ppl=28.27, wps=11266.2, ups=0.17, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.59, loss_scale=16, train_wall=559, gb_free=3.3, wall=127861
2022-02-25 22:53:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 22:57:21 | INFO | train_inner | epoch 028:    669 / 788 loss=5.591, nll_loss=4.835, ppl=28.55, wps=11155.6, ups=0.17, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.595, loss_scale=8, train_wall=565, gb_free=3.3, wall=128448
2022-02-25 23:07:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 23:07:09 | INFO | train_inner | epoch 028:    770 / 788 loss=5.604, nll_loss=4.849, ppl=28.82, wps=11155.8, ups=0.17, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.579, loss_scale=8, train_wall=565, gb_free=3.3, wall=129036
2022-02-25 23:08:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 23:08:59 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.176 | nll_loss 5.433 | ppl 43.19 | wps 28264.2 | wpb 510.9 | bsz 1 | num_updates 21918 | best_loss 6.171
2022-02-25 23:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 21918 updates
2022-02-25 23:08:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-25 23:09:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-25 23:09:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 28 @ 21918 updates, score 6.176) (writing took 5.802440273575485 seconds)
2022-02-25 23:09:04 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-25 23:09:04 | INFO | train | epoch 028 | loss 5.545 | nll_loss 4.787 | ppl 27.61 | wps 11163.2 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 21918 | lr 0.000213599 | gnorm 0.59 | loss_scale 8 | train_wall 4404 | gb_free 3.3 | wall 129151
2022-02-25 23:09:04 | INFO | fairseq.trainer | begin training epoch 29
2022-02-25 23:09:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 23:17:01 | INFO | train_inner | epoch 029:     82 / 788 loss=5.485, nll_loss=4.723, ppl=26.41, wps=11008.8, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.599, loss_scale=8, train_wall=557, gb_free=3.3, wall=129628
2022-02-25 23:26:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 23:26:49 | INFO | train_inner | epoch 029:    183 / 788 loss=5.478, nll_loss=4.716, ppl=26.28, wps=11156.6, ups=0.17, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.587, loss_scale=8, train_wall=565, gb_free=3.3, wall=130216
2022-02-25 23:36:30 | INFO | train_inner | epoch 029:    283 / 788 loss=5.495, nll_loss=4.734, ppl=26.61, wps=11273.2, ups=0.17, wpb=65534.7, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.595, loss_scale=8, train_wall=559, gb_free=3.3, wall=130797
2022-02-25 23:38:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 23:46:17 | INFO | train_inner | epoch 029:    384 / 788 loss=5.526, nll_loss=4.767, ppl=27.22, wps=11163.8, ups=0.17, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.593, loss_scale=8, train_wall=564, gb_free=3.3, wall=131384
2022-02-25 23:51:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 23:56:04 | INFO | train_inner | epoch 029:    485 / 788 loss=5.555, nll_loss=4.798, ppl=27.82, wps=11162, ups=0.17, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.582, loss_scale=8, train_wall=565, gb_free=3.3, wall=131971
2022-02-26 00:05:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 00:05:51 | INFO | train_inner | epoch 029:    586 / 788 loss=5.567, nll_loss=4.81, ppl=28.06, wps=11163.5, ups=0.17, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.583, loss_scale=8, train_wall=565, gb_free=3.3, wall=132558
2022-02-26 00:15:33 | INFO | train_inner | epoch 029:    686 / 788 loss=5.575, nll_loss=4.818, ppl=28.22, wps=11271.5, ups=0.17, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.589, loss_scale=8, train_wall=559, gb_free=3.3, wall=133140
2022-02-26 00:20:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 00:25:20 | INFO | train_inner | epoch 029:    787 / 788 loss=5.581, nll_loss=4.825, ppl=28.34, wps=11161.4, ups=0.17, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.587, loss_scale=8, train_wall=565, gb_free=3.3, wall=133727
2022-02-26 00:25:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 00:25:31 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.176 | nll_loss 5.445 | ppl 43.56 | wps 28266.9 | wpb 510.9 | bsz 1 | num_updates 22701 | best_loss 6.171
2022-02-26 00:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22701 updates
2022-02-26 00:25:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 00:25:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 00:25:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 29 @ 22701 updates, score 6.176) (writing took 8.44573512300849 seconds)
2022-02-26 00:25:39 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-26 00:25:39 | INFO | train | epoch 029 | loss 5.531 | nll_loss 4.772 | ppl 27.32 | wps 11160.9 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 22701 | lr 0.000209883 | gnorm 0.589 | loss_scale 8 | train_wall 4403 | gb_free 3.3 | wall 133746
2022-02-26 00:25:39 | INFO | fairseq.trainer | begin training epoch 30
2022-02-26 00:25:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 00:35:15 | INFO | train_inner | epoch 030:     99 / 788 loss=5.448, nll_loss=4.684, ppl=25.71, wps=10957.3, ups=0.17, wpb=65232.6, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.603, loss_scale=16, train_wall=557, gb_free=3.3, wall=134322
2022-02-26 00:44:57 | INFO | train_inner | epoch 030:    199 / 788 loss=5.471, nll_loss=4.709, ppl=26.15, wps=11265.4, ups=0.17, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.584, loss_scale=16, train_wall=559, gb_free=3.3, wall=134904
2022-02-26 00:45:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 00:47:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 00:54:50 | INFO | train_inner | epoch 030:    301 / 788 loss=5.491, nll_loss=4.729, ppl=26.52, wps=11053.7, ups=0.17, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.629, loss_scale=8, train_wall=570, gb_free=3.3, wall=135497
2022-02-26 01:04:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 01:04:37 | INFO | train_inner | epoch 030:    402 / 788 loss=5.522, nll_loss=4.763, ppl=27.14, wps=11165.2, ups=0.17, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.577, loss_scale=8, train_wall=564, gb_free=3.3, wall=136084
2022-02-26 01:14:18 | INFO | train_inner | epoch 030:    502 / 788 loss=5.532, nll_loss=4.773, ppl=27.35, wps=11280, ups=0.17, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.586, loss_scale=8, train_wall=559, gb_free=3.3, wall=136665
2022-02-26 01:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 01:24:05 | INFO | train_inner | epoch 030:    603 / 788 loss=5.553, nll_loss=4.795, ppl=27.77, wps=11165.4, ups=0.17, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.591, loss_scale=8, train_wall=564, gb_free=3.3, wall=137252
2022-02-26 01:33:46 | INFO | train_inner | epoch 030:    703 / 788 loss=5.557, nll_loss=4.8, ppl=27.85, wps=11274.9, ups=0.17, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.595, loss_scale=16, train_wall=559, gb_free=3.3, wall=137833
2022-02-26 01:34:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 01:41:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 01:42:05 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.173 | nll_loss 5.431 | ppl 43.14 | wps 28104.9 | wpb 510.9 | bsz 1 | num_updates 23484 | best_loss 6.171
2022-02-26 01:42:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23484 updates
2022-02-26 01:42:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 01:42:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 01:42:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 30 @ 23484 updates, score 6.173) (writing took 5.947924351319671 seconds)
2022-02-26 01:42:11 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-26 01:42:11 | INFO | train | epoch 030 | loss 5.518 | nll_loss 4.758 | ppl 27.06 | wps 11168.6 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 23484 | lr 0.000206355 | gnorm 0.596 | loss_scale 8 | train_wall 4402 | gb_free 3.3 | wall 138338
2022-02-26 01:42:11 | INFO | fairseq.trainer | begin training epoch 31
2022-02-26 01:42:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 01:43:44 | INFO | train_inner | epoch 031:     16 / 788 loss=5.556, nll_loss=4.799, ppl=27.83, wps=10904.5, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=23500, lr=0.000206284, gnorm=0.604, loss_scale=8, train_wall=562, gb_free=3.3, wall=138431
2022-02-26 01:47:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 01:53:31 | INFO | train_inner | epoch 031:    117 / 788 loss=5.44, nll_loss=4.676, ppl=25.56, wps=11164.1, ups=0.17, wpb=65536, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.611, loss_scale=8, train_wall=564, gb_free=3.3, wall=139018
2022-02-26 02:00:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:03:18 | INFO | train_inner | epoch 031:    218 / 788 loss=5.462, nll_loss=4.699, ppl=25.98, wps=11164.7, ups=0.17, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.603, loss_scale=8, train_wall=564, gb_free=3.3, wall=139605
2022-02-26 02:13:00 | INFO | train_inner | epoch 031:    318 / 788 loss=5.477, nll_loss=4.715, ppl=26.26, wps=11275, ups=0.17, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.595, loss_scale=16, train_wall=559, gb_free=3.3, wall=140187
2022-02-26 02:17:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:22:47 | INFO | train_inner | epoch 031:    419 / 788 loss=5.516, nll_loss=4.755, ppl=27.01, wps=11163.7, ups=0.17, wpb=65534.7, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.611, loss_scale=8, train_wall=565, gb_free=3.3, wall=140774
2022-02-26 02:29:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:32:34 | INFO | train_inner | epoch 031:    520 / 788 loss=5.52, nll_loss=4.761, ppl=27.11, wps=11162.1, ups=0.17, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.599, loss_scale=8, train_wall=565, gb_free=3.3, wall=141361
2022-02-26 02:42:15 | INFO | train_inner | epoch 031:    620 / 788 loss=5.537, nll_loss=4.778, ppl=27.43, wps=11272.5, ups=0.17, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.605, loss_scale=16, train_wall=559, gb_free=3.3, wall=141942
2022-02-26 02:44:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 02:52:02 | INFO | train_inner | epoch 031:    721 / 788 loss=5.555, nll_loss=4.797, ppl=27.81, wps=11165.6, ups=0.17, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.592, loss_scale=8, train_wall=564, gb_free=3.3, wall=142529
2022-02-26 02:58:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 02:58:37 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.179 | nll_loss 5.441 | ppl 43.45 | wps 28199.9 | wpb 510.9 | bsz 1 | num_updates 24267 | best_loss 6.171
2022-02-26 02:58:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24267 updates
2022-02-26 02:58:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 02:58:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 02:58:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 31 @ 24267 updates, score 6.179) (writing took 5.759819057770073 seconds)
2022-02-26 02:58:42 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-26 02:58:42 | INFO | train | epoch 031 | loss 5.506 | nll_loss 4.745 | ppl 26.82 | wps 11170.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 24267 | lr 0.000202998 | gnorm 0.602 | loss_scale 16 | train_wall 4402 | gb_free 3.3 | wall 142929
2022-02-26 02:58:42 | INFO | fairseq.trainer | begin training epoch 32
2022-02-26 02:58:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 03:01:54 | INFO | train_inner | epoch 032:     33 / 788 loss=5.518, nll_loss=4.759, ppl=27.07, wps=11015.8, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=24300, lr=0.00020286, gnorm=0.596, loss_scale=16, train_wall=556, gb_free=3.3, wall=143121
2022-02-26 03:05:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 03:11:42 | INFO | train_inner | epoch 032:    134 / 788 loss=5.439, nll_loss=4.675, ppl=25.54, wps=11159.3, ups=0.17, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.592, loss_scale=8, train_wall=565, gb_free=3.3, wall=143709
2022-02-26 03:21:23 | INFO | train_inner | epoch 032:    234 / 788 loss=5.45, nll_loss=4.686, ppl=25.75, wps=11269.9, ups=0.17, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.602, loss_scale=16, train_wall=559, gb_free=3.3, wall=144290
2022-02-26 03:21:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 03:31:10 | INFO | train_inner | epoch 032:    335 / 788 loss=5.477, nll_loss=4.714, ppl=26.24, wps=11159.5, ups=0.17, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.594, loss_scale=8, train_wall=565, gb_free=3.3, wall=144877
2022-02-26 03:35:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 03:40:57 | INFO | train_inner | epoch 032:    436 / 788 loss=5.494, nll_loss=4.732, ppl=26.58, wps=11162.7, ups=0.17, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.609, loss_scale=8, train_wall=565, gb_free=3.3, wall=145465
2022-02-26 03:48:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 03:50:44 | INFO | train_inner | epoch 032:    537 / 788 loss=5.514, nll_loss=4.754, ppl=26.98, wps=11164.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.601, loss_scale=8, train_wall=564, gb_free=3.3, wall=146052
2022-02-26 04:00:26 | INFO | train_inner | epoch 032:    637 / 788 loss=5.533, nll_loss=4.774, ppl=27.36, wps=11273.1, ups=0.17, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.615, loss_scale=8, train_wall=559, gb_free=3.3, wall=146633
2022-02-26 04:03:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:10:13 | INFO | train_inner | epoch 032:    738 / 788 loss=5.544, nll_loss=4.785, ppl=27.57, wps=11163, ups=0.17, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.589, loss_scale=8, train_wall=565, gb_free=3.3, wall=147220
2022-02-26 04:15:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 04:15:09 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.175 | nll_loss 5.432 | ppl 43.17 | wps 28336.2 | wpb 510.9 | bsz 1 | num_updates 25050 | best_loss 6.171
2022-02-26 04:15:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25050 updates
2022-02-26 04:15:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 04:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 04:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 32 @ 25050 updates, score 6.175) (writing took 5.7809069734066725 seconds)
2022-02-26 04:15:14 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-26 04:15:14 | INFO | train | epoch 032 | loss 5.493 | nll_loss 4.732 | ppl 26.57 | wps 11168.1 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 25050 | lr 0.0001998 | gnorm 0.602 | loss_scale 8 | train_wall 4403 | gb_free 3.3 | wall 147521
2022-02-26 04:15:15 | INFO | fairseq.trainer | begin training epoch 33
2022-02-26 04:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 04:20:05 | INFO | train_inner | epoch 033:     50 / 788 loss=5.479, nll_loss=4.717, ppl=26.29, wps=11012, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=25100, lr=0.000199601, gnorm=0.614, loss_scale=16, train_wall=556, gb_free=3.3, wall=147812
2022-02-26 04:26:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:29:52 | INFO | train_inner | epoch 033:    151 / 788 loss=5.432, nll_loss=4.667, ppl=25.4, wps=11160.8, ups=0.17, wpb=65534.7, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.609, loss_scale=8, train_wall=565, gb_free=3.3, wall=148400
2022-02-26 04:39:34 | INFO | train_inner | epoch 033:    251 / 788 loss=5.447, nll_loss=4.683, ppl=25.69, wps=11275.1, ups=0.17, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.61, loss_scale=16, train_wall=559, gb_free=3.3, wall=148981
2022-02-26 04:41:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:49:21 | INFO | train_inner | epoch 033:    352 / 788 loss=5.472, nll_loss=4.709, ppl=26.16, wps=11163.5, ups=0.17, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.603, loss_scale=8, train_wall=564, gb_free=3.3, wall=149568
2022-02-26 04:54:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:59:08 | INFO | train_inner | epoch 033:    453 / 788 loss=5.492, nll_loss=4.73, ppl=26.54, wps=11166.5, ups=0.17, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.615, loss_scale=8, train_wall=564, gb_free=3.3, wall=150155
2022-02-26 05:08:49 | INFO | train_inner | epoch 033:    553 / 788 loss=5.504, nll_loss=4.743, ppl=26.77, wps=11279.2, ups=0.17, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.605, loss_scale=16, train_wall=559, gb_free=3.3, wall=150736
2022-02-26 05:09:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:18:36 | INFO | train_inner | epoch 033:    654 / 788 loss=5.522, nll_loss=4.762, ppl=27.13, wps=11163, ups=0.17, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.628, loss_scale=8, train_wall=565, gb_free=3.3, wall=151323
2022-02-26 05:24:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:28:23 | INFO | train_inner | epoch 033:    755 / 788 loss=5.529, nll_loss=4.77, ppl=27.28, wps=11165.5, ups=0.17, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.61, loss_scale=8, train_wall=564, gb_free=3.3, wall=151910
2022-02-26 05:31:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 05:31:40 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.178 | nll_loss 5.433 | ppl 43.21 | wps 28256.2 | wpb 510.9 | bsz 1 | num_updates 25833 | best_loss 6.171
2022-02-26 05:31:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25833 updates
2022-02-26 05:31:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 05:31:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 05:31:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 33 @ 25833 updates, score 6.178) (writing took 5.878078922629356 seconds)
2022-02-26 05:31:46 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-26 05:31:46 | INFO | train | epoch 033 | loss 5.482 | nll_loss 4.72 | ppl 26.36 | wps 11170.3 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 25833 | lr 0.000196749 | gnorm 0.614 | loss_scale 8 | train_wall 4402 | gb_free 3.3 | wall 152113
2022-02-26 05:31:46 | INFO | fairseq.trainer | begin training epoch 34
2022-02-26 05:31:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 05:38:15 | INFO | train_inner | epoch 034:     67 / 788 loss=5.436, nll_loss=4.671, ppl=25.48, wps=11005.6, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.634, loss_scale=16, train_wall=557, gb_free=3.3, wall=152503
2022-02-26 05:42:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:48:03 | INFO | train_inner | epoch 034:    168 / 788 loss=5.417, nll_loss=4.651, ppl=25.12, wps=11159.9, ups=0.17, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.598, loss_scale=8, train_wall=565, gb_free=3.3, wall=153090
2022-02-26 05:57:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:57:50 | INFO | train_inner | epoch 034:    269 / 788 loss=5.454, nll_loss=4.69, ppl=25.81, wps=11164.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.611, loss_scale=8, train_wall=564, gb_free=3.3, wall=153677
2022-02-26 06:07:31 | INFO | train_inner | epoch 034:    369 / 788 loss=5.46, nll_loss=4.696, ppl=25.93, wps=11273, ups=0.17, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.608, loss_scale=8, train_wall=559, gb_free=3.3, wall=154258
2022-02-26 06:17:12 | INFO | train_inner | epoch 034:    469 / 788 loss=5.478, nll_loss=4.715, ppl=26.27, wps=11276.3, ups=0.17, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.606, loss_scale=16, train_wall=559, gb_free=3.3, wall=154839
2022-02-26 06:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 06:26:59 | INFO | train_inner | epoch 034:    570 / 788 loss=5.502, nll_loss=4.741, ppl=26.74, wps=11161, ups=0.17, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.614, loss_scale=8, train_wall=565, gb_free=3.3, wall=155426
2022-02-26 06:32:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 06:36:47 | INFO | train_inner | epoch 034:    671 / 788 loss=5.507, nll_loss=4.746, ppl=26.84, wps=11162.7, ups=0.17, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.604, loss_scale=8, train_wall=565, gb_free=3.3, wall=156014
2022-02-26 06:46:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 06:46:33 | INFO | train_inner | epoch 034:    772 / 788 loss=5.525, nll_loss=4.765, ppl=27.2, wps=11167.8, ups=0.17, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.618, loss_scale=8, train_wall=564, gb_free=3.3, wall=156600
2022-02-26 06:48:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 06:48:11 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.172 | nll_loss 5.43 | ppl 43.13 | wps 28239.7 | wpb 510.9 | bsz 1 | num_updates 26616 | best_loss 6.171
2022-02-26 06:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 26616 updates
2022-02-26 06:48:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 06:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 06:48:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 34 @ 26616 updates, score 6.172) (writing took 5.880129211582243 seconds)
2022-02-26 06:48:17 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-26 06:48:17 | INFO | train | epoch 034 | loss 5.471 | nll_loss 4.708 | ppl 26.14 | wps 11168.8 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 26616 | lr 0.000193833 | gnorm 0.609 | loss_scale 8 | train_wall 4402 | gb_free 3.3 | wall 156704
2022-02-26 06:48:17 | INFO | fairseq.trainer | begin training epoch 35
2022-02-26 06:48:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 06:56:26 | INFO | train_inner | epoch 035:     84 / 788 loss=5.411, nll_loss=4.645, ppl=25.01, wps=11012.5, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.629, loss_scale=8, train_wall=556, gb_free=3.3, wall=157193
2022-02-26 07:05:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:06:13 | INFO | train_inner | epoch 035:    185 / 788 loss=5.417, nll_loss=4.651, ppl=25.12, wps=11159, ups=0.17, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.599, loss_scale=8, train_wall=565, gb_free=3.3, wall=157780
2022-02-26 07:15:54 | INFO | train_inner | epoch 035:    285 / 788 loss=5.433, nll_loss=4.668, ppl=25.42, wps=11273.7, ups=0.17, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.613, loss_scale=8, train_wall=559, gb_free=3.3, wall=158361
2022-02-26 07:22:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:25:41 | INFO | train_inner | epoch 035:    386 / 788 loss=5.448, nll_loss=4.684, ppl=25.7, wps=11161.9, ups=0.17, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.605, loss_scale=8, train_wall=565, gb_free=3.3, wall=158949
2022-02-26 07:35:23 | INFO | train_inner | epoch 035:    486 / 788 loss=5.474, nll_loss=4.712, ppl=26.2, wps=11275.4, ups=0.17, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.608, loss_scale=16, train_wall=559, gb_free=3.3, wall=159530
2022-02-26 07:36:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:45:10 | INFO | train_inner | epoch 035:    587 / 788 loss=5.491, nll_loss=4.729, ppl=26.52, wps=11157.9, ups=0.17, wpb=65534.7, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.613, loss_scale=8, train_wall=565, gb_free=3.3, wall=160117
2022-02-26 07:50:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:54:57 | INFO | train_inner | epoch 035:    688 / 788 loss=5.513, nll_loss=4.753, ppl=26.96, wps=11159.8, ups=0.17, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.62, loss_scale=8, train_wall=565, gb_free=3.3, wall=160704
2022-02-26 08:04:36 | INFO | train_inner | epoch 035:    788 / 788 loss=5.514, nll_loss=4.754, ppl=26.98, wps=11272.6, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=27400, lr=0.00019104, gnorm=0.614, loss_scale=16, train_wall=557, gb_free=3.3, wall=161283
2022-02-26 08:04:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 08:04:44 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.169 | nll_loss 5.428 | ppl 43.04 | wps 28187.7 | wpb 510.9 | bsz 1 | num_updates 27400 | best_loss 6.169
2022-02-26 08:04:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 27400 updates
2022-02-26 08:04:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-26 08:04:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-02-26 08:04:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 35 @ 27400 updates, score 6.169) (writing took 12.794623706489801 seconds)
2022-02-26 08:04:57 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-26 08:04:57 | INFO | train | epoch 035 | loss 5.461 | nll_loss 4.698 | ppl 25.95 | wps 11164.8 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 27400 | lr 0.00019104 | gnorm 0.612 | loss_scale 16 | train_wall 4403 | gb_free 3.3 | wall 161304
2022-02-26 08:04:57 | INFO | fairseq.trainer | begin training epoch 36
2022-02-26 08:04:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 08:10:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:14:44 | INFO | train_inner | epoch 036:    101 / 788 loss=5.382, nll_loss=4.614, ppl=24.49, wps=10777.7, ups=0.16, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.608, loss_scale=8, train_wall=565, gb_free=3.3, wall=161891
2022-02-26 08:23:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:24:31 | INFO | train_inner | epoch 036:    202 / 788 loss=5.409, nll_loss=4.642, ppl=24.97, wps=11160, ups=0.17, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.614, loss_scale=8, train_wall=565, gb_free=3.3, wall=162478
2022-02-26 08:34:13 | INFO | train_inner | epoch 036:    302 / 788 loss=5.425, nll_loss=4.66, ppl=25.27, wps=11271.7, ups=0.17, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.612, loss_scale=8, train_wall=559, gb_free=3.3, wall=163060
2022-02-26 08:37:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:44:00 | INFO | train_inner | epoch 036:    403 / 788 loss=5.441, nll_loss=4.676, ppl=25.57, wps=11160.6, ups=0.17, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.615, loss_scale=8, train_wall=565, gb_free=3.3, wall=163647
2022-02-26 08:50:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:53:47 | INFO | train_inner | epoch 036:    504 / 788 loss=5.467, nll_loss=4.703, ppl=26.05, wps=11164.6, ups=0.17, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.621, loss_scale=8, train_wall=565, gb_free=3.3, wall=164234
2022-02-26 09:03:28 | INFO | train_inner | epoch 036:    604 / 788 loss=5.476, nll_loss=4.713, ppl=26.22, wps=11274.4, ups=0.17, wpb=65534.7, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.612, loss_scale=16, train_wall=559, gb_free=3.3, wall=164815
2022-02-26 09:06:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:13:15 | INFO | train_inner | epoch 036:    705 / 788 loss=5.5, nll_loss=4.739, ppl=26.7, wps=11163.4, ups=0.17, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.611, loss_scale=8, train_wall=565, gb_free=3.3, wall=165402
2022-02-26 09:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 09:21:23 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.172 | nll_loss 5.433 | ppl 43.21 | wps 28211.5 | wpb 510.9 | bsz 1 | num_updates 28183 | best_loss 6.169
2022-02-26 09:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 28183 updates
2022-02-26 09:21:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 09:21:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 09:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 36 @ 28183 updates, score 6.172) (writing took 5.936664000153542 seconds)
2022-02-26 09:21:29 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-26 09:21:29 | INFO | train | epoch 036 | loss 5.451 | nll_loss 4.686 | ppl 25.75 | wps 11167.4 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 28183 | lr 0.000188368 | gnorm 0.614 | loss_scale 16 | train_wall 4403 | gb_free 3.3 | wall 165896
2022-02-26 09:21:29 | INFO | fairseq.trainer | begin training epoch 37
2022-02-26 09:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 09:21:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:23:14 | INFO | train_inner | epoch 037:     18 / 788 loss=5.491, nll_loss=4.73, ppl=26.53, wps=10901.7, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=28200, lr=0.000188311, gnorm=0.624, loss_scale=8, train_wall=562, gb_free=3.3, wall=166001
2022-02-26 09:32:55 | INFO | train_inner | epoch 037:    118 / 788 loss=5.377, nll_loss=4.608, ppl=24.39, wps=11271.6, ups=0.17, wpb=65536, bsz=128, num_updates=28300, lr=0.000187978, gnorm=0.61, loss_scale=8, train_wall=559, gb_free=3.3, wall=166582
2022-02-26 09:36:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:42:42 | INFO | train_inner | epoch 037:    219 / 788 loss=5.393, nll_loss=4.625, ppl=24.67, wps=11159.4, ups=0.17, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.632, loss_scale=8, train_wall=565, gb_free=3.3, wall=167169
2022-02-26 09:52:24 | INFO | train_inner | epoch 037:    319 / 788 loss=5.418, nll_loss=4.651, ppl=25.13, wps=11268.6, ups=0.17, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.604, loss_scale=16, train_wall=559, gb_free=3.3, wall=167751
2022-02-26 10:00:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 10:02:11 | INFO | train_inner | epoch 037:    420 / 788 loss=5.449, nll_loss=4.685, ppl=25.72, wps=11163.4, ups=0.17, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.623, loss_scale=8, train_wall=564, gb_free=3.3, wall=168338
2022-02-26 10:11:52 | INFO | train_inner | epoch 037:    520 / 788 loss=5.458, nll_loss=4.694, ppl=25.88, wps=11277.7, ups=0.17, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.611, loss_scale=8, train_wall=559, gb_free=3.3, wall=168919
2022-02-26 10:13:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 10:21:39 | INFO | train_inner | epoch 037:    621 / 788 loss=5.486, nll_loss=4.723, ppl=26.41, wps=11164.3, ups=0.17, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.646, loss_scale=8, train_wall=565, gb_free=3.3, wall=169506
2022-02-26 10:31:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 10:31:27 | INFO | train_inner | epoch 037:    722 / 788 loss=5.486, nll_loss=4.724, ppl=26.43, wps=11156.6, ups=0.17, wpb=65534.7, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.611, loss_scale=8, train_wall=565, gb_free=3.3, wall=170094
2022-02-26 10:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 10:37:55 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.172 | nll_loss 5.43 | ppl 43.12 | wps 28230.9 | wpb 510.9 | bsz 1 | num_updates 28966 | best_loss 6.169
2022-02-26 10:37:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 28966 updates
2022-02-26 10:37:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 10:38:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-02-26 10:38:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.5_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 37 @ 28966 updates, score 6.172) (writing took 6.09403607621789 seconds)
2022-02-26 10:38:01 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-26 10:38:01 | INFO | train | epoch 037 | loss 5.442 | nll_loss 4.677 | ppl 25.58 | wps 11166.9 | ups 0.17 | wpb 65497.3 | bsz 127.9 | num_updates 28966 | lr 0.000185804 | gnorm 0.622 | loss_scale 8 | train_wall 4403 | gb_free 3.3 | wall 170488
2022-02-26 10:38:02 | INFO | fairseq.trainer | begin training epoch 38
2022-02-26 10:38:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 10:41:19 | INFO | train_inner | epoch 038:     34 / 788 loss=5.452, nll_loss=4.688, ppl=25.77, wps=11005.6, ups=0.17, wpb=65233.9, bsz=127.4, num_updates=29000, lr=0.000185695, gnorm=0.64, loss_scale=8, train_wall=557, gb_free=3.3, wall=170686
2022-02-26 10:51:01 | INFO | train_inner | epoch 038:    134 / 788 loss=5.367, nll_loss=4.598, ppl=24.22, wps=11268.1, ups=0.17, wpb=65534.7, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.606, loss_scale=16, train_wall=559, gb_free=3.3, wall=171268
2022-02-26 10:55:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 11:00:48 | INFO | train_inner | epoch 038:    235 / 788 loss=5.399, nll_loss=4.632, ppl=24.8, wps=11161.6, ups=0.17, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.614, loss_scale=8, train_wall=565, gb_free=3.3, wall=171855
2022-02-26 11:08:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 11:10:35 | INFO | train_inner | epoch 038:    336 / 788 loss=5.416, nll_loss=4.649, ppl=25.1, wps=11159.7, ups=0.17, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.617, loss_scale=8, train_wall=565, gb_free=3.3, wall=172442
User defined signal 2
