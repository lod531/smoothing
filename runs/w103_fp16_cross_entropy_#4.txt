Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 205309586: <w103_fp16_cross_entropy_#4> in cluster <euler> Exited

Job <w103_fp16_cross_entropy_#4> was submitted from host <eu-login-39> by user <andriusb> in cluster <euler> at Wed Feb 16 05:27:28 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Wed Feb 16 07:15:47 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Feb 16 07:15:47 2022
Terminated at Thu Feb 17 18:30:20 2022
Results reported at Thu Feb 17 18:30:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 6658484 --fp16 --max-update 50000
------------------------------------------------------------

Exited with exit code 134.

Resource usage summary:

    CPU time :                                   126807.88 sec.
    Max Memory :                                 18533 MB
    Average Memory :                             3867.93 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               1467.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   126873 sec.
    Turnaround time :                            133372 sec.

The output (if any) follows:

2022-02-16 07:15:57 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 6658484, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 6658484, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-16 07:15:58 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-16 07:16:05 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-16 07:16:05 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-16 07:16:05 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-16 07:16:05 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-16 07:16:05 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-16 07:16:05 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-16 07:16:06 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-16 07:16:09 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-16 07:16:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-16 07:16:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-16 07:16:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-16 07:16:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-16 07:16:09 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-16 07:16:09 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint_last.pt
2022-02-16 07:16:09 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint_last.pt
2022-02-16 07:16:09 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-16 07:16:15 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-16 07:16:16 | INFO | fairseq.trainer | begin training epoch 1
2022-02-16 07:16:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 07:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-16 07:16:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-16 07:16:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 07:16:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-16 07:17:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-16 07:24:57 | INFO | train_inner | epoch 001:    105 / 1576 loss=18.026, ppl=266989, wps=13722.5, ups=0.21, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.662, loss_scale=4, train_wall=509, gb_free=10, wall=528
2022-02-16 07:32:50 | INFO | train_inner | epoch 001:    205 / 1576 loss=15.416, ppl=43728.6, wps=13850.6, ups=0.21, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.762, loss_scale=4, train_wall=462, gb_free=10, wall=1001
2022-02-16 07:40:43 | INFO | train_inner | epoch 001:    305 / 1576 loss=12.974, ppl=8046.19, wps=13861.7, ups=0.21, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.272, loss_scale=8, train_wall=462, gb_free=10, wall=1474
2022-02-16 07:48:35 | INFO | train_inner | epoch 001:    405 / 1576 loss=11.051, ppl=2122.1, wps=13866.4, ups=0.21, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.749, loss_scale=8, train_wall=462, gb_free=10, wall=1946
2022-02-16 07:56:28 | INFO | train_inner | epoch 001:    505 / 1576 loss=10.211, ppl=1184.97, wps=13875.2, ups=0.21, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.52, loss_scale=8, train_wall=461, gb_free=10, wall=2419
2022-02-16 08:04:20 | INFO | train_inner | epoch 001:    605 / 1576 loss=9.83, ppl=909.95, wps=13874.9, ups=0.21, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.581, loss_scale=16, train_wall=461, gb_free=10, wall=2891
2022-02-16 08:12:12 | INFO | train_inner | epoch 001:    705 / 1576 loss=9.53, ppl=739.52, wps=13878.4, ups=0.21, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.619, loss_scale=16, train_wall=461, gb_free=10, wall=3363
2022-02-16 08:20:04 | INFO | train_inner | epoch 001:    805 / 1576 loss=9.285, ppl=624, wps=13879.3, ups=0.21, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.731, loss_scale=32, train_wall=461, gb_free=10, wall=3836
2022-02-16 08:27:57 | INFO | train_inner | epoch 001:    905 / 1576 loss=9.057, ppl=532.66, wps=13872.3, ups=0.21, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.867, loss_scale=32, train_wall=461, gb_free=10, wall=4308
2022-02-16 08:35:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 08:35:54 | INFO | train_inner | epoch 001:   1006 / 1576 loss=8.839, ppl=457.87, wps=13736.9, ups=0.21, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.87, loss_scale=16, train_wall=466, gb_free=10, wall=4785
2022-02-16 08:43:46 | INFO | train_inner | epoch 001:   1106 / 1576 loss=8.649, ppl=401.49, wps=13881, ups=0.21, wpb=65532.3, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.944, loss_scale=16, train_wall=461, gb_free=10, wall=5257
2022-02-16 08:51:38 | INFO | train_inner | epoch 001:   1206 / 1576 loss=8.484, ppl=358.01, wps=13884.6, ups=0.21, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.954, loss_scale=16, train_wall=461, gb_free=10, wall=5729
2022-02-16 08:59:30 | INFO | train_inner | epoch 001:   1306 / 1576 loss=8.337, ppl=323.36, wps=13885.4, ups=0.21, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.948, loss_scale=32, train_wall=461, gb_free=10, wall=6201
2022-02-16 09:07:22 | INFO | train_inner | epoch 001:   1406 / 1576 loss=8.197, ppl=293.53, wps=13883.5, ups=0.21, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.973, loss_scale=32, train_wall=461, gb_free=10, wall=6673
2022-02-16 09:15:14 | INFO | train_inner | epoch 001:   1506 / 1576 loss=8.074, ppl=269.46, wps=13882.3, ups=0.21, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.976, loss_scale=32, train_wall=461, gb_free=10, wall=7145
2022-02-16 09:16:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-16 09:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 09:20:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.763 | ppl 217.28 | wps 35359.2 | wpb 1021.8 | bsz 2 | num_updates 1569
2022-02-16 09:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1569 updates
2022-02-16 09:20:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint1.pt
2022-02-16 09:20:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint1.pt
2022-02-16 09:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint1.pt (epoch 1 @ 1569 updates, score 7.763) (writing took 29.57912196777761 seconds)
2022-02-16 09:21:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-16 09:21:16 | INFO | train | epoch 001 | loss 10.292 | ppl 1253.82 | wps 13781.1 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 1569 | lr 0.000196186 | gnorm 1.09 | loss_scale 32 | train_wall 7291 | gb_free 10 | wall 7507
2022-02-16 09:21:16 | INFO | fairseq.trainer | begin training epoch 2
2022-02-16 09:21:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 09:23:43 | INFO | train_inner | epoch 002:     31 / 1576 loss=7.937, ppl=245.11, wps=12774.5, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=0.98, loss_scale=32, train_wall=461, gb_free=10, wall=7654
2022-02-16 09:31:35 | INFO | train_inner | epoch 002:    131 / 1576 loss=7.802, ppl=223.16, wps=13879.3, ups=0.21, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.938, loss_scale=32, train_wall=461, gb_free=10, wall=8126
2022-02-16 09:37:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-16 09:39:32 | INFO | train_inner | epoch 002:    232 / 1576 loss=7.699, ppl=207.82, wps=13745.4, ups=0.21, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.98, loss_scale=32, train_wall=466, gb_free=10, wall=8603
2022-02-16 09:47:24 | INFO | train_inner | epoch 002:    332 / 1576 loss=7.611, ppl=195.52, wps=13881, ups=0.21, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.942, loss_scale=32, train_wall=461, gb_free=10, wall=9075
2022-02-16 09:55:16 | INFO | train_inner | epoch 002:    432 / 1576 loss=7.509, ppl=182.21, wps=13889.6, ups=0.21, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.935, loss_scale=32, train_wall=461, gb_free=10, wall=9547
2022-02-16 09:58:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-16 10:03:12 | INFO | train_inner | epoch 002:    533 / 1576 loss=7.413, ppl=170.41, wps=13748.9, ups=0.21, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.913, loss_scale=32, train_wall=465, gb_free=10, wall=10023
2022-02-16 10:11:05 | INFO | train_inner | epoch 002:    633 / 1576 loss=7.324, ppl=160.26, wps=13859.5, ups=0.21, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.931, loss_scale=32, train_wall=462, gb_free=10, wall=10496
2022-02-16 10:18:58 | INFO | train_inner | epoch 002:    733 / 1576 loss=7.251, ppl=152.34, wps=13866.5, ups=0.21, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.872, loss_scale=64, train_wall=462, gb_free=10, wall=10969
2022-02-16 10:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-16 10:26:55 | INFO | train_inner | epoch 002:    834 / 1576 loss=7.164, ppl=143.39, wps=13732.4, ups=0.21, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.871, loss_scale=32, train_wall=466, gb_free=10, wall=11446
2022-02-16 10:34:48 | INFO | train_inner | epoch 002:    934 / 1576 loss=7.099, ppl=137.11, wps=13867.1, ups=0.21, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.86, loss_scale=32, train_wall=462, gb_free=10, wall=11919
2022-02-16 10:40:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-16 10:42:45 | INFO | train_inner | epoch 002:   1035 / 1576 loss=7.039, ppl=131.47, wps=13724.9, ups=0.21, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.854, loss_scale=32, train_wall=466, gb_free=10, wall=12396
2022-02-16 10:50:38 | INFO | train_inner | epoch 002:   1135 / 1576 loss=6.97, ppl=125.37, wps=13855.3, ups=0.21, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.825, loss_scale=32, train_wall=462, gb_free=10, wall=12869
2022-02-16 10:56:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 10:58:35 | INFO | train_inner | epoch 002:   1236 / 1576 loss=6.904, ppl=119.77, wps=13740.4, ups=0.21, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.803, loss_scale=16, train_wall=466, gb_free=10, wall=13346
2022-02-16 11:06:26 | INFO | train_inner | epoch 002:   1336 / 1576 loss=6.84, ppl=114.53, wps=13919.3, ups=0.21, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.822, loss_scale=16, train_wall=460, gb_free=10, wall=13817
2022-02-16 11:14:17 | INFO | train_inner | epoch 002:   1436 / 1576 loss=6.784, ppl=110.2, wps=13922.7, ups=0.21, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.798, loss_scale=16, train_wall=460, gb_free=10, wall=14288
2022-02-16 11:21:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 11:22:12 | INFO | train_inner | epoch 002:   1537 / 1576 loss=6.708, ppl=104.57, wps=13785.2, ups=0.21, wpb=65532.3, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.783, loss_scale=16, train_wall=464, gb_free=10, wall=14763
2022-02-16 11:25:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 11:25:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.52 | ppl 91.78 | wps 35647.7 | wpb 1021.8 | bsz 2 | num_updates 3139 | best_loss 6.52
2022-02-16 11:25:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3139 updates
2022-02-16 11:25:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint2.pt
2022-02-16 11:25:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint2.pt
2022-02-16 11:25:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint2.pt (epoch 2 @ 3139 updates, score 6.52) (writing took 28.81061434932053 seconds)
2022-02-16 11:25:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-16 11:25:46 | INFO | train | epoch 002 | loss 7.208 | ppl 147.84 | wps 13765.8 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 3139 | lr 0.000392397 | gnorm 0.875 | loss_scale 16 | train_wall 7261 | gb_free 10 | wall 14978
2022-02-16 11:25:47 | INFO | fairseq.trainer | begin training epoch 3
2022-02-16 11:25:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 11:30:34 | INFO | train_inner | epoch 003:     61 / 1576 loss=6.624, ppl=98.64, wps=12942, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.767, loss_scale=16, train_wall=456, gb_free=10, wall=15265
2022-02-16 11:38:25 | INFO | train_inner | epoch 003:    161 / 1576 loss=6.567, ppl=94.8, wps=13912.1, ups=0.21, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.779, loss_scale=16, train_wall=460, gb_free=10, wall=15736
2022-02-16 11:46:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 11:46:21 | INFO | train_inner | epoch 003:    262 / 1576 loss=6.513, ppl=91.34, wps=13776.6, ups=0.21, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.766, loss_scale=16, train_wall=465, gb_free=10, wall=16212
2022-02-16 11:54:12 | INFO | train_inner | epoch 003:    362 / 1576 loss=6.483, ppl=89.43, wps=13898.9, ups=0.21, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.752, loss_scale=16, train_wall=461, gb_free=10, wall=16683
2022-02-16 12:02:04 | INFO | train_inner | epoch 003:    462 / 1576 loss=6.445, ppl=87.13, wps=13895.5, ups=0.21, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.734, loss_scale=16, train_wall=461, gb_free=10, wall=17155
2022-02-16 12:06:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 12:10:00 | INFO | train_inner | epoch 003:    563 / 1576 loss=6.415, ppl=85.34, wps=13759.9, ups=0.21, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.737, loss_scale=16, train_wall=465, gb_free=10, wall=17631
2022-02-16 12:17:52 | INFO | train_inner | epoch 003:    663 / 1576 loss=6.371, ppl=82.8, wps=13899.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.72, loss_scale=16, train_wall=460, gb_free=10, wall=18103
2022-02-16 12:25:43 | INFO | train_inner | epoch 003:    763 / 1576 loss=6.33, ppl=80.46, wps=13898.4, ups=0.21, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.705, loss_scale=16, train_wall=461, gb_free=10, wall=18574
2022-02-16 12:31:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 12:33:40 | INFO | train_inner | epoch 003:    864 / 1576 loss=6.303, ppl=78.98, wps=13753, ups=0.21, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.721, loss_scale=16, train_wall=465, gb_free=10, wall=19051
2022-02-16 12:41:31 | INFO | train_inner | epoch 003:    964 / 1576 loss=6.267, ppl=77, wps=13898.8, ups=0.21, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.673, loss_scale=16, train_wall=461, gb_free=10, wall=19522
2022-02-16 12:49:23 | INFO | train_inner | epoch 003:   1064 / 1576 loss=6.231, ppl=75.14, wps=13900, ups=0.21, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.682, loss_scale=16, train_wall=460, gb_free=10, wall=19994
2022-02-16 12:57:14 | INFO | train_inner | epoch 003:   1164 / 1576 loss=6.217, ppl=74.4, wps=13896.6, ups=0.21, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.644, loss_scale=32, train_wall=461, gb_free=10, wall=20465
2022-02-16 12:58:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 13:05:10 | INFO | train_inner | epoch 003:   1265 / 1576 loss=6.174, ppl=72.22, wps=13760.8, ups=0.21, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.653, loss_scale=16, train_wall=465, gb_free=10, wall=20942
2022-02-16 13:13:02 | INFO | train_inner | epoch 003:   1365 / 1576 loss=6.143, ppl=70.65, wps=13897, ups=0.21, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.637, loss_scale=16, train_wall=461, gb_free=10, wall=21413
2022-02-16 13:20:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 13:20:58 | INFO | train_inner | epoch 003:   1466 / 1576 loss=6.114, ppl=69.28, wps=13756.2, ups=0.21, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.628, loss_scale=16, train_wall=465, gb_free=10, wall=21890
2022-02-16 13:28:50 | INFO | train_inner | epoch 003:   1566 / 1576 loss=6.087, ppl=67.96, wps=13903.2, ups=0.21, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.606, loss_scale=16, train_wall=460, gb_free=10, wall=22361
2022-02-16 13:29:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 13:29:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.91 | ppl 60.12 | wps 35728.9 | wpb 1021.8 | bsz 2 | num_updates 4710 | best_loss 5.91
2022-02-16 13:29:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4710 updates
2022-02-16 13:29:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint3.pt
2022-02-16 13:29:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint3.pt
2022-02-16 13:30:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint3.pt (epoch 3 @ 4710 updates, score 5.91) (writing took 28.4176144618541 seconds)
2022-02-16 13:30:08 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-16 13:30:08 | INFO | train | epoch 003 | loss 6.32 | ppl 79.9 | wps 13791.5 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 4710 | lr 0.000460776 | gnorm 0.698 | loss_scale 16 | train_wall 7253 | gb_free 10 | wall 22439
2022-02-16 13:30:08 | INFO | fairseq.trainer | begin training epoch 4
2022-02-16 13:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 13:37:12 | INFO | train_inner | epoch 004:     90 / 1576 loss=5.966, ppl=62.52, wps=12940.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.621, loss_scale=16, train_wall=456, gb_free=10, wall=22863
2022-02-16 13:45:03 | INFO | train_inner | epoch 004:    190 / 1576 loss=5.951, ppl=61.84, wps=13895.7, ups=0.21, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.606, loss_scale=32, train_wall=461, gb_free=10, wall=23335
2022-02-16 13:52:55 | INFO | train_inner | epoch 004:    290 / 1576 loss=5.933, ppl=61.11, wps=13892.9, ups=0.21, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.615, loss_scale=32, train_wall=461, gb_free=10, wall=23806
2022-02-16 13:54:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 14:00:52 | INFO | train_inner | epoch 004:    391 / 1576 loss=5.899, ppl=59.66, wps=13756.1, ups=0.21, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.586, loss_scale=16, train_wall=465, gb_free=10, wall=24283
2022-02-16 14:08:43 | INFO | train_inner | epoch 004:    491 / 1576 loss=5.915, ppl=60.35, wps=13894.4, ups=0.21, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.598, loss_scale=16, train_wall=461, gb_free=10, wall=24754
2022-02-16 14:16:35 | INFO | train_inner | epoch 004:    591 / 1576 loss=5.898, ppl=59.64, wps=13892.2, ups=0.21, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.575, loss_scale=32, train_wall=461, gb_free=10, wall=25226
2022-02-16 14:24:27 | INFO | train_inner | epoch 004:    691 / 1576 loss=5.865, ppl=58.27, wps=13889.1, ups=0.21, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.573, loss_scale=32, train_wall=461, gb_free=10, wall=25698
2022-02-16 14:25:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 14:32:23 | INFO | train_inner | epoch 004:    792 / 1576 loss=5.858, ppl=58.01, wps=13758.2, ups=0.21, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.587, loss_scale=16, train_wall=465, gb_free=10, wall=26174
2022-02-16 14:40:15 | INFO | train_inner | epoch 004:    892 / 1576 loss=5.851, ppl=57.74, wps=13895.2, ups=0.21, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.575, loss_scale=16, train_wall=461, gb_free=10, wall=26646
2022-02-16 14:48:06 | INFO | train_inner | epoch 004:    992 / 1576 loss=5.838, ppl=57.2, wps=13896.2, ups=0.21, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.581, loss_scale=32, train_wall=461, gb_free=10, wall=27118
2022-02-16 14:53:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 14:56:03 | INFO | train_inner | epoch 004:   1093 / 1576 loss=5.825, ppl=56.69, wps=13757.9, ups=0.21, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.591, loss_scale=16, train_wall=465, gb_free=10, wall=27594
2022-02-16 15:03:55 | INFO | train_inner | epoch 004:   1193 / 1576 loss=5.797, ppl=55.61, wps=13867.3, ups=0.21, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.57, loss_scale=16, train_wall=462, gb_free=10, wall=28067
2022-02-16 15:11:47 | INFO | train_inner | epoch 004:   1293 / 1576 loss=5.794, ppl=55.48, wps=13908.9, ups=0.21, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.554, loss_scale=16, train_wall=460, gb_free=10, wall=28538
2022-02-16 15:14:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 15:19:43 | INFO | train_inner | epoch 004:   1394 / 1576 loss=5.787, ppl=55.22, wps=13751, ups=0.21, wpb=65532.3, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.56, loss_scale=16, train_wall=465, gb_free=10, wall=29014
2022-02-16 15:27:36 | INFO | train_inner | epoch 004:   1494 / 1576 loss=5.776, ppl=54.79, wps=13855.9, ups=0.21, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.566, loss_scale=16, train_wall=462, gb_free=10, wall=29487
2022-02-16 15:34:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 15:34:06 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.634 | ppl 49.67 | wps 34901.5 | wpb 1021.8 | bsz 2 | num_updates 6282 | best_loss 5.634
2022-02-16 15:34:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6282 updates
2022-02-16 15:34:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint4.pt
2022-02-16 15:34:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint4.pt
2022-02-16 15:34:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint4.pt (epoch 4 @ 6282 updates, score 5.634) (writing took 28.55077145434916 seconds)
2022-02-16 15:34:35 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-16 15:34:35 | INFO | train | epoch 004 | loss 5.857 | ppl 57.96 | wps 13789.3 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 6282 | lr 0.00039898 | gnorm 0.583 | loss_scale 16 | train_wall 7258 | gb_free 10 | wall 29906
2022-02-16 15:34:35 | INFO | fairseq.trainer | begin training epoch 5
2022-02-16 15:34:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 15:36:00 | INFO | train_inner | epoch 005:     18 / 1576 loss=5.738, ppl=53.38, wps=12903, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.562, loss_scale=32, train_wall=457, gb_free=10, wall=29991
2022-02-16 15:41:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 15:43:58 | INFO | train_inner | epoch 005:    119 / 1576 loss=5.631, ppl=49.56, wps=13705, ups=0.21, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.553, loss_scale=16, train_wall=467, gb_free=10, wall=30469
2022-02-16 15:51:50 | INFO | train_inner | epoch 005:    219 / 1576 loss=5.633, ppl=49.62, wps=13873.6, ups=0.21, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.559, loss_scale=16, train_wall=461, gb_free=10, wall=30941
2022-02-16 15:59:41 | INFO | train_inner | epoch 005:    319 / 1576 loss=5.626, ppl=49.4, wps=13923, ups=0.21, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.557, loss_scale=16, train_wall=460, gb_free=10, wall=31412
2022-02-16 16:06:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 16:07:36 | INFO | train_inner | epoch 005:    420 / 1576 loss=5.627, ppl=49.42, wps=13782.3, ups=0.21, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.553, loss_scale=16, train_wall=464, gb_free=10, wall=31888
2022-02-16 16:15:28 | INFO | train_inner | epoch 005:    520 / 1576 loss=5.634, ppl=49.66, wps=13910.2, ups=0.21, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.556, loss_scale=16, train_wall=460, gb_free=10, wall=32359
2022-02-16 16:23:19 | INFO | train_inner | epoch 005:    620 / 1576 loss=5.619, ppl=49.15, wps=13902, ups=0.21, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.55, loss_scale=16, train_wall=460, gb_free=10, wall=32830
2022-02-16 16:31:11 | INFO | train_inner | epoch 005:    720 / 1576 loss=5.624, ppl=49.33, wps=13896.5, ups=0.21, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.55, loss_scale=32, train_wall=461, gb_free=10, wall=33302
2022-02-16 16:31:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 16:39:07 | INFO | train_inner | epoch 005:    821 / 1576 loss=5.624, ppl=49.33, wps=13761.1, ups=0.21, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.551, loss_scale=16, train_wall=465, gb_free=10, wall=33778
2022-02-16 16:46:58 | INFO | train_inner | epoch 005:    921 / 1576 loss=5.622, ppl=49.25, wps=13898.9, ups=0.21, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.548, loss_scale=16, train_wall=461, gb_free=10, wall=34250
2022-02-16 16:54:50 | INFO | train_inner | epoch 005:   1021 / 1576 loss=5.598, ppl=48.45, wps=13894.3, ups=0.21, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.55, loss_scale=32, train_wall=461, gb_free=10, wall=34721
2022-02-16 16:57:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 17:02:46 | INFO | train_inner | epoch 005:   1122 / 1576 loss=5.6, ppl=48.5, wps=13760.4, ups=0.21, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.566, loss_scale=16, train_wall=465, gb_free=10, wall=35197
2022-02-16 17:10:38 | INFO | train_inner | epoch 005:   1222 / 1576 loss=5.594, ppl=48.29, wps=13893.8, ups=0.21, wpb=65532.3, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.529, loss_scale=16, train_wall=461, gb_free=10, wall=35669
2022-02-16 17:18:29 | INFO | train_inner | epoch 005:   1322 / 1576 loss=5.592, ppl=48.23, wps=13899.6, ups=0.21, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.569, loss_scale=32, train_wall=461, gb_free=10, wall=36141
2022-02-16 17:22:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 17:26:26 | INFO | train_inner | epoch 005:   1423 / 1576 loss=5.595, ppl=48.34, wps=13761.8, ups=0.21, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.538, loss_scale=16, train_wall=465, gb_free=10, wall=36617
2022-02-16 17:34:17 | INFO | train_inner | epoch 005:   1523 / 1576 loss=5.582, ppl=47.89, wps=13899.3, ups=0.21, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.539, loss_scale=16, train_wall=461, gb_free=10, wall=37088
2022-02-16 17:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 17:38:29 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.488 | ppl 44.87 | wps 35383.5 | wpb 1021.8 | bsz 2 | num_updates 7853 | best_loss 5.488
2022-02-16 17:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7853 updates
2022-02-16 17:38:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint5.pt
2022-02-16 17:38:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint5.pt
2022-02-16 17:38:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint5.pt (epoch 5 @ 7853 updates, score 5.488) (writing took 28.322456632740796 seconds)
2022-02-16 17:38:58 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-16 17:38:58 | INFO | train | epoch 005 | loss 5.612 | ppl 48.91 | wps 13787.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 7853 | lr 0.000356847 | gnorm 0.552 | loss_scale 16 | train_wall 7254 | gb_free 10 | wall 37369
2022-02-16 17:38:58 | INFO | fairseq.trainer | begin training epoch 6
2022-02-16 17:38:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 17:42:39 | INFO | train_inner | epoch 006:     47 / 1576 loss=5.519, ppl=45.85, wps=12939.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.562, loss_scale=16, train_wall=456, gb_free=10, wall=37590
2022-02-16 17:45:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 17:50:35 | INFO | train_inner | epoch 006:    148 / 1576 loss=5.437, ppl=43.31, wps=13760.3, ups=0.21, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.573, loss_scale=16, train_wall=465, gb_free=10, wall=38067
2022-02-16 17:58:27 | INFO | train_inner | epoch 006:    248 / 1576 loss=5.465, ppl=44.16, wps=13906.6, ups=0.21, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.534, loss_scale=16, train_wall=460, gb_free=10, wall=38538
2022-02-16 18:05:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 18:06:23 | INFO | train_inner | epoch 006:    349 / 1576 loss=5.455, ppl=43.88, wps=13767.9, ups=0.21, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.555, loss_scale=16, train_wall=465, gb_free=10, wall=39014
2022-02-16 18:14:14 | INFO | train_inner | epoch 006:    449 / 1576 loss=5.461, ppl=44.06, wps=13913.7, ups=0.21, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.557, loss_scale=16, train_wall=460, gb_free=10, wall=39485
2022-02-16 18:22:05 | INFO | train_inner | epoch 006:    549 / 1576 loss=5.466, ppl=44.2, wps=13905.1, ups=0.21, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.545, loss_scale=16, train_wall=460, gb_free=10, wall=39956
2022-02-16 18:29:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 18:30:01 | INFO | train_inner | epoch 006:    650 / 1576 loss=5.461, ppl=44.06, wps=13765.2, ups=0.21, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.537, loss_scale=16, train_wall=465, gb_free=10, wall=40432
2022-02-16 18:37:52 | INFO | train_inner | epoch 006:    750 / 1576 loss=5.479, ppl=44.59, wps=13904.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.535, loss_scale=16, train_wall=460, gb_free=10, wall=40904
2022-02-16 18:45:44 | INFO | train_inner | epoch 006:    850 / 1576 loss=5.466, ppl=44.19, wps=13909.3, ups=0.21, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.541, loss_scale=16, train_wall=460, gb_free=10, wall=41375
2022-02-16 18:50:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 18:53:40 | INFO | train_inner | epoch 006:    951 / 1576 loss=5.466, ppl=44.19, wps=13767.4, ups=0.21, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.576, loss_scale=16, train_wall=465, gb_free=10, wall=41851
2022-02-16 19:01:31 | INFO | train_inner | epoch 006:   1051 / 1576 loss=5.466, ppl=44.21, wps=13912.7, ups=0.21, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.551, loss_scale=16, train_wall=460, gb_free=10, wall=42322
2022-02-16 19:09:22 | INFO | train_inner | epoch 006:   1151 / 1576 loss=5.466, ppl=44.19, wps=13915.1, ups=0.21, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.537, loss_scale=16, train_wall=460, gb_free=10, wall=42793
2022-02-16 19:12:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 19:17:18 | INFO | train_inner | epoch 006:   1252 / 1576 loss=5.453, ppl=43.8, wps=13768.6, ups=0.21, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.549, loss_scale=16, train_wall=465, gb_free=10, wall=43269
2022-02-16 19:25:09 | INFO | train_inner | epoch 006:   1352 / 1576 loss=5.465, ppl=44.17, wps=13903.6, ups=0.21, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.542, loss_scale=16, train_wall=460, gb_free=10, wall=43740
2022-02-16 19:33:01 | INFO | train_inner | epoch 006:   1452 / 1576 loss=5.462, ppl=44.09, wps=13899.1, ups=0.21, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.538, loss_scale=32, train_wall=461, gb_free=10, wall=44212
2022-02-16 19:33:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 19:40:57 | INFO | train_inner | epoch 006:   1553 / 1576 loss=5.448, ppl=43.66, wps=13766, ups=0.21, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.535, loss_scale=16, train_wall=465, gb_free=10, wall=44688
2022-02-16 19:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 19:42:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.386 | ppl 41.82 | wps 35380.4 | wpb 1021.8 | bsz 2 | num_updates 9423 | best_loss 5.386
2022-02-16 19:42:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9423 updates
2022-02-16 19:42:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint6.pt
2022-02-16 19:42:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint6.pt
2022-02-16 19:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint6.pt (epoch 6 @ 9423 updates, score 5.386) (writing took 28.26927190553397 seconds)
2022-02-16 19:43:15 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-16 19:43:15 | INFO | train | epoch 006 | loss 5.461 | ppl 44.05 | wps 13788.6 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 9423 | lr 0.000325766 | gnorm 0.548 | loss_scale 16 | train_wall 7250 | gb_free 10 | wall 44827
2022-02-16 19:43:16 | INFO | fairseq.trainer | begin training epoch 7
2022-02-16 19:43:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 19:49:19 | INFO | train_inner | epoch 007:     77 / 1576 loss=5.347, ppl=40.71, wps=12929.9, ups=0.2, wpb=64958.8, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.545, loss_scale=16, train_wall=457, gb_free=10, wall=45190
2022-02-16 19:53:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 19:57:17 | INFO | train_inner | epoch 007:    178 / 1576 loss=5.345, ppl=40.63, wps=13703.1, ups=0.21, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.532, loss_scale=16, train_wall=467, gb_free=10, wall=45668
2022-02-16 20:05:10 | INFO | train_inner | epoch 007:    278 / 1576 loss=5.344, ppl=40.62, wps=13851.4, ups=0.21, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.569, loss_scale=16, train_wall=462, gb_free=10, wall=46142
2022-02-16 20:13:04 | INFO | train_inner | epoch 007:    378 / 1576 loss=5.342, ppl=40.57, wps=13839.9, ups=0.21, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.547, loss_scale=16, train_wall=462, gb_free=10, wall=46615
2022-02-16 20:18:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 20:21:02 | INFO | train_inner | epoch 007:    479 / 1576 loss=5.344, ppl=40.61, wps=13721.7, ups=0.21, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.513, loss_scale=16, train_wall=466, gb_free=10, wall=47093
2022-02-16 20:28:54 | INFO | train_inner | epoch 007:    579 / 1576 loss=5.361, ppl=41.08, wps=13882.9, ups=0.21, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.575, loss_scale=16, train_wall=461, gb_free=10, wall=47565
2022-02-16 20:36:46 | INFO | train_inner | epoch 007:    679 / 1576 loss=5.362, ppl=41.14, wps=13879.5, ups=0.21, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.545, loss_scale=16, train_wall=461, gb_free=10, wall=48037
2022-02-16 20:38:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 20:44:43 | INFO | train_inner | epoch 007:    780 / 1576 loss=5.365, ppl=41.23, wps=13744.8, ups=0.21, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.517, loss_scale=16, train_wall=466, gb_free=10, wall=48514
2022-02-16 20:46:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-16 20:52:38 | INFO | train_inner | epoch 007:    881 / 1576 loss=5.36, ppl=41.06, wps=13774.6, ups=0.21, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.57, loss_scale=8, train_wall=465, gb_free=10, wall=48990
2022-02-16 21:00:29 | INFO | train_inner | epoch 007:    981 / 1576 loss=5.36, ppl=41.06, wps=13913.6, ups=0.21, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.55, loss_scale=8, train_wall=460, gb_free=10, wall=49461
2022-02-16 21:08:20 | INFO | train_inner | epoch 007:   1081 / 1576 loss=5.377, ppl=41.56, wps=13918.9, ups=0.21, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.533, loss_scale=16, train_wall=460, gb_free=10, wall=49931
2022-02-16 21:16:11 | INFO | train_inner | epoch 007:   1181 / 1576 loss=5.357, ppl=40.97, wps=13916.5, ups=0.21, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.556, loss_scale=16, train_wall=460, gb_free=10, wall=50402
2022-02-16 21:24:02 | INFO | train_inner | epoch 007:   1281 / 1576 loss=5.358, ppl=41.02, wps=13911.4, ups=0.21, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.52, loss_scale=16, train_wall=460, gb_free=10, wall=50873
2022-02-16 21:26:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 21:31:58 | INFO | train_inner | epoch 007:   1382 / 1576 loss=5.355, ppl=40.92, wps=13777.1, ups=0.21, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.565, loss_scale=16, train_wall=465, gb_free=10, wall=51349
2022-02-16 21:39:49 | INFO | train_inner | epoch 007:   1482 / 1576 loss=5.365, ppl=41.22, wps=13906.4, ups=0.21, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.555, loss_scale=16, train_wall=460, gb_free=10, wall=51820
2022-02-16 21:47:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 21:47:14 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.327 | ppl 40.13 | wps 35801.5 | wpb 1021.8 | bsz 2 | num_updates 10994 | best_loss 5.327
2022-02-16 21:47:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10994 updates
2022-02-16 21:47:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint7.pt
2022-02-16 21:47:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint7.pt
2022-02-16 21:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint7.pt (epoch 7 @ 10994 updates, score 5.327) (writing took 30.734635985456407 seconds)
2022-02-16 21:47:45 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-16 21:47:45 | INFO | train | epoch 007 | loss 5.355 | ppl 40.93 | wps 13775.9 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 10994 | lr 0.000301594 | gnorm 0.548 | loss_scale 32 | train_wall 7258 | gb_free 10 | wall 52296
2022-02-16 21:47:45 | INFO | fairseq.trainer | begin training epoch 8
2022-02-16 21:47:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 21:48:13 | INFO | train_inner | epoch 008:      6 / 1576 loss=5.351, ppl=40.83, wps=12885.4, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.579, loss_scale=32, train_wall=456, gb_free=10, wall=52325
2022-02-16 21:50:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 21:56:09 | INFO | train_inner | epoch 008:    107 / 1576 loss=5.224, ppl=37.37, wps=13768.2, ups=0.21, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.522, loss_scale=16, train_wall=465, gb_free=10, wall=52801
2022-02-16 22:04:00 | INFO | train_inner | epoch 008:    207 / 1576 loss=5.251, ppl=38.08, wps=13913.8, ups=0.21, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.552, loss_scale=16, train_wall=460, gb_free=10, wall=53272
2022-02-16 22:05:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-16 22:11:55 | INFO | train_inner | epoch 008:    308 / 1576 loss=5.263, ppl=38.4, wps=13797.3, ups=0.21, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.573, loss_scale=8, train_wall=464, gb_free=10, wall=53747
2022-02-16 22:19:46 | INFO | train_inner | epoch 008:    408 / 1576 loss=5.257, ppl=38.24, wps=13917.6, ups=0.21, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.537, loss_scale=8, train_wall=460, gb_free=10, wall=54217
2022-02-16 22:27:37 | INFO | train_inner | epoch 008:    508 / 1576 loss=5.264, ppl=38.44, wps=13909.8, ups=0.21, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.566, loss_scale=16, train_wall=460, gb_free=10, wall=54689
2022-02-16 22:35:29 | INFO | train_inner | epoch 008:    608 / 1576 loss=5.265, ppl=38.46, wps=13901.2, ups=0.21, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.534, loss_scale=16, train_wall=460, gb_free=10, wall=55160
2022-02-16 22:43:20 | INFO | train_inner | epoch 008:    708 / 1576 loss=5.267, ppl=38.51, wps=13899.4, ups=0.21, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.549, loss_scale=16, train_wall=460, gb_free=10, wall=55632
2022-02-16 22:46:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 22:46:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-16 22:51:21 | INFO | train_inner | epoch 008:    810 / 1576 loss=5.281, ppl=38.87, wps=13634.1, ups=0.21, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.558, loss_scale=8, train_wall=469, gb_free=10, wall=56112
2022-02-16 22:59:12 | INFO | train_inner | epoch 008:    910 / 1576 loss=5.288, ppl=39.06, wps=13908.8, ups=0.21, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.549, loss_scale=8, train_wall=460, gb_free=10, wall=56583
2022-02-16 23:07:03 | INFO | train_inner | epoch 008:   1010 / 1576 loss=5.294, ppl=39.22, wps=13905.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.529, loss_scale=16, train_wall=460, gb_free=10, wall=57055
2022-02-16 23:14:55 | INFO | train_inner | epoch 008:   1110 / 1576 loss=5.286, ppl=39.01, wps=13900.6, ups=0.21, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.564, loss_scale=16, train_wall=460, gb_free=10, wall=57526
2022-02-16 23:22:46 | INFO | train_inner | epoch 008:   1210 / 1576 loss=5.293, ppl=39.2, wps=13900.7, ups=0.21, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.546, loss_scale=16, train_wall=460, gb_free=10, wall=57998
2022-02-16 23:27:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-16 23:30:43 | INFO | train_inner | epoch 008:   1311 / 1576 loss=5.288, ppl=39.08, wps=13762.5, ups=0.21, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.529, loss_scale=16, train_wall=465, gb_free=10, wall=58474
2022-02-16 23:38:34 | INFO | train_inner | epoch 008:   1411 / 1576 loss=5.303, ppl=39.47, wps=13908.4, ups=0.21, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.537, loss_scale=16, train_wall=460, gb_free=10, wall=58945
2022-02-16 23:44:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-16 23:46:30 | INFO | train_inner | epoch 008:   1512 / 1576 loss=5.295, ppl=39.27, wps=13770.7, ups=0.21, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.574, loss_scale=8, train_wall=465, gb_free=10, wall=59421
2022-02-16 23:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-16 23:51:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.282 | ppl 38.9 | wps 35490.1 | wpb 1021.8 | bsz 2 | num_updates 12564 | best_loss 5.282
2022-02-16 23:51:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12564 updates
2022-02-16 23:51:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint8.pt
2022-02-16 23:51:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint8.pt
2022-02-16 23:52:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint8.pt (epoch 8 @ 12564 updates, score 5.282) (writing took 28.344566809013486 seconds)
2022-02-16 23:52:02 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-16 23:52:02 | INFO | train | epoch 008 | loss 5.275 | ppl 38.71 | wps 13790.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 12564 | lr 0.000282121 | gnorm 0.548 | loss_scale 8 | train_wall 7248 | gb_free 10 | wall 59753
2022-02-16 23:52:02 | INFO | fairseq.trainer | begin training epoch 9
2022-02-16 23:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-16 23:54:51 | INFO | train_inner | epoch 009:     36 / 1576 loss=5.248, ppl=38.01, wps=12949.6, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.548, loss_scale=8, train_wall=456, gb_free=10, wall=59923
2022-02-17 00:02:43 | INFO | train_inner | epoch 009:    136 / 1576 loss=5.169, ppl=35.97, wps=13908.1, ups=0.21, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.546, loss_scale=8, train_wall=460, gb_free=10, wall=60394
2022-02-17 00:09:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 00:10:39 | INFO | train_inner | epoch 009:    237 / 1576 loss=5.188, ppl=36.44, wps=13767.1, ups=0.21, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.544, loss_scale=8, train_wall=465, gb_free=10, wall=60870
2022-02-17 00:18:30 | INFO | train_inner | epoch 009:    337 / 1576 loss=5.197, ppl=36.68, wps=13905.2, ups=0.21, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.558, loss_scale=8, train_wall=460, gb_free=10, wall=61341
2022-02-17 00:26:21 | INFO | train_inner | epoch 009:    437 / 1576 loss=5.193, ppl=36.59, wps=13909.1, ups=0.21, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.545, loss_scale=8, train_wall=460, gb_free=10, wall=61812
2022-02-17 00:34:12 | INFO | train_inner | epoch 009:    537 / 1576 loss=5.193, ppl=36.58, wps=13903.6, ups=0.21, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.535, loss_scale=16, train_wall=460, gb_free=10, wall=62284
2022-02-17 00:42:04 | INFO | train_inner | epoch 009:    637 / 1576 loss=5.196, ppl=36.67, wps=13896.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.566, loss_scale=16, train_wall=461, gb_free=10, wall=62755
2022-02-17 00:49:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 00:50:01 | INFO | train_inner | epoch 009:    738 / 1576 loss=5.219, ppl=37.23, wps=13738.4, ups=0.21, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.564, loss_scale=8, train_wall=466, gb_free=10, wall=63232
2022-02-17 00:57:53 | INFO | train_inner | epoch 009:    838 / 1576 loss=5.217, ppl=37.19, wps=13889.4, ups=0.21, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.559, loss_scale=8, train_wall=461, gb_free=10, wall=63704
2022-02-17 01:05:45 | INFO | train_inner | epoch 009:    938 / 1576 loss=5.218, ppl=37.22, wps=13883.8, ups=0.21, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.532, loss_scale=8, train_wall=461, gb_free=10, wall=64176
2022-02-17 01:11:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 01:13:42 | INFO | train_inner | epoch 009:   1039 / 1576 loss=5.217, ppl=37.2, wps=13749.6, ups=0.21, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.546, loss_scale=8, train_wall=465, gb_free=10, wall=64653
2022-02-17 01:21:32 | INFO | train_inner | epoch 009:   1139 / 1576 loss=5.219, ppl=37.24, wps=13916.3, ups=0.21, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.543, loss_scale=8, train_wall=460, gb_free=10, wall=65124
2022-02-17 01:29:25 | INFO | train_inner | epoch 009:   1239 / 1576 loss=5.239, ppl=37.77, wps=13860.1, ups=0.21, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.557, loss_scale=8, train_wall=462, gb_free=10, wall=65597
2022-02-17 01:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 01:37:21 | INFO | train_inner | epoch 009:   1340 / 1576 loss=5.225, ppl=37.4, wps=13764.1, ups=0.21, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.549, loss_scale=8, train_wall=465, gb_free=10, wall=66073
2022-02-17 01:45:12 | INFO | train_inner | epoch 009:   1440 / 1576 loss=5.245, ppl=37.91, wps=13931.7, ups=0.21, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.536, loss_scale=8, train_wall=459, gb_free=10, wall=66543
2022-02-17 01:53:02 | INFO | train_inner | epoch 009:   1540 / 1576 loss=5.253, ppl=38.12, wps=13933.4, ups=0.21, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.572, loss_scale=8, train_wall=459, gb_free=10, wall=67013
2022-02-17 01:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 01:55:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.25 | ppl 38.06 | wps 35732 | wpb 1021.8 | bsz 2 | num_updates 14136 | best_loss 5.25
2022-02-17 01:55:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14136 updates
2022-02-17 01:55:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint9.pt
2022-02-17 01:56:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint9.pt
2022-02-17 01:56:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint9.pt (epoch 9 @ 14136 updates, score 5.25) (writing took 28.325616191141307 seconds)
2022-02-17 01:56:22 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-17 01:56:22 | INFO | train | epoch 009 | loss 5.212 | ppl 37.07 | wps 13801.5 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 14136 | lr 0.000265972 | gnorm 0.551 | loss_scale 16 | train_wall 7252 | gb_free 10 | wall 67213
2022-02-17 01:56:22 | INFO | fairseq.trainer | begin training epoch 10
2022-02-17 01:56:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 02:01:24 | INFO | train_inner | epoch 010:     64 / 1576 loss=5.148, ppl=35.45, wps=12957.2, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=14200, lr=0.000265372, gnorm=0.554, loss_scale=16, train_wall=456, gb_free=10, wall=67515
2022-02-17 02:08:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 02:09:19 | INFO | train_inner | epoch 010:    165 / 1576 loss=5.122, ppl=34.82, wps=13771.4, ups=0.21, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.55, loss_scale=8, train_wall=465, gb_free=10, wall=67991
2022-02-17 02:17:11 | INFO | train_inner | epoch 010:    265 / 1576 loss=5.126, ppl=34.91, wps=13909.7, ups=0.21, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.571, loss_scale=8, train_wall=460, gb_free=10, wall=68462
2022-02-17 02:25:02 | INFO | train_inner | epoch 010:    365 / 1576 loss=5.145, ppl=35.38, wps=13903.9, ups=0.21, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.535, loss_scale=8, train_wall=460, gb_free=10, wall=68933
2022-02-17 02:32:53 | INFO | train_inner | epoch 010:    465 / 1576 loss=5.15, ppl=35.51, wps=13902.5, ups=0.21, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.573, loss_scale=16, train_wall=460, gb_free=10, wall=69405
2022-02-17 02:40:45 | INFO | train_inner | epoch 010:    565 / 1576 loss=5.146, ppl=35.4, wps=13896.8, ups=0.21, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.563, loss_scale=16, train_wall=461, gb_free=10, wall=69876
2022-02-17 02:45:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 02:48:41 | INFO | train_inner | epoch 010:    666 / 1576 loss=5.166, ppl=35.9, wps=13767, ups=0.21, wpb=65532.3, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.58, loss_scale=8, train_wall=465, gb_free=10, wall=70352
2022-02-17 02:56:32 | INFO | train_inner | epoch 010:    766 / 1576 loss=5.156, ppl=35.65, wps=13909.6, ups=0.21, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.58, loss_scale=8, train_wall=460, gb_free=10, wall=70823
2022-02-17 03:04:23 | INFO | train_inner | epoch 010:    866 / 1576 loss=5.172, ppl=36.05, wps=13903.9, ups=0.21, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.546, loss_scale=8, train_wall=460, gb_free=10, wall=71295
2022-02-17 03:05:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 03:12:19 | INFO | train_inner | epoch 010:    967 / 1576 loss=5.182, ppl=36.31, wps=13772.3, ups=0.21, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.551, loss_scale=8, train_wall=465, gb_free=10, wall=71771
2022-02-17 03:20:11 | INFO | train_inner | epoch 010:   1067 / 1576 loss=5.17, ppl=35.99, wps=13906.9, ups=0.21, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.566, loss_scale=8, train_wall=460, gb_free=10, wall=72242
2022-02-17 03:28:02 | INFO | train_inner | epoch 010:   1167 / 1576 loss=5.176, ppl=36.14, wps=13904.4, ups=0.21, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.54, loss_scale=16, train_wall=460, gb_free=10, wall=72713
2022-02-17 03:29:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 03:35:58 | INFO | train_inner | epoch 010:   1268 / 1576 loss=5.184, ppl=36.34, wps=13770.8, ups=0.21, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.559, loss_scale=8, train_wall=465, gb_free=10, wall=73189
2022-02-17 03:43:49 | INFO | train_inner | epoch 010:   1368 / 1576 loss=5.184, ppl=36.36, wps=13904.3, ups=0.21, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.579, loss_scale=8, train_wall=460, gb_free=10, wall=73660
2022-02-17 03:50:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 03:51:45 | INFO | train_inner | epoch 010:   1469 / 1576 loss=5.182, ppl=36.29, wps=13770.3, ups=0.21, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.552, loss_scale=8, train_wall=465, gb_free=10, wall=74136
2022-02-17 03:59:36 | INFO | train_inner | epoch 010:   1569 / 1576 loss=5.188, ppl=36.45, wps=13905.6, ups=0.21, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.563, loss_scale=8, train_wall=460, gb_free=10, wall=74608
2022-02-17 04:00:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 04:00:11 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.22 | ppl 37.27 | wps 35463.6 | wpb 1021.8 | bsz 2 | num_updates 15707 | best_loss 5.22
2022-02-17 04:00:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15707 updates
2022-02-17 04:00:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint10.pt
2022-02-17 04:00:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint10.pt
2022-02-17 04:00:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint10.pt (epoch 10 @ 15707 updates, score 5.22) (writing took 28.411375952884555 seconds)
2022-02-17 04:00:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-17 04:00:40 | INFO | train | epoch 010 | loss 5.161 | ppl 35.77 | wps 13797.5 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 15707 | lr 0.000252321 | gnorm 0.56 | loss_scale 8 | train_wall 7249 | gb_free 10 | wall 74671
2022-02-17 04:00:40 | INFO | fairseq.trainer | begin training epoch 11
2022-02-17 04:00:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 04:07:58 | INFO | train_inner | epoch 011:     93 / 1576 loss=5.073, ppl=33.66, wps=12939.9, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=15800, lr=0.000251577, gnorm=0.556, loss_scale=8, train_wall=456, gb_free=10, wall=75110
2022-02-17 04:15:50 | INFO | train_inner | epoch 011:    193 / 1576 loss=5.071, ppl=33.62, wps=13902, ups=0.21, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.579, loss_scale=16, train_wall=460, gb_free=10, wall=75581
2022-02-17 04:23:41 | INFO | train_inner | epoch 011:    293 / 1576 loss=5.09, ppl=34.06, wps=13901.4, ups=0.21, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.577, loss_scale=16, train_wall=460, gb_free=10, wall=76052
2022-02-17 04:31:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-17 04:31:37 | INFO | train_inner | epoch 011:    394 / 1576 loss=5.083, ppl=33.9, wps=13769.8, ups=0.21, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.528, loss_scale=16, train_wall=465, gb_free=10, wall=76528
2022-02-17 04:31:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 04:39:33 | INFO | train_inner | epoch 011:    495 / 1576 loss=5.095, ppl=34.19, wps=13776.2, ups=0.21, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.557, loss_scale=8, train_wall=465, gb_free=10, wall=77004
2022-02-17 04:47:24 | INFO | train_inner | epoch 011:    595 / 1576 loss=5.106, ppl=34.43, wps=13911.9, ups=0.21, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.594, loss_scale=8, train_wall=460, gb_free=10, wall=77475
2022-02-17 04:55:15 | INFO | train_inner | epoch 011:    695 / 1576 loss=5.124, ppl=34.88, wps=13905.8, ups=0.21, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.567, loss_scale=16, train_wall=460, gb_free=10, wall=77947
2022-02-17 05:00:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 05:03:11 | INFO | train_inner | epoch 011:    796 / 1576 loss=5.114, ppl=34.63, wps=13766.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.569, loss_scale=8, train_wall=465, gb_free=10, wall=78423
2022-02-17 05:11:02 | INFO | train_inner | epoch 011:    896 / 1576 loss=5.139, ppl=35.24, wps=13910.2, ups=0.21, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.567, loss_scale=8, train_wall=460, gb_free=10, wall=78894
2022-02-17 05:18:54 | INFO | train_inner | epoch 011:    996 / 1576 loss=5.13, ppl=35.01, wps=13911.7, ups=0.21, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.543, loss_scale=8, train_wall=460, gb_free=10, wall=79365
2022-02-17 05:25:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 05:26:50 | INFO | train_inner | epoch 011:   1097 / 1576 loss=5.132, ppl=35.06, wps=13763.7, ups=0.21, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.58, loss_scale=8, train_wall=465, gb_free=10, wall=79841
2022-02-17 05:34:41 | INFO | train_inner | epoch 011:   1197 / 1576 loss=5.14, ppl=35.26, wps=13915.6, ups=0.21, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.572, loss_scale=8, train_wall=460, gb_free=10, wall=80312
2022-02-17 05:42:33 | INFO | train_inner | epoch 011:   1297 / 1576 loss=5.137, ppl=35.19, wps=13878.3, ups=0.21, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.544, loss_scale=8, train_wall=461, gb_free=10, wall=80784
2022-02-17 05:50:26 | INFO | train_inner | epoch 011:   1397 / 1576 loss=5.147, ppl=35.42, wps=13863.3, ups=0.21, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.556, loss_scale=16, train_wall=462, gb_free=10, wall=81257
2022-02-17 05:55:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 05:58:21 | INFO | train_inner | epoch 011:   1498 / 1576 loss=5.159, ppl=35.73, wps=13795, ups=0.21, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.582, loss_scale=8, train_wall=464, gb_free=10, wall=81732
2022-02-17 06:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 06:04:31 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.194 | ppl 36.61 | wps 34973.7 | wpb 1021.8 | bsz 2 | num_updates 17278 | best_loss 5.194
2022-02-17 06:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17278 updates
2022-02-17 06:04:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint11.pt
2022-02-17 06:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint11.pt
2022-02-17 06:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint11.pt (epoch 11 @ 17278 updates, score 5.194) (writing took 28.68244187068194 seconds)
2022-02-17 06:05:00 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-17 06:05:00 | INFO | train | epoch 011 | loss 5.118 | ppl 34.73 | wps 13793.8 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 17278 | lr 0.000240577 | gnorm 0.564 | loss_scale 8 | train_wall 7251 | gb_free 10 | wall 82131
2022-02-17 06:05:00 | INFO | fairseq.trainer | begin training epoch 12
2022-02-17 06:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 06:06:44 | INFO | train_inner | epoch 012:     22 / 1576 loss=5.126, ppl=34.93, wps=12911.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=17300, lr=0.000240424, gnorm=0.555, loss_scale=8, train_wall=457, gb_free=10, wall=82235
2022-02-17 06:14:36 | INFO | train_inner | epoch 012:    122 / 1576 loss=5.028, ppl=32.63, wps=13894.5, ups=0.21, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.586, loss_scale=8, train_wall=461, gb_free=10, wall=82707
2022-02-17 06:22:27 | INFO | train_inner | epoch 012:    222 / 1576 loss=5.037, ppl=32.82, wps=13895, ups=0.21, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.561, loss_scale=16, train_wall=461, gb_free=10, wall=83178
2022-02-17 06:30:19 | INFO | train_inner | epoch 012:    322 / 1576 loss=5.058, ppl=33.31, wps=13891.7, ups=0.21, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.563, loss_scale=16, train_wall=461, gb_free=10, wall=83650
2022-02-17 06:35:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 06:38:15 | INFO | train_inner | epoch 012:    423 / 1576 loss=5.068, ppl=33.53, wps=13772.5, ups=0.21, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.598, loss_scale=8, train_wall=465, gb_free=10, wall=84126
2022-02-17 06:46:06 | INFO | train_inner | epoch 012:    523 / 1576 loss=5.069, ppl=33.57, wps=13909.6, ups=0.21, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.568, loss_scale=8, train_wall=460, gb_free=10, wall=84597
2022-02-17 06:53:57 | INFO | train_inner | epoch 012:    623 / 1576 loss=5.073, ppl=33.65, wps=13910.1, ups=0.21, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.574, loss_scale=8, train_wall=460, gb_free=10, wall=85068
2022-02-17 06:56:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 07:01:53 | INFO | train_inner | epoch 012:    724 / 1576 loss=5.09, ppl=34.05, wps=13776.4, ups=0.21, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.563, loss_scale=8, train_wall=465, gb_free=10, wall=85544
2022-02-17 07:09:43 | INFO | train_inner | epoch 012:    824 / 1576 loss=5.096, ppl=34.19, wps=13926.4, ups=0.21, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.596, loss_scale=8, train_wall=460, gb_free=10, wall=86015
2022-02-17 07:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 07:17:39 | INFO | train_inner | epoch 012:    925 / 1576 loss=5.082, ppl=33.87, wps=13777.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.571, loss_scale=8, train_wall=465, gb_free=10, wall=86490
2022-02-17 07:25:30 | INFO | train_inner | epoch 012:   1025 / 1576 loss=5.101, ppl=34.33, wps=13909.9, ups=0.21, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.565, loss_scale=8, train_wall=460, gb_free=10, wall=86961
2022-02-17 07:33:21 | INFO | train_inner | epoch 012:   1125 / 1576 loss=5.099, ppl=34.27, wps=13911.8, ups=0.21, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.572, loss_scale=8, train_wall=460, gb_free=10, wall=87432
2022-02-17 07:41:12 | INFO | train_inner | epoch 012:   1225 / 1576 loss=5.098, ppl=34.25, wps=13912, ups=0.21, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.556, loss_scale=16, train_wall=460, gb_free=10, wall=87904
2022-02-17 07:42:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 07:49:08 | INFO | train_inner | epoch 012:   1326 / 1576 loss=5.107, ppl=34.47, wps=13777.7, ups=0.21, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.577, loss_scale=8, train_wall=465, gb_free=10, wall=88379
2022-02-17 07:56:59 | INFO | train_inner | epoch 012:   1426 / 1576 loss=5.105, ppl=34.42, wps=13913.2, ups=0.21, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.558, loss_scale=8, train_wall=460, gb_free=10, wall=88850
2022-02-17 08:04:50 | INFO | train_inner | epoch 012:   1526 / 1576 loss=5.113, ppl=34.6, wps=13912.6, ups=0.21, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.576, loss_scale=16, train_wall=460, gb_free=10, wall=89321
2022-02-17 08:07:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 08:08:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 08:08:48 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.182 | ppl 36.31 | wps 35715 | wpb 1021.8 | bsz 2 | num_updates 18849 | best_loss 5.182
2022-02-17 08:08:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18849 updates
2022-02-17 08:08:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint12.pt
2022-02-17 08:08:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint12.pt
2022-02-17 08:09:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint12.pt (epoch 12 @ 18849 updates, score 5.182) (writing took 28.52486235462129 seconds)
2022-02-17 08:09:16 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-17 08:09:16 | INFO | train | epoch 012 | loss 5.082 | ppl 33.86 | wps 13800.2 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 18849 | lr 0.000230333 | gnorm 0.574 | loss_scale 8 | train_wall 7248 | gb_free 10 | wall 89587
2022-02-17 08:09:16 | INFO | fairseq.trainer | begin training epoch 13
2022-02-17 08:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 08:13:16 | INFO | train_inner | epoch 013:     51 / 1576 loss=5.053, ppl=33.19, wps=12833.3, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=18900, lr=0.000230022, gnorm=0.601, loss_scale=8, train_wall=460, gb_free=10, wall=89827
2022-02-17 08:21:07 | INFO | train_inner | epoch 013:    151 / 1576 loss=5.003, ppl=32.06, wps=13909.5, ups=0.21, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.571, loss_scale=8, train_wall=460, gb_free=10, wall=90299
2022-02-17 08:28:59 | INFO | train_inner | epoch 013:    251 / 1576 loss=5.023, ppl=32.51, wps=13912.1, ups=0.21, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.563, loss_scale=16, train_wall=460, gb_free=10, wall=90770
2022-02-17 08:31:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 08:36:54 | INFO | train_inner | epoch 013:    352 / 1576 loss=5.023, ppl=32.52, wps=13779.7, ups=0.21, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.594, loss_scale=8, train_wall=464, gb_free=10, wall=91245
2022-02-17 08:44:45 | INFO | train_inner | epoch 013:    452 / 1576 loss=5.037, ppl=32.84, wps=13916.7, ups=0.21, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.578, loss_scale=8, train_wall=460, gb_free=10, wall=91716
2022-02-17 08:52:36 | INFO | train_inner | epoch 013:    552 / 1576 loss=5.034, ppl=32.76, wps=13907.5, ups=0.21, wpb=65532.3, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.58, loss_scale=16, train_wall=460, gb_free=10, wall=92187
2022-02-17 09:00:27 | INFO | train_inner | epoch 013:    652 / 1576 loss=5.039, ppl=32.87, wps=13907.7, ups=0.21, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.571, loss_scale=16, train_wall=460, gb_free=10, wall=92659
2022-02-17 09:08:19 | INFO | train_inner | epoch 013:    752 / 1576 loss=5.051, ppl=33.16, wps=13908.4, ups=0.21, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.56, loss_scale=16, train_wall=460, gb_free=10, wall=93130
2022-02-17 09:11:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-17 09:12:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 09:16:19 | INFO | train_inner | epoch 013:    854 / 1576 loss=5.059, ppl=33.34, wps=13641.4, ups=0.21, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.594, loss_scale=8, train_wall=469, gb_free=10, wall=93610
2022-02-17 09:24:10 | INFO | train_inner | epoch 013:    954 / 1576 loss=5.057, ppl=33.28, wps=13910.8, ups=0.21, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.559, loss_scale=8, train_wall=460, gb_free=10, wall=94081
2022-02-17 09:32:01 | INFO | train_inner | epoch 013:   1054 / 1576 loss=5.074, ppl=33.68, wps=13910.5, ups=0.21, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.584, loss_scale=8, train_wall=460, gb_free=10, wall=94553
2022-02-17 09:39:53 | INFO | train_inner | epoch 013:   1154 / 1576 loss=5.077, ppl=33.75, wps=13905.5, ups=0.21, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.552, loss_scale=16, train_wall=460, gb_free=10, wall=95024
2022-02-17 09:47:44 | INFO | train_inner | epoch 013:   1254 / 1576 loss=5.086, ppl=33.96, wps=13900, ups=0.21, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.574, loss_scale=16, train_wall=461, gb_free=10, wall=95495
2022-02-17 09:52:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-17 09:55:40 | INFO | train_inner | epoch 013:   1355 / 1576 loss=5.075, ppl=33.72, wps=13765.9, ups=0.21, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.57, loss_scale=16, train_wall=465, gb_free=10, wall=95971
2022-02-17 10:03:31 | INFO | train_inner | epoch 013:   1455 / 1576 loss=5.07, ppl=33.6, wps=13905.8, ups=0.21, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.589, loss_scale=16, train_wall=460, gb_free=10, wall=96443
2022-02-17 10:11:23 | INFO | train_inner | epoch 013:   1555 / 1576 loss=5.079, ppl=33.81, wps=13904.2, ups=0.21, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.571, loss_scale=16, train_wall=460, gb_free=10, wall=96914
2022-02-17 10:12:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 10:13:04 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.164 | ppl 35.84 | wps 35505.8 | wpb 1021.8 | bsz 2 | num_updates 20421 | best_loss 5.164
2022-02-17 10:13:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20421 updates
2022-02-17 10:13:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint13.pt
2022-02-17 10:13:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint13.pt
2022-02-17 10:13:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint13.pt (epoch 13 @ 20421 updates, score 5.164) (writing took 28.44288717303425 seconds)
2022-02-17 10:13:32 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-17 10:13:32 | INFO | train | epoch 013 | loss 5.051 | ppl 33.15 | wps 13809.3 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 20421 | lr 0.00022129 | gnorm 0.574 | loss_scale 32 | train_wall 7248 | gb_free 10 | wall 97044
2022-02-17 10:13:32 | INFO | fairseq.trainer | begin training epoch 14
2022-02-17 10:13:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 10:13:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-17 10:19:49 | INFO | train_inner | epoch 014:     80 / 1576 loss=4.99, ppl=31.77, wps=12823.1, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=20500, lr=0.000220863, gnorm=0.572, loss_scale=16, train_wall=461, gb_free=10, wall=97421
2022-02-17 10:25:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 10:27:45 | INFO | train_inner | epoch 014:    181 / 1576 loss=4.965, ppl=31.23, wps=13775.8, ups=0.21, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.587, loss_scale=8, train_wall=465, gb_free=10, wall=97896
2022-02-17 10:35:37 | INFO | train_inner | epoch 014:    281 / 1576 loss=4.993, ppl=31.85, wps=13881.1, ups=0.21, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.576, loss_scale=8, train_wall=461, gb_free=10, wall=98368
2022-02-17 10:43:29 | INFO | train_inner | epoch 014:    381 / 1576 loss=4.991, ppl=31.81, wps=13887.6, ups=0.21, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.549, loss_scale=8, train_wall=461, gb_free=10, wall=98840
2022-02-17 10:50:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 10:51:27 | INFO | train_inner | epoch 014:    482 / 1576 loss=5.01, ppl=32.22, wps=13720.1, ups=0.21, wpb=65532.3, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.587, loss_scale=8, train_wall=466, gb_free=10, wall=99318
2022-02-17 10:59:20 | INFO | train_inner | epoch 014:    582 / 1576 loss=5.013, ppl=32.28, wps=13860.4, ups=0.21, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.578, loss_scale=8, train_wall=462, gb_free=10, wall=99791
2022-02-17 11:07:12 | INFO | train_inner | epoch 014:    682 / 1576 loss=5.021, ppl=32.47, wps=13864.9, ups=0.21, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.577, loss_scale=8, train_wall=462, gb_free=10, wall=100264
2022-02-17 11:12:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 11:15:10 | INFO | train_inner | epoch 014:    783 / 1576 loss=5.036, ppl=32.81, wps=13726.1, ups=0.21, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.573, loss_scale=8, train_wall=466, gb_free=10, wall=100741
2022-02-17 11:23:02 | INFO | train_inner | epoch 014:    883 / 1576 loss=5.043, ppl=32.97, wps=13872.5, ups=0.21, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.598, loss_scale=8, train_wall=461, gb_free=10, wall=101213
2022-02-17 11:30:53 | INFO | train_inner | epoch 014:    983 / 1576 loss=5.034, ppl=32.77, wps=13912.1, ups=0.21, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.592, loss_scale=8, train_wall=460, gb_free=10, wall=101684
2022-02-17 11:32:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 11:38:49 | INFO | train_inner | epoch 014:   1084 / 1576 loss=5.036, ppl=32.8, wps=13770.6, ups=0.21, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.592, loss_scale=8, train_wall=465, gb_free=10, wall=102160
2022-02-17 11:46:40 | INFO | train_inner | epoch 014:   1184 / 1576 loss=5.05, ppl=33.12, wps=13910.1, ups=0.21, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.571, loss_scale=8, train_wall=460, gb_free=10, wall=102632
2022-02-17 11:54:32 | INFO | train_inner | epoch 014:   1284 / 1576 loss=5.051, ppl=33.15, wps=13908, ups=0.21, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.611, loss_scale=16, train_wall=460, gb_free=10, wall=103103
2022-02-17 12:02:23 | INFO | train_inner | epoch 014:   1384 / 1576 loss=5.054, ppl=33.21, wps=13901.3, ups=0.21, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.558, loss_scale=16, train_wall=460, gb_free=10, wall=103574
2022-02-17 12:10:14 | INFO | train_inner | epoch 014:   1484 / 1576 loss=5.054, ppl=33.23, wps=13904.5, ups=0.21, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.587, loss_scale=16, train_wall=460, gb_free=10, wall=104046
2022-02-17 12:13:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-17 12:15:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 12:17:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 12:17:30 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.145 | ppl 35.39 | wps 35347 | wpb 1021.8 | bsz 2 | num_updates 21990 | best_loss 5.145
2022-02-17 12:17:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21990 updates
2022-02-17 12:17:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint14.pt
2022-02-17 12:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint14.pt
2022-02-17 12:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint14.pt (epoch 14 @ 21990 updates, score 5.145) (writing took 28.36775918956846 seconds)
2022-02-17 12:17:58 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-17 12:17:58 | INFO | train | epoch 014 | loss 5.024 | ppl 32.54 | wps 13764.7 | ups 0.21 | wpb 65499.2 | bsz 127.9 | num_updates 21990 | lr 0.000213249 | gnorm 0.583 | loss_scale 8 | train_wall 7257 | gb_free 10 | wall 104510
2022-02-17 12:17:59 | INFO | fairseq.trainer | begin training epoch 15
2022-02-17 12:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 12:18:46 | INFO | train_inner | epoch 015:     10 / 1576 loss=5.044, ppl=33, wps=12703.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=22000, lr=0.000213201, gnorm=0.613, loss_scale=8, train_wall=465, gb_free=10, wall=104557
2022-02-17 12:26:37 | INFO | train_inner | epoch 015:    110 / 1576 loss=4.935, ppl=30.59, wps=13904.6, ups=0.21, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.578, loss_scale=8, train_wall=460, gb_free=10, wall=105028
2022-02-17 12:34:28 | INFO | train_inner | epoch 015:    210 / 1576 loss=4.951, ppl=30.93, wps=13904.5, ups=0.21, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.59, loss_scale=8, train_wall=460, gb_free=10, wall=105500
2022-02-17 12:41:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 12:42:24 | INFO | train_inner | epoch 015:    311 / 1576 loss=4.967, ppl=31.28, wps=13772.5, ups=0.21, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.569, loss_scale=8, train_wall=465, gb_free=10, wall=105975
2022-02-17 12:50:15 | INFO | train_inner | epoch 015:    411 / 1576 loss=4.973, ppl=31.41, wps=13909.9, ups=0.21, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.578, loss_scale=8, train_wall=460, gb_free=10, wall=106447
2022-02-17 12:58:06 | INFO | train_inner | epoch 015:    511 / 1576 loss=4.995, ppl=31.89, wps=13911.2, ups=0.21, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.586, loss_scale=8, train_wall=460, gb_free=10, wall=106918
2022-02-17 13:05:58 | INFO | train_inner | epoch 015:    611 / 1576 loss=5.009, ppl=32.2, wps=13906.7, ups=0.21, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.601, loss_scale=16, train_wall=460, gb_free=10, wall=107389
2022-02-17 13:13:49 | INFO | train_inner | epoch 015:    711 / 1576 loss=4.991, ppl=31.8, wps=13900.4, ups=0.21, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.581, loss_scale=16, train_wall=460, gb_free=10, wall=107860
2022-02-17 13:21:40 | INFO | train_inner | epoch 015:    811 / 1576 loss=5.003, ppl=32.07, wps=13904.1, ups=0.21, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.597, loss_scale=16, train_wall=460, gb_free=10, wall=108332
2022-02-17 13:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-17 13:22:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 13:29:41 | INFO | train_inner | epoch 015:    913 / 1576 loss=5.018, ppl=32.41, wps=13641.8, ups=0.21, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.577, loss_scale=8, train_wall=469, gb_free=10, wall=108812
2022-02-17 13:37:32 | INFO | train_inner | epoch 015:   1013 / 1576 loss=5.009, ppl=32.2, wps=13915.1, ups=0.21, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.586, loss_scale=8, train_wall=460, gb_free=10, wall=109283
2022-02-17 13:45:23 | INFO | train_inner | epoch 015:   1113 / 1576 loss=5.02, ppl=32.44, wps=13910.6, ups=0.21, wpb=65532.3, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.586, loss_scale=16, train_wall=460, gb_free=10, wall=109754
2022-02-17 13:46:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 13:53:19 | INFO | train_inner | epoch 015:   1214 / 1576 loss=5.021, ppl=32.46, wps=13771.6, ups=0.21, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.577, loss_scale=8, train_wall=465, gb_free=10, wall=110230
2022-02-17 14:01:10 | INFO | train_inner | epoch 015:   1314 / 1576 loss=5.03, ppl=32.67, wps=13914.2, ups=0.21, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.576, loss_scale=8, train_wall=460, gb_free=10, wall=110701
2022-02-17 14:09:01 | INFO | train_inner | epoch 015:   1414 / 1576 loss=5.037, ppl=32.83, wps=13910.1, ups=0.21, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.598, loss_scale=16, train_wall=460, gb_free=10, wall=111172
2022-02-17 14:14:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 14:16:57 | INFO | train_inner | epoch 015:   1515 / 1576 loss=5.033, ppl=32.74, wps=13775.8, ups=0.21, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.597, loss_scale=8, train_wall=465, gb_free=10, wall=111648
2022-02-17 14:21:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 14:21:46 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.136 | ppl 35.16 | wps 35752.2 | wpb 1021.8 | bsz 2 | num_updates 23561 | best_loss 5.136
2022-02-17 14:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23561 updates
2022-02-17 14:21:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint15.pt
2022-02-17 14:21:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint15.pt
2022-02-17 14:22:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint15.pt (epoch 15 @ 23561 updates, score 5.136) (writing took 28.589223633520305 seconds)
2022-02-17 14:22:14 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-17 14:22:15 | INFO | train | epoch 015 | loss 5 | ppl 32.01 | wps 13800.7 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 23561 | lr 0.000206017 | gnorm 0.586 | loss_scale 8 | train_wall 7248 | gb_free 10 | wall 111966
2022-02-17 14:22:15 | INFO | fairseq.trainer | begin training epoch 16
2022-02-17 14:22:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 14:25:18 | INFO | train_inner | epoch 016:     39 / 1576 loss=4.985, ppl=31.67, wps=12956, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=23600, lr=0.000205847, gnorm=0.583, loss_scale=8, train_wall=456, gb_free=10, wall=112149
2022-02-17 14:33:09 | INFO | train_inner | epoch 016:    139 / 1576 loss=4.922, ppl=30.31, wps=13913.4, ups=0.21, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.625, loss_scale=8, train_wall=460, gb_free=10, wall=112620
2022-02-17 14:38:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 14:41:05 | INFO | train_inner | epoch 016:    240 / 1576 loss=4.926, ppl=30.39, wps=13777.1, ups=0.21, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.569, loss_scale=8, train_wall=465, gb_free=10, wall=113096
2022-02-17 14:48:56 | INFO | train_inner | epoch 016:    340 / 1576 loss=4.955, ppl=31.02, wps=13911.8, ups=0.21, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.594, loss_scale=8, train_wall=460, gb_free=10, wall=113567
2022-02-17 14:56:47 | INFO | train_inner | epoch 016:    440 / 1576 loss=4.949, ppl=30.9, wps=13918.3, ups=0.21, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.587, loss_scale=8, train_wall=460, gb_free=10, wall=114038
2022-02-17 15:00:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 15:04:43 | INFO | train_inner | epoch 016:    541 / 1576 loss=4.96, ppl=31.13, wps=13775.1, ups=0.21, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.601, loss_scale=8, train_wall=465, gb_free=10, wall=114514
2022-02-17 15:12:34 | INFO | train_inner | epoch 016:    641 / 1576 loss=4.972, ppl=31.39, wps=13911.8, ups=0.21, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.605, loss_scale=8, train_wall=460, gb_free=10, wall=114985
2022-02-17 15:20:25 | INFO | train_inner | epoch 016:    741 / 1576 loss=4.985, ppl=31.67, wps=13911.7, ups=0.21, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.587, loss_scale=8, train_wall=460, gb_free=10, wall=115456
2022-02-17 15:21:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 15:28:22 | INFO | train_inner | epoch 016:    842 / 1576 loss=4.983, ppl=31.62, wps=13736.9, ups=0.21, wpb=65532.3, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.617, loss_scale=8, train_wall=466, gb_free=10, wall=115933
2022-02-17 15:36:15 | INFO | train_inner | epoch 016:    942 / 1576 loss=5.003, ppl=32.06, wps=13862.2, ups=0.21, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.577, loss_scale=8, train_wall=462, gb_free=10, wall=116406
2022-02-17 15:41:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 15:44:12 | INFO | train_inner | epoch 016:   1043 / 1576 loss=5.002, ppl=32.05, wps=13738.1, ups=0.21, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.637, loss_scale=8, train_wall=466, gb_free=10, wall=116883
2022-02-17 15:52:04 | INFO | train_inner | epoch 016:   1143 / 1576 loss=5.007, ppl=32.15, wps=13879.9, ups=0.21, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.585, loss_scale=8, train_wall=461, gb_free=10, wall=117355
2022-02-17 15:59:56 | INFO | train_inner | epoch 016:   1243 / 1576 loss=5.003, ppl=32.08, wps=13875.2, ups=0.21, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.625, loss_scale=8, train_wall=461, gb_free=10, wall=117827
2022-02-17 16:03:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 16:07:53 | INFO | train_inner | epoch 016:   1344 / 1576 loss=5.012, ppl=32.26, wps=13729.3, ups=0.21, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.589, loss_scale=8, train_wall=466, gb_free=10, wall=118305
2022-02-17 16:15:45 | INFO | train_inner | epoch 016:   1444 / 1576 loss=5.013, ppl=32.29, wps=13889.7, ups=0.21, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.592, loss_scale=8, train_wall=461, gb_free=10, wall=118776
2022-02-17 16:23:36 | INFO | train_inner | epoch 016:   1544 / 1576 loss=5.023, ppl=32.5, wps=13924.2, ups=0.21, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.598, loss_scale=8, train_wall=460, gb_free=10, wall=119247
2022-02-17 16:26:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 16:26:09 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.13 | ppl 35.02 | wps 35659.7 | wpb 1021.8 | bsz 2 | num_updates 25132 | best_loss 5.13
2022-02-17 16:26:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 25132 updates
2022-02-17 16:26:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint16.pt
2022-02-17 16:26:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint16.pt
2022-02-17 16:26:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint16.pt (epoch 16 @ 25132 updates, score 5.13) (writing took 28.700119994580746 seconds)
2022-02-17 16:26:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-17 16:26:37 | INFO | train | epoch 016 | loss 4.98 | ppl 31.55 | wps 13788.1 | ups 0.21 | wpb 65499.3 | bsz 127.9 | num_updates 25132 | lr 0.000199474 | gnorm 0.598 | loss_scale 16 | train_wall 7254 | gb_free 10 | wall 119429
2022-02-17 16:26:38 | INFO | fairseq.trainer | begin training epoch 17
2022-02-17 16:26:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-17 16:26:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 16:32:02 | INFO | train_inner | epoch 017:     69 / 1576 loss=4.928, ppl=30.44, wps=12831.8, ups=0.2, wpb=64962.6, bsz=126.9, num_updates=25200, lr=0.000199205, gnorm=0.583, loss_scale=8, train_wall=460, gb_free=10, wall=119753
2022-02-17 16:39:53 | INFO | train_inner | epoch 017:    169 / 1576 loss=4.899, ppl=29.84, wps=13917.8, ups=0.21, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.601, loss_scale=8, train_wall=460, gb_free=10, wall=120224
2022-02-17 16:47:44 | INFO | train_inner | epoch 017:    269 / 1576 loss=4.924, ppl=30.36, wps=13918.3, ups=0.21, wpb=65532.3, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.566, loss_scale=16, train_wall=460, gb_free=10, wall=120695
2022-02-17 16:49:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 16:55:39 | INFO | train_inner | epoch 017:    370 / 1576 loss=4.942, ppl=30.73, wps=13785.5, ups=0.21, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.625, loss_scale=8, train_wall=464, gb_free=10, wall=121170
2022-02-17 17:03:30 | INFO | train_inner | epoch 017:    470 / 1576 loss=4.945, ppl=30.8, wps=13920.9, ups=0.21, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.607, loss_scale=8, train_wall=460, gb_free=10, wall=121641
2022-02-17 17:10:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 17:11:25 | INFO | train_inner | epoch 017:    571 / 1576 loss=4.947, ppl=30.85, wps=13790.3, ups=0.21, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.589, loss_scale=8, train_wall=464, gb_free=10, wall=122116
2022-02-17 17:19:16 | INFO | train_inner | epoch 017:    671 / 1576 loss=4.954, ppl=31, wps=13927.8, ups=0.21, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.604, loss_scale=8, train_wall=460, gb_free=10, wall=122587
2022-02-17 17:27:06 | INFO | train_inner | epoch 017:    771 / 1576 loss=4.948, ppl=30.87, wps=13942.6, ups=0.21, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.604, loss_scale=8, train_wall=459, gb_free=10, wall=123057
2022-02-17 17:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 17:35:01 | INFO | train_inner | epoch 017:    872 / 1576 loss=4.971, ppl=31.35, wps=13800.2, ups=0.21, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.602, loss_scale=8, train_wall=464, gb_free=10, wall=123532
2022-02-17 17:42:51 | INFO | train_inner | epoch 017:    972 / 1576 loss=4.979, ppl=31.54, wps=13937, ups=0.21, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.607, loss_scale=8, train_wall=459, gb_free=10, wall=124002
2022-02-17 17:50:41 | INFO | train_inner | epoch 017:   1072 / 1576 loss=4.99, ppl=31.77, wps=13936.6, ups=0.21, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.602, loss_scale=16, train_wall=459, gb_free=10, wall=124472
2022-02-17 17:51:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 17:58:36 | INFO | train_inner | epoch 017:   1173 / 1576 loss=4.979, ppl=31.54, wps=13802.8, ups=0.21, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.595, loss_scale=8, train_wall=464, gb_free=10, wall=124947
2022-02-17 18:06:26 | INFO | train_inner | epoch 017:   1273 / 1576 loss=4.984, ppl=31.64, wps=13933.3, ups=0.21, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.582, loss_scale=8, train_wall=459, gb_free=10, wall=125418
2022-02-17 18:12:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-17 18:14:21 | INFO | train_inner | epoch 017:   1374 / 1576 loss=4.996, ppl=31.91, wps=13793.8, ups=0.21, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.595, loss_scale=8, train_wall=464, gb_free=10, wall=125893
2022-02-17 18:22:12 | INFO | train_inner | epoch 017:   1474 / 1576 loss=4.998, ppl=31.95, wps=13932.7, ups=0.21, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.617, loss_scale=8, train_wall=459, gb_free=10, wall=126363
2022-02-17 18:30:02 | INFO | train_inner | epoch 017:   1574 / 1576 loss=5.003, ppl=32.07, wps=13932, ups=0.21, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.609, loss_scale=8, train_wall=459, gb_free=10, wall=126833
2022-02-17 18:30:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-17 18:30:14 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.117 | ppl 34.69 | wps 35467.1 | wpb 1021.8 | bsz 2 | num_updates 26702 | best_loss 5.117
2022-02-17 18:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26702 updates
2022-02-17 18:30:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4/checkpoint17.pt
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 33472 vs 33360
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x2b313692a6a7 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1c99500 (0x2b309d8c3500 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1c956d3 (0x2b309d8bf6d3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x2b309d8c4609 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x2b309d8c5141 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x2b309d8c5935 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x6543f3 (0x2b309ad953f3 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x2c2c60 (0x2b309aa03c60 in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x2c3dce (0x2b309aa04dce in /cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/lib/libtorch_python.so)
<omitting python frames>
frame #46: __libc_start_main + 0xf5 (0x2b308a4fd555 in /lib64/libc.so.6)
frame #47: /cluster/home/andriusb/fq/fairseq/envapex/bin/python() [0x40071e]

/cluster/shadow/.lsbatch/1644985648.205309586: line 8: 86360 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-cross_entropy_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 6658484 --fp16 --max-update 50000
