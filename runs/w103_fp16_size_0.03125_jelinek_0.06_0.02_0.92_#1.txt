Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 207346208: <w103_fp16_size_0.03125_jelinek_0.06_0.02_0.92_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.03125_jelinek_0.06_0.02_0.92_#1> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 13:27:36 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Sun Mar  6 14:07:22 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 14:07:22 2022
Terminated at Sun Mar  6 20:38:52 2022
Results reported at Sun Mar  6 20:38:52 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.03125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.06, 0.02, 0.92)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --no-epoch-checkpoints --no-last-checkpoints --seed 66575621 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   23466.34 sec.
    Max Memory :                                 5601 MB
    Average Memory :                             3029.77 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14399.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   23490 sec.
    Turnaround time :                            25876 sec.

The output (if any) follows:

2022-03-06 14:07:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575621, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.03125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575621, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.06, 0.02, 0.92)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 14:07:30 | INFO | fairseq.tasks.language_modeling | dictionary: 96056 types
2022-03-06 14:07:32 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
Calculating frequency stats:
  0%|          | 0/56292 [00:00<?, ?it/s]  1%|          | 646/56292 [00:00<00:08, 6454.45it/s]  2%|▏         | 1292/56292 [00:00<00:09, 5779.87it/s]  3%|▎         | 1875/56292 [00:00<00:09, 5701.26it/s]  4%|▍         | 2448/56292 [00:00<00:09, 5524.77it/s]  6%|▌         | 3181/56292 [00:00<00:08, 6139.83it/s]  7%|▋         | 3800/56292 [00:00<00:08, 5939.27it/s]  8%|▊         | 4510/56292 [00:00<00:08, 6286.72it/s]  9%|▉         | 5220/56292 [00:00<00:07, 6533.12it/s] 10%|█         | 5910/56292 [00:00<00:07, 6634.45it/s] 12%|█▏        | 6577/56292 [00:01<00:08, 6033.12it/s] 13%|█▎        | 7192/56292 [00:01<00:08, 6025.83it/s] 14%|█▍        | 7803/56292 [00:01<00:08, 6000.06it/s] 15%|█▍        | 8409/56292 [00:01<00:08, 5869.27it/s] 16%|█▌        | 9008/56292 [00:01<00:08, 5903.68it/s] 17%|█▋        | 9602/56292 [00:01<00:08, 5816.57it/s] 18%|█▊        | 10279/56292 [00:01<00:07, 6086.83it/s] 19%|█▉        | 10891/56292 [00:01<00:07, 5839.16it/s] 20%|██        | 11479/56292 [00:01<00:07, 5836.32it/s] 22%|██▏       | 12113/56292 [00:02<00:07, 5978.80it/s] 23%|██▎       | 12714/56292 [00:02<00:07, 5790.72it/s] 24%|██▎       | 13336/56292 [00:02<00:07, 5911.63it/s] 25%|██▍       | 13993/56292 [00:02<00:06, 6087.31it/s] 26%|██▌       | 14604/56292 [00:02<00:06, 5999.47it/s] 27%|██▋       | 15288/56292 [00:02<00:06, 6243.82it/s] 28%|██▊       | 15915/56292 [00:02<00:06, 5930.98it/s] 29%|██▉       | 16513/56292 [00:02<00:06, 5824.25it/s] 30%|███       | 17125/56292 [00:02<00:06, 5908.13it/s] 31%|███▏      | 17719/56292 [00:02<00:06, 5880.69it/s] 33%|███▎      | 18309/56292 [00:03<00:06, 5866.95it/s] 34%|███▍      | 19018/56292 [00:03<00:05, 6224.62it/s] 35%|███▍      | 19678/56292 [00:03<00:05, 6333.25it/s] 36%|███▌      | 20313/56292 [00:03<00:06, 5959.01it/s] 37%|███▋      | 20943/56292 [00:03<00:05, 6050.05it/s] 38%|███▊      | 21553/56292 [00:03<00:06, 5738.95it/s] 39%|███▉      | 22166/56292 [00:03<00:05, 5848.18it/s] 41%|████      | 22847/56292 [00:03<00:05, 6121.83it/s] 42%|████▏     | 23465/56292 [00:03<00:05, 6132.71it/s] 43%|████▎     | 24209/56292 [00:04<00:04, 6509.11it/s] 44%|████▍     | 24888/56292 [00:04<00:04, 6589.68it/s] 45%|████▌     | 25566/56292 [00:04<00:04, 6633.54it/s] 47%|████▋     | 26231/56292 [00:04<00:04, 6265.20it/s] 48%|████▊     | 26863/56292 [00:04<00:04, 5914.76it/s] 49%|████▉     | 27462/56292 [00:04<00:05, 5703.52it/s] 50%|████▉     | 28072/56292 [00:04<00:04, 5805.62it/s] 51%|█████     | 28794/56292 [00:04<00:04, 6200.74it/s] 52%|█████▏    | 29420/56292 [00:04<00:04, 5852.36it/s] 53%|█████▎    | 30090/56292 [00:04<00:04, 6081.49it/s] 55%|█████▍    | 30705/56292 [00:05<00:04, 5705.81it/s] 56%|█████▌    | 31284/56292 [00:05<00:04, 5638.67it/s] 57%|█████▋    | 31882/56292 [00:05<00:04, 5728.49it/s] 58%|█████▊    | 32460/56292 [00:05<00:04, 5689.97it/s] 59%|█████▊    | 33032/56292 [00:05<00:04, 5580.50it/s] 60%|█████▉    | 33610/56292 [00:05<00:04, 5631.64it/s] 61%|██████    | 34194/56292 [00:05<00:03, 5687.63it/s] 62%|██████▏   | 34904/56292 [00:05<00:03, 6096.56it/s] 63%|██████▎   | 35516/56292 [00:05<00:03, 5846.21it/s] 64%|██████▍   | 36118/56292 [00:06<00:03, 5893.59it/s] 65%|██████▌   | 36728/56292 [00:06<00:03, 5948.43it/s] 66%|██████▋   | 37325/56292 [00:06<00:03, 5665.10it/s] 67%|██████▋   | 37896/56292 [00:06<00:03, 5578.83it/s] 68%|██████▊   | 38480/56292 [00:06<00:03, 5649.30it/s] 69%|██████▉   | 39054/56292 [00:06<00:03, 5666.81it/s] 70%|███████   | 39648/56292 [00:06<00:02, 5739.13it/s] 72%|███████▏  | 40267/56292 [00:06<00:02, 5866.02it/s] 73%|███████▎  | 40924/56292 [00:06<00:02, 6068.72it/s] 74%|███████▍  | 41532/56292 [00:06<00:02, 5851.75it/s] 75%|███████▍  | 42120/56292 [00:07<00:02, 5583.65it/s] 76%|███████▌  | 42682/56292 [00:07<00:02, 5542.96it/s] 77%|███████▋  | 43239/56292 [00:07<00:02, 5501.19it/s] 78%|███████▊  | 43910/56292 [00:07<00:02, 5843.25it/s] 79%|███████▉  | 44525/56292 [00:07<00:01, 5931.59it/s] 80%|████████  | 45121/56292 [00:07<00:01, 5842.24it/s] 81%|████████▏ | 45743/56292 [00:07<00:01, 5949.21it/s] 83%|████████▎ | 46464/56292 [00:07<00:01, 6318.71it/s] 84%|████████▎ | 47098/56292 [00:07<00:01, 6223.23it/s] 85%|████████▌ | 48053/56292 [00:08<00:01, 7199.89it/s] 87%|████████▋ | 48776/56292 [00:08<00:01, 6980.61it/s] 88%|████████▊ | 49518/56292 [00:08<00:00, 7107.40it/s] 89%|████████▉ | 50232/56292 [00:08<00:00, 6487.74it/s] 90%|█████████ | 50893/56292 [00:08<00:00, 6151.84it/s] 92%|█████████▏| 51519/56292 [00:08<00:00, 6055.90it/s] 93%|█████████▎| 52386/56292 [00:08<00:00, 6775.12it/s] 94%|█████████▍| 53075/56292 [00:08<00:00, 6571.00it/s] 95%|█████████▌| 53741/56292 [00:08<00:00, 6393.16it/s] 97%|█████████▋| 54387/56292 [00:09<00:00, 6000.25it/s] 98%|█████████▊| 54995/56292 [00:09<00:00, 5925.68it/s] 99%|█████████▉| 55683/56292 [00:09<00:00, 6189.37it/s]100%|██████████| 56292/56292 [00:09<00:00, 6021.47it/s]

gathering stats for n=1
  0%|          | 0/56292 [00:00<?, ?it/s]  3%|▎         | 1892/56292 [00:00<00:02, 18916.18it/s]  7%|▋         | 3841/56292 [00:00<00:02, 19252.52it/s] 11%|█         | 6118/56292 [00:00<00:02, 20853.65it/s] 15%|█▍        | 8204/56292 [00:00<00:02, 19912.97it/s] 18%|█▊        | 10222/56292 [00:00<00:02, 20004.36it/s] 22%|██▏       | 12227/56292 [00:00<00:02, 19716.36it/s] 25%|██▌       | 14242/56292 [00:00<00:02, 19849.90it/s] 29%|██▉       | 16230/56292 [00:00<00:02, 19827.37it/s] 32%|███▏      | 18215/56292 [00:00<00:01, 19680.37it/s] 36%|███▌      | 20291/56292 [00:01<00:01, 20007.64it/s] 40%|███▉      | 22294/56292 [00:01<00:01, 19954.83it/s] 44%|████▎     | 24572/56292 [00:01<00:01, 20796.08it/s] 47%|████▋     | 26653/56292 [00:01<00:01, 20598.18it/s] 51%|█████     | 28714/56292 [00:01<00:01, 20464.28it/s] 55%|█████▍    | 30762/56292 [00:01<00:01, 19814.16it/s] 58%|█████▊    | 32748/56292 [00:01<00:01, 19594.74it/s] 62%|██████▏   | 34711/56292 [00:01<00:01, 19543.85it/s] 65%|██████▌   | 36668/56292 [00:01<00:01, 19368.02it/s] 69%|██████▊   | 38607/56292 [00:01<00:00, 18974.88it/s] 72%|███████▏  | 40676/56292 [00:02<00:00, 19470.16it/s] 76%|███████▌  | 42626/56292 [00:02<00:00, 19138.25it/s] 79%|███████▉  | 44641/56292 [00:02<00:00, 19428.56it/s] 83%|████████▎ | 46715/56292 [00:02<00:00, 19809.90it/s] 87%|████████▋ | 49126/56292 [00:02<00:00, 21079.01it/s] 91%|█████████ | 51238/56292 [00:02<00:00, 20425.39it/s] 95%|█████████▍| 53368/56292 [00:02<00:00, 20667.46it/s] 98%|█████████▊| 55440/56292 [00:02<00:00, 20186.09it/s]100%|██████████| 56292/56292 [00:02<00:00, 19971.12it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 259.15it/s]2022-03-06 14:07:53 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(96056, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=96056, bias=False)
  )
)
2022-03-06 14:07:53 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 14:07:53 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 14:07:53 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-06 14:07:53 | INFO | fairseq_cli.train | num. shared model params: 68,094,976 (num. trained: 68,094,976)
2022-03-06 14:07:53 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 14:07:53 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.03125/valid
2022-03-06 14:07:53 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 14:07:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 14:07:53 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-06 14:07:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 14:07:53 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 14:07:53 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 14:07:53 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_last.pt
2022-03-06 14:07:53 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_last.pt
2022-03-06 14:07:53 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 14:07:53 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
2022-03-06 14:07:53 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 14:07:53 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-06 14:08:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 14:08:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:08:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 14:08:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 14:08:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 14:10:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:10:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.622 | ppl 50429.5 | wps 38767.5 | wpb 510.9 | bsz 1 | num_updates 44
2022-03-06 14:10:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 44 updates
2022-03-06 14:10:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:10:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 1 @ 44 updates, score 15.622) (writing took 2.0546247577294707 seconds)
2022-03-06 14:10:38 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 14:10:38 | INFO | train | epoch 001 | loss 16.794 | ppl 113657 | wps 21514.6 | ups 0.33 | wpb 64791.3 | bsz 126.5 | num_updates 44 | lr 5.5989e-06 | gnorm 5.129 | loss_scale 4 | train_wall 146 | gb_free 21.5 | wall 165
2022-03-06 14:10:38 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 14:10:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:13:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.092 | ppl 17457.6 | wps 39059.6 | wpb 510.9 | bsz 1 | num_updates 93 | best_loss 14.092
2022-03-06 14:13:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 93 updates
2022-03-06 14:13:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:13:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:13:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 2 @ 93 updates, score 14.092) (writing took 2.0642385333776474 seconds)
2022-03-06 14:13:05 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 14:13:05 | INFO | train | epoch 002 | loss 14.783 | ppl 28200.7 | wps 21659 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 93 | lr 1.17227e-05 | gnorm 2.224 | loss_scale 4 | train_wall 128 | gb_free 21.5 | wall 312
2022-03-06 14:13:05 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 14:13:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:13:25 | INFO | train_inner | epoch 003:      7 / 49 loss=15.621, ppl=50385.2, wps=21671.6, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=100, lr=1.25975e-05, gnorm=3.453, loss_scale=4, train_wall=292, gb_free=21.5, wall=332
2022-03-06 14:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:15:30 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.458 | ppl 11253.6 | wps 39179 | wpb 510.9 | bsz 1 | num_updates 142 | best_loss 13.458
2022-03-06 14:15:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 142 updates
2022-03-06 14:15:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 3 @ 142 updates, score 13.458) (writing took 2.090915166772902 seconds)
2022-03-06 14:15:32 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 14:15:32 | INFO | train | epoch 003 | loss 13.858 | ppl 14848.3 | wps 21663.9 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 142 | lr 1.78465e-05 | gnorm 1.436 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 458
2022-03-06 14:15:32 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 14:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:17:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:17:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.676 | ppl 6544.09 | wps 38959.7 | wpb 510.9 | bsz 1 | num_updates 191 | best_loss 12.676
2022-03-06 14:17:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 191 updates
2022-03-06 14:17:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:17:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 4 @ 191 updates, score 12.676) (writing took 2.053358164615929 seconds)
2022-03-06 14:17:58 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 14:17:58 | INFO | train | epoch 004 | loss 13.155 | ppl 9120.81 | wps 21652.5 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 191 | lr 2.39702e-05 | gnorm 1.257 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 605
2022-03-06 14:17:58 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 14:17:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:18:24 | INFO | train_inner | epoch 005:      9 / 49 loss=13.384, ppl=10689.5, wps=21678.3, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=200, lr=2.5095e-05, gnorm=1.31, loss_scale=8, train_wall=261, gb_free=21.5, wall=631
2022-03-06 14:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:20:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.893 | ppl 3803.76 | wps 39068.6 | wpb 510.9 | bsz 1 | num_updates 240 | best_loss 11.893
2022-03-06 14:20:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 240 updates
2022-03-06 14:20:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:20:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:20:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 5 @ 240 updates, score 11.893) (writing took 2.0805276380851865 seconds)
2022-03-06 14:20:25 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 14:20:25 | INFO | train | epoch 005 | loss 12.309 | ppl 5075.73 | wps 21630.2 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 240 | lr 3.0094e-05 | gnorm 0.975 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 752
2022-03-06 14:20:25 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 14:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:22:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:22:50 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.307 | ppl 2532.92 | wps 38869 | wpb 510.9 | bsz 1 | num_updates 289 | best_loss 11.307
2022-03-06 14:22:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 289 updates
2022-03-06 14:22:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:22:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:22:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 6 @ 289 updates, score 11.307) (writing took 2.0714654568582773 seconds)
2022-03-06 14:22:52 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 14:22:52 | INFO | train | epoch 006 | loss 11.582 | ppl 3065.37 | wps 21633.5 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 289 | lr 3.62178e-05 | gnorm 0.722 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 899
2022-03-06 14:22:52 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 14:22:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:23:24 | INFO | train_inner | epoch 007:     11 / 49 loss=11.804, ppl=3574.98, wps=21656.9, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=300, lr=3.75925e-05, gnorm=0.804, loss_scale=16, train_wall=261, gb_free=21.5, wall=931
2022-03-06 14:25:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:25:17 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.953 | ppl 1982.36 | wps 38993.4 | wpb 510.9 | bsz 1 | num_updates 338 | best_loss 10.953
2022-03-06 14:25:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 338 updates
2022-03-06 14:25:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 7 @ 338 updates, score 10.953) (writing took 2.0629823841154575 seconds)
2022-03-06 14:25:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 14:25:19 | INFO | train | epoch 007 | loss 11.076 | ppl 2158.18 | wps 21641.5 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 338 | lr 4.23416e-05 | gnorm 0.531 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 1046
2022-03-06 14:25:19 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 14:25:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:27:44 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.737 | ppl 1706.44 | wps 39017.3 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.737
2022-03-06 14:27:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 387 updates
2022-03-06 14:27:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:27:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 8 @ 387 updates, score 10.737) (writing took 2.0463330605998635 seconds)
2022-03-06 14:27:46 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 14:27:46 | INFO | train | epoch 008 | loss 10.781 | ppl 1759.41 | wps 21650.2 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 387 | lr 4.84653e-05 | gnorm 0.469 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1193
2022-03-06 14:27:46 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 14:27:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:28:23 | INFO | train_inner | epoch 009:     13 / 49 loss=10.859, ppl=1857.2, wps=21669, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=400, lr=5.009e-05, gnorm=0.478, loss_scale=32, train_wall=261, gb_free=21.5, wall=1230
2022-03-06 14:30:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:30:11 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.58 | ppl 1530.66 | wps 38891.2 | wpb 510.9 | bsz 1 | num_updates 436 | best_loss 10.58
2022-03-06 14:30:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 436 updates
2022-03-06 14:30:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:30:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:30:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 9 @ 436 updates, score 10.58) (writing took 2.0633740266785026 seconds)
2022-03-06 14:30:13 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 14:30:13 | INFO | train | epoch 009 | loss 10.592 | ppl 1544.01 | wps 21631.7 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 436 | lr 5.45891e-05 | gnorm 0.435 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1340
2022-03-06 14:30:13 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 14:30:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:32:38 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.435 | ppl 1384.09 | wps 38920.8 | wpb 510.9 | bsz 1 | num_updates 485 | best_loss 10.435
2022-03-06 14:32:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 485 updates
2022-03-06 14:32:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:32:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 10 @ 485 updates, score 10.435) (writing took 2.074409387074411 seconds)
2022-03-06 14:32:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 14:32:40 | INFO | train | epoch 010 | loss 10.436 | ppl 1385.12 | wps 21639 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 485 | lr 6.07129e-05 | gnorm 0.489 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1486
2022-03-06 14:32:40 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 14:32:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:33:23 | INFO | train_inner | epoch 011:     15 / 49 loss=10.467, ppl=1415.47, wps=21656.3, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=500, lr=6.25875e-05, gnorm=0.462, loss_scale=32, train_wall=261, gb_free=21.5, wall=1530
2022-03-06 14:34:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:34:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:35:05 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.301 | ppl 1261.32 | wps 38882 | wpb 510.9 | bsz 1 | num_updates 533 | best_loss 10.301
2022-03-06 14:35:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 533 updates
2022-03-06 14:35:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:35:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:35:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 11 @ 533 updates, score 10.301) (writing took 2.0485872756689787 seconds)
2022-03-06 14:35:07 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 14:35:07 | INFO | train | epoch 011 | loss 10.284 | ppl 1246.92 | wps 21188.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 533 | lr 6.67117e-05 | gnorm 0.519 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1633
2022-03-06 14:35:07 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 14:35:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:37:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:37:31 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.165 | ppl 1148.43 | wps 38750.6 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.165
2022-03-06 14:37:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 582 updates
2022-03-06 14:37:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:37:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:37:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 12 @ 582 updates, score 10.165) (writing took 2.0866972589865327 seconds)
2022-03-06 14:37:34 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 14:37:34 | INFO | train | epoch 012 | loss 10.14 | ppl 1128.22 | wps 21619.3 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 582 | lr 7.28355e-05 | gnorm 0.598 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1780
2022-03-06 14:37:34 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 14:37:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:38:25 | INFO | train_inner | epoch 013:     18 / 49 loss=10.16, ppl=1144.03, wps=21449.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=600, lr=7.5085e-05, gnorm=0.572, loss_scale=32, train_wall=264, gb_free=21.5, wall=1832
2022-03-06 14:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:39:58 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.048 | ppl 1058.91 | wps 38808.1 | wpb 510.9 | bsz 1 | num_updates 631 | best_loss 10.048
2022-03-06 14:39:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 631 updates
2022-03-06 14:39:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:40:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 13 @ 631 updates, score 10.048) (writing took 2.0734888836741447 seconds)
2022-03-06 14:40:01 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 14:40:01 | INFO | train | epoch 013 | loss 9.997 | ppl 1022.01 | wps 21626.5 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 631 | lr 7.89592e-05 | gnorm 0.588 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 1927
2022-03-06 14:40:01 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 14:40:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:40:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:42:25 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.932 | ppl 976.98 | wps 38730.2 | wpb 510.9 | bsz 1 | num_updates 679 | best_loss 9.932
2022-03-06 14:42:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 679 updates
2022-03-06 14:42:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:42:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:42:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 14 @ 679 updates, score 9.932) (writing took 2.0684284269809723 seconds)
2022-03-06 14:42:28 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 14:42:28 | INFO | train | epoch 014 | loss 9.866 | ppl 932.85 | wps 21170.4 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 679 | lr 8.4958e-05 | gnorm 0.617 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 2074
2022-03-06 14:42:28 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 14:42:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:43:28 | INFO | train_inner | epoch 015:     21 / 49 loss=9.88, ppl=941.98, wps=21441, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=700, lr=8.75825e-05, gnorm=0.655, loss_scale=32, train_wall=264, gb_free=21.5, wall=2135
2022-03-06 14:44:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:44:52 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.84 | ppl 916.81 | wps 39093.7 | wpb 510.9 | bsz 1 | num_updates 728 | best_loss 9.84
2022-03-06 14:44:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 728 updates
2022-03-06 14:44:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:44:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:44:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 15 @ 728 updates, score 9.84) (writing took 2.098136283457279 seconds)
2022-03-06 14:44:54 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 14:44:54 | INFO | train | epoch 015 | loss 9.739 | ppl 854.73 | wps 21634.2 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 728 | lr 9.10818e-05 | gnorm 0.716 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 2221
2022-03-06 14:44:54 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 14:44:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:47:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:47:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:47:19 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.748 | ppl 859.97 | wps 38866.8 | wpb 510.9 | bsz 1 | num_updates 776 | best_loss 9.748
2022-03-06 14:47:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 776 updates
2022-03-06 14:47:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:47:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:47:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 16 @ 776 updates, score 9.748) (writing took 2.0658182743936777 seconds)
2022-03-06 14:47:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 14:47:21 | INFO | train | epoch 016 | loss 9.617 | ppl 785.09 | wps 21171.4 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 776 | lr 9.70806e-05 | gnorm 0.719 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 2368
2022-03-06 14:47:21 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 14:47:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:48:30 | INFO | train_inner | epoch 017:     24 / 49 loss=9.622, ppl=787.72, wps=21440.2, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=800, lr=0.00010008, gnorm=0.73, loss_scale=32, train_wall=264, gb_free=21.5, wall=2437
2022-03-06 14:49:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:49:46 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.673 | ppl 816.28 | wps 38904.4 | wpb 510.9 | bsz 1 | num_updates 825 | best_loss 9.673
2022-03-06 14:49:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 825 updates
2022-03-06 14:49:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:49:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:49:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 17 @ 825 updates, score 9.673) (writing took 2.0850015357136726 seconds)
2022-03-06 14:49:48 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 14:49:48 | INFO | train | epoch 017 | loss 9.5 | ppl 724.29 | wps 21616.5 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 825 | lr 0.000103204 | gnorm 0.838 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 2515
2022-03-06 14:49:49 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 14:49:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:52:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:52:13 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.587 | ppl 768.99 | wps 39049.4 | wpb 510.9 | bsz 1 | num_updates 874 | best_loss 9.587
2022-03-06 14:52:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 874 updates
2022-03-06 14:52:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:52:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 18 @ 874 updates, score 9.587) (writing took 2.085152913816273 seconds)
2022-03-06 14:52:16 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 14:52:16 | INFO | train | epoch 018 | loss 9.386 | ppl 668.9 | wps 21613.1 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 874 | lr 0.000109328 | gnorm 0.819 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 2662
2022-03-06 14:52:16 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 14:52:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:53:30 | INFO | train_inner | epoch 019:     26 / 49 loss=9.383, ppl=667.87, wps=21633.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=900, lr=0.000112578, gnorm=0.818, loss_scale=32, train_wall=262, gb_free=21.5, wall=2737
2022-03-06 14:53:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:54:40 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.539 | ppl 743.9 | wps 38875.1 | wpb 510.9 | bsz 1 | num_updates 922 | best_loss 9.539
2022-03-06 14:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 922 updates
2022-03-06 14:54:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:54:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 19 @ 922 updates, score 9.539) (writing took 2.038397199474275 seconds)
2022-03-06 14:54:42 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 14:54:42 | INFO | train | epoch 019 | loss 9.276 | ppl 619.77 | wps 21183.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 922 | lr 0.000115327 | gnorm 0.821 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 2809
2022-03-06 14:54:42 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 14:54:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:57:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.456 | ppl 702.48 | wps 38984.7 | wpb 510.9 | bsz 1 | num_updates 971 | best_loss 9.456
2022-03-06 14:57:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 971 updates
2022-03-06 14:57:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:57:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:57:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 20 @ 971 updates, score 9.456) (writing took 2.069035591557622 seconds)
2022-03-06 14:57:09 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 14:57:09 | INFO | train | epoch 020 | loss 9.169 | ppl 575.76 | wps 21630.6 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 971 | lr 0.000121451 | gnorm 0.783 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 2956
2022-03-06 14:57:09 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 14:57:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:58:33 | INFO | train_inner | epoch 021:     29 / 49 loss=9.161, ppl=572.43, wps=21446.4, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=1000, lr=0.000125075, gnorm=0.839, loss_scale=32, train_wall=264, gb_free=21.5, wall=3039
2022-03-06 14:59:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:59:34 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.393 | ppl 672.51 | wps 38983.2 | wpb 510.9 | bsz 1 | num_updates 1020 | best_loss 9.393
2022-03-06 14:59:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1020 updates
2022-03-06 14:59:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:59:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 14:59:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 21 @ 1020 updates, score 9.393) (writing took 2.083426807075739 seconds)
2022-03-06 14:59:36 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 14:59:36 | INFO | train | epoch 021 | loss 9.069 | ppl 536.99 | wps 21608.1 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1020 | lr 0.000127575 | gnorm 0.906 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 3103
2022-03-06 14:59:36 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 14:59:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:00:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:01:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:02:01 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.342 | ppl 648.96 | wps 38919.4 | wpb 510.9 | bsz 1 | num_updates 1068 | best_loss 9.342
2022-03-06 15:02:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1068 updates
2022-03-06 15:02:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:02:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:02:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 22 @ 1068 updates, score 9.342) (writing took 2.036072788760066 seconds)
2022-03-06 15:02:03 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 15:02:03 | INFO | train | epoch 022 | loss 8.967 | ppl 500.37 | wps 21190.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1068 | lr 0.000133573 | gnorm 0.809 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 3250
2022-03-06 15:02:03 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 15:02:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:03:35 | INFO | train_inner | epoch 023:     32 / 49 loss=8.957, ppl=497.04, wps=21448.4, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=1100, lr=0.000137573, gnorm=0.842, loss_scale=32, train_wall=264, gb_free=21.5, wall=3342
2022-03-06 15:04:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:04:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.283 | ppl 622.78 | wps 38769.2 | wpb 510.9 | bsz 1 | num_updates 1117 | best_loss 9.283
2022-03-06 15:04:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1117 updates
2022-03-06 15:04:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:04:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:04:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 23 @ 1117 updates, score 9.283) (writing took 2.0294093806296587 seconds)
2022-03-06 15:04:30 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 15:04:30 | INFO | train | epoch 023 | loss 8.871 | ppl 468.29 | wps 21629.9 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1117 | lr 0.000139697 | gnorm 0.862 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 3397
2022-03-06 15:04:30 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 15:04:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:06:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:06:55 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.241 | ppl 605.23 | wps 38883.5 | wpb 510.9 | bsz 1 | num_updates 1165 | best_loss 9.241
2022-03-06 15:06:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1165 updates
2022-03-06 15:06:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:06:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 24 @ 1165 updates, score 9.241) (writing took 2.0357918348163366 seconds)
2022-03-06 15:06:57 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 15:06:57 | INFO | train | epoch 024 | loss 8.777 | ppl 438.65 | wps 21177.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1165 | lr 0.000145696 | gnorm 0.854 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 3544
2022-03-06 15:06:57 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 15:06:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:08:38 | INFO | train_inner | epoch 025:     35 / 49 loss=8.757, ppl=432.57, wps=21437.9, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=1200, lr=0.00015007, gnorm=0.845, loss_scale=32, train_wall=264, gb_free=21.5, wall=3645
2022-03-06 15:09:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:09:22 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.196 | ppl 586.31 | wps 38916.7 | wpb 510.9 | bsz 1 | num_updates 1214 | best_loss 9.196
2022-03-06 15:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1214 updates
2022-03-06 15:09:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:09:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:09:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 25 @ 1214 updates, score 9.196) (writing took 2.1088002929463983 seconds)
2022-03-06 15:09:24 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 15:09:24 | INFO | train | epoch 025 | loss 8.683 | ppl 411.09 | wps 21609.2 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1214 | lr 0.00015182 | gnorm 0.828 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 3691
2022-03-06 15:09:24 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 15:09:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:11:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:11:49 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.134 | ppl 561.92 | wps 38741.9 | wpb 510.9 | bsz 1 | num_updates 1263 | best_loss 9.134
2022-03-06 15:11:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1263 updates
2022-03-06 15:11:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:11:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 26 @ 1263 updates, score 9.134) (writing took 2.0868289368227124 seconds)
2022-03-06 15:11:51 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 15:11:51 | INFO | train | epoch 026 | loss 8.594 | ppl 386.3 | wps 21598.9 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1263 | lr 0.000157943 | gnorm 0.927 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 3838
2022-03-06 15:11:51 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 15:11:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:13:38 | INFO | train_inner | epoch 027:     37 / 49 loss=8.572, ppl=380.44, wps=21632.7, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=1300, lr=0.000162568, gnorm=0.87, loss_scale=64, train_wall=262, gb_free=21.5, wall=3944
2022-03-06 15:14:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:14:16 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.089 | ppl 544.75 | wps 38826.4 | wpb 510.9 | bsz 1 | num_updates 1312 | best_loss 9.089
2022-03-06 15:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1312 updates
2022-03-06 15:14:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 27 @ 1312 updates, score 9.089) (writing took 2.059106775559485 seconds)
2022-03-06 15:14:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 15:14:18 | INFO | train | epoch 027 | loss 8.496 | ppl 361.04 | wps 21624.3 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1312 | lr 0.000164067 | gnorm 0.823 | loss_scale 64 | train_wall 128 | gb_free 21.5 | wall 3985
2022-03-06 15:14:18 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 15:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:16:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:16:43 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.058 | ppl 532.83 | wps 38734.9 | wpb 510.9 | bsz 1 | num_updates 1360 | best_loss 9.058
2022-03-06 15:16:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1360 updates
2022-03-06 15:16:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:16:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:16:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 28 @ 1360 updates, score 9.058) (writing took 2.0940525894984603 seconds)
2022-03-06 15:16:45 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 15:16:45 | INFO | train | epoch 028 | loss 8.407 | ppl 339.55 | wps 21168.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1360 | lr 0.000170066 | gnorm 0.929 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 4132
2022-03-06 15:16:45 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 15:16:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:18:40 | INFO | train_inner | epoch 029:     40 / 49 loss=8.379, ppl=332.81, wps=21436.6, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=1400, lr=0.000175065, gnorm=0.903, loss_scale=32, train_wall=264, gb_free=21.5, wall=4247
2022-03-06 15:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:19:10 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.024 | ppl 520.49 | wps 38876.3 | wpb 510.9 | bsz 1 | num_updates 1409 | best_loss 9.024
2022-03-06 15:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1409 updates
2022-03-06 15:19:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:19:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 29 @ 1409 updates, score 9.024) (writing took 2.0485475435853004 seconds)
2022-03-06 15:19:12 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 15:19:12 | INFO | train | epoch 029 | loss 8.314 | ppl 318.14 | wps 21620.8 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1409 | lr 0.00017619 | gnorm 0.894 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 4279
2022-03-06 15:19:12 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 15:19:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:21:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:21:37 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.998 | ppl 511.29 | wps 38878.2 | wpb 510.9 | bsz 1 | num_updates 1458 | best_loss 8.998
2022-03-06 15:21:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1458 updates
2022-03-06 15:21:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:21:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:21:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 30 @ 1458 updates, score 8.998) (writing took 2.0790178077295423 seconds)
2022-03-06 15:21:40 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 15:21:40 | INFO | train | epoch 030 | loss 8.221 | ppl 298.47 | wps 21605.1 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1458 | lr 0.000182314 | gnorm 0.916 | loss_scale 64 | train_wall 128 | gb_free 21.5 | wall 4426
2022-03-06 15:21:40 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 15:21:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:21:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:23:43 | INFO | train_inner | epoch 031:     43 / 49 loss=8.191, ppl=292.17, wps=21436.2, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=1500, lr=0.000187563, gnorm=0.912, loss_scale=32, train_wall=264, gb_free=21.5, wall=4550
2022-03-06 15:23:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:24:04 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.969 | ppl 501.07 | wps 38767.7 | wpb 510.9 | bsz 1 | num_updates 1506 | best_loss 8.969
2022-03-06 15:24:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1506 updates
2022-03-06 15:24:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:24:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:24:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 31 @ 1506 updates, score 8.969) (writing took 2.046691712923348 seconds)
2022-03-06 15:24:06 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 15:24:06 | INFO | train | epoch 031 | loss 8.129 | ppl 279.85 | wps 21185.1 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1506 | lr 0.000188312 | gnorm 0.908 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 4573
2022-03-06 15:24:06 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 15:24:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:26:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:26:31 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.945 | ppl 492.98 | wps 38931.6 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 8.945
2022-03-06 15:26:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1555 updates
2022-03-06 15:26:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:26:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:26:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 32 @ 1555 updates, score 8.945) (writing took 2.0451931934803724 seconds)
2022-03-06 15:26:33 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 15:26:33 | INFO | train | epoch 032 | loss 8.041 | ppl 263.47 | wps 21618.7 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1555 | lr 0.000194436 | gnorm 0.955 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 4720
2022-03-06 15:26:33 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 15:26:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:28:43 | INFO | train_inner | epoch 033:     45 / 49 loss=8.002, ppl=256.35, wps=21640.7, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=1600, lr=0.00020006, gnorm=0.918, loss_scale=64, train_wall=262, gb_free=21.5, wall=4849
2022-03-06 15:28:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:28:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:28:58 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.921 | ppl 484.65 | wps 39087.2 | wpb 510.9 | bsz 1 | num_updates 1603 | best_loss 8.921
2022-03-06 15:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1603 updates
2022-03-06 15:28:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:29:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 33 @ 1603 updates, score 8.921) (writing took 2.039788962341845 seconds)
2022-03-06 15:29:00 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 15:29:00 | INFO | train | epoch 033 | loss 7.947 | ppl 246.72 | wps 21181.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1603 | lr 0.000200435 | gnorm 0.889 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 4867
2022-03-06 15:29:00 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 15:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:31:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:31:25 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.915 | ppl 482.64 | wps 38931.1 | wpb 510.9 | bsz 1 | num_updates 1652 | best_loss 8.915
2022-03-06 15:31:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1652 updates
2022-03-06 15:31:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:31:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 34 @ 1652 updates, score 8.915) (writing took 2.049716374836862 seconds)
2022-03-06 15:31:27 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 15:31:27 | INFO | train | epoch 034 | loss 7.864 | ppl 233.02 | wps 21626.7 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1652 | lr 0.000206559 | gnorm 0.983 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 5014
2022-03-06 15:31:27 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 15:31:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:33:45 | INFO | train_inner | epoch 035:     48 / 49 loss=7.821, ppl=226.17, wps=21445.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=1700, lr=0.000212558, gnorm=0.953, loss_scale=32, train_wall=264, gb_free=21.5, wall=5152
2022-03-06 15:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:33:52 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.883 | ppl 472.1 | wps 39035.8 | wpb 510.9 | bsz 1 | num_updates 1701 | best_loss 8.883
2022-03-06 15:33:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1701 updates
2022-03-06 15:33:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:33:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 35 @ 1701 updates, score 8.883) (writing took 2.0368144288659096 seconds)
2022-03-06 15:33:54 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 15:33:54 | INFO | train | epoch 035 | loss 7.772 | ppl 218.55 | wps 21624.5 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1701 | lr 0.000212682 | gnorm 0.921 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 5161
2022-03-06 15:33:54 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 15:33:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:35:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:36:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:36:19 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.878 | ppl 470.53 | wps 39109.5 | wpb 510.9 | bsz 1 | num_updates 1749 | best_loss 8.878
2022-03-06 15:36:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1749 updates
2022-03-06 15:36:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:36:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:36:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 36 @ 1749 updates, score 8.878) (writing took 2.040307126007974 seconds)
2022-03-06 15:36:21 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 15:36:21 | INFO | train | epoch 036 | loss 7.687 | ppl 206.02 | wps 21209.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1749 | lr 0.000218681 | gnorm 0.969 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 5308
2022-03-06 15:36:21 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 15:36:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:38:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:38:46 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.866 | ppl 466.56 | wps 39054.5 | wpb 510.9 | bsz 1 | num_updates 1798 | best_loss 8.866
2022-03-06 15:38:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1798 updates
2022-03-06 15:38:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:38:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:38:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 37 @ 1798 updates, score 8.866) (writing took 2.0514382300898433 seconds)
2022-03-06 15:38:48 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 15:38:48 | INFO | train | epoch 037 | loss 7.601 | ppl 194.09 | wps 21631 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1798 | lr 0.000224805 | gnorm 0.981 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 5455
2022-03-06 15:38:48 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 15:38:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:38:54 | INFO | train_inner | epoch 038:      2 / 49 loss=7.642, ppl=199.68, wps=20911.8, ups=0.32, wpb=64544.1, bsz=126.1, num_updates=1800, lr=0.000225055, gnorm=0.974, loss_scale=32, train_wall=263, gb_free=21.5, wall=5461
2022-03-06 15:41:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:41:13 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.848 | ppl 460.67 | wps 38908.2 | wpb 510.9 | bsz 1 | num_updates 1847 | best_loss 8.848
2022-03-06 15:41:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1847 updates
2022-03-06 15:41:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:41:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt
2022-03-06 15:41:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.06_0.02_0.92_#1/checkpoint_best.pt (epoch 38 @ 1847 updates, score 8.848) (writing took 2.052010854706168 seconds)
2022-03-06 15:41:15 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 15:41:15 | INFO | train | epoch 038 | loss 7.513 | ppl 182.6 | wps 21630 | ups 0.33 | wpb 64858.2 | bsz 126.7 | num_updates 1847 | lr 0.000230929 | gnorm 0.953 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 5602
2022-03-06 15:41:15 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 15:41:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:41:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:43:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:43:40 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.861 | ppl 465.04 | wps 39046.6 | wpb 510.9 | bsz 1 | num_updates 1895 | best_loss 8.848
2022-03-06 15:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1895 updates
2022-03-06 15:43:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 15:43:40 | INFO | train | epoch 039 | loss 7.427 | ppl 172.14 | wps 21462.5 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1895 | lr 0.000236928 | gnorm 0.981 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 5747
2022-03-06 15:43:40 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 15:43:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:43:54 | INFO | train_inner | epoch 040:      5 / 49 loss=7.464, ppl=176.56, wps=21586, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=1900, lr=0.000237553, gnorm=0.968, loss_scale=32, train_wall=264, gb_free=21.5, wall=5761
2022-03-06 15:45:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:46:05 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.875 | ppl 469.36 | wps 38590.9 | wpb 510.9 | bsz 1 | num_updates 1944 | best_loss 8.848
2022-03-06 15:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1944 updates
2022-03-06 15:46:05 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 15:46:05 | INFO | train | epoch 040 | loss 7.343 | ppl 162.35 | wps 21901.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1944 | lr 0.000243051 | gnorm 0.935 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 5892
2022-03-06 15:46:05 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 15:46:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:48:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:48:30 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.897 | ppl 476.56 | wps 38679.5 | wpb 510.9 | bsz 1 | num_updates 1993 | best_loss 8.848
2022-03-06 15:48:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1993 updates
2022-03-06 15:48:30 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 15:48:30 | INFO | train | epoch 041 | loss 7.26 | ppl 153.23 | wps 21904.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1993 | lr 0.000249175 | gnorm 0.985 | loss_scale 64 | train_wall 128 | gb_free 21.5 | wall 6037
2022-03-06 15:48:30 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 15:48:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:48:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:48:53 | INFO | train_inner | epoch 042:      8 / 49 loss=7.288, ppl=156.32, wps=21705.8, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=2000, lr=0.00025005, gnorm=0.963, loss_scale=32, train_wall=265, gb_free=21.5, wall=6060
2022-03-06 15:50:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:50:55 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.894 | ppl 475.58 | wps 38706 | wpb 510.9 | bsz 1 | num_updates 2041 | best_loss 8.848
2022-03-06 15:50:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2041 updates
2022-03-06 15:50:55 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 15:50:55 | INFO | train | epoch 042 | loss 7.175 | ppl 144.53 | wps 21458.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 2041 | lr 0.000255174 | gnorm 1.002 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 6182
2022-03-06 15:50:55 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 15:50:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:53:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:53:20 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.981 | ppl 505.3 | wps 38932 | wpb 510.9 | bsz 1 | num_updates 2090 | best_loss 8.848
2022-03-06 15:53:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2090 updates
2022-03-06 15:53:20 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 15:53:20 | INFO | train | epoch 043 | loss 7.097 | ppl 136.88 | wps 21908.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2090 | lr 0.000261298 | gnorm 1.049 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 6327
2022-03-06 15:53:20 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 15:53:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:53:49 | INFO | train_inner | epoch 044:     10 / 49 loss=7.122, ppl=139.31, wps=21931.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=2100, lr=0.000262548, gnorm=1.036, loss_scale=32, train_wall=262, gb_free=21.5, wall=6356
2022-03-06 15:55:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:55:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:55:45 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.937 | ppl 490.28 | wps 38722.9 | wpb 510.9 | bsz 1 | num_updates 2138 | best_loss 8.848
2022-03-06 15:55:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2138 updates
2022-03-06 15:55:45 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 15:55:45 | INFO | train | epoch 044 | loss 7.015 | ppl 129.37 | wps 21467.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 2138 | lr 0.000267297 | gnorm 1.017 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 6472
2022-03-06 15:55:45 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 15:55:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:58:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:58:10 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.998 | ppl 511.18 | wps 38687.7 | wpb 510.9 | bsz 1 | num_updates 2187 | best_loss 8.848
2022-03-06 15:58:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2187 updates
2022-03-06 15:58:10 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 15:58:10 | INFO | train | epoch 045 | loss 6.941 | ppl 122.87 | wps 21908 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2187 | lr 0.00027342 | gnorm 1.095 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 6617
2022-03-06 15:58:10 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 15:58:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:58:48 | INFO | train_inner | epoch 046:     13 / 49 loss=6.957, ppl=124.23, wps=21720, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=2200, lr=0.000275045, gnorm=1.049, loss_scale=32, train_wall=264, gb_free=21.5, wall=6654
2022-03-06 16:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:00:35 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.019 | ppl 518.82 | wps 38831.8 | wpb 510.9 | bsz 1 | num_updates 2236 | best_loss 8.848
2022-03-06 16:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2236 updates
2022-03-06 16:00:35 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 16:00:35 | INFO | train | epoch 046 | loss 6.85 | ppl 115.38 | wps 21901.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2236 | lr 0.000279544 | gnorm 1.045 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 6762
2022-03-06 16:00:35 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 16:00:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:01:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:02:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:03:00 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.04 | ppl 526.38 | wps 38660.5 | wpb 510.9 | bsz 1 | num_updates 2284 | best_loss 8.848
2022-03-06 16:03:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2284 updates
2022-03-06 16:03:00 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 16:03:00 | INFO | train | epoch 047 | loss 6.771 | ppl 109.18 | wps 21477.1 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 2284 | lr 0.000285543 | gnorm 1.056 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 6907
2022-03-06 16:03:00 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 16:03:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:03:46 | INFO | train_inner | epoch 048:     16 / 49 loss=6.782, ppl=110.03, wps=21710.3, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=2300, lr=0.000287543, gnorm=1.048, loss_scale=32, train_wall=265, gb_free=21.5, wall=6953
2022-03-06 16:04:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:05:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:05:26 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.076 | ppl 539.68 | wps 38611.5 | wpb 510.9 | bsz 1 | num_updates 2332 | best_loss 8.848
2022-03-06 16:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2332 updates
2022-03-06 16:05:26 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 16:05:26 | INFO | train | epoch 048 | loss 6.693 | ppl 103.5 | wps 21439.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 2332 | lr 0.000291542 | gnorm 1.113 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 7052
2022-03-06 16:05:26 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 16:05:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:07:51 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.137 | ppl 563.06 | wps 38662.4 | wpb 510.9 | bsz 1 | num_updates 2381 | best_loss 8.848
2022-03-06 16:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2381 updates
2022-03-06 16:07:51 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 16:07:51 | INFO | train | epoch 049 | loss 6.617 | ppl 98.13 | wps 21896.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2381 | lr 0.000297665 | gnorm 1.119 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 7197
2022-03-06 16:07:51 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 16:07:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:08:45 | INFO | train_inner | epoch 050:     19 / 49 loss=6.628, ppl=98.94, wps=21714.8, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=2400, lr=0.00030004, gnorm=1.125, loss_scale=16, train_wall=264, gb_free=21.5, wall=7252
2022-03-06 16:10:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:10:16 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.171 | ppl 576.39 | wps 38848.9 | wpb 510.9 | bsz 1 | num_updates 2430 | best_loss 8.848
2022-03-06 16:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2430 updates
2022-03-06 16:10:16 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 16:10:16 | INFO | train | epoch 050 | loss 6.535 | ppl 92.76 | wps 21922.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2430 | lr 0.000303789 | gnorm 1.063 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 7342
2022-03-06 16:10:16 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 16:10:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:12:41 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.219 | ppl 595.81 | wps 38853.3 | wpb 510.9 | bsz 1 | num_updates 2479 | best_loss 8.848
2022-03-06 16:12:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2479 updates
2022-03-06 16:12:41 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 16:12:41 | INFO | train | epoch 051 | loss 6.455 | ppl 87.74 | wps 21907 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2479 | lr 0.000309913 | gnorm 1.124 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 7487
2022-03-06 16:12:41 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 16:12:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:13:41 | INFO | train_inner | epoch 052:     21 / 49 loss=6.462, ppl=88.16, wps=21930.9, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2500, lr=0.000312538, gnorm=1.138, loss_scale=32, train_wall=262, gb_free=21.5, wall=7548
2022-03-06 16:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:15:06 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.213 | ppl 593.47 | wps 38687.1 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 8.848
2022-03-06 16:15:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2528 updates
2022-03-06 16:15:06 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 16:15:06 | INFO | train | epoch 052 | loss 6.379 | ppl 83.21 | wps 21917.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2528 | lr 0.000316037 | gnorm 1.151 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 7632
2022-03-06 16:15:06 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 16:15:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:15:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:17:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:17:31 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.252 | ppl 609.75 | wps 38729.4 | wpb 510.9 | bsz 1 | num_updates 2576 | best_loss 8.848
2022-03-06 16:17:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2576 updates
2022-03-06 16:17:31 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 16:17:31 | INFO | train | epoch 053 | loss 6.299 | ppl 78.75 | wps 21449.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 2576 | lr 0.000322036 | gnorm 1.181 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 7778
2022-03-06 16:17:31 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 16:17:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:18:40 | INFO | train_inner | epoch 054:     24 / 49 loss=6.301, ppl=78.84, wps=21710.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=2600, lr=0.000325035, gnorm=1.136, loss_scale=16, train_wall=264, gb_free=21.5, wall=7847
2022-03-06 16:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:19:56 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.278 | ppl 620.85 | wps 38986.3 | wpb 510.9 | bsz 1 | num_updates 2625 | best_loss 8.848
2022-03-06 16:19:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2625 updates
2022-03-06 16:19:56 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 16:19:56 | INFO | train | epoch 054 | loss 6.228 | ppl 74.94 | wps 21906.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2625 | lr 0.000328159 | gnorm 1.226 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 7923
2022-03-06 16:19:56 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 16:19:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:22:21 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.371 | ppl 662.14 | wps 38945.5 | wpb 510.9 | bsz 1 | num_updates 2674 | best_loss 8.848
2022-03-06 16:22:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2674 updates
2022-03-06 16:22:21 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 16:22:21 | INFO | train | epoch 055 | loss 6.143 | ppl 70.67 | wps 21903.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2674 | lr 0.000334283 | gnorm 1.156 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 8068
2022-03-06 16:22:21 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 16:22:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:23:36 | INFO | train_inner | epoch 056:     26 / 49 loss=6.141, ppl=70.56, wps=21923.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2700, lr=0.000337533, gnorm=1.186, loss_scale=32, train_wall=262, gb_free=21.5, wall=8143
2022-03-06 16:24:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:24:46 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.37 | ppl 661.63 | wps 38869.2 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 8.848
2022-03-06 16:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2723 updates
2022-03-06 16:24:46 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 16:24:46 | INFO | train | epoch 056 | loss 6.065 | ppl 66.97 | wps 21908.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2723 | lr 0.000340407 | gnorm 1.205 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 8213
2022-03-06 16:24:46 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 16:24:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:27:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:27:11 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.418 | ppl 684.23 | wps 38763.9 | wpb 510.9 | bsz 1 | num_updates 2772 | best_loss 8.848
2022-03-06 16:27:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2772 updates
2022-03-06 16:27:11 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 16:27:11 | INFO | train | epoch 057 | loss 5.99 | ppl 63.55 | wps 21902.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2772 | lr 0.000346531 | gnorm 1.205 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 8358
2022-03-06 16:27:11 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 16:27:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:27:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:28:34 | INFO | train_inner | epoch 058:     29 / 49 loss=5.99, ppl=63.57, wps=21727.9, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=2800, lr=0.00035003, gnorm=1.269, loss_scale=16, train_wall=264, gb_free=21.5, wall=8441
2022-03-06 16:29:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:29:36 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.426 | ppl 687.84 | wps 38646.3 | wpb 510.9 | bsz 1 | num_updates 2820 | best_loss 8.848
2022-03-06 16:29:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2820 updates
2022-03-06 16:29:36 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 16:29:36 | INFO | train | epoch 058 | loss 5.92 | ppl 60.53 | wps 21486.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 2820 | lr 0.00035253 | gnorm 1.33 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 8503
2022-03-06 16:29:36 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 16:29:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:31:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:32:01 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.545 | ppl 747.12 | wps 38695 | wpb 510.9 | bsz 1 | num_updates 2869 | best_loss 8.848
2022-03-06 16:32:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2869 updates
2022-03-06 16:32:01 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 16:32:01 | INFO | train | epoch 059 | loss 5.837 | ppl 57.15 | wps 21891.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2869 | lr 0.000358653 | gnorm 1.198 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 8648
2022-03-06 16:32:01 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 16:32:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:33:30 | INFO | train_inner | epoch 060:     31 / 49 loss=5.83, ppl=56.9, wps=21924, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=2900, lr=0.000362528, gnorm=1.208, loss_scale=16, train_wall=262, gb_free=21.5, wall=8737
2022-03-06 16:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:34:26 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.52 | ppl 734.09 | wps 38627.1 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 8.848
2022-03-06 16:34:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2918 updates
2022-03-06 16:34:26 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 16:34:26 | INFO | train | epoch 060 | loss 5.764 | ppl 54.33 | wps 21900.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2918 | lr 0.000364777 | gnorm 1.263 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 8793
2022-03-06 16:34:26 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 16:34:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:36:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:36:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:36:51 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.591 | ppl 771.34 | wps 38736.4 | wpb 510.9 | bsz 1 | num_updates 2966 | best_loss 8.848
2022-03-06 16:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 2966 updates
2022-03-06 16:36:51 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 16:36:51 | INFO | train | epoch 061 | loss 5.688 | ppl 51.54 | wps 21472.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 2966 | lr 0.000370776 | gnorm 1.285 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 8938
2022-03-06 16:36:51 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 16:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:38:29 | INFO | train_inner | epoch 062:     34 / 49 loss=5.677, ppl=51.16, wps=21710.9, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=3000, lr=0.000375025, gnorm=1.318, loss_scale=16, train_wall=264, gb_free=21.5, wall=9036
2022-03-06 16:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:39:16 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.707 | ppl 835.63 | wps 38675.8 | wpb 510.9 | bsz 1 | num_updates 3015 | best_loss 8.848
2022-03-06 16:39:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3015 updates
2022-03-06 16:39:16 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 16:39:16 | INFO | train | epoch 062 | loss 5.615 | ppl 49.01 | wps 21884.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3015 | lr 0.0003769 | gnorm 1.255 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 9083
2022-03-06 16:39:16 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 16:39:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:41:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:41:41 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.756 | ppl 864.63 | wps 38922.9 | wpb 510.9 | bsz 1 | num_updates 3064 | best_loss 8.848
2022-03-06 16:41:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3064 updates
2022-03-06 16:41:41 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 16:41:41 | INFO | train | epoch 063 | loss 5.54 | ppl 46.51 | wps 21912.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3064 | lr 0.000383023 | gnorm 1.295 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 9228
2022-03-06 16:41:41 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 16:41:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:43:25 | INFO | train_inner | epoch 064:     36 / 49 loss=5.526, ppl=46.07, wps=21923.7, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=3100, lr=0.000387523, gnorm=1.288, loss_scale=32, train_wall=262, gb_free=21.5, wall=9332
2022-03-06 16:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:44:07 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.75 | ppl 861.34 | wps 38775.1 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 8.848
2022-03-06 16:44:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3113 updates
2022-03-06 16:44:07 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 16:44:07 | INFO | train | epoch 064 | loss 5.467 | ppl 44.22 | wps 21908.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3113 | lr 0.000389147 | gnorm 1.309 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 9373
2022-03-06 16:44:07 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 16:44:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:45:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:46:32 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.823 | ppl 905.8 | wps 38531.2 | wpb 510.9 | bsz 1 | num_updates 3161 | best_loss 8.848
2022-03-06 16:46:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3161 updates
2022-03-06 16:46:32 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-06 16:46:32 | INFO | train | epoch 065 | loss 5.403 | ppl 42.3 | wps 21452.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3161 | lr 0.000395146 | gnorm 1.365 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 9518
2022-03-06 16:46:32 | INFO | fairseq.trainer | begin training epoch 66
2022-03-06 16:46:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:48:24 | INFO | train_inner | epoch 066:     39 / 49 loss=5.378, ppl=41.58, wps=21712.8, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=3200, lr=0.00040002, gnorm=1.314, loss_scale=16, train_wall=264, gb_free=21.5, wall=9630
2022-03-06 16:48:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:48:57 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.867 | ppl 933.8 | wps 38724.8 | wpb 510.9 | bsz 1 | num_updates 3210 | best_loss 8.848
2022-03-06 16:48:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3210 updates
2022-03-06 16:48:57 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-06 16:48:57 | INFO | train | epoch 066 | loss 5.319 | ppl 39.92 | wps 21913.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3210 | lr 0.00040127 | gnorm 1.278 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 9663
2022-03-06 16:48:57 | INFO | fairseq.trainer | begin training epoch 67
2022-03-06 16:48:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:51:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:51:22 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.962 | ppl 997.26 | wps 38847 | wpb 510.9 | bsz 1 | num_updates 3259 | best_loss 8.848
2022-03-06 16:51:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3259 updates
2022-03-06 16:51:22 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-06 16:51:22 | INFO | train | epoch 067 | loss 5.253 | ppl 38.14 | wps 21899.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3259 | lr 0.000407394 | gnorm 1.391 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 9809
2022-03-06 16:51:22 | INFO | fairseq.trainer | begin training epoch 68
2022-03-06 16:51:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:52:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:53:22 | INFO | train_inner | epoch 068:     42 / 49 loss=5.228, ppl=37.48, wps=21710.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=3300, lr=0.000412518, gnorm=1.337, loss_scale=16, train_wall=265, gb_free=21.5, wall=9929
2022-03-06 16:53:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:53:47 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10 | ppl 1024.18 | wps 38726.1 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 8.848
2022-03-06 16:53:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3307 updates
2022-03-06 16:53:47 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-06 16:53:47 | INFO | train | epoch 068 | loss 5.178 | ppl 36.21 | wps 21450.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3307 | lr 0.000413392 | gnorm 1.303 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 9954
2022-03-06 16:53:47 | INFO | fairseq.trainer | begin training epoch 69
2022-03-06 16:53:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:56:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:56:12 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.023 | ppl 1040.3 | wps 38732.6 | wpb 510.9 | bsz 1 | num_updates 3356 | best_loss 8.848
2022-03-06 16:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3356 updates
2022-03-06 16:56:12 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-06 16:56:12 | INFO | train | epoch 069 | loss 5.133 | ppl 35.09 | wps 21917.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3356 | lr 0.000419516 | gnorm 1.432 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 10099
2022-03-06 16:56:12 | INFO | fairseq.trainer | begin training epoch 70
2022-03-06 16:56:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:58:18 | INFO | train_inner | epoch 070:     44 / 49 loss=5.093, ppl=34.13, wps=21925.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3400, lr=0.000425015, gnorm=1.374, loss_scale=16, train_wall=262, gb_free=21.5, wall=10225
2022-03-06 16:58:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:58:37 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.075 | ppl 1078.96 | wps 38773.6 | wpb 510.9 | bsz 1 | num_updates 3405 | best_loss 8.848
2022-03-06 16:58:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3405 updates
2022-03-06 16:58:37 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-06 16:58:37 | INFO | train | epoch 070 | loss 5.04 | ppl 32.89 | wps 21898.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3405 | lr 0.00042564 | gnorm 1.321 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 10244
2022-03-06 16:58:37 | INFO | fairseq.trainer | begin training epoch 71
2022-03-06 16:58:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:59:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:01:02 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.192 | ppl 1169.36 | wps 38708.1 | wpb 510.9 | bsz 1 | num_updates 3453 | best_loss 8.848
2022-03-06 17:01:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3453 updates
2022-03-06 17:01:02 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-06 17:01:02 | INFO | train | epoch 071 | loss 4.973 | ppl 31.41 | wps 21449.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3453 | lr 0.000431639 | gnorm 1.383 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 10389
2022-03-06 17:01:02 | INFO | fairseq.trainer | begin training epoch 72
2022-03-06 17:01:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:03:17 | INFO | train_inner | epoch 072:     47 / 49 loss=4.952, ppl=30.94, wps=21711.2, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=3500, lr=0.000437513, gnorm=1.38, loss_scale=16, train_wall=264, gb_free=21.5, wall=10524
2022-03-06 17:03:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:03:27 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.212 | ppl 1186.47 | wps 38829.7 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 8.848
2022-03-06 17:03:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 3502 updates
2022-03-06 17:03:27 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-06 17:03:27 | INFO | train | epoch 072 | loss 4.913 | ppl 30.13 | wps 21909.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3502 | lr 0.000437762 | gnorm 1.375 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 10534
2022-03-06 17:03:27 | INFO | fairseq.trainer | begin training epoch 73
2022-03-06 17:03:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:05:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:05:52 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.271 | ppl 1235.25 | wps 38730.2 | wpb 510.9 | bsz 1 | num_updates 3551 | best_loss 8.848
2022-03-06 17:05:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 3551 updates
2022-03-06 17:05:52 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-06 17:05:52 | INFO | train | epoch 073 | loss 4.845 | ppl 28.74 | wps 21910.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3551 | lr 0.000443886 | gnorm 1.408 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 10679
2022-03-06 17:05:52 | INFO | fairseq.trainer | begin training epoch 74
2022-03-06 17:05:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:06:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:08:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:08:17 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.261 | ppl 1226.73 | wps 38657.2 | wpb 510.9 | bsz 1 | num_updates 3599 | best_loss 8.848
2022-03-06 17:08:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 3599 updates
2022-03-06 17:08:17 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-06 17:08:17 | INFO | train | epoch 074 | loss 4.769 | ppl 27.27 | wps 21452.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3599 | lr 0.000449885 | gnorm 1.361 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 10824
2022-03-06 17:08:17 | INFO | fairseq.trainer | begin training epoch 75
2022-03-06 17:08:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:08:20 | INFO | train_inner | epoch 075:      1 / 49 loss=4.808, ppl=28.01, wps=21293.4, ups=0.33, wpb=64544.1, bsz=126.1, num_updates=3600, lr=0.00045001, gnorm=1.38, loss_scale=16, train_wall=263, gb_free=21.5, wall=10827
2022-03-06 17:10:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:10:42 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.34 | ppl 1296.12 | wps 38801.1 | wpb 510.9 | bsz 1 | num_updates 3648 | best_loss 8.848
2022-03-06 17:10:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 3648 updates
2022-03-06 17:10:42 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-06 17:10:42 | INFO | train | epoch 075 | loss 4.713 | ppl 26.23 | wps 21896 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3648 | lr 0.000456009 | gnorm 1.379 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 10969
2022-03-06 17:10:42 | INFO | fairseq.trainer | begin training epoch 76
2022-03-06 17:10:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:12:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:13:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:13:08 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.425 | ppl 1374.85 | wps 38781.1 | wpb 510.9 | bsz 1 | num_updates 3696 | best_loss 8.848
2022-03-06 17:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 3696 updates
2022-03-06 17:13:08 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-06 17:13:08 | INFO | train | epoch 076 | loss 4.647 | ppl 25.05 | wps 21440.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3696 | lr 0.000462008 | gnorm 1.403 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 11114
2022-03-06 17:13:08 | INFO | fairseq.trainer | begin training epoch 77
2022-03-06 17:13:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:13:19 | INFO | train_inner | epoch 077:      4 / 49 loss=4.674, ppl=25.53, wps=21703.1, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=3700, lr=0.000462508, gnorm=1.399, loss_scale=16, train_wall=265, gb_free=21.5, wall=11126
2022-03-06 17:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:15:32 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.52 | ppl 1468.44 | wps 38658.6 | wpb 510.9 | bsz 1 | num_updates 3745 | best_loss 8.848
2022-03-06 17:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 3745 updates
2022-03-06 17:15:32 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-06 17:15:32 | INFO | train | epoch 077 | loss 4.586 | ppl 24.01 | wps 21939.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3745 | lr 0.000468131 | gnorm 1.389 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 11259
2022-03-06 17:15:32 | INFO | fairseq.trainer | begin training epoch 78
2022-03-06 17:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:17:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:17:58 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.548 | ppl 1496.82 | wps 38675.9 | wpb 510.9 | bsz 1 | num_updates 3794 | best_loss 8.848
2022-03-06 17:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 3794 updates
2022-03-06 17:17:58 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-06 17:17:58 | INFO | train | epoch 078 | loss 4.523 | ppl 22.99 | wps 21904.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3794 | lr 0.000474255 | gnorm 1.41 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 11404
2022-03-06 17:17:58 | INFO | fairseq.trainer | begin training epoch 79
2022-03-06 17:17:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:18:15 | INFO | train_inner | epoch 079:      6 / 49 loss=4.545, ppl=23.35, wps=21938.2, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=3800, lr=0.000475005, gnorm=1.382, loss_scale=16, train_wall=262, gb_free=21.5, wall=11422
2022-03-06 17:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:20:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:20:23 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.619 | ppl 1572.37 | wps 38954.6 | wpb 510.9 | bsz 1 | num_updates 3842 | best_loss 8.848
2022-03-06 17:20:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 3842 updates
2022-03-06 17:20:23 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-06 17:20:23 | INFO | train | epoch 079 | loss 4.462 | ppl 22.04 | wps 21466.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3842 | lr 0.000480254 | gnorm 1.414 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 11549
2022-03-06 17:20:23 | INFO | fairseq.trainer | begin training epoch 80
2022-03-06 17:20:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:22:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:22:48 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.674 | ppl 1633.58 | wps 38739.1 | wpb 510.9 | bsz 1 | num_updates 3891 | best_loss 8.848
2022-03-06 17:22:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 3891 updates
2022-03-06 17:22:48 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-06 17:22:48 | INFO | train | epoch 080 | loss 4.4 | ppl 21.11 | wps 21899.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3891 | lr 0.000486378 | gnorm 1.388 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 11694
2022-03-06 17:22:48 | INFO | fairseq.trainer | begin training epoch 81
2022-03-06 17:22:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:23:14 | INFO | train_inner | epoch 081:      9 / 49 loss=4.423, ppl=21.45, wps=21714.9, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=3900, lr=0.000487503, gnorm=1.416, loss_scale=16, train_wall=264, gb_free=21.5, wall=11720
2022-03-06 17:24:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:25:13 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.767 | ppl 1741.97 | wps 38512.2 | wpb 510.9 | bsz 1 | num_updates 3939 | best_loss 8.848
2022-03-06 17:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 3939 updates
2022-03-06 17:25:13 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-06 17:25:13 | INFO | train | epoch 081 | loss 4.348 | ppl 20.37 | wps 21423.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 3939 | lr 0.000492377 | gnorm 1.479 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 11840
2022-03-06 17:25:13 | INFO | fairseq.trainer | begin training epoch 82
2022-03-06 17:25:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:27:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:27:38 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.804 | ppl 1787.75 | wps 38765.7 | wpb 510.9 | bsz 1 | num_updates 3988 | best_loss 8.848
2022-03-06 17:27:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 3988 updates
2022-03-06 17:27:38 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-06 17:27:38 | INFO | train | epoch 082 | loss 4.285 | ppl 19.5 | wps 21917.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3988 | lr 0.0004985 | gnorm 1.347 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 11985
2022-03-06 17:27:38 | INFO | fairseq.trainer | begin training epoch 83
2022-03-06 17:27:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:28:12 | INFO | train_inner | epoch 083:     12 / 49 loss=4.302, ppl=19.72, wps=21706.3, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=4000, lr=0.0005, gnorm=1.396, loss_scale=8, train_wall=265, gb_free=21.5, wall=12019
2022-03-06 17:29:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:30:03 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.89 | ppl 1897.23 | wps 38937.1 | wpb 510.9 | bsz 1 | num_updates 4037 | best_loss 8.848
2022-03-06 17:30:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 4037 updates
2022-03-06 17:30:03 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-06 17:30:03 | INFO | train | epoch 083 | loss 4.254 | ppl 19.08 | wps 21905.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4037 | lr 0.000497703 | gnorm 1.508 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 12130
2022-03-06 17:30:03 | INFO | fairseq.trainer | begin training epoch 84
2022-03-06 17:30:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:32:28 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.904 | ppl 1916.69 | wps 38862.5 | wpb 510.9 | bsz 1 | num_updates 4086 | best_loss 8.848
2022-03-06 17:32:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 4086 updates
2022-03-06 17:32:28 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-06 17:32:28 | INFO | train | epoch 084 | loss 4.165 | ppl 17.94 | wps 21917.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4086 | lr 0.00049471 | gnorm 1.213 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 12275
2022-03-06 17:32:28 | INFO | fairseq.trainer | begin training epoch 85
2022-03-06 17:32:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:33:08 | INFO | train_inner | epoch 085:     14 / 49 loss=4.19, ppl=18.25, wps=21927.1, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4100, lr=0.000493865, gnorm=1.35, loss_scale=16, train_wall=262, gb_free=21.5, wall=12315
2022-03-06 17:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:34:53 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.941 | ppl 1966.23 | wps 38707.5 | wpb 510.9 | bsz 1 | num_updates 4135 | best_loss 8.848
2022-03-06 17:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 4135 updates
2022-03-06 17:34:53 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-06 17:34:53 | INFO | train | epoch 085 | loss 4.113 | ppl 17.3 | wps 21903.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4135 | lr 0.00049177 | gnorm 1.371 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 12420
2022-03-06 17:34:53 | INFO | fairseq.trainer | begin training epoch 86
2022-03-06 17:34:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:36:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:37:18 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.988 | ppl 2030.91 | wps 38987.6 | wpb 510.9 | bsz 1 | num_updates 4183 | best_loss 8.848
2022-03-06 17:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 4183 updates
2022-03-06 17:37:18 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-06 17:37:18 | INFO | train | epoch 086 | loss 4.05 | ppl 16.56 | wps 21461.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4183 | lr 0.000488941 | gnorm 1.299 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 12565
2022-03-06 17:37:18 | INFO | fairseq.trainer | begin training epoch 87
2022-03-06 17:37:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:38:07 | INFO | train_inner | epoch 087:     17 / 49 loss=4.064, ppl=16.72, wps=21717.6, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=4200, lr=0.00048795, gnorm=1.344, loss_scale=8, train_wall=264, gb_free=21.5, wall=12614
2022-03-06 17:39:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:39:43 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.083 | ppl 2169.26 | wps 38809.8 | wpb 510.9 | bsz 1 | num_updates 4232 | best_loss 8.848
2022-03-06 17:39:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 4232 updates
2022-03-06 17:39:43 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-06 17:39:43 | INFO | train | epoch 087 | loss 4 | ppl 16.01 | wps 21904.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4232 | lr 0.000486102 | gnorm 1.311 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 12710
2022-03-06 17:39:43 | INFO | fairseq.trainer | begin training epoch 88
2022-03-06 17:39:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:42:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:42:08 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.129 | ppl 2239.85 | wps 38527.7 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 8.848
2022-03-06 17:42:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 4281 updates
2022-03-06 17:42:08 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-06 17:42:08 | INFO | train | epoch 088 | loss 3.952 | ppl 15.48 | wps 21881.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4281 | lr 0.000483312 | gnorm 1.368 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 12855
2022-03-06 17:42:08 | INFO | fairseq.trainer | begin training epoch 89
2022-03-06 17:42:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:43:03 | INFO | train_inner | epoch 089:     19 / 49 loss=3.955, ppl=15.51, wps=21906.3, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=4300, lr=0.000482243, gnorm=1.308, loss_scale=16, train_wall=262, gb_free=21.5, wall=12910
2022-03-06 17:43:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:44:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:44:34 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.171 | ppl 2305.49 | wps 38619.8 | wpb 510.9 | bsz 1 | num_updates 4329 | best_loss 8.848
2022-03-06 17:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 4329 updates
2022-03-06 17:44:34 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-06 17:44:34 | INFO | train | epoch 089 | loss 3.889 | ppl 14.82 | wps 21434 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4329 | lr 0.000480625 | gnorm 1.201 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13000
2022-03-06 17:44:34 | INFO | fairseq.trainer | begin training epoch 90
2022-03-06 17:44:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:46:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:46:59 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.291 | ppl 2505.77 | wps 38817.5 | wpb 510.9 | bsz 1 | num_updates 4378 | best_loss 8.848
2022-03-06 17:46:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 4378 updates
2022-03-06 17:46:59 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-06 17:46:59 | INFO | train | epoch 090 | loss 3.85 | ppl 14.42 | wps 21908.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4378 | lr 0.000477928 | gnorm 1.337 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13146
2022-03-06 17:46:59 | INFO | fairseq.trainer | begin training epoch 91
2022-03-06 17:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:48:02 | INFO | train_inner | epoch 091:     22 / 49 loss=3.85, ppl=14.42, wps=21704.5, ups=0.33, wpb=64867.4, bsz=126.7, num_updates=4400, lr=0.000476731, gnorm=1.263, loss_scale=8, train_wall=265, gb_free=21.5, wall=13209
2022-03-06 17:49:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:49:24 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.311 | ppl 2541.26 | wps 38722.1 | wpb 510.9 | bsz 1 | num_updates 4427 | best_loss 8.848
2022-03-06 17:49:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 4427 updates
2022-03-06 17:49:24 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-06 17:49:24 | INFO | train | epoch 091 | loss 3.792 | ppl 13.85 | wps 21916.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4427 | lr 0.000475275 | gnorm 1.19 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13291
2022-03-06 17:49:24 | INFO | fairseq.trainer | begin training epoch 92
2022-03-06 17:49:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:51:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:51:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:51:49 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.339 | ppl 2591.31 | wps 38156 | wpb 510.9 | bsz 1 | num_updates 4475 | best_loss 8.848
2022-03-06 17:51:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 4475 updates
2022-03-06 17:51:49 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-06 17:51:49 | INFO | train | epoch 092 | loss 3.744 | ppl 13.4 | wps 21433.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4475 | lr 0.000472719 | gnorm 1.238 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13436
2022-03-06 17:51:49 | INFO | fairseq.trainer | begin training epoch 93
2022-03-06 17:51:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:53:01 | INFO | train_inner | epoch 093:     25 / 49 loss=3.745, ppl=13.41, wps=21713.1, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=4500, lr=0.000471405, gnorm=1.233, loss_scale=8, train_wall=264, gb_free=21.5, wall=13508
2022-03-06 17:54:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:54:14 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.398 | ppl 2698.46 | wps 38922.6 | wpb 510.9 | bsz 1 | num_updates 4524 | best_loss 8.848
2022-03-06 17:54:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 4524 updates
2022-03-06 17:54:14 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-06 17:54:14 | INFO | train | epoch 093 | loss 3.703 | ppl 13.03 | wps 21922.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4524 | lr 0.000470152 | gnorm 1.211 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13581
2022-03-06 17:54:14 | INFO | fairseq.trainer | begin training epoch 94
2022-03-06 17:54:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:56:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:56:39 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.506 | ppl 2907.59 | wps 38796.9 | wpb 510.9 | bsz 1 | num_updates 4573 | best_loss 8.848
2022-03-06 17:56:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 4573 updates
2022-03-06 17:56:39 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-06 17:56:39 | INFO | train | epoch 094 | loss 3.661 | ppl 12.65 | wps 21909 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4573 | lr 0.000467627 | gnorm 1.234 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13726
2022-03-06 17:56:39 | INFO | fairseq.trainer | begin training epoch 95
2022-03-06 17:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:57:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:57:59 | INFO | train_inner | epoch 095:     28 / 49 loss=3.66, ppl=12.64, wps=21724.3, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=4600, lr=0.000466252, gnorm=1.218, loss_scale=8, train_wall=264, gb_free=21.5, wall=13806
2022-03-06 17:58:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:59:04 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.507 | ppl 2910.28 | wps 38984.1 | wpb 510.9 | bsz 1 | num_updates 4621 | best_loss 8.848
2022-03-06 17:59:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 4621 updates
2022-03-06 17:59:04 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-06 17:59:04 | INFO | train | epoch 095 | loss 3.616 | ppl 12.26 | wps 21462.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4621 | lr 0.000465192 | gnorm 1.201 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 13871
2022-03-06 17:59:04 | INFO | fairseq.trainer | begin training epoch 96
2022-03-06 17:59:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:01:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:01:29 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.543 | ppl 2984.32 | wps 38781.5 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 8.848
2022-03-06 18:01:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 4670 updates
2022-03-06 18:01:29 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-06 18:01:29 | INFO | train | epoch 096 | loss 3.578 | ppl 11.94 | wps 21915.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4670 | lr 0.000462745 | gnorm 1.195 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 14016
2022-03-06 18:01:29 | INFO | fairseq.trainer | begin training epoch 97
2022-03-06 18:01:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:02:55 | INFO | train_inner | epoch 097:     30 / 49 loss=3.571, ppl=11.89, wps=21925.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4700, lr=0.000461266, gnorm=1.195, loss_scale=8, train_wall=262, gb_free=21.5, wall=14102
2022-03-06 18:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:03:54 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.597 | ppl 3097.65 | wps 38930.4 | wpb 510.9 | bsz 1 | num_updates 4719 | best_loss 8.848
2022-03-06 18:03:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 4719 updates
2022-03-06 18:03:54 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-06 18:03:54 | INFO | train | epoch 097 | loss 3.54 | ppl 11.63 | wps 21922.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4719 | lr 0.000460336 | gnorm 1.171 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 14161
2022-03-06 18:03:54 | INFO | fairseq.trainer | begin training epoch 98
2022-03-06 18:03:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:04:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:06:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:06:19 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.615 | ppl 3136.06 | wps 38983.1 | wpb 510.9 | bsz 1 | num_updates 4767 | best_loss 8.848
2022-03-06 18:06:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 4767 updates
2022-03-06 18:06:19 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-06 18:06:19 | INFO | train | epoch 098 | loss 3.503 | ppl 11.34 | wps 21467.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4767 | lr 0.000458013 | gnorm 1.22 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 14306
2022-03-06 18:06:19 | INFO | fairseq.trainer | begin training epoch 99
2022-03-06 18:06:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:07:54 | INFO | train_inner | epoch 099:     33 / 49 loss=3.499, ppl=11.31, wps=21735.3, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4800, lr=0.000456435, gnorm=1.177, loss_scale=8, train_wall=264, gb_free=21.5, wall=14401
2022-03-06 18:08:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:08:44 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 11.693 | ppl 3310.91 | wps 38825.9 | wpb 510.9 | bsz 1 | num_updates 4816 | best_loss 8.848
2022-03-06 18:08:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 4816 updates
2022-03-06 18:08:44 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-06 18:08:44 | INFO | train | epoch 099 | loss 3.463 | ppl 11.03 | wps 21925.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4816 | lr 0.000455677 | gnorm 1.137 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 14451
2022-03-06 18:08:44 | INFO | fairseq.trainer | begin training epoch 100
2022-03-06 18:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:11:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:11:09 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 11.732 | ppl 3400.66 | wps 38976.4 | wpb 510.9 | bsz 1 | num_updates 4865 | best_loss 8.848
2022-03-06 18:11:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 4865 updates
2022-03-06 18:11:09 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-06 18:11:09 | INFO | train | epoch 100 | loss 3.428 | ppl 10.76 | wps 21918.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4865 | lr 0.000453376 | gnorm 1.141 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 14596
2022-03-06 18:11:09 | INFO | fairseq.trainer | begin training epoch 101
2022-03-06 18:11:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:11:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:12:52 | INFO | train_inner | epoch 101:     36 / 49 loss=3.422, ppl=10.72, wps=21728.3, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=4900, lr=0.000451754, gnorm=1.133, loss_scale=8, train_wall=264, gb_free=21.5, wall=14699
2022-03-06 18:13:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:13:34 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 11.823 | ppl 3622.8 | wps 38793.8 | wpb 510.9 | bsz 1 | num_updates 4913 | best_loss 8.848
2022-03-06 18:13:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 4913 updates
2022-03-06 18:13:34 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-06 18:13:34 | INFO | train | epoch 101 | loss 3.395 | ppl 10.52 | wps 21461.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 4913 | lr 0.000451156 | gnorm 1.145 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 14741
2022-03-06 18:13:34 | INFO | fairseq.trainer | begin training epoch 102
2022-03-06 18:13:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:15:59 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 11.738 | ppl 3415.84 | wps 38840 | wpb 510.9 | bsz 1 | num_updates 4962 | best_loss 8.848
2022-03-06 18:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 4962 updates
2022-03-06 18:15:59 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-06 18:15:59 | INFO | train | epoch 102 | loss 3.365 | ppl 10.31 | wps 21906 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4962 | lr 0.000448923 | gnorm 1.135 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 14886
2022-03-06 18:15:59 | INFO | fairseq.trainer | begin training epoch 103
2022-03-06 18:15:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:17:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:17:51 | INFO | train_inner | epoch 103:     39 / 49 loss=3.358, ppl=10.25, wps=21717.6, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=5000, lr=0.000447214, gnorm=1.138, loss_scale=8, train_wall=264, gb_free=21.5, wall=14998
2022-03-06 18:18:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:18:24 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 11.81 | ppl 3590.35 | wps 38511.2 | wpb 510.9 | bsz 1 | num_updates 5010 | best_loss 8.848
2022-03-06 18:18:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 5010 updates
2022-03-06 18:18:24 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-06 18:18:24 | INFO | train | epoch 103 | loss 3.332 | ppl 10.07 | wps 21457.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5010 | lr 0.000446767 | gnorm 1.115 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 15031
2022-03-06 18:18:24 | INFO | fairseq.trainer | begin training epoch 104
2022-03-06 18:18:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:20:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:20:50 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 11.851 | ppl 3694.72 | wps 38263.1 | wpb 510.9 | bsz 1 | num_updates 5059 | best_loss 8.848
2022-03-06 18:20:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 5059 updates
2022-03-06 18:20:50 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-06 18:20:50 | INFO | train | epoch 104 | loss 3.304 | ppl 9.88 | wps 21830.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5059 | lr 0.000444598 | gnorm 1.102 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 15176
2022-03-06 18:20:50 | INFO | fairseq.trainer | begin training epoch 105
2022-03-06 18:20:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:22:48 | INFO | train_inner | epoch 105:     41 / 49 loss=3.295, ppl=9.82, wps=21849.2, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=5100, lr=0.000442807, gnorm=1.097, loss_scale=8, train_wall=263, gb_free=21.5, wall=15295
2022-03-06 18:23:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:23:15 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 11.904 | ppl 3832.04 | wps 38039.1 | wpb 510.9 | bsz 1 | num_updates 5108 | best_loss 8.848
2022-03-06 18:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 5108 updates
2022-03-06 18:23:15 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-06 18:23:15 | INFO | train | epoch 105 | loss 3.276 | ppl 9.69 | wps 21806.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5108 | lr 0.000442461 | gnorm 1.098 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 15322
2022-03-06 18:23:15 | INFO | fairseq.trainer | begin training epoch 106
2022-03-06 18:23:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:25:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:25:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:25:41 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 11.92 | ppl 3875.21 | wps 38303.3 | wpb 510.9 | bsz 1 | num_updates 5156 | best_loss 8.848
2022-03-06 18:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 5156 updates
2022-03-06 18:25:41 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-06 18:25:41 | INFO | train | epoch 106 | loss 3.246 | ppl 9.48 | wps 21388.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5156 | lr 0.000440396 | gnorm 1.1 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 15468
2022-03-06 18:25:41 | INFO | fairseq.trainer | begin training epoch 107
2022-03-06 18:25:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:27:48 | INFO | train_inner | epoch 107:     44 / 49 loss=3.238, ppl=9.43, wps=21633, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=5200, lr=0.000438529, gnorm=1.078, loss_scale=8, train_wall=265, gb_free=21.5, wall=15595
2022-03-06 18:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:28:07 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 11.962 | ppl 3989.08 | wps 38061.4 | wpb 510.9 | bsz 1 | num_updates 5205 | best_loss 8.848
2022-03-06 18:28:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 5205 updates
2022-03-06 18:28:07 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-06 18:28:07 | INFO | train | epoch 107 | loss 3.218 | ppl 9.3 | wps 21815.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5205 | lr 0.000438318 | gnorm 1.05 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 15613
2022-03-06 18:28:07 | INFO | fairseq.trainer | begin training epoch 108
2022-03-06 18:28:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:30:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:30:32 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 11.99 | ppl 4067.45 | wps 38228.5 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 8.848
2022-03-06 18:30:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 5254 updates
2022-03-06 18:30:32 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-06 18:30:32 | INFO | train | epoch 108 | loss 3.196 | ppl 9.16 | wps 21815.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5254 | lr 0.00043627 | gnorm 1.095 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 15759
2022-03-06 18:30:32 | INFO | fairseq.trainer | begin training epoch 109
2022-03-06 18:30:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:32:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:32:48 | INFO | train_inner | epoch 109:     47 / 49 loss=3.183, ppl=9.08, wps=21619.2, ups=0.33, wpb=64876.2, bsz=126.7, num_updates=5300, lr=0.000434372, gnorm=1.07, loss_scale=8, train_wall=265, gb_free=21.5, wall=15895
2022-03-06 18:32:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:32:58 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 12.014 | ppl 4137 | wps 38162.4 | wpb 510.9 | bsz 1 | num_updates 5302 | best_loss 8.848
2022-03-06 18:32:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 5302 updates
2022-03-06 18:32:58 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-06 18:32:58 | INFO | train | epoch 109 | loss 3.164 | ppl 8.96 | wps 21356.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5302 | lr 0.00043429 | gnorm 1.044 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 15905
2022-03-06 18:32:58 | INFO | fairseq.trainer | begin training epoch 110
2022-03-06 18:32:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:35:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:35:24 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 11.975 | ppl 4026.52 | wps 38215.7 | wpb 510.9 | bsz 1 | num_updates 5351 | best_loss 8.848
2022-03-06 18:35:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 5351 updates
2022-03-06 18:35:24 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-06 18:35:24 | INFO | train | epoch 110 | loss 3.146 | ppl 8.85 | wps 21826.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5351 | lr 0.000432297 | gnorm 1.06 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 16050
2022-03-06 18:35:24 | INFO | fairseq.trainer | begin training epoch 111
2022-03-06 18:35:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:37:44 | INFO | train_inner | epoch 111:     49 / 49 loss=3.135, ppl=8.78, wps=21824.6, ups=0.34, wpb=64539.7, bsz=126.1, num_updates=5400, lr=0.000430331, gnorm=1.058, loss_scale=8, train_wall=261, gb_free=21.5, wall=16190
2022-03-06 18:37:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:37:49 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 12.045 | ppl 4226.89 | wps 38189.8 | wpb 510.9 | bsz 1 | num_updates 5400 | best_loss 8.848
2022-03-06 18:37:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 5400 updates
2022-03-06 18:37:49 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-06 18:37:49 | INFO | train | epoch 111 | loss 3.121 | ppl 8.7 | wps 21804 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5400 | lr 0.000430331 | gnorm 1.051 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 16196
2022-03-06 18:37:49 | INFO | fairseq.trainer | begin training epoch 112
2022-03-06 18:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:40:15 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 12.085 | ppl 4344.1 | wps 38286.5 | wpb 510.9 | bsz 1 | num_updates 5449 | best_loss 8.848
2022-03-06 18:40:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 5449 updates
2022-03-06 18:40:15 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-06 18:40:15 | INFO | train | epoch 112 | loss 3.099 | ppl 8.57 | wps 21835.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5449 | lr 0.000428392 | gnorm 1.042 | loss_scale 16 | train_wall 129 | gb_free 21.5 | wall 16342
2022-03-06 18:40:15 | INFO | fairseq.trainer | begin training epoch 113
2022-03-06 18:40:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:41:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:42:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:42:41 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 12.031 | ppl 4184.39 | wps 38300.2 | wpb 510.9 | bsz 1 | num_updates 5497 | best_loss 8.848
2022-03-06 18:42:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 5497 updates
2022-03-06 18:42:41 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-06 18:42:41 | INFO | train | epoch 113 | loss 3.076 | ppl 8.43 | wps 21367.5 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5497 | lr 0.000426518 | gnorm 1.037 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 16487
2022-03-06 18:42:41 | INFO | fairseq.trainer | begin training epoch 114
2022-03-06 18:42:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:42:49 | INFO | train_inner | epoch 114:      3 / 49 loss=3.085, ppl=8.49, wps=21220.3, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=5500, lr=0.000426401, gnorm=1.036, loss_scale=8, train_wall=265, gb_free=21.5, wall=16496
2022-03-06 18:45:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:45:06 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 12.103 | ppl 4398.47 | wps 38232.4 | wpb 510.9 | bsz 1 | num_updates 5546 | best_loss 8.848
2022-03-06 18:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 5546 updates
2022-03-06 18:45:06 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-06 18:45:06 | INFO | train | epoch 114 | loss 3.053 | ppl 8.3 | wps 21817.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5546 | lr 0.000424629 | gnorm 0.989 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 16633
2022-03-06 18:45:06 | INFO | fairseq.trainer | begin training epoch 115
2022-03-06 18:45:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:47:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:47:32 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 12.119 | ppl 4446.98 | wps 38355.3 | wpb 510.9 | bsz 1 | num_updates 5595 | best_loss 8.848
2022-03-06 18:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 5595 updates
2022-03-06 18:47:32 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-06 18:47:32 | INFO | train | epoch 115 | loss 3.032 | ppl 8.18 | wps 21841 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5595 | lr 0.000422766 | gnorm 0.98 | loss_scale 16 | train_wall 129 | gb_free 21.5 | wall 16779
2022-03-06 18:47:32 | INFO | fairseq.trainer | begin training epoch 116
2022-03-06 18:47:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:47:46 | INFO | train_inner | epoch 116:      5 / 49 loss=3.04, ppl=8.22, wps=21847.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=5600, lr=0.000422577, gnorm=0.986, loss_scale=16, train_wall=262, gb_free=21.5, wall=16793
2022-03-06 18:48:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:49:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:49:57 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 12.16 | ppl 4575 | wps 38200.5 | wpb 510.9 | bsz 1 | num_updates 5643 | best_loss 8.848
2022-03-06 18:49:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 5643 updates
2022-03-06 18:49:57 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-06 18:49:57 | INFO | train | epoch 116 | loss 3.017 | ppl 8.1 | wps 21363.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5643 | lr 0.000420964 | gnorm 1.024 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 16924
2022-03-06 18:49:57 | INFO | fairseq.trainer | begin training epoch 117
2022-03-06 18:49:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:52:23 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 12.166 | ppl 4595.28 | wps 38187.5 | wpb 510.9 | bsz 1 | num_updates 5692 | best_loss 8.848
2022-03-06 18:52:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 5692 updates
2022-03-06 18:52:23 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-06 18:52:23 | INFO | train | epoch 117 | loss 2.994 | ppl 7.97 | wps 21821.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5692 | lr 0.000419148 | gnorm 0.952 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 17070
2022-03-06 18:52:23 | INFO | fairseq.trainer | begin training epoch 118
2022-03-06 18:52:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:52:46 | INFO | train_inner | epoch 118:      8 / 49 loss=3.003, ppl=8.01, wps=21621.7, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=5700, lr=0.000418854, gnorm=0.985, loss_scale=8, train_wall=265, gb_free=21.5, wall=17093
2022-03-06 18:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:54:49 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 12.135 | ppl 4498.93 | wps 38147.3 | wpb 510.9 | bsz 1 | num_updates 5741 | best_loss 8.848
2022-03-06 18:54:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 5741 updates
2022-03-06 18:54:49 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-06 18:54:49 | INFO | train | epoch 118 | loss 2.979 | ppl 7.88 | wps 21822.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5741 | lr 0.000417356 | gnorm 0.989 | loss_scale 16 | train_wall 129 | gb_free 21.5 | wall 17216
2022-03-06 18:54:49 | INFO | fairseq.trainer | begin training epoch 119
2022-03-06 18:54:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:54:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:57:14 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 12.196 | ppl 4690.6 | wps 38153.7 | wpb 510.9 | bsz 1 | num_updates 5789 | best_loss 8.848
2022-03-06 18:57:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 5789 updates
2022-03-06 18:57:14 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-06 18:57:14 | INFO | train | epoch 119 | loss 2.957 | ppl 7.77 | wps 21359.1 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5789 | lr 0.000415622 | gnorm 0.956 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 17361
2022-03-06 18:57:14 | INFO | fairseq.trainer | begin training epoch 120
2022-03-06 18:57:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:57:46 | INFO | train_inner | epoch 120:     11 / 49 loss=2.964, ppl=7.8, wps=21627.9, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=5800, lr=0.000415227, gnorm=0.974, loss_scale=8, train_wall=265, gb_free=21.5, wall=17393
2022-03-06 18:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:59:40 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 12.165 | ppl 4592.85 | wps 38185.5 | wpb 510.9 | bsz 1 | num_updates 5838 | best_loss 8.848
2022-03-06 18:59:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 5838 updates
2022-03-06 18:59:40 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-06 18:59:40 | INFO | train | epoch 120 | loss 2.942 | ppl 7.68 | wps 21840.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5838 | lr 0.000413874 | gnorm 0.956 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 17507
2022-03-06 18:59:40 | INFO | fairseq.trainer | begin training epoch 121
2022-03-06 18:59:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:01:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:02:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:02:06 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 12.205 | ppl 4720.69 | wps 38184.7 | wpb 510.9 | bsz 1 | num_updates 5886 | best_loss 8.848
2022-03-06 19:02:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 5886 updates
2022-03-06 19:02:06 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-06 19:02:06 | INFO | train | epoch 121 | loss 2.923 | ppl 7.59 | wps 21357.1 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 5886 | lr 0.000412183 | gnorm 0.956 | loss_scale 8 | train_wall 129 | gb_free 21.5 | wall 17652
2022-03-06 19:02:06 | INFO | fairseq.trainer | begin training epoch 122
2022-03-06 19:02:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:02:46 | INFO | train_inner | epoch 122:     14 / 49 loss=2.927, ppl=7.6, wps=21631.9, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=5900, lr=0.000411693, gnorm=0.951, loss_scale=8, train_wall=265, gb_free=21.5, wall=17693
2022-03-06 19:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:04:31 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 12.173 | ppl 4617.64 | wps 38829.1 | wpb 510.9 | bsz 1 | num_updates 5935 | best_loss 8.848
2022-03-06 19:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 5935 updates
2022-03-06 19:04:31 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-06 19:04:31 | INFO | train | epoch 122 | loss 2.911 | ppl 7.52 | wps 21878.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5935 | lr 0.000410478 | gnorm 0.959 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 17798
2022-03-06 19:04:31 | INFO | fairseq.trainer | begin training epoch 123
2022-03-06 19:04:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:06:56 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 12.192 | ppl 4679.47 | wps 38632.1 | wpb 510.9 | bsz 1 | num_updates 5984 | best_loss 8.848
2022-03-06 19:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 5984 updates
2022-03-06 19:06:56 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-06 19:06:56 | INFO | train | epoch 123 | loss 2.894 | ppl 7.43 | wps 21942.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5984 | lr 0.000408794 | gnorm 0.942 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 17943
2022-03-06 19:06:56 | INFO | fairseq.trainer | begin training epoch 124
2022-03-06 19:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:07:42 | INFO | train_inner | epoch 124:     16 / 49 loss=2.897, ppl=7.45, wps=21940.5, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=6000, lr=0.000408248, gnorm=0.948, loss_scale=16, train_wall=262, gb_free=21.5, wall=17989
2022-03-06 19:09:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:09:21 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 12.251 | ppl 4876 | wps 38804.5 | wpb 510.9 | bsz 1 | num_updates 6033 | best_loss 8.848
2022-03-06 19:09:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 6033 updates
2022-03-06 19:09:21 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-06 19:09:21 | INFO | train | epoch 124 | loss 2.878 | ppl 7.35 | wps 21937.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6033 | lr 0.00040713 | gnorm 0.937 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 18087
2022-03-06 19:09:21 | INFO | fairseq.trainer | begin training epoch 125
2022-03-06 19:09:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:11:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:11:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:11:45 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 12.242 | ppl 4843.23 | wps 38826.3 | wpb 510.9 | bsz 1 | num_updates 6081 | best_loss 8.848
2022-03-06 19:11:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 6081 updates
2022-03-06 19:11:45 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-06 19:11:45 | INFO | train | epoch 125 | loss 2.864 | ppl 7.28 | wps 21498.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 6081 | lr 0.00040552 | gnorm 0.932 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 18232
2022-03-06 19:11:45 | INFO | fairseq.trainer | begin training epoch 126
2022-03-06 19:11:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:12:40 | INFO | train_inner | epoch 126:     19 / 49 loss=2.865, ppl=7.28, wps=21755.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=6100, lr=0.000404888, gnorm=0.931, loss_scale=8, train_wall=264, gb_free=21.5, wall=18287
2022-03-06 19:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:14:10 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 12.229 | ppl 4801.24 | wps 38836.3 | wpb 510.9 | bsz 1 | num_updates 6130 | best_loss 8.848
2022-03-06 19:14:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 6130 updates
2022-03-06 19:14:10 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-06 19:14:10 | INFO | train | epoch 126 | loss 2.847 | ppl 7.2 | wps 21950.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6130 | lr 0.000403896 | gnorm 0.894 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 18377
2022-03-06 19:14:10 | INFO | fairseq.trainer | begin training epoch 127
2022-03-06 19:14:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:16:35 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 12.2 | ppl 4704.24 | wps 38823.5 | wpb 510.9 | bsz 1 | num_updates 6179 | best_loss 8.848
2022-03-06 19:16:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 6179 updates
2022-03-06 19:16:35 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-06 19:16:35 | INFO | train | epoch 127 | loss 2.834 | ppl 7.13 | wps 21944.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6179 | lr 0.000402292 | gnorm 0.897 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 18522
2022-03-06 19:16:35 | INFO | fairseq.trainer | begin training epoch 128
2022-03-06 19:16:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:17:35 | INFO | train_inner | epoch 128:     21 / 49 loss=2.835, ppl=7.13, wps=21964.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=6200, lr=0.00040161, gnorm=0.89, loss_scale=8, train_wall=261, gb_free=21.5, wall=18582
2022-03-06 19:18:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:19:00 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 12.248 | ppl 4864.44 | wps 38909.6 | wpb 510.9 | bsz 1 | num_updates 6228 | best_loss 8.848
2022-03-06 19:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 6228 updates
2022-03-06 19:19:00 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-06 19:19:00 | INFO | train | epoch 128 | loss 2.823 | ppl 7.08 | wps 21962.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6228 | lr 0.000400706 | gnorm 0.902 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 18667
2022-03-06 19:19:00 | INFO | fairseq.trainer | begin training epoch 129
2022-03-06 19:19:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:19:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:21:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:21:25 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 12.232 | ppl 4810.2 | wps 38840.1 | wpb 510.9 | bsz 1 | num_updates 6276 | best_loss 8.848
2022-03-06 19:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 6276 updates
2022-03-06 19:21:25 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-06 19:21:25 | INFO | train | epoch 129 | loss 2.808 | ppl 7 | wps 21476.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 6276 | lr 0.000399171 | gnorm 0.895 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 18811
2022-03-06 19:21:25 | INFO | fairseq.trainer | begin training epoch 130
2022-03-06 19:21:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:22:34 | INFO | train_inner | epoch 130:     24 / 49 loss=2.81, ppl=7.01, wps=21749.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=6300, lr=0.00039841, gnorm=0.899, loss_scale=8, train_wall=264, gb_free=21.5, wall=18880
2022-03-06 19:23:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:23:50 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 12.251 | ppl 4875.51 | wps 38802.1 | wpb 510.9 | bsz 1 | num_updates 6325 | best_loss 8.848
2022-03-06 19:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 6325 updates
2022-03-06 19:23:50 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-06 19:23:50 | INFO | train | epoch 130 | loss 2.796 | ppl 6.94 | wps 21930.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6325 | lr 0.000397621 | gnorm 0.892 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 18956
2022-03-06 19:23:50 | INFO | fairseq.trainer | begin training epoch 131
2022-03-06 19:23:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:26:14 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 12.297 | ppl 5031.78 | wps 38775.4 | wpb 510.9 | bsz 1 | num_updates 6374 | best_loss 8.848
2022-03-06 19:26:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 6374 updates
2022-03-06 19:26:14 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-06 19:26:14 | INFO | train | epoch 131 | loss 2.783 | ppl 6.88 | wps 21939 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6374 | lr 0.00039609 | gnorm 0.883 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 19101
2022-03-06 19:26:14 | INFO | fairseq.trainer | begin training epoch 132
2022-03-06 19:26:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:27:29 | INFO | train_inner | epoch 132:     26 / 49 loss=2.783, ppl=6.88, wps=21952, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=6400, lr=0.000395285, gnorm=0.876, loss_scale=16, train_wall=261, gb_free=21.5, wall=19176
2022-03-06 19:28:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:28:39 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 12.229 | ppl 4801.86 | wps 38908.8 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 8.848
2022-03-06 19:28:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 6423 updates
2022-03-06 19:28:39 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-06 19:28:39 | INFO | train | epoch 132 | loss 2.77 | ppl 6.82 | wps 21941.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6423 | lr 0.000394576 | gnorm 0.866 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 19246
2022-03-06 19:28:39 | INFO | fairseq.trainer | begin training epoch 133
2022-03-06 19:28:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:30:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:31:04 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 12.25 | ppl 4869.67 | wps 38452.2 | wpb 510.9 | bsz 1 | num_updates 6471 | best_loss 8.848
2022-03-06 19:31:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 6471 updates
2022-03-06 19:31:04 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-06 19:31:04 | INFO | train | epoch 133 | loss 2.76 | ppl 6.77 | wps 21490.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 6471 | lr 0.00039311 | gnorm 0.87 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 19391
2022-03-06 19:31:04 | INFO | fairseq.trainer | begin training epoch 134
2022-03-06 19:31:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:32:27 | INFO | train_inner | epoch 134:     29 / 49 loss=2.759, ppl=6.77, wps=21753.6, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=6500, lr=0.000392232, gnorm=0.868, loss_scale=8, train_wall=264, gb_free=21.5, wall=19474
2022-03-06 19:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:33:29 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 12.246 | ppl 4858.08 | wps 38780 | wpb 510.9 | bsz 1 | num_updates 6520 | best_loss 8.848
2022-03-06 19:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 6520 updates
2022-03-06 19:33:29 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-06 19:33:29 | INFO | train | epoch 134 | loss 2.748 | ppl 6.72 | wps 21950 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6520 | lr 0.00039163 | gnorm 0.849 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 19536
2022-03-06 19:33:29 | INFO | fairseq.trainer | begin training epoch 135
2022-03-06 19:33:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:35:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:35:54 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 12.277 | ppl 4962.2 | wps 38577.1 | wpb 510.9 | bsz 1 | num_updates 6569 | best_loss 8.848
2022-03-06 19:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 6569 updates
2022-03-06 19:35:54 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-06 19:35:54 | INFO | train | epoch 135 | loss 2.739 | ppl 6.68 | wps 21931.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6569 | lr 0.000390167 | gnorm 0.86 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 19681
2022-03-06 19:35:54 | INFO | fairseq.trainer | begin training epoch 136
2022-03-06 19:35:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:37:23 | INFO | train_inner | epoch 136:     31 / 49 loss=2.737, ppl=6.67, wps=21952.2, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=6600, lr=0.000389249, gnorm=0.855, loss_scale=16, train_wall=261, gb_free=21.5, wall=19770
2022-03-06 19:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:38:19 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 12.256 | ppl 4889.62 | wps 38715.2 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 8.848
2022-03-06 19:38:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 6618 updates
2022-03-06 19:38:19 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-06 19:38:19 | INFO | train | epoch 136 | loss 2.728 | ppl 6.62 | wps 21917.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6618 | lr 0.00038872 | gnorm 0.862 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 19826
2022-03-06 19:38:19 | INFO | fairseq.trainer | begin training epoch 137
2022-03-06 19:38:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:40:44 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 12.243 | ppl 4846.61 | wps 38959.6 | wpb 510.9 | bsz 1 | num_updates 6667 | best_loss 8.848
2022-03-06 19:40:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 6667 updates
2022-03-06 19:40:44 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-06 19:40:44 | INFO | train | epoch 137 | loss 2.715 | ppl 6.56 | wps 21934.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6667 | lr 0.000387289 | gnorm 0.843 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 19970
2022-03-06 19:40:44 | INFO | fairseq.trainer | begin training epoch 138
2022-03-06 19:40:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:42:18 | INFO | train_inner | epoch 138:     33 / 49 loss=2.713, ppl=6.56, wps=21946.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=6700, lr=0.000386334, gnorm=0.838, loss_scale=16, train_wall=261, gb_free=21.5, wall=20065
2022-03-06 19:43:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:43:09 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 12.284 | ppl 4985.73 | wps 39025.6 | wpb 510.9 | bsz 1 | num_updates 6716 | best_loss 8.848
2022-03-06 19:43:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 6716 updates
2022-03-06 19:43:09 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-06 19:43:09 | INFO | train | epoch 138 | loss 2.703 | ppl 6.51 | wps 21943.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6716 | lr 0.000385873 | gnorm 0.814 | loss_scale 32 | train_wall 128 | gb_free 21.5 | wall 20115
2022-03-06 19:43:09 | INFO | fairseq.trainer | begin training epoch 139
2022-03-06 19:43:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:43:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:45:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:45:33 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 12.254 | ppl 4885.15 | wps 38780 | wpb 510.9 | bsz 1 | num_updates 6764 | best_loss 8.848
2022-03-06 19:45:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 6764 updates
2022-03-06 19:45:33 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-06 19:45:33 | INFO | train | epoch 139 | loss 2.697 | ppl 6.48 | wps 21506.5 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 6764 | lr 0.000384502 | gnorm 0.831 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 20260
2022-03-06 19:45:33 | INFO | fairseq.trainer | begin training epoch 140
2022-03-06 19:45:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:47:17 | INFO | train_inner | epoch 140:     36 / 49 loss=2.696, ppl=6.48, wps=21756.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=6800, lr=0.000383482, gnorm=0.829, loss_scale=16, train_wall=264, gb_free=21.5, wall=20363
2022-03-06 19:47:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:47:58 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 12.189 | ppl 4669.26 | wps 38947.4 | wpb 510.9 | bsz 1 | num_updates 6813 | best_loss 8.848
2022-03-06 19:47:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 6813 updates
2022-03-06 19:47:58 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-06 19:47:58 | INFO | train | epoch 140 | loss 2.688 | ppl 6.44 | wps 21934.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6813 | lr 0.000383116 | gnorm 0.825 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 20405
2022-03-06 19:47:58 | INFO | fairseq.trainer | begin training epoch 141
2022-03-06 19:47:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:49:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:50:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:50:23 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 12.282 | ppl 4978.91 | wps 38635 | wpb 510.9 | bsz 1 | num_updates 6861 | best_loss 8.848
2022-03-06 19:50:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 6861 updates
2022-03-06 19:50:23 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-06 19:50:23 | INFO | train | epoch 141 | loss 2.674 | ppl 6.38 | wps 21470.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 6861 | lr 0.000381774 | gnorm 0.8 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 20550
2022-03-06 19:50:23 | INFO | fairseq.trainer | begin training epoch 142
2022-03-06 19:50:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:52:15 | INFO | train_inner | epoch 142:     39 / 49 loss=2.673, ppl=6.38, wps=21740.6, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=6900, lr=0.000380693, gnorm=0.808, loss_scale=16, train_wall=264, gb_free=21.5, wall=20662
2022-03-06 19:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:52:48 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 12.227 | ppl 4794.25 | wps 38727.8 | wpb 510.9 | bsz 1 | num_updates 6910 | best_loss 8.848
2022-03-06 19:52:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 6910 updates
2022-03-06 19:52:48 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-06 19:52:48 | INFO | train | epoch 142 | loss 2.668 | ppl 6.36 | wps 21945.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 6910 | lr 0.000380418 | gnorm 0.821 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 20695
2022-03-06 19:52:48 | INFO | fairseq.trainer | begin training epoch 143
2022-03-06 19:52:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:53:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:55:13 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 12.218 | ppl 4763.48 | wps 38719.3 | wpb 510.9 | bsz 1 | num_updates 6958 | best_loss 8.848
2022-03-06 19:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 6958 updates
2022-03-06 19:55:13 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-06 19:55:13 | INFO | train | epoch 143 | loss 2.659 | ppl 6.31 | wps 21474 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 6958 | lr 0.000379103 | gnorm 0.808 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 20840
2022-03-06 19:55:13 | INFO | fairseq.trainer | begin training epoch 144
2022-03-06 19:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:57:13 | INFO | train_inner | epoch 144:     42 / 49 loss=2.658, ppl=6.31, wps=21743, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=7000, lr=0.000377964, gnorm=0.812, loss_scale=8, train_wall=264, gb_free=21.5, wall=20960
2022-03-06 19:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:57:38 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 12.301 | ppl 5046.45 | wps 38576.7 | wpb 510.9 | bsz 1 | num_updates 7007 | best_loss 8.848
2022-03-06 19:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 7007 updates
2022-03-06 19:57:38 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-06 19:57:38 | INFO | train | epoch 144 | loss 2.651 | ppl 6.28 | wps 21946.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7007 | lr 0.000377776 | gnorm 0.809 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 20984
2022-03-06 19:57:38 | INFO | fairseq.trainer | begin training epoch 145
2022-03-06 19:57:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:00:02 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 12.274 | ppl 4953.56 | wps 39061.5 | wpb 510.9 | bsz 1 | num_updates 7056 | best_loss 8.848
2022-03-06 20:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 7056 updates
2022-03-06 20:00:02 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-06 20:00:02 | INFO | train | epoch 145 | loss 2.639 | ppl 6.23 | wps 21957.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7056 | lr 0.000376462 | gnorm 0.79 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 21129
2022-03-06 20:00:02 | INFO | fairseq.trainer | begin training epoch 146
2022-03-06 20:00:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:02:09 | INFO | train_inner | epoch 146:     44 / 49 loss=2.637, ppl=6.22, wps=21959.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=7100, lr=0.000375293, gnorm=0.795, loss_scale=16, train_wall=261, gb_free=21.5, wall=21256
2022-03-06 20:02:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:02:27 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 12.242 | ppl 4845.37 | wps 38785 | wpb 510.9 | bsz 1 | num_updates 7105 | best_loss 8.848
2022-03-06 20:02:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 7105 updates
2022-03-06 20:02:27 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-06 20:02:27 | INFO | train | epoch 146 | loss 2.633 | ppl 6.2 | wps 21925.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7105 | lr 0.000375161 | gnorm 0.798 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 21274
2022-03-06 20:02:27 | INFO | fairseq.trainer | begin training epoch 147
2022-03-06 20:02:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:04:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:04:52 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 12.325 | ppl 5132.67 | wps 38815.2 | wpb 510.9 | bsz 1 | num_updates 7153 | best_loss 8.848
2022-03-06 20:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 7153 updates
2022-03-06 20:04:52 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-06 20:04:52 | INFO | train | epoch 147 | loss 2.623 | ppl 6.16 | wps 21487.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 7153 | lr 0.0003739 | gnorm 0.776 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 21419
2022-03-06 20:04:52 | INFO | fairseq.trainer | begin training epoch 148
2022-03-06 20:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:07:07 | INFO | train_inner | epoch 148:     47 / 49 loss=2.621, ppl=6.15, wps=21751.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=7200, lr=0.000372678, gnorm=0.779, loss_scale=8, train_wall=264, gb_free=21.5, wall=21554
2022-03-06 20:07:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:07:17 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 12.285 | ppl 4990.28 | wps 38757.6 | wpb 510.9 | bsz 1 | num_updates 7202 | best_loss 8.848
2022-03-06 20:07:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 7202 updates
2022-03-06 20:07:17 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-06 20:07:17 | INFO | train | epoch 148 | loss 2.617 | ppl 6.13 | wps 21953.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7202 | lr 0.000372626 | gnorm 0.782 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 21564
2022-03-06 20:07:17 | INFO | fairseq.trainer | begin training epoch 149
2022-03-06 20:07:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:09:42 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 12.314 | ppl 5091.79 | wps 38732.5 | wpb 510.9 | bsz 1 | num_updates 7251 | best_loss 8.848
2022-03-06 20:09:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 7251 updates
2022-03-06 20:09:42 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-06 20:09:42 | INFO | train | epoch 149 | loss 2.608 | ppl 6.09 | wps 21952.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7251 | lr 0.000371365 | gnorm 0.766 | loss_scale 8 | train_wall 128 | gb_free 21.5 | wall 21709
2022-03-06 20:09:42 | INFO | fairseq.trainer | begin training epoch 150
2022-03-06 20:09:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:12:01 | INFO | train_inner | epoch 150:     49 / 49 loss=2.605, ppl=6.09, wps=21959.8, ups=0.34, wpb=64544.1, bsz=126.1, num_updates=7300, lr=0.000370117, gnorm=0.778, loss_scale=16, train_wall=260, gb_free=21.5, wall=21848
2022-03-06 20:12:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:12:07 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 12.283 | ppl 4985.16 | wps 38851.8 | wpb 510.9 | bsz 1 | num_updates 7300 | best_loss 8.848
2022-03-06 20:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 7300 updates
2022-03-06 20:12:07 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-06 20:12:07 | INFO | train | epoch 150 | loss 2.601 | ppl 6.07 | wps 21945.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7300 | lr 0.000370117 | gnorm 0.786 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 21853
2022-03-06 20:12:07 | INFO | fairseq.trainer | begin training epoch 151
2022-03-06 20:12:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:14:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:14:31 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 12.298 | ppl 5035.79 | wps 38733.6 | wpb 510.9 | bsz 1 | num_updates 7349 | best_loss 8.848
2022-03-06 20:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 7349 updates
2022-03-06 20:14:31 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-06 20:14:31 | INFO | train | epoch 151 | loss 2.592 | ppl 6.03 | wps 21941.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7349 | lr 0.000368881 | gnorm 0.757 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 21998
2022-03-06 20:14:31 | INFO | fairseq.trainer | begin training epoch 152
2022-03-06 20:14:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:16:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:16:56 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 12.279 | ppl 4969.3 | wps 38804.9 | wpb 510.9 | bsz 1 | num_updates 7398 | best_loss 8.848
2022-03-06 20:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 7398 updates
2022-03-06 20:16:56 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-06 20:16:56 | INFO | train | epoch 152 | loss 2.587 | ppl 6.01 | wps 21938.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7398 | lr 0.000367657 | gnorm 0.796 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 22143
2022-03-06 20:16:56 | INFO | fairseq.trainer | begin training epoch 153
2022-03-06 20:16:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:17:02 | INFO | train_inner | epoch 153:      2 / 49 loss=2.589, ppl=6.02, wps=21538.8, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=7400, lr=0.000367607, gnorm=0.776, loss_scale=16, train_wall=261, gb_free=21.5, wall=22149
2022-03-06 20:17:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:19:21 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 12.284 | ppl 4985.86 | wps 38794.8 | wpb 510.9 | bsz 1 | num_updates 7446 | best_loss 8.848
2022-03-06 20:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 7446 updates
2022-03-06 20:19:21 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-06 20:19:21 | INFO | train | epoch 153 | loss 2.576 | ppl 5.96 | wps 21488.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 7446 | lr 0.00036647 | gnorm 0.748 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 22288
2022-03-06 20:19:21 | INFO | fairseq.trainer | begin training epoch 154
2022-03-06 20:19:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:21:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:21:46 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 12.274 | ppl 4953.72 | wps 38713 | wpb 510.9 | bsz 1 | num_updates 7495 | best_loss 8.848
2022-03-06 20:21:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 7495 updates
2022-03-06 20:21:46 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-06 20:21:46 | INFO | train | epoch 154 | loss 2.572 | ppl 5.94 | wps 21945.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7495 | lr 0.00036527 | gnorm 0.766 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 22433
2022-03-06 20:21:46 | INFO | fairseq.trainer | begin training epoch 155
2022-03-06 20:21:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:22:00 | INFO | train_inner | epoch 155:      5 / 49 loss=2.573, ppl=5.95, wps=21750.5, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=7500, lr=0.000365148, gnorm=0.757, loss_scale=16, train_wall=264, gb_free=21.5, wall=22447
2022-03-06 20:23:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:24:11 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 12.242 | ppl 4843.76 | wps 38700.8 | wpb 510.9 | bsz 1 | num_updates 7543 | best_loss 8.848
2022-03-06 20:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 7543 updates
2022-03-06 20:24:11 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-06 20:24:11 | INFO | train | epoch 155 | loss 2.565 | ppl 5.92 | wps 21485.6 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 7543 | lr 0.000364106 | gnorm 0.751 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 22578
2022-03-06 20:24:11 | INFO | fairseq.trainer | begin training epoch 156
2022-03-06 20:24:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:26:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:26:36 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 12.277 | ppl 4962 | wps 38702.2 | wpb 510.9 | bsz 1 | num_updates 7592 | best_loss 8.848
2022-03-06 20:26:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 7592 updates
2022-03-06 20:26:36 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-06 20:26:36 | INFO | train | epoch 156 | loss 2.559 | ppl 5.89 | wps 21940.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7592 | lr 0.000362929 | gnorm 0.753 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 22722
2022-03-06 20:26:36 | INFO | fairseq.trainer | begin training epoch 157
2022-03-06 20:26:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:26:59 | INFO | train_inner | epoch 157:      8 / 49 loss=2.559, ppl=5.89, wps=21740.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=7600, lr=0.000362738, gnorm=0.747, loss_scale=16, train_wall=264, gb_free=21.5, wall=22746
2022-03-06 20:28:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:29:01 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 12.239 | ppl 4835.35 | wps 38741.1 | wpb 510.9 | bsz 1 | num_updates 7641 | best_loss 8.848
2022-03-06 20:29:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 7641 updates
2022-03-06 20:29:01 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-06 20:29:01 | INFO | train | epoch 157 | loss 2.551 | ppl 5.86 | wps 21935.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7641 | lr 0.000361764 | gnorm 0.729 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 22867
2022-03-06 20:29:01 | INFO | fairseq.trainer | begin training epoch 158
2022-03-06 20:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:30:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:31:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:31:25 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 12.219 | ppl 4768.65 | wps 38713.7 | wpb 510.9 | bsz 1 | num_updates 7689 | best_loss 8.848
2022-03-06 20:31:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 7689 updates
2022-03-06 20:31:25 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-06 20:31:25 | INFO | train | epoch 158 | loss 2.544 | ppl 5.83 | wps 21505.6 | ups 0.33 | wpb 64853.3 | bsz 126.7 | num_updates 7689 | lr 0.000360633 | gnorm 0.721 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 23012
2022-03-06 20:31:25 | INFO | fairseq.trainer | begin training epoch 159
2022-03-06 20:31:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:31:57 | INFO | train_inner | epoch 159:     11 / 49 loss=2.545, ppl=5.84, wps=21757.6, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=7700, lr=0.000360375, gnorm=0.724, loss_scale=16, train_wall=264, gb_free=21.5, wall=23044
2022-03-06 20:33:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:33:50 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 12.229 | ppl 4802.18 | wps 38599.8 | wpb 510.9 | bsz 1 | num_updates 7738 | best_loss 8.848
2022-03-06 20:33:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 7738 updates
2022-03-06 20:33:50 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-06 20:33:50 | INFO | train | epoch 159 | loss 2.537 | ppl 5.8 | wps 21940.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7738 | lr 0.000359489 | gnorm 0.718 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 23157
2022-03-06 20:33:50 | INFO | fairseq.trainer | begin training epoch 160
2022-03-06 20:33:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:36:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:36:15 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 12.236 | ppl 4825.15 | wps 38618.8 | wpb 510.9 | bsz 1 | num_updates 7787 | best_loss 8.848
2022-03-06 20:36:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 7787 updates
2022-03-06 20:36:15 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-06 20:36:15 | INFO | train | epoch 160 | loss 2.534 | ppl 5.79 | wps 21928 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 7787 | lr 0.000358356 | gnorm 0.731 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 23302
2022-03-06 20:36:15 | INFO | fairseq.trainer | begin training epoch 161
2022-03-06 20:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:36:52 | INFO | train_inner | epoch 161:     13 / 49 loss=2.534, ppl=5.79, wps=21951.9, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=7800, lr=0.000358057, gnorm=0.726, loss_scale=32, train_wall=261, gb_free=21.5, wall=23339
2022-03-06 20:38:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:38:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:38:40 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 12.273 | ppl 4950.78 | wps 38763.9 | wpb 510.9 | bsz 1 | num_updates 7835 | best_loss 8.848
2022-03-06 20:38:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 7835 updates
2022-03-06 20:38:40 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-06 20:38:40 | INFO | train | epoch 161 | loss 2.525 | ppl 5.76 | wps 21484.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 7835 | lr 0.000357257 | gnorm 0.721 | loss_scale 16 | train_wall 128 | gb_free 21.5 | wall 23447
2022-03-06 20:38:40 | INFO | fairseq.trainer | begin training epoch 162
2022-03-06 20:38:40 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 98, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 428, in forward
    x = self.dropout_module(x)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/fairseq_dropout.py", line 25, in forward
    return F.dropout(x, p=self.p, training=True, inplace=inplace)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 983, in dropout
    else _VF.dropout(input, p, training))
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/_VF.py", line 25, in __getattr__
    def __getattr__(self, attr):
KeyboardInterrupt
