Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 206457058: <w103_fp16_size_0.5_jelinek_0.036_0.004_0.96_#2> in cluster <euler> Exited

Job <w103_fp16_size_0.5_jelinek_0.036_0.004_0.96_#2> was submitted from host <eu-login-05> by user <andriusb> in cluster <euler> at Fri Feb 25 10:43:55 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Feb 25 10:44:18 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 25 10:44:18 2022
Terminated at Sun Feb 27 10:44:46 2022
Results reported at Sun Feb 27 10:44:46 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.5 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.036, 0.004, 0.96)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321672 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   172742.00 sec.
    Max Memory :                                 11088 MB
    Average Memory :                             3268.89 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               8912.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   172828 sec.
    Turnaround time :                            172851 sec.

The output (if any) follows:

2022-02-25 10:44:25 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321672, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.5', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321672, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.036, 0.004, 0.96)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-02-25 10:44:26 | INFO | fairseq.tasks.language_modeling | dictionary: 430640 types
2022-02-25 10:44:33 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
Calculating frequency stats:
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 703/900675 [00:00<02:08, 7005.12it/s]  0%|          | 1404/900675 [00:00<02:21, 6356.24it/s]  0%|          | 2044/900675 [00:00<02:33, 5867.46it/s]  0%|          | 2698/900675 [00:00<02:27, 6105.46it/s]  0%|          | 3371/900675 [00:00<02:22, 6314.89it/s]  0%|          | 4058/900675 [00:00<02:18, 6491.58it/s]  1%|          | 4830/900675 [00:00<02:10, 6882.17it/s]  1%|          | 5531/900675 [00:00<02:09, 6913.09it/s]  1%|          | 6229/900675 [00:00<02:09, 6924.68it/s]  1%|          | 6924/900675 [00:01<02:20, 6356.65it/s]  1%|          | 7591/900675 [00:01<02:18, 6443.81it/s]  1%|          | 8243/900675 [00:01<02:21, 6296.32it/s]  1%|          | 8878/900675 [00:01<02:21, 6287.95it/s]  1%|          | 9511/900675 [00:01<02:22, 6235.11it/s]  1%|          | 10225/900675 [00:01<02:17, 6498.31it/s]  1%|          | 10878/900675 [00:01<02:22, 6238.03it/s]  1%|▏         | 11528/900675 [00:01<02:20, 6307.57it/s]  1%|▏         | 12184/900675 [00:01<02:19, 6380.42it/s]  1%|▏         | 12825/900675 [00:02<02:23, 6192.84it/s]  2%|▏         | 13522/900675 [00:02<02:18, 6413.46it/s]  2%|▏         | 14168/900675 [00:02<02:18, 6417.95it/s]  2%|▏         | 14842/900675 [00:02<02:16, 6506.68it/s]  2%|▏         | 15495/900675 [00:02<02:19, 6355.28it/s]  2%|▏         | 16133/900675 [00:02<02:21, 6250.72it/s]  2%|▏         | 16760/900675 [00:02<02:23, 6180.63it/s]  2%|▏         | 17465/900675 [00:02<02:17, 6430.03it/s]  2%|▏         | 18110/900675 [00:02<02:19, 6305.96it/s]  2%|▏         | 18775/900675 [00:02<02:17, 6405.96it/s]  2%|▏         | 19566/900675 [00:03<02:08, 6843.39it/s]  2%|▏         | 20253/900675 [00:03<02:16, 6440.02it/s]  2%|▏         | 20903/900675 [00:03<02:16, 6443.21it/s]  2%|▏         | 21552/900675 [00:03<02:24, 6099.64it/s]  2%|▏         | 22230/900675 [00:03<02:19, 6289.50it/s]  3%|▎         | 22959/900675 [00:03<02:13, 6564.74it/s]  3%|▎         | 23640/900675 [00:03<02:12, 6632.57it/s]  3%|▎         | 24382/900675 [00:03<02:07, 6862.77it/s]  3%|▎         | 25147/900675 [00:03<02:03, 7092.91it/s]  3%|▎         | 25859/900675 [00:03<02:07, 6861.35it/s]  3%|▎         | 26549/900675 [00:04<02:15, 6432.46it/s]  3%|▎         | 27200/900675 [00:04<02:22, 6123.23it/s]  3%|▎         | 27819/900675 [00:04<02:22, 6106.80it/s]  3%|▎         | 28595/900675 [00:04<02:12, 6569.47it/s]  3%|▎         | 29259/900675 [00:04<02:15, 6454.09it/s]  3%|▎         | 29909/900675 [00:04<02:15, 6443.41it/s]  3%|▎         | 30557/900675 [00:04<02:19, 6220.13it/s]  3%|▎         | 31183/900675 [00:04<02:26, 5937.11it/s]  4%|▎         | 31844/900675 [00:04<02:21, 6121.13it/s]  4%|▎         | 32461/900675 [00:05<02:23, 6038.48it/s]  4%|▎         | 33068/900675 [00:05<02:28, 5848.17it/s]  4%|▎         | 33694/900675 [00:05<02:25, 5961.79it/s]  4%|▍         | 34298/900675 [00:05<02:24, 5982.39it/s]  4%|▍         | 35046/900675 [00:05<02:14, 6418.24it/s]  4%|▍         | 35691/900675 [00:05<02:19, 6222.61it/s]  4%|▍         | 36349/900675 [00:05<02:16, 6323.84it/s]  4%|▍         | 36984/900675 [00:05<02:21, 6093.36it/s]  4%|▍         | 37597/900675 [00:05<02:27, 5869.77it/s]  4%|▍         | 38228/900675 [00:06<02:23, 5993.45it/s]  4%|▍         | 38851/900675 [00:06<02:22, 6058.24it/s]  4%|▍         | 39460/900675 [00:06<02:24, 5963.73it/s]  4%|▍         | 40144/900675 [00:06<02:18, 6217.34it/s]  5%|▍         | 40867/900675 [00:06<02:12, 6513.18it/s]  5%|▍         | 41521/900675 [00:06<02:15, 6321.65it/s]  5%|▍         | 42156/900675 [00:06<02:23, 5982.89it/s]  5%|▍         | 42760/900675 [00:06<02:23, 5958.76it/s]  5%|▍         | 43360/900675 [00:06<02:24, 5926.70it/s]  5%|▍         | 44052/900675 [00:06<02:18, 6206.14it/s]  5%|▍         | 44738/900675 [00:07<02:14, 6381.54it/s]  5%|▌         | 45379/900675 [00:07<02:16, 6257.40it/s]  5%|▌         | 46102/900675 [00:07<02:10, 6537.20it/s]  5%|▌         | 46763/900675 [00:07<02:10, 6554.54it/s]  5%|▌         | 47770/900675 [00:07<01:52, 7590.45it/s]  5%|▌         | 48532/900675 [00:07<01:56, 7324.62it/s]  5%|▌         | 49286/900675 [00:07<01:55, 7380.77it/s]  6%|▌         | 50027/900675 [00:07<02:00, 7048.95it/s]  6%|▌         | 50737/900675 [00:07<02:09, 6566.98it/s]  6%|▌         | 51402/900675 [00:08<02:10, 6502.30it/s]  6%|▌         | 52139/900675 [00:08<02:05, 6741.93it/s]  6%|▌         | 52950/900675 [00:08<01:58, 7124.61it/s]  6%|▌         | 53669/900675 [00:08<02:04, 6777.64it/s]  6%|▌         | 54354/900675 [00:08<02:12, 6394.47it/s]  6%|▌         | 55002/900675 [00:08<02:13, 6332.98it/s]  6%|▌         | 55737/900675 [00:08<02:07, 6610.65it/s]  6%|▋         | 56404/900675 [00:08<02:10, 6478.31it/s]  6%|▋         | 57056/900675 [00:08<02:18, 6106.99it/s]  6%|▋         | 57673/900675 [00:09<02:18, 6087.66it/s]  6%|▋         | 58349/900675 [00:09<02:14, 6270.77it/s]  7%|▋         | 59070/900675 [00:09<02:08, 6528.81it/s]  7%|▋         | 59727/900675 [00:09<02:13, 6284.35it/s]  7%|▋         | 60360/900675 [00:09<02:13, 6274.44it/s]  7%|▋         | 61129/900675 [00:09<02:05, 6682.48it/s]  7%|▋         | 61801/900675 [00:09<02:07, 6585.03it/s]  7%|▋         | 62463/900675 [00:09<02:07, 6569.36it/s]  7%|▋         | 63122/900675 [00:09<02:11, 6359.50it/s]  7%|▋         | 63791/900675 [00:09<02:09, 6450.75it/s]  7%|▋         | 64439/900675 [00:10<02:14, 6238.47it/s]  7%|▋         | 65105/900675 [00:10<02:11, 6355.85it/s]  7%|▋         | 65743/900675 [00:10<02:11, 6360.96it/s]  7%|▋         | 66381/900675 [00:10<02:11, 6364.63it/s]  7%|▋         | 67051/900675 [00:10<02:09, 6462.17it/s]  8%|▊         | 67699/900675 [00:10<02:15, 6152.81it/s]  8%|▊         | 68318/900675 [00:10<02:16, 6118.22it/s]  8%|▊         | 69068/900675 [00:10<02:07, 6515.89it/s]  8%|▊         | 69750/900675 [00:10<02:05, 6600.93it/s]  8%|▊         | 70450/900675 [00:10<02:03, 6716.26it/s]  8%|▊         | 71124/900675 [00:11<02:10, 6372.71it/s]  8%|▊         | 71778/900675 [00:11<02:09, 6416.95it/s]  8%|▊         | 72444/900675 [00:11<02:07, 6487.40it/s]  8%|▊         | 73242/900675 [00:11<01:59, 6921.71it/s]  8%|▊         | 74043/900675 [00:11<01:54, 7229.83it/s]  8%|▊         | 74769/900675 [00:11<01:55, 7173.98it/s]  8%|▊         | 75489/900675 [00:11<02:01, 6817.99it/s]  8%|▊         | 76180/900675 [00:11<02:00, 6843.95it/s]  9%|▊         | 76868/900675 [00:11<02:01, 6786.68it/s]  9%|▊         | 77549/900675 [00:12<02:02, 6724.98it/s]  9%|▊         | 78224/900675 [00:12<02:06, 6527.25it/s]  9%|▉         | 78879/900675 [00:12<02:14, 6115.93it/s]  9%|▉         | 79497/900675 [00:12<02:16, 6022.32it/s]  9%|▉         | 80103/900675 [00:12<02:22, 5775.37it/s]  9%|▉         | 80769/900675 [00:12<02:16, 6018.61it/s]  9%|▉         | 81481/900675 [00:12<02:09, 6328.63it/s]  9%|▉         | 82119/900675 [00:12<02:10, 6278.47it/s]  9%|▉         | 82825/900675 [00:12<02:05, 6500.49it/s]  9%|▉         | 83508/900675 [00:13<02:03, 6593.22it/s]  9%|▉         | 84170/900675 [00:13<02:05, 6487.34it/s]  9%|▉         | 84870/900675 [00:13<02:02, 6635.15it/s]  9%|▉         | 85536/900675 [00:13<02:08, 6352.57it/s] 10%|▉         | 86175/900675 [00:13<02:08, 6328.31it/s] 10%|▉         | 86866/900675 [00:13<02:05, 6495.08it/s] 10%|▉         | 87595/900675 [00:13<02:00, 6726.90it/s] 10%|▉         | 88280/900675 [00:13<02:00, 6757.46it/s] 10%|▉         | 88958/900675 [00:13<02:10, 6226.31it/s] 10%|▉         | 89590/900675 [00:13<02:10, 6234.24it/s] 10%|█         | 90220/900675 [00:14<02:13, 6068.68it/s] 10%|█         | 90832/900675 [00:14<02:15, 5972.75it/s] 10%|█         | 91583/900675 [00:14<02:06, 6409.87it/s] 10%|█         | 92229/900675 [00:14<02:07, 6356.19it/s] 10%|█         | 92912/900675 [00:14<02:04, 6486.15it/s] 10%|█         | 93564/900675 [00:14<02:04, 6482.36it/s] 10%|█         | 94215/900675 [00:14<02:07, 6343.07it/s] 11%|█         | 94947/900675 [00:14<02:01, 6622.75it/s] 11%|█         | 95612/900675 [00:14<02:07, 6333.36it/s] 11%|█         | 96324/900675 [00:15<02:02, 6553.23it/s] 11%|█         | 97137/900675 [00:15<01:54, 7005.15it/s] 11%|█         | 97842/900675 [00:15<02:01, 6597.60it/s] 11%|█         | 98509/900675 [00:15<02:01, 6589.10it/s] 11%|█         | 99173/900675 [00:15<02:04, 6432.52it/s] 11%|█         | 99820/900675 [00:15<02:08, 6256.45it/s] 11%|█         | 100449/900675 [00:15<02:12, 6055.15it/s] 11%|█         | 101140/900675 [00:15<02:07, 6291.67it/s] 11%|█▏        | 101773/900675 [00:15<02:08, 6239.91it/s] 11%|█▏        | 102412/900675 [00:15<02:07, 6281.37it/s] 11%|█▏        | 103059/900675 [00:16<02:05, 6330.70it/s] 12%|█▏        | 103694/900675 [00:16<02:07, 6240.18it/s] 12%|█▏        | 104320/900675 [00:16<02:11, 6042.19it/s] 12%|█▏        | 104989/900675 [00:16<02:07, 6228.57it/s] 12%|█▏        | 105643/900675 [00:16<02:05, 6319.04it/s] 12%|█▏        | 106277/900675 [00:16<02:06, 6276.83it/s] 12%|█▏        | 106906/900675 [00:16<02:13, 5956.84it/s] 12%|█▏        | 107520/900675 [00:16<02:12, 6006.33it/s] 12%|█▏        | 108159/900675 [00:16<02:09, 6105.83it/s] 12%|█▏        | 108772/900675 [00:17<02:14, 5869.64it/s] 12%|█▏        | 109401/900675 [00:17<02:12, 5988.80it/s] 12%|█▏        | 110047/900675 [00:17<02:09, 6120.38it/s] 12%|█▏        | 110761/900675 [00:17<02:03, 6415.87it/s] 12%|█▏        | 111455/900675 [00:17<02:00, 6570.23it/s] 12%|█▏        | 112114/900675 [00:17<02:03, 6381.38it/s] 13%|█▎        | 112795/900675 [00:17<02:01, 6505.56it/s] 13%|█▎        | 113448/900675 [00:17<02:04, 6323.50it/s] 13%|█▎        | 114083/900675 [00:17<02:04, 6309.22it/s] 13%|█▎        | 114716/900675 [00:17<02:05, 6282.39it/s] 13%|█▎        | 115346/900675 [00:18<02:05, 6243.93it/s] 13%|█▎        | 115972/900675 [00:18<02:11, 5955.50it/s] 13%|█▎        | 116571/900675 [00:18<02:11, 5945.48it/s] 13%|█▎        | 117168/900675 [00:18<02:12, 5913.14it/s] 13%|█▎        | 117766/900675 [00:18<02:11, 5931.76it/s] 13%|█▎        | 118488/900675 [00:18<02:04, 6301.26it/s] 13%|█▎        | 119120/900675 [00:18<02:07, 6124.34it/s] 13%|█▎        | 119735/900675 [00:18<02:09, 6049.24it/s] 13%|█▎        | 120556/900675 [00:18<01:56, 6676.68it/s] 13%|█▎        | 121227/900675 [00:18<01:59, 6503.07it/s] 14%|█▎        | 121881/900675 [00:19<02:05, 6220.52it/s] 14%|█▎        | 122507/900675 [00:19<02:04, 6231.53it/s] 14%|█▎        | 123133/900675 [00:19<02:05, 6181.74it/s] 14%|█▎        | 123756/900675 [00:19<02:05, 6191.99it/s] 14%|█▍        | 124384/900675 [00:19<02:04, 6211.91it/s] 14%|█▍        | 125020/900675 [00:19<02:04, 6247.45it/s] 14%|█▍        | 125667/900675 [00:19<02:02, 6312.25it/s] 14%|█▍        | 126353/900675 [00:19<01:59, 6466.57it/s] 14%|█▍        | 127044/900675 [00:19<01:57, 6594.86it/s] 14%|█▍        | 127704/900675 [00:20<02:03, 6281.33it/s] 14%|█▍        | 128361/900675 [00:20<02:01, 6356.01it/s] 14%|█▍        | 129000/900675 [00:20<02:01, 6331.22it/s] 14%|█▍        | 129672/900675 [00:20<01:59, 6441.91it/s] 14%|█▍        | 130318/900675 [00:20<02:04, 6171.84it/s] 15%|█▍        | 130939/900675 [00:20<02:06, 6098.14it/s] 15%|█▍        | 131551/900675 [00:20<02:07, 6048.19it/s] 15%|█▍        | 132158/900675 [00:20<02:13, 5752.08it/s] 15%|█▍        | 132884/900675 [00:20<02:04, 6177.18it/s] 15%|█▍        | 133507/900675 [00:20<02:04, 6184.18it/s] 15%|█▍        | 134229/900675 [00:21<01:58, 6480.05it/s] 15%|█▍        | 134975/900675 [00:21<01:53, 6761.12it/s] 15%|█▌        | 135693/900675 [00:21<01:51, 6875.25it/s] 15%|█▌        | 136383/900675 [00:21<01:53, 6745.68it/s] 15%|█▌        | 137195/900675 [00:21<01:46, 7147.61it/s] 15%|█▌        | 137913/900675 [00:21<01:51, 6829.45it/s] 15%|█▌        | 138601/900675 [00:21<01:51, 6814.25it/s] 15%|█▌        | 139286/900675 [00:21<01:56, 6521.68it/s] 16%|█▌        | 139947/900675 [00:21<01:56, 6546.03it/s] 16%|█▌        | 140605/900675 [00:22<01:56, 6526.33it/s] 16%|█▌        | 141260/900675 [00:22<02:00, 6306.79it/s] 16%|█▌        | 141897/900675 [00:22<02:00, 6322.65it/s] 16%|█▌        | 142532/900675 [00:22<01:59, 6322.87it/s] 16%|█▌        | 143166/900675 [00:22<02:06, 5985.81it/s] 16%|█▌        | 143990/900675 [00:22<01:54, 6623.18it/s] 16%|█▌        | 144659/900675 [00:22<01:54, 6590.78it/s] 16%|█▌        | 145408/900675 [00:22<01:50, 6850.73it/s] 16%|█▌        | 146098/900675 [00:22<01:50, 6813.64it/s] 16%|█▋        | 146783/900675 [00:22<01:57, 6427.08it/s] 16%|█▋        | 147432/900675 [00:23<02:02, 6153.47it/s] 16%|█▋        | 148053/900675 [00:23<02:02, 6146.92it/s] 17%|█▋        | 148672/900675 [00:23<02:02, 6148.84it/s] 17%|█▋        | 149290/900675 [00:23<02:05, 5976.26it/s] 17%|█▋        | 149894/900675 [00:23<02:05, 5989.08it/s] 17%|█▋        | 150495/900675 [00:23<02:06, 5926.56it/s] 17%|█▋        | 151094/900675 [00:23<02:06, 5941.25it/s] 17%|█▋        | 151718/900675 [00:23<02:04, 6023.94it/s] 17%|█▋        | 152322/900675 [00:23<02:06, 5919.64it/s] 17%|█▋        | 153017/900675 [00:24<02:00, 6218.13it/s] 17%|█▋        | 153641/900675 [00:24<02:00, 6212.33it/s] 17%|█▋        | 154264/900675 [00:24<02:00, 6182.06it/s] 17%|█▋        | 154889/900675 [00:24<02:00, 6201.35it/s] 17%|█▋        | 155556/900675 [00:24<01:57, 6337.15it/s] 17%|█▋        | 156244/900675 [00:24<01:54, 6497.20it/s] 17%|█▋        | 156895/900675 [00:24<01:56, 6393.13it/s] 17%|█▋        | 157599/900675 [00:24<01:52, 6580.10it/s] 18%|█▊        | 158314/900675 [00:24<01:50, 6747.12it/s] 18%|█▊        | 158990/900675 [00:24<01:56, 6353.67it/s] 18%|█▊        | 159631/900675 [00:25<01:58, 6262.38it/s] 18%|█▊        | 160302/900675 [00:25<01:55, 6383.98it/s] 18%|█▊        | 161013/900675 [00:25<01:52, 6589.51it/s] 18%|█▊        | 161675/900675 [00:25<01:57, 6280.83it/s] 18%|█▊        | 162357/900675 [00:25<01:54, 6434.22it/s] 18%|█▊        | 163096/900675 [00:25<01:50, 6700.64it/s] 18%|█▊        | 163770/900675 [00:25<01:57, 6249.58it/s] 18%|█▊        | 164428/900675 [00:25<01:56, 6333.58it/s] 18%|█▊        | 165119/900675 [00:25<01:53, 6492.08it/s] 18%|█▊        | 165833/900675 [00:25<01:50, 6679.13it/s] 18%|█▊        | 166506/900675 [00:26<01:51, 6595.53it/s] 19%|█▊        | 167225/900675 [00:26<01:48, 6768.32it/s] 19%|█▊        | 167905/900675 [00:26<01:54, 6399.54it/s] 19%|█▊        | 168551/900675 [00:26<01:54, 6370.25it/s] 19%|█▉        | 169204/900675 [00:26<01:54, 6406.14it/s] 19%|█▉        | 169848/900675 [00:26<01:58, 6168.30it/s] 19%|█▉        | 170469/900675 [00:26<02:01, 5997.68it/s] 19%|█▉        | 171072/900675 [00:26<02:02, 5953.40it/s] 19%|█▉        | 171752/900675 [00:26<01:57, 6195.48it/s] 19%|█▉        | 172374/900675 [00:27<01:59, 6089.30it/s] 19%|█▉        | 173016/900675 [00:27<01:57, 6180.49it/s] 19%|█▉        | 173652/900675 [00:27<01:56, 6230.98it/s] 19%|█▉        | 174277/900675 [00:27<01:58, 6112.77it/s] 19%|█▉        | 175014/900675 [00:27<01:52, 6478.93it/s] 20%|█▉        | 175664/900675 [00:27<01:58, 6109.67it/s] 20%|█▉        | 176308/900675 [00:27<01:56, 6202.71it/s] 20%|█▉        | 176933/900675 [00:27<01:59, 6037.49it/s] 20%|█▉        | 177541/900675 [00:27<02:07, 5690.05it/s] 20%|█▉        | 178133/900675 [00:28<02:05, 5752.74it/s] 20%|█▉        | 178806/900675 [00:28<01:59, 6024.56it/s] 20%|█▉        | 179478/900675 [00:28<01:55, 6224.84it/s] 20%|█▉        | 180105/900675 [00:28<02:00, 5995.73it/s] 20%|██        | 180784/900675 [00:28<01:55, 6217.32it/s] 20%|██        | 181410/900675 [00:28<01:58, 6084.62it/s] 20%|██        | 182022/900675 [00:28<01:59, 6003.44it/s] 20%|██        | 182641/900675 [00:28<01:58, 6054.08it/s] 20%|██        | 183249/900675 [00:28<01:59, 6002.59it/s] 20%|██        | 183851/900675 [00:28<02:05, 5714.49it/s] 20%|██        | 184461/900675 [00:29<02:03, 5822.06it/s] 21%|██        | 185113/900675 [00:29<01:58, 6023.11it/s] 21%|██        | 185719/900675 [00:29<02:03, 5778.50it/s] 21%|██        | 186313/900675 [00:29<02:02, 5819.40it/s] 21%|██        | 186940/900675 [00:29<01:59, 5948.53it/s] 21%|██        | 187620/900675 [00:29<01:55, 6196.41it/s] 21%|██        | 188242/900675 [00:29<01:55, 6141.96it/s] 21%|██        | 188932/900675 [00:29<01:51, 6361.47it/s] 21%|██        | 189570/900675 [00:29<01:52, 6320.75it/s] 21%|██        | 190217/900675 [00:29<01:51, 6362.33it/s] 21%|██        | 190864/900675 [00:30<01:51, 6392.25it/s] 21%|██▏       | 191504/900675 [00:30<01:53, 6237.35it/s] 21%|██▏       | 192219/900675 [00:30<01:48, 6504.69it/s] 21%|██▏       | 192871/900675 [00:30<01:49, 6481.28it/s] 21%|██▏       | 193521/900675 [00:30<01:49, 6450.97it/s] 22%|██▏       | 194199/900675 [00:30<01:47, 6547.41it/s] 22%|██▏       | 194855/900675 [00:30<01:49, 6421.39it/s] 22%|██▏       | 195499/900675 [00:30<01:49, 6419.89it/s] 22%|██▏       | 196142/900675 [00:30<01:52, 6241.53it/s] 22%|██▏       | 196768/900675 [00:31<01:53, 6177.96it/s] 22%|██▏       | 197788/900675 [00:31<01:35, 7341.68it/s] 22%|██▏       | 198526/900675 [00:31<01:49, 6394.91it/s] 22%|██▏       | 199190/900675 [00:31<01:49, 6388.16it/s] 22%|██▏       | 199846/900675 [00:31<01:50, 6318.90it/s] 22%|██▏       | 200490/900675 [00:31<01:53, 6144.58it/s] 22%|██▏       | 201113/900675 [00:31<01:53, 6158.90it/s] 22%|██▏       | 201735/900675 [00:31<01:54, 6086.95it/s] 22%|██▏       | 202365/900675 [00:31<01:53, 6143.20it/s] 23%|██▎       | 203031/900675 [00:31<01:50, 6287.00it/s] 23%|██▎       | 203747/900675 [00:32<01:46, 6538.60it/s] 23%|██▎       | 204404/900675 [00:32<01:50, 6300.18it/s] 23%|██▎       | 205038/900675 [00:32<01:52, 6193.05it/s] 23%|██▎       | 205697/900675 [00:32<01:50, 6297.24it/s] 23%|██▎       | 206355/900675 [00:32<01:48, 6378.60it/s] 23%|██▎       | 206995/900675 [00:32<01:51, 6196.73it/s] 23%|██▎       | 207617/900675 [00:32<01:51, 6190.06it/s] 23%|██▎       | 208238/900675 [00:32<01:53, 6107.61it/s] 23%|██▎       | 208850/900675 [00:32<01:58, 5837.64it/s] 23%|██▎       | 209456/900675 [00:33<01:57, 5900.36it/s] 23%|██▎       | 210190/900675 [00:33<01:49, 6310.49it/s] 23%|██▎       | 210848/900675 [00:33<01:48, 6379.05it/s] 23%|██▎       | 211500/900675 [00:33<01:47, 6420.15it/s] 24%|██▎       | 212144/900675 [00:33<01:50, 6239.24it/s] 24%|██▎       | 212897/900675 [00:33<01:44, 6611.30it/s] 24%|██▎       | 213561/900675 [00:33<01:49, 6275.87it/s] 24%|██▍       | 214194/900675 [00:33<01:50, 6209.24it/s] 24%|██▍       | 214819/900675 [00:33<01:54, 6001.59it/s] 24%|██▍       | 215443/900675 [00:33<01:52, 6066.45it/s] 24%|██▍       | 216055/900675 [00:34<01:52, 6079.03it/s] 24%|██▍       | 216665/900675 [00:34<01:53, 6038.10it/s] 24%|██▍       | 217271/900675 [00:34<01:53, 6036.66it/s] 24%|██▍       | 217969/900675 [00:34<01:48, 6312.53it/s] 24%|██▍       | 218620/900675 [00:34<01:47, 6365.47it/s] 24%|██▍       | 219303/900675 [00:34<01:44, 6501.73it/s] 24%|██▍       | 219954/900675 [00:34<01:47, 6312.93it/s] 24%|██▍       | 220587/900675 [00:34<01:51, 6106.89it/s] 25%|██▍       | 221300/900675 [00:34<01:46, 6397.13it/s] 25%|██▍       | 221943/900675 [00:35<01:50, 6145.28it/s] 25%|██▍       | 222562/900675 [00:35<01:54, 5901.27it/s] 25%|██▍       | 223198/900675 [00:35<01:52, 6026.86it/s] 25%|██▍       | 223805/900675 [00:35<01:57, 5781.39it/s] 25%|██▍       | 224555/900675 [00:35<01:48, 6260.21it/s] 25%|██▌       | 225224/900675 [00:35<01:45, 6377.75it/s] 25%|██▌       | 226011/900675 [00:35<01:39, 6808.36it/s] 25%|██▌       | 226697/900675 [00:35<01:42, 6547.33it/s] 25%|██▌       | 227357/900675 [00:35<01:43, 6535.89it/s] 25%|██▌       | 228015/900675 [00:36<01:46, 6295.12it/s] 25%|██▌       | 228649/900675 [00:36<01:49, 6132.71it/s] 25%|██▌       | 229266/900675 [00:36<01:49, 6122.76it/s] 26%|██▌       | 229881/900675 [00:36<01:56, 5748.43it/s] 26%|██▌       | 230554/900675 [00:36<01:51, 6019.73it/s] 26%|██▌       | 231178/900675 [00:36<01:50, 6078.35it/s] 26%|██▌       | 231791/900675 [00:36<01:52, 5950.58it/s] 26%|██▌       | 232408/900675 [00:36<01:51, 6006.78it/s] 26%|██▌       | 233195/900675 [00:36<01:41, 6547.90it/s] 26%|██▌       | 233854/900675 [00:36<01:41, 6538.61it/s] 26%|██▌       | 234612/900675 [00:37<01:37, 6845.17it/s] 26%|██▌       | 235305/900675 [00:37<01:36, 6869.28it/s] 26%|██▌       | 235994/900675 [00:37<01:39, 6690.70it/s] 26%|██▋       | 236666/900675 [00:37<01:45, 6276.16it/s] 26%|██▋       | 237300/900675 [00:37<01:45, 6271.74it/s] 26%|██▋       | 237932/900675 [00:37<01:49, 6078.84it/s] 27%|██▋       | 238747/900675 [00:37<01:39, 6665.09it/s] 27%|██▋       | 239420/900675 [00:37<01:42, 6436.72it/s] 27%|██▋       | 240069/900675 [00:37<01:46, 6195.10it/s] 27%|██▋       | 240914/900675 [00:38<01:36, 6825.33it/s] 27%|██▋       | 241605/900675 [00:38<01:38, 6698.54it/s] 27%|██▋       | 242293/900675 [00:38<01:37, 6749.20it/s] 27%|██▋       | 242972/900675 [00:38<01:40, 6572.70it/s] 27%|██▋       | 243657/900675 [00:38<01:38, 6645.31it/s] 27%|██▋       | 244325/900675 [00:38<01:41, 6458.37it/s] 27%|██▋       | 244974/900675 [00:38<01:42, 6408.06it/s] 27%|██▋       | 245665/900675 [00:38<01:39, 6550.83it/s] 27%|██▋       | 246353/900675 [00:38<01:38, 6645.61it/s] 27%|██▋       | 247161/900675 [00:38<01:32, 7067.69it/s] 28%|██▊       | 247870/900675 [00:39<01:39, 6544.37it/s] 28%|██▊       | 248534/900675 [00:39<01:42, 6386.92it/s] 28%|██▊       | 249282/900675 [00:39<01:37, 6684.44it/s] 28%|██▊       | 249973/900675 [00:39<01:36, 6743.89it/s] 28%|██▊       | 250652/900675 [00:39<01:38, 6596.62it/s] 28%|██▊       | 251328/900675 [00:39<01:37, 6642.36it/s] 28%|██▊       | 251995/900675 [00:39<01:38, 6582.59it/s] 28%|██▊       | 252656/900675 [00:39<01:39, 6512.45it/s] 28%|██▊       | 253309/900675 [00:39<01:43, 6272.10it/s] 28%|██▊       | 254077/900675 [00:40<01:36, 6673.45it/s] 28%|██▊       | 254748/900675 [00:40<01:40, 6400.01it/s] 28%|██▊       | 255393/900675 [00:40<01:42, 6326.05it/s] 28%|██▊       | 256137/900675 [00:40<01:37, 6642.75it/s] 29%|██▊       | 256814/900675 [00:40<01:36, 6677.94it/s] 29%|██▊       | 257485/900675 [00:40<01:37, 6595.05it/s] 29%|██▊       | 258147/900675 [00:40<01:39, 6437.75it/s] 29%|██▊       | 258793/900675 [00:40<01:46, 6043.94it/s] 29%|██▉       | 259403/900675 [00:40<01:47, 5966.75it/s] 29%|██▉       | 260036/900675 [00:40<01:45, 6069.05it/s] 29%|██▉       | 260839/900675 [00:41<01:36, 6627.77it/s] 29%|██▉       | 261507/900675 [00:41<01:38, 6481.24it/s] 29%|██▉       | 262159/900675 [00:41<01:42, 6219.83it/s] 29%|██▉       | 262785/900675 [00:41<01:42, 6206.03it/s] 29%|██▉       | 263409/900675 [00:41<01:45, 6037.49it/s] 29%|██▉       | 264047/900675 [00:41<01:43, 6132.16it/s] 29%|██▉       | 264671/900675 [00:41<01:43, 6162.64it/s] 29%|██▉       | 265289/900675 [00:41<01:44, 6058.67it/s] 30%|██▉       | 265897/900675 [00:41<01:44, 6055.03it/s] 30%|██▉       | 266519/900675 [00:42<01:43, 6101.32it/s] 30%|██▉       | 267141/900675 [00:42<01:43, 6135.73it/s] 30%|██▉       | 267756/900675 [00:42<01:45, 6007.81it/s] 30%|██▉       | 268386/900675 [00:42<01:43, 6088.37it/s] 30%|██▉       | 269135/900675 [00:42<01:37, 6499.46it/s] 30%|██▉       | 269787/900675 [00:42<01:41, 6207.69it/s] 30%|███       | 270441/900675 [00:42<01:40, 6298.13it/s] 30%|███       | 271078/900675 [00:42<01:39, 6312.70it/s] 30%|███       | 271735/900675 [00:42<01:38, 6385.16it/s] 30%|███       | 272512/900675 [00:42<01:32, 6791.40it/s] 30%|███       | 273242/900675 [00:43<01:30, 6942.20it/s] 30%|███       | 273938/900675 [00:43<01:31, 6863.78it/s] 30%|███       | 274626/900675 [00:43<01:35, 6563.51it/s] 31%|███       | 275286/900675 [00:43<01:40, 6200.27it/s] 31%|███       | 275950/900675 [00:43<01:38, 6320.23it/s] 31%|███       | 276587/900675 [00:43<01:44, 5950.08it/s] 31%|███       | 277244/900675 [00:43<01:41, 6117.18it/s] 31%|███       | 277885/900675 [00:43<01:40, 6200.05it/s] 31%|███       | 278602/900675 [00:43<01:36, 6470.73it/s] 31%|███       | 279254/900675 [00:44<01:39, 6218.78it/s] 31%|███       | 279887/900675 [00:44<01:39, 6248.69it/s] 31%|███       | 280545/900675 [00:44<01:37, 6341.93it/s] 31%|███       | 281182/900675 [00:44<01:40, 6170.99it/s] 31%|███▏      | 281860/900675 [00:44<01:37, 6346.50it/s] 31%|███▏      | 282498/900675 [00:44<01:41, 6104.95it/s] 31%|███▏      | 283177/900675 [00:44<01:38, 6299.59it/s] 32%|███▏      | 283816/900675 [00:44<01:37, 6321.28it/s] 32%|███▏      | 284471/900675 [00:44<01:36, 6381.83it/s] 32%|███▏      | 285115/900675 [00:44<01:36, 6393.50it/s] 32%|███▏      | 285756/900675 [00:45<01:36, 6356.00it/s] 32%|███▏      | 286393/900675 [00:45<01:37, 6276.87it/s] 32%|███▏      | 287022/900675 [00:45<01:40, 6118.76it/s] 32%|███▏      | 287653/900675 [00:45<01:39, 6171.78it/s] 32%|███▏      | 288371/900675 [00:45<01:34, 6464.16it/s] 32%|███▏      | 289046/900675 [00:45<01:33, 6539.98it/s] 32%|███▏      | 289702/900675 [00:45<01:40, 6103.65it/s] 32%|███▏      | 290319/900675 [00:45<01:40, 6050.34it/s] 32%|███▏      | 290975/900675 [00:45<01:38, 6193.00it/s] 32%|███▏      | 291646/900675 [00:45<01:36, 6332.22it/s] 32%|███▏      | 292283/900675 [00:46<01:38, 6168.25it/s] 33%|███▎      | 292903/900675 [00:46<01:39, 6099.29it/s] 33%|███▎      | 293567/900675 [00:46<01:37, 6245.84it/s] 33%|███▎      | 294197/900675 [00:46<01:37, 6247.91it/s] 33%|███▎      | 294903/900675 [00:46<01:33, 6485.77it/s] 33%|███▎      | 295553/900675 [00:46<01:33, 6459.70it/s] 33%|███▎      | 296224/900675 [00:46<01:32, 6522.95it/s] 33%|███▎      | 296878/900675 [00:46<01:33, 6461.25it/s] 33%|███▎      | 297525/900675 [00:46<01:38, 6153.82it/s] 33%|███▎      | 298224/900675 [00:47<01:34, 6392.34it/s] 33%|███▎      | 298867/900675 [00:47<01:36, 6266.92it/s] 33%|███▎      | 299549/900675 [00:47<01:33, 6420.78it/s] 33%|███▎      | 300194/900675 [00:47<01:34, 6379.80it/s] 33%|███▎      | 300834/900675 [00:47<01:34, 6362.03it/s] 33%|███▎      | 301472/900675 [00:47<01:37, 6121.54it/s] 34%|███▎      | 302087/900675 [00:47<01:41, 5904.87it/s] 34%|███▎      | 302820/900675 [00:47<01:34, 6306.26it/s] 34%|███▎      | 303492/900675 [00:47<01:33, 6416.21it/s] 34%|███▍      | 304141/900675 [00:47<01:32, 6428.89it/s] 34%|███▍      | 304787/900675 [00:48<01:34, 6327.40it/s] 34%|███▍      | 305422/900675 [00:48<01:34, 6293.81it/s] 34%|███▍      | 306064/900675 [00:48<01:33, 6329.04it/s] 34%|███▍      | 306698/900675 [00:48<01:36, 6184.43it/s] 34%|███▍      | 307359/900675 [00:48<01:34, 6303.56it/s] 34%|███▍      | 307998/900675 [00:48<01:33, 6328.64it/s] 34%|███▍      | 308632/900675 [00:48<01:41, 5849.36it/s] 34%|███▍      | 309559/900675 [00:48<01:26, 6801.69it/s] 34%|███▍      | 310251/900675 [00:48<01:29, 6577.33it/s] 35%|███▍      | 310923/900675 [00:49<01:29, 6616.15it/s] 35%|███▍      | 311592/900675 [00:49<01:30, 6528.60it/s] 35%|███▍      | 312250/900675 [00:49<01:34, 6202.34it/s] 35%|███▍      | 312914/900675 [00:49<01:32, 6322.78it/s] 35%|███▍      | 313589/900675 [00:49<01:31, 6443.90it/s] 35%|███▍      | 314328/900675 [00:49<01:27, 6717.51it/s] 35%|███▍      | 315004/900675 [00:49<01:34, 6181.19it/s] 35%|███▌      | 315634/900675 [00:49<01:34, 6213.49it/s] 35%|███▌      | 316263/900675 [00:49<01:34, 6207.09it/s] 35%|███▌      | 316889/900675 [00:49<01:34, 6176.48it/s] 35%|███▌      | 317511/900675 [00:50<01:36, 6063.14it/s] 35%|███▌      | 318120/900675 [00:50<01:38, 5941.46it/s] 35%|███▌      | 318719/900675 [00:50<01:37, 5951.78it/s] 35%|███▌      | 319401/900675 [00:50<01:33, 6199.56it/s] 36%|███▌      | 320023/900675 [00:50<01:33, 6204.30it/s] 36%|███▌      | 320645/900675 [00:50<01:37, 5966.75it/s] 36%|███▌      | 321457/900675 [00:50<01:27, 6585.31it/s] 36%|███▌      | 322120/900675 [00:50<01:32, 6273.50it/s] 36%|███▌      | 322753/900675 [00:50<01:35, 6067.32it/s] 36%|███▌      | 323398/900675 [00:51<01:33, 6174.52it/s] 36%|███▌      | 324029/900675 [00:51<01:32, 6205.49it/s] 36%|███▌      | 324685/900675 [00:51<01:31, 6307.52it/s] 36%|███▌      | 325319/900675 [00:51<01:36, 5938.91it/s] 36%|███▌      | 325919/900675 [00:51<01:36, 5939.21it/s] 36%|███▋      | 326581/900675 [00:51<01:33, 6133.96it/s] 36%|███▋      | 327326/900675 [00:51<01:28, 6510.00it/s] 36%|███▋      | 328165/900675 [00:51<01:21, 7059.80it/s] 37%|███▋      | 328875/900675 [00:51<01:21, 7011.63it/s] 37%|███▋      | 329579/900675 [00:52<01:27, 6528.41it/s] 37%|███▋      | 330240/900675 [00:52<01:32, 6153.76it/s] 37%|███▋      | 330905/900675 [00:52<01:30, 6288.13it/s] 37%|███▋      | 331542/900675 [00:52<01:33, 6105.07it/s] 37%|███▋      | 332176/900675 [00:52<01:32, 6169.49it/s] 37%|███▋      | 332836/900675 [00:52<01:30, 6291.88it/s] 37%|███▋      | 333469/900675 [00:52<01:31, 6232.12it/s] 37%|███▋      | 334095/900675 [00:52<01:31, 6197.73it/s] 37%|███▋      | 334768/900675 [00:52<01:29, 6343.27it/s] 37%|███▋      | 335414/900675 [00:52<01:28, 6377.12it/s] 37%|███▋      | 336090/900675 [00:53<01:27, 6488.02it/s] 37%|███▋      | 336740/900675 [00:53<01:27, 6416.41it/s] 37%|███▋      | 337435/900675 [00:53<01:25, 6560.52it/s] 38%|███▊      | 338119/900675 [00:53<01:24, 6643.01it/s] 38%|███▊      | 338784/900675 [00:53<01:25, 6594.20it/s] 38%|███▊      | 339464/900675 [00:53<01:24, 6644.25it/s] 38%|███▊      | 340129/900675 [00:53<01:34, 5960.96it/s] 38%|███▊      | 340803/900675 [00:53<01:30, 6173.16it/s] 38%|███▊      | 341446/900675 [00:53<01:29, 6244.76it/s] 38%|███▊      | 342091/900675 [00:53<01:28, 6301.94it/s] 38%|███▊      | 342735/900675 [00:54<01:28, 6337.77it/s] 38%|███▊      | 343376/900675 [00:54<01:27, 6358.35it/s] 38%|███▊      | 344104/900675 [00:54<01:24, 6618.70it/s] 38%|███▊      | 344769/900675 [00:54<01:27, 6389.28it/s] 38%|███▊      | 345432/900675 [00:54<01:26, 6449.17it/s] 38%|███▊      | 346080/900675 [00:54<01:33, 5940.72it/s] 39%|███▊      | 346801/900675 [00:54<01:28, 6290.42it/s] 39%|███▊      | 347440/900675 [00:54<01:28, 6238.23it/s] 39%|███▊      | 348070/900675 [00:54<01:29, 6150.57it/s] 39%|███▊      | 348690/900675 [00:55<01:30, 6124.34it/s] 39%|███▉      | 349309/900675 [00:55<01:29, 6140.04it/s] 39%|███▉      | 350123/900675 [00:55<01:21, 6723.13it/s] 39%|███▉      | 350806/900675 [00:55<01:21, 6743.61it/s] 39%|███▉      | 351483/900675 [00:55<01:22, 6662.94it/s] 39%|███▉      | 352151/900675 [00:55<01:28, 6177.16it/s] 39%|███▉      | 352777/900675 [00:55<01:32, 5914.37it/s] 39%|███▉      | 353450/900675 [00:55<01:29, 6138.81it/s] 39%|███▉      | 354071/900675 [00:55<01:32, 5925.52it/s] 39%|███▉      | 354823/900675 [00:56<01:25, 6367.61it/s] 39%|███▉      | 355467/900675 [00:56<01:29, 6087.99it/s] 40%|███▉      | 356137/900675 [00:56<01:27, 6252.37it/s] 40%|███▉      | 356768/900675 [00:56<01:27, 6200.74it/s] 40%|███▉      | 357392/900675 [00:56<01:33, 5818.30it/s] 40%|███▉      | 358083/900675 [00:56<01:28, 6120.29it/s] 40%|███▉      | 358708/900675 [00:56<01:28, 6156.84it/s] 40%|███▉      | 359349/900675 [00:56<01:26, 6226.09it/s] 40%|███▉      | 359976/900675 [00:56<01:26, 6228.17it/s] 40%|████      | 360688/900675 [00:56<01:23, 6488.36it/s] 40%|████      | 361366/900675 [00:57<01:22, 6569.17it/s] 40%|████      | 362167/900675 [00:57<01:17, 6991.84it/s] 40%|████      | 362868/900675 [00:57<01:18, 6813.84it/s] 40%|████      | 363552/900675 [00:57<01:19, 6720.44it/s] 40%|████      | 364246/900675 [00:57<01:19, 6782.64it/s] 41%|████      | 364936/900675 [00:57<01:18, 6816.07it/s] 41%|████      | 365641/900675 [00:57<01:17, 6877.38it/s] 41%|████      | 366330/900675 [00:57<01:19, 6709.34it/s] 41%|████      | 367003/900675 [00:57<01:22, 6477.62it/s] 41%|████      | 367653/900675 [00:58<01:22, 6440.20it/s] 41%|████      | 368299/900675 [00:58<01:28, 6043.58it/s] 41%|████      | 369009/900675 [00:58<01:23, 6338.77it/s] 41%|████      | 369660/900675 [00:58<01:23, 6385.25it/s] 41%|████      | 370303/900675 [00:58<01:23, 6370.06it/s] 41%|████      | 370943/900675 [00:58<01:24, 6239.87it/s] 41%|████▏     | 371570/900675 [00:58<01:24, 6240.42it/s] 41%|████▏     | 372196/900675 [00:58<01:27, 6019.23it/s] 41%|████▏     | 372859/900675 [00:58<01:25, 6189.95it/s] 41%|████▏     | 373509/900675 [00:58<01:23, 6278.70it/s] 42%|████▏     | 374146/900675 [00:59<01:23, 6305.18it/s] 42%|████▏     | 374871/900675 [00:59<01:19, 6576.71it/s] 42%|████▏     | 375531/900675 [00:59<01:25, 6170.26it/s] 42%|████▏     | 376155/900675 [00:59<01:24, 6184.07it/s] 42%|████▏     | 376778/900675 [00:59<01:28, 5916.74it/s] 42%|████▏     | 377399/900675 [00:59<01:27, 5987.02it/s] 42%|████▏     | 378083/900675 [00:59<01:23, 6231.77it/s] 42%|████▏     | 378722/900675 [00:59<01:23, 6274.98it/s] 42%|████▏     | 379410/900675 [00:59<01:20, 6445.87it/s] 42%|████▏     | 380168/900675 [00:59<01:16, 6779.10it/s] 42%|████▏     | 380848/900675 [01:00<01:17, 6692.79it/s] 42%|████▏     | 381519/900675 [01:00<01:17, 6667.93it/s] 42%|████▏     | 382187/900675 [01:00<01:23, 6215.64it/s] 43%|████▎     | 382816/900675 [01:00<01:24, 6099.10it/s] 43%|████▎     | 383437/900675 [01:00<01:24, 6128.51it/s] 43%|████▎     | 384127/900675 [01:00<01:21, 6345.29it/s] 43%|████▎     | 384818/900675 [01:00<01:19, 6507.00it/s] 43%|████▎     | 385472/900675 [01:00<01:21, 6326.03it/s] 43%|████▎     | 386108/900675 [01:00<01:21, 6320.22it/s] 43%|████▎     | 386742/900675 [01:01<01:22, 6221.03it/s] 43%|████▎     | 387545/900675 [01:01<01:16, 6743.42it/s] 43%|████▎     | 388222/900675 [01:01<01:17, 6646.88it/s] 43%|████▎     | 388889/900675 [01:01<01:19, 6405.10it/s] 43%|████▎     | 389584/900675 [01:01<01:17, 6556.48it/s] 43%|████▎     | 390286/900675 [01:01<01:16, 6689.06it/s] 43%|████▎     | 390958/900675 [01:01<01:17, 6537.01it/s] 43%|████▎     | 391614/900675 [01:01<01:22, 6179.49it/s] 44%|████▎     | 392237/900675 [01:01<01:24, 6036.89it/s] 44%|████▎     | 392861/900675 [01:02<01:23, 6091.73it/s] 44%|████▎     | 393473/900675 [01:02<01:25, 5961.56it/s] 44%|████▍     | 394177/900675 [01:02<01:20, 6263.29it/s] 44%|████▍     | 394807/900675 [01:02<01:21, 6187.37it/s] 44%|████▍     | 395482/900675 [01:02<01:19, 6349.78it/s] 44%|████▍     | 396119/900675 [01:02<01:20, 6253.30it/s] 44%|████▍     | 396746/900675 [01:02<01:22, 6077.90it/s] 44%|████▍     | 397356/900675 [01:02<01:26, 5834.10it/s] 44%|████▍     | 398078/900675 [01:02<01:20, 6226.15it/s] 44%|████▍     | 398758/900675 [01:02<01:18, 6387.47it/s] 44%|████▍     | 399458/900675 [01:03<01:16, 6565.78it/s] 44%|████▍     | 400118/900675 [01:03<01:16, 6569.75it/s] 45%|████▍     | 400815/900675 [01:03<01:14, 6682.66it/s] 45%|████▍     | 401485/900675 [01:03<01:19, 6259.92it/s] 45%|████▍     | 402118/900675 [01:03<01:23, 5962.94it/s] 45%|████▍     | 402721/900675 [01:03<01:25, 5850.70it/s] 45%|████▍     | 403368/900675 [01:03<01:22, 6019.95it/s] 45%|████▍     | 403975/900675 [01:03<01:23, 5970.79it/s] 45%|████▍     | 404667/900675 [01:03<01:19, 6238.87it/s] 45%|████▍     | 405294/900675 [01:04<01:23, 5968.16it/s] 45%|████▌     | 405923/900675 [01:04<01:21, 6055.39it/s] 45%|████▌     | 406532/900675 [01:04<01:24, 5874.91it/s] 45%|████▌     | 407204/900675 [01:04<01:20, 6111.18it/s] 45%|████▌     | 407819/900675 [01:04<01:22, 5965.54it/s] 45%|████▌     | 408481/900675 [01:04<01:20, 6151.77it/s] 45%|████▌     | 409172/900675 [01:04<01:17, 6369.43it/s] 46%|████▌     | 409812/900675 [01:04<01:19, 6210.60it/s] 46%|████▌     | 410455/900675 [01:04<01:18, 6271.86it/s] 46%|████▌     | 411085/900675 [01:04<01:19, 6188.60it/s] 46%|████▌     | 411709/900675 [01:05<01:18, 6197.96it/s] 46%|████▌     | 412330/900675 [01:05<01:23, 5854.52it/s] 46%|████▌     | 412920/900675 [01:05<01:25, 5735.01it/s] 46%|████▌     | 413547/900675 [01:05<01:22, 5880.24it/s] 46%|████▌     | 414138/900675 [01:05<01:25, 5690.72it/s] 46%|████▌     | 414710/900675 [01:05<01:27, 5566.96it/s] 46%|████▌     | 415348/900675 [01:05<01:23, 5797.12it/s] 46%|████▌     | 415961/900675 [01:05<01:22, 5889.96it/s] 46%|████▌     | 416553/900675 [01:05<01:22, 5853.26it/s] 46%|████▋     | 417161/900675 [01:06<01:21, 5918.61it/s] 46%|████▋     | 417856/900675 [01:06<01:17, 6222.66it/s] 46%|████▋     | 418480/900675 [01:06<01:20, 5985.01it/s] 47%|████▋     | 419197/900675 [01:06<01:16, 6316.94it/s] 47%|████▋     | 419832/900675 [01:06<01:17, 6234.16it/s] 47%|████▋     | 420469/900675 [01:06<01:16, 6271.39it/s] 47%|████▋     | 421098/900675 [01:06<01:17, 6163.84it/s] 47%|████▋     | 421716/900675 [01:06<01:20, 5931.01it/s] 47%|████▋     | 422325/900675 [01:06<01:20, 5973.12it/s] 47%|████▋     | 423065/900675 [01:06<01:14, 6381.48it/s] 47%|████▋     | 423706/900675 [01:07<01:16, 6260.00it/s] 47%|████▋     | 424335/900675 [01:07<01:16, 6240.92it/s] 47%|████▋     | 424961/900675 [01:07<01:18, 6027.26it/s] 47%|████▋     | 425568/900675 [01:07<01:18, 6038.29it/s] 47%|████▋     | 426359/900675 [01:07<01:12, 6582.67it/s] 47%|████▋     | 427025/900675 [01:07<01:11, 6597.30it/s] 47%|████▋     | 427687/900675 [01:07<01:12, 6550.02it/s] 48%|████▊     | 428394/900675 [01:07<01:10, 6698.58it/s] 48%|████▊     | 429066/900675 [01:07<01:13, 6456.29it/s] 48%|████▊     | 429750/900675 [01:07<01:11, 6559.12it/s] 48%|████▊     | 430409/900675 [01:08<01:11, 6532.01it/s] 48%|████▊     | 431064/900675 [01:08<01:16, 6143.91it/s] 48%|████▊     | 431684/900675 [01:08<01:16, 6091.76it/s] 48%|████▊     | 432365/900675 [01:08<01:14, 6290.26it/s] 48%|████▊     | 433223/900675 [01:08<01:07, 6945.42it/s] 48%|████▊     | 433923/900675 [01:08<01:10, 6612.57it/s] 48%|████▊     | 434591/900675 [01:08<01:15, 6160.39it/s] 48%|████▊     | 435216/900675 [01:08<01:16, 6073.00it/s] 48%|████▊     | 435917/900675 [01:08<01:13, 6323.53it/s] 48%|████▊     | 436556/900675 [01:09<01:15, 6130.84it/s] 49%|████▊     | 437276/900675 [01:09<01:12, 6424.34it/s] 49%|████▊     | 437924/900675 [01:09<01:13, 6303.79it/s] 49%|████▊     | 438559/900675 [01:09<01:13, 6247.98it/s] 49%|████▉     | 439187/900675 [01:09<01:16, 6047.14it/s] 49%|████▉     | 439795/900675 [01:09<01:17, 5977.55it/s] 49%|████▉     | 440395/900675 [01:09<01:17, 5957.84it/s] 49%|████▉     | 441020/900675 [01:09<01:16, 6038.82it/s] 49%|████▉     | 441674/900675 [01:09<01:14, 6181.14it/s] 49%|████▉     | 442349/900675 [01:10<01:12, 6348.55it/s] 49%|████▉     | 442985/900675 [01:10<01:12, 6271.00it/s] 49%|████▉     | 443682/900675 [01:10<01:10, 6473.29it/s] 49%|████▉     | 444331/900675 [01:10<01:11, 6386.59it/s] 49%|████▉     | 444971/900675 [01:10<01:16, 5992.43it/s] 49%|████▉     | 445598/900675 [01:10<01:15, 6064.28it/s] 50%|████▉     | 446209/900675 [01:10<01:15, 5982.20it/s] 50%|████▉     | 446810/900675 [01:10<01:16, 5968.36it/s] 50%|████▉     | 447497/900675 [01:10<01:12, 6228.55it/s] 50%|████▉     | 448122/900675 [01:10<01:12, 6231.21it/s] 50%|████▉     | 448827/900675 [01:11<01:09, 6471.99it/s] 50%|████▉     | 449476/900675 [01:11<01:13, 6166.14it/s] 50%|████▉     | 450158/900675 [01:11<01:10, 6349.97it/s] 50%|█████     | 450828/900675 [01:11<01:09, 6450.91it/s] 50%|█████     | 451477/900675 [01:11<01:09, 6459.38it/s] 50%|█████     | 452125/900675 [01:11<01:13, 6067.31it/s] 50%|█████     | 452738/900675 [01:11<01:14, 6019.44it/s] 50%|█████     | 453411/900675 [01:11<01:11, 6218.71it/s] 50%|█████     | 454037/900675 [01:11<01:13, 6057.37it/s] 50%|█████     | 454646/900675 [01:12<01:13, 6046.97it/s] 51%|█████     | 455263/900675 [01:12<01:13, 6075.91it/s] 51%|█████     | 455873/900675 [01:12<01:14, 6002.42it/s] 51%|█████     | 456604/900675 [01:12<01:09, 6379.42it/s] 51%|█████     | 457364/900675 [01:12<01:05, 6732.34it/s] 51%|█████     | 458040/900675 [01:12<01:08, 6482.75it/s] 51%|█████     | 458692/900675 [01:12<01:10, 6289.77it/s] 51%|█████     | 459436/900675 [01:12<01:06, 6613.38it/s] 51%|█████     | 460101/900675 [01:12<01:08, 6396.17it/s] 51%|█████     | 460745/900675 [01:12<01:11, 6134.47it/s] 51%|█████     | 461385/900675 [01:13<01:10, 6205.89it/s] 51%|█████▏    | 462009/900675 [01:13<01:11, 6176.59it/s] 51%|█████▏    | 462629/900675 [01:13<01:11, 6156.16it/s] 51%|█████▏    | 463296/900675 [01:13<01:09, 6303.86it/s] 52%|█████▏    | 463928/900675 [01:13<01:11, 6111.40it/s] 52%|█████▏    | 464577/900675 [01:13<01:10, 6220.00it/s] 52%|█████▏    | 465228/900675 [01:13<01:09, 6299.99it/s] 52%|█████▏    | 465860/900675 [01:13<01:09, 6266.08it/s] 52%|█████▏    | 466506/900675 [01:13<01:08, 6320.66it/s] 52%|█████▏    | 467139/900675 [01:13<01:09, 6246.18it/s] 52%|█████▏    | 467796/900675 [01:14<01:08, 6337.33it/s] 52%|█████▏    | 468450/900675 [01:14<01:07, 6389.65it/s] 52%|█████▏    | 469100/900675 [01:14<01:07, 6421.59it/s] 52%|█████▏    | 469743/900675 [01:14<01:09, 6196.18it/s] 52%|█████▏    | 470365/900675 [01:14<01:10, 6095.15it/s] 52%|█████▏    | 471094/900675 [01:14<01:06, 6436.92it/s] 52%|█████▏    | 471740/900675 [01:14<01:07, 6384.67it/s] 52%|█████▏    | 472380/900675 [01:14<01:09, 6137.07it/s] 53%|█████▎    | 473049/900675 [01:14<01:07, 6295.39it/s] 53%|█████▎    | 473711/900675 [01:15<01:06, 6382.52it/s] 53%|█████▎    | 474352/900675 [01:15<01:09, 6161.02it/s] 53%|█████▎    | 475069/900675 [01:15<01:05, 6449.46it/s] 53%|█████▎    | 475891/900675 [01:15<01:01, 6956.50it/s] 53%|█████▎    | 476591/900675 [01:15<01:06, 6412.46it/s] 53%|█████▎    | 477243/900675 [01:15<01:07, 6278.07it/s] 53%|█████▎    | 477878/900675 [01:15<01:07, 6258.62it/s] 53%|█████▎    | 478621/900675 [01:15<01:04, 6590.14it/s] 53%|█████▎    | 479344/900675 [01:15<01:02, 6774.38it/s] 53%|█████▎    | 480026/900675 [01:16<01:07, 6241.36it/s] 53%|█████▎    | 480676/900675 [01:16<01:06, 6310.52it/s] 53%|█████▎    | 481315/900675 [01:16<01:07, 6242.97it/s] 54%|█████▎    | 481945/900675 [01:16<01:07, 6199.69it/s] 54%|█████▎    | 482569/900675 [01:16<01:10, 5958.99it/s] 54%|█████▎    | 483268/900675 [01:16<01:06, 6249.19it/s] 54%|█████▎    | 483898/900675 [01:16<01:08, 6120.49it/s] 54%|█████▍    | 484568/900675 [01:16<01:06, 6285.55it/s] 54%|█████▍    | 485329/900675 [01:16<01:02, 6668.83it/s] 54%|█████▍    | 486028/900675 [01:16<01:01, 6762.74it/s] 54%|█████▍    | 486707/900675 [01:17<01:05, 6277.08it/s] 54%|█████▍    | 487359/900675 [01:17<01:05, 6345.08it/s] 54%|█████▍    | 488008/900675 [01:17<01:04, 6376.19it/s] 54%|█████▍    | 488688/900675 [01:17<01:03, 6498.84it/s] 54%|█████▍    | 489342/900675 [01:17<01:04, 6379.13it/s] 54%|█████▍    | 489983/900675 [01:17<01:07, 6067.78it/s] 54%|█████▍    | 490595/900675 [01:17<01:08, 6025.96it/s] 55%|█████▍    | 491212/900675 [01:17<01:07, 6067.01it/s] 55%|█████▍    | 491895/900675 [01:17<01:05, 6287.73it/s] 55%|█████▍    | 492527/900675 [01:17<01:04, 6290.86it/s] 55%|█████▍    | 493164/900675 [01:18<01:04, 6310.98it/s] 55%|█████▍    | 493797/900675 [01:18<01:06, 6131.32it/s] 55%|█████▍    | 494573/900675 [01:18<01:01, 6604.49it/s] 55%|█████▍    | 495237/900675 [01:18<01:01, 6591.64it/s] 55%|█████▌    | 495899/900675 [01:18<01:01, 6552.75it/s] 55%|█████▌    | 496556/900675 [01:18<01:03, 6362.05it/s] 55%|█████▌    | 497209/900675 [01:18<01:02, 6408.53it/s] 55%|█████▌    | 497852/900675 [01:18<01:02, 6410.79it/s] 55%|█████▌    | 498495/900675 [01:18<01:03, 6306.58it/s] 55%|█████▌    | 499203/900675 [01:19<01:01, 6528.43it/s] 56%|█████▌    | 499897/900675 [01:19<01:00, 6649.63it/s] 56%|█████▌    | 500565/900675 [01:19<01:00, 6657.69it/s] 56%|█████▌    | 501232/900675 [01:19<01:00, 6632.13it/s] 56%|█████▌    | 501896/900675 [01:19<01:03, 6328.66it/s] 56%|█████▌    | 502532/900675 [01:19<01:04, 6205.63it/s] 56%|█████▌    | 503155/900675 [01:19<01:04, 6210.96it/s] 56%|█████▌    | 503787/900675 [01:19<01:03, 6241.10it/s] 56%|█████▌    | 504413/900675 [01:19<01:05, 6066.60it/s] 56%|█████▌    | 505059/900675 [01:19<01:04, 6180.33it/s] 56%|█████▌    | 505728/900675 [01:20<01:02, 6323.15it/s] 56%|█████▌    | 506438/900675 [01:20<01:00, 6548.51it/s] 56%|█████▋    | 507095/900675 [01:20<01:01, 6393.51it/s] 56%|█████▋    | 507736/900675 [01:20<01:05, 6043.28it/s] 56%|█████▋    | 508378/900675 [01:20<01:03, 6141.06it/s] 57%|█████▋    | 508996/900675 [01:20<01:04, 6112.17it/s] 57%|█████▋    | 509725/900675 [01:20<01:00, 6451.44it/s] 57%|█████▋    | 510374/900675 [01:20<01:01, 6373.50it/s] 57%|█████▋    | 511014/900675 [01:20<01:01, 6351.62it/s] 57%|█████▋    | 511790/900675 [01:20<00:57, 6762.13it/s] 57%|█████▋    | 512469/900675 [01:21<01:00, 6432.30it/s] 57%|█████▋    | 513209/900675 [01:21<00:57, 6707.34it/s] 57%|█████▋    | 513885/900675 [01:21<00:58, 6576.90it/s] 57%|█████▋    | 514546/900675 [01:21<01:00, 6430.69it/s] 57%|█████▋    | 515192/900675 [01:21<01:03, 6047.05it/s] 57%|█████▋    | 515970/900675 [01:21<00:58, 6527.88it/s] 57%|█████▋    | 516631/900675 [01:21<01:01, 6285.05it/s] 57%|█████▋    | 517266/900675 [01:21<01:02, 6174.18it/s] 58%|█████▊    | 517960/900675 [01:21<00:59, 6380.89it/s] 58%|█████▊    | 518617/900675 [01:22<00:59, 6429.41it/s] 58%|█████▊    | 519264/900675 [01:22<01:01, 6192.05it/s] 58%|█████▊    | 519948/900675 [01:22<00:59, 6376.63it/s] 58%|█████▊    | 520590/900675 [01:22<01:01, 6197.37it/s] 58%|█████▊    | 521283/900675 [01:22<00:59, 6405.75it/s] 58%|█████▊    | 522011/900675 [01:22<00:56, 6651.80it/s] 58%|█████▊    | 522680/900675 [01:22<01:00, 6280.70it/s] 58%|█████▊    | 523314/900675 [01:22<01:00, 6213.87it/s] 58%|█████▊    | 523946/900675 [01:22<01:00, 6239.33it/s] 58%|█████▊    | 524573/900675 [01:23<01:01, 6101.38it/s] 58%|█████▊    | 525186/900675 [01:23<01:02, 6036.05it/s] 58%|█████▊    | 525792/900675 [01:23<01:03, 5935.41it/s] 58%|█████▊    | 526455/900675 [01:23<01:01, 6130.16it/s] 59%|█████▊    | 527212/900675 [01:23<00:57, 6543.35it/s] 59%|█████▊    | 527869/900675 [01:23<01:01, 6106.12it/s] 59%|█████▊    | 528592/900675 [01:23<00:57, 6417.94it/s] 59%|█████▉    | 529242/900675 [01:23<00:57, 6428.72it/s] 59%|█████▉    | 529890/900675 [01:23<00:58, 6319.00it/s] 59%|█████▉    | 530526/900675 [01:24<01:02, 5950.95it/s] 59%|█████▉    | 531186/900675 [01:24<01:00, 6120.58it/s] 59%|█████▉    | 531870/900675 [01:24<00:58, 6315.67it/s] 59%|█████▉    | 532507/900675 [01:24<00:59, 6208.94it/s] 59%|█████▉    | 533195/900675 [01:24<00:57, 6398.19it/s] 59%|█████▉    | 533838/900675 [01:24<01:00, 6112.74it/s] 59%|█████▉    | 534454/900675 [01:24<01:00, 6011.85it/s] 59%|█████▉    | 535112/900675 [01:24<00:59, 6173.25it/s] 59%|█████▉    | 535733/900675 [01:24<00:59, 6156.04it/s] 60%|█████▉    | 536351/900675 [01:24<00:59, 6107.86it/s] 60%|█████▉    | 537018/900675 [01:25<00:58, 6267.37it/s] 60%|█████▉    | 537647/900675 [01:25<00:59, 6120.96it/s] 60%|█████▉    | 538265/900675 [01:25<00:59, 6133.47it/s] 60%|█████▉    | 539033/900675 [01:25<00:54, 6586.65it/s] 60%|█████▉    | 539694/900675 [01:25<00:58, 6196.94it/s] 60%|█████▉    | 540336/900675 [01:25<00:57, 6258.00it/s] 60%|██████    | 540967/900675 [01:25<00:58, 6192.58it/s] 60%|██████    | 541598/900675 [01:25<00:57, 6224.79it/s] 60%|██████    | 542223/900675 [01:25<00:58, 6116.43it/s] 60%|██████    | 542991/900675 [01:25<00:54, 6568.37it/s] 60%|██████    | 543651/900675 [01:26<00:56, 6359.16it/s] 60%|██████    | 544319/900675 [01:26<00:55, 6445.07it/s] 61%|██████    | 544966/900675 [01:26<00:56, 6254.52it/s] 61%|██████    | 545594/900675 [01:26<00:56, 6253.66it/s] 61%|██████    | 546222/900675 [01:26<00:56, 6250.78it/s] 61%|██████    | 546870/900675 [01:26<00:56, 6317.12it/s] 61%|██████    | 547503/900675 [01:26<00:56, 6300.05it/s] 61%|██████    | 548228/900675 [01:26<00:53, 6573.38it/s] 61%|██████    | 548887/900675 [01:26<00:54, 6512.91it/s] 61%|██████    | 549539/900675 [01:27<00:55, 6338.11it/s] 61%|██████    | 550204/900675 [01:27<00:54, 6427.84it/s] 61%|██████    | 550857/900675 [01:27<00:54, 6457.55it/s] 61%|██████    | 551504/900675 [01:27<00:54, 6410.82it/s] 61%|██████▏   | 552146/900675 [01:27<00:55, 6245.79it/s] 61%|██████▏   | 552772/900675 [01:27<00:56, 6197.07it/s] 61%|██████▏   | 553402/900675 [01:27<00:55, 6220.38it/s] 62%|██████▏   | 554043/900675 [01:27<00:55, 6273.19it/s] 62%|██████▏   | 554671/900675 [01:27<00:56, 6126.61it/s] 62%|██████▏   | 555285/900675 [01:27<00:56, 6100.02it/s] 62%|██████▏   | 555982/900675 [01:28<00:54, 6339.99it/s] 62%|██████▏   | 556649/900675 [01:28<00:53, 6436.69it/s] 62%|██████▏   | 557294/900675 [01:28<00:55, 6190.49it/s] 62%|██████▏   | 557933/900675 [01:28<00:54, 6247.93it/s] 62%|██████▏   | 558560/900675 [01:28<00:55, 6120.12it/s] 62%|██████▏   | 559174/900675 [01:28<00:58, 5838.70it/s] 62%|██████▏   | 559808/900675 [01:28<00:57, 5974.06it/s] 62%|██████▏   | 560409/900675 [01:28<00:57, 5910.67it/s] 62%|██████▏   | 561062/900675 [01:28<00:55, 6087.48it/s] 62%|██████▏   | 561764/900675 [01:28<00:53, 6357.06it/s] 62%|██████▏   | 562479/900675 [01:29<00:51, 6583.13it/s] 63%|██████▎   | 563140/900675 [01:29<00:51, 6587.20it/s] 63%|██████▎   | 563801/900675 [01:29<00:54, 6132.85it/s] 63%|██████▎   | 564430/900675 [01:29<00:54, 6170.57it/s] 63%|██████▎   | 565053/900675 [01:29<00:55, 6030.73it/s] 63%|██████▎   | 565756/900675 [01:29<00:53, 6315.37it/s] 63%|██████▎   | 566392/900675 [01:29<00:56, 5957.80it/s] 63%|██████▎   | 566995/900675 [01:29<00:56, 5854.68it/s] 63%|██████▎   | 567767/900675 [01:29<00:52, 6379.04it/s] 63%|██████▎   | 568412/900675 [01:30<00:53, 6203.48it/s] 63%|██████▎   | 569061/900675 [01:30<00:52, 6282.39it/s] 63%|██████▎   | 569694/900675 [01:30<00:53, 6244.67it/s] 63%|██████▎   | 570420/900675 [01:30<00:50, 6533.46it/s] 63%|██████▎   | 571239/900675 [01:30<00:46, 7014.88it/s] 64%|██████▎   | 571944/900675 [01:30<00:47, 6879.80it/s] 64%|██████▎   | 572635/900675 [01:30<00:49, 6598.78it/s] 64%|██████▎   | 573299/900675 [01:30<00:51, 6333.51it/s] 64%|██████▎   | 573978/900675 [01:30<00:50, 6459.02it/s] 64%|██████▍   | 574628/900675 [01:31<00:52, 6153.01it/s] 64%|██████▍   | 575248/900675 [01:31<00:53, 6129.51it/s] 64%|██████▍   | 575864/900675 [01:31<00:53, 6112.50it/s] 64%|██████▍   | 576491/900675 [01:31<00:52, 6153.77it/s] 64%|██████▍   | 577200/900675 [01:31<00:50, 6420.77it/s] 64%|██████▍   | 577844/900675 [01:31<00:52, 6207.56it/s] 64%|██████▍   | 578468/900675 [01:31<00:54, 5933.75it/s] 64%|██████▍   | 579105/900675 [01:31<00:53, 6048.98it/s] 64%|██████▍   | 579810/900675 [01:31<00:50, 6330.91it/s] 64%|██████▍   | 580452/900675 [01:31<00:50, 6354.70it/s] 65%|██████▍   | 581105/900675 [01:32<00:49, 6399.79it/s] 65%|██████▍   | 581767/900675 [01:32<00:49, 6461.92it/s] 65%|██████▍   | 582415/900675 [01:32<00:49, 6425.05it/s] 65%|██████▍   | 583104/900675 [01:32<00:48, 6555.74it/s] 65%|██████▍   | 583873/900675 [01:32<00:45, 6892.64it/s] 65%|██████▍   | 584652/900675 [01:32<00:44, 7159.53it/s] 65%|██████▍   | 585369/900675 [01:32<00:44, 7105.99it/s] 65%|██████▌   | 586081/900675 [01:32<00:47, 6598.73it/s] 65%|██████▌   | 586749/900675 [01:32<00:48, 6510.43it/s] 65%|██████▌   | 587406/900675 [01:33<00:52, 6002.61it/s] 65%|██████▌   | 588077/900675 [01:33<00:50, 6186.66it/s] 65%|██████▌   | 588705/900675 [01:33<00:50, 6209.11it/s] 65%|██████▌   | 589370/900675 [01:33<00:49, 6333.95it/s] 66%|██████▌   | 590038/900675 [01:33<00:48, 6432.38it/s] 66%|██████▌   | 590712/900675 [01:33<00:47, 6522.08it/s] 66%|██████▌   | 591368/900675 [01:33<00:50, 6143.49it/s] 66%|██████▌   | 592105/900675 [01:33<00:47, 6488.55it/s] 66%|██████▌   | 592761/900675 [01:33<00:50, 6135.93it/s] 66%|██████▌   | 593383/900675 [01:33<00:50, 6096.50it/s] 66%|██████▌   | 594018/900675 [01:34<00:49, 6164.38it/s] 66%|██████▌   | 594674/900675 [01:34<00:48, 6275.84it/s] 66%|██████▌   | 595305/900675 [01:34<00:50, 6059.75it/s] 66%|██████▌   | 595949/900675 [01:34<00:49, 6161.96it/s] 66%|██████▌   | 596608/900675 [01:34<00:48, 6281.01it/s] 66%|██████▋   | 597285/900675 [01:34<00:47, 6416.10it/s] 66%|██████▋   | 597929/900675 [01:34<00:49, 6149.78it/s] 66%|██████▋   | 598614/900675 [01:34<00:47, 6348.22it/s] 67%|██████▋   | 599253/900675 [01:34<00:48, 6262.48it/s] 67%|██████▋   | 599941/900675 [01:35<00:46, 6440.96it/s] 67%|██████▋   | 600588/900675 [01:35<00:47, 6343.90it/s] 67%|██████▋   | 601225/900675 [01:35<00:48, 6123.35it/s] 67%|██████▋   | 601840/900675 [01:35<00:51, 5831.90it/s] 67%|██████▋   | 602473/900675 [01:35<00:49, 5971.47it/s] 67%|██████▋   | 603074/900675 [01:35<00:49, 5960.28it/s] 67%|██████▋   | 603790/900675 [01:35<00:47, 6307.21it/s] 67%|██████▋   | 604424/900675 [01:35<00:47, 6258.02it/s] 67%|██████▋   | 605069/900675 [01:35<00:46, 6307.67it/s] 67%|██████▋   | 605702/900675 [01:35<00:47, 6150.41it/s] 67%|██████▋   | 606375/900675 [01:36<00:46, 6312.10it/s] 67%|██████▋   | 607032/900675 [01:36<00:45, 6387.55it/s] 67%|██████▋   | 607673/900675 [01:36<00:46, 6271.97it/s] 68%|██████▊   | 608304/900675 [01:36<00:46, 6280.09it/s] 68%|██████▊   | 608933/900675 [01:36<00:49, 5922.13it/s] 68%|██████▊   | 609626/900675 [01:36<00:46, 6207.48it/s] 68%|██████▊   | 610284/900675 [01:36<00:46, 6308.50it/s] 68%|██████▊   | 611014/900675 [01:36<00:43, 6592.52it/s] 68%|██████▊   | 611677/900675 [01:36<00:45, 6418.56it/s] 68%|██████▊   | 612322/900675 [01:36<00:45, 6351.78it/s] 68%|██████▊   | 612960/900675 [01:37<00:47, 6105.93it/s] 68%|██████▊   | 613574/900675 [01:37<00:47, 6062.72it/s] 68%|██████▊   | 614183/900675 [01:37<00:48, 5949.02it/s] 68%|██████▊   | 614780/900675 [01:37<00:49, 5829.53it/s] 68%|██████▊   | 615424/900675 [01:37<00:47, 6003.43it/s] 68%|██████▊   | 616026/900675 [01:37<00:48, 5838.46it/s] 68%|██████▊   | 616818/900675 [01:37<00:44, 6435.36it/s] 69%|██████▊   | 617545/900675 [01:37<00:42, 6673.86it/s] 69%|██████▊   | 618246/900675 [01:37<00:41, 6769.97it/s] 69%|██████▊   | 618974/900675 [01:38<00:40, 6915.22it/s] 69%|██████▉   | 619731/900675 [01:38<00:39, 7104.83it/s] 69%|██████▉   | 620547/900675 [01:38<00:37, 7415.67it/s] 69%|██████▉   | 621290/900675 [01:38<00:41, 6707.20it/s] 69%|██████▉   | 621988/900675 [01:38<00:41, 6776.76it/s] 69%|██████▉   | 622730/900675 [01:38<00:39, 6958.89it/s] 69%|██████▉   | 623437/900675 [01:38<00:39, 6990.46it/s] 69%|██████▉   | 624142/900675 [01:38<00:42, 6460.12it/s] 69%|██████▉   | 624800/900675 [01:38<00:43, 6280.36it/s] 69%|██████▉   | 625656/900675 [01:39<00:39, 6909.73it/s] 70%|██████▉   | 626358/900675 [01:39<00:40, 6707.58it/s] 70%|██████▉   | 627037/900675 [01:39<00:42, 6384.44it/s] 70%|██████▉   | 627692/900675 [01:39<00:42, 6426.76it/s] 70%|██████▉   | 628341/900675 [01:39<00:42, 6367.45it/s] 70%|██████▉   | 629027/900675 [01:39<00:41, 6503.63it/s] 70%|██████▉   | 629681/900675 [01:39<00:41, 6463.06it/s] 70%|██████▉   | 630330/900675 [01:39<00:43, 6173.77it/s] 70%|███████   | 631101/900675 [01:39<00:40, 6609.46it/s] 70%|███████   | 631825/900675 [01:39<00:39, 6789.70it/s] 70%|███████   | 632509/900675 [01:40<00:40, 6697.58it/s] 70%|███████   | 633271/900675 [01:40<00:38, 6965.80it/s] 70%|███████   | 633971/900675 [01:40<00:38, 6914.49it/s] 70%|███████   | 634665/900675 [01:40<00:40, 6628.18it/s] 71%|███████   | 635332/900675 [01:40<00:40, 6619.77it/s] 71%|███████   | 636042/900675 [01:40<00:39, 6757.12it/s] 71%|███████   | 636720/900675 [01:40<00:42, 6272.95it/s] 71%|███████   | 637377/900675 [01:40<00:41, 6353.87it/s] 71%|███████   | 638105/900675 [01:40<00:39, 6616.46it/s] 71%|███████   | 638773/900675 [01:41<00:40, 6504.19it/s] 71%|███████   | 639428/900675 [01:41<00:40, 6385.38it/s] 71%|███████   | 640070/900675 [01:41<00:41, 6258.14it/s] 71%|███████   | 640808/900675 [01:41<00:39, 6579.12it/s] 71%|███████   | 641575/900675 [01:41<00:37, 6891.47it/s] 71%|███████▏  | 642268/900675 [01:41<00:37, 6808.37it/s] 71%|███████▏  | 642952/900675 [01:41<00:39, 6493.92it/s] 71%|███████▏  | 643606/900675 [01:41<00:41, 6258.07it/s] 72%|███████▏  | 644236/900675 [01:41<00:41, 6166.02it/s] 72%|███████▏  | 644856/900675 [01:41<00:42, 6075.15it/s] 72%|███████▏  | 645554/900675 [01:42<00:40, 6330.84it/s] 72%|███████▏  | 646222/900675 [01:42<00:39, 6423.71it/s] 72%|███████▏  | 646867/900675 [01:42<00:40, 6272.46it/s] 72%|███████▏  | 647497/900675 [01:42<00:41, 6105.04it/s] 72%|███████▏  | 648110/900675 [01:42<00:44, 5654.19it/s] 72%|███████▏  | 648724/900675 [01:42<00:43, 5782.04it/s] 72%|███████▏  | 649316/900675 [01:42<00:43, 5814.34it/s] 72%|███████▏  | 649902/900675 [01:42<00:43, 5725.77it/s] 72%|███████▏  | 650512/900675 [01:42<00:42, 5832.30it/s] 72%|███████▏  | 651108/900675 [01:43<00:42, 5861.74it/s] 72%|███████▏  | 651844/900675 [01:43<00:39, 6299.85it/s] 72%|███████▏  | 652477/900675 [01:43<00:40, 6137.60it/s] 73%|███████▎  | 653234/900675 [01:43<00:37, 6552.40it/s] 73%|███████▎  | 653893/900675 [01:43<00:38, 6379.19it/s] 73%|███████▎  | 654602/900675 [01:43<00:37, 6578.15it/s] 73%|███████▎  | 655263/900675 [01:43<00:38, 6436.45it/s] 73%|███████▎  | 655909/900675 [01:43<00:38, 6386.54it/s] 73%|███████▎  | 656550/900675 [01:43<00:38, 6318.48it/s] 73%|███████▎  | 657223/900675 [01:43<00:37, 6436.12it/s] 73%|███████▎  | 657868/900675 [01:44<00:38, 6253.69it/s] 73%|███████▎  | 658496/900675 [01:44<00:38, 6261.06it/s] 73%|███████▎  | 659124/900675 [01:44<00:38, 6251.83it/s] 73%|███████▎  | 659797/900675 [01:44<00:37, 6390.90it/s] 73%|███████▎  | 660443/900675 [01:44<00:37, 6408.05it/s] 73%|███████▎  | 661085/900675 [01:44<00:37, 6365.84it/s] 73%|███████▎  | 661723/900675 [01:44<00:37, 6363.80it/s] 74%|███████▎  | 662377/900675 [01:44<00:37, 6409.13it/s] 74%|███████▎  | 663019/900675 [01:44<00:38, 6225.35it/s] 74%|███████▎  | 663643/900675 [01:45<00:38, 6153.59it/s] 74%|███████▍  | 664359/900675 [01:45<00:36, 6443.23it/s] 74%|███████▍  | 665005/900675 [01:45<00:37, 6340.00it/s] 74%|███████▍  | 665664/900675 [01:45<00:36, 6409.43it/s] 74%|███████▍  | 666366/900675 [01:45<00:35, 6588.67it/s] 74%|███████▍  | 667026/900675 [01:45<00:36, 6447.72it/s] 74%|███████▍  | 667673/900675 [01:45<00:36, 6340.22it/s] 74%|███████▍  | 668331/900675 [01:45<00:36, 6403.31it/s] 74%|███████▍  | 669034/900675 [01:45<00:35, 6585.84it/s] 74%|███████▍  | 669765/900675 [01:45<00:33, 6798.23it/s] 74%|███████▍  | 670446/900675 [01:46<00:35, 6548.52it/s] 75%|███████▍  | 671140/900675 [01:46<00:34, 6661.02it/s] 75%|███████▍  | 671813/900675 [01:46<00:34, 6673.33it/s] 75%|███████▍  | 672482/900675 [01:46<00:34, 6531.18it/s] 75%|███████▍  | 673184/900675 [01:46<00:34, 6672.26it/s] 75%|███████▍  | 673853/900675 [01:46<00:34, 6482.02it/s] 75%|███████▍  | 674504/900675 [01:46<00:35, 6393.66it/s] 75%|███████▍  | 675182/900675 [01:46<00:34, 6498.45it/s] 75%|███████▌  | 675877/900675 [01:46<00:33, 6627.38it/s] 75%|███████▌  | 676541/900675 [01:46<00:35, 6334.68it/s] 75%|███████▌  | 677178/900675 [01:47<00:35, 6214.01it/s] 75%|███████▌  | 677823/900675 [01:47<00:35, 6272.57it/s] 75%|███████▌  | 678453/900675 [01:47<00:36, 6153.86it/s] 75%|███████▌  | 679070/900675 [01:47<00:35, 6157.23it/s] 75%|███████▌  | 679687/900675 [01:47<00:36, 5991.03it/s] 76%|███████▌  | 680451/900675 [01:47<00:34, 6464.92it/s] 76%|███████▌  | 681145/900675 [01:47<00:33, 6593.32it/s] 76%|███████▌  | 681807/900675 [01:47<00:34, 6431.06it/s] 76%|███████▌  | 682453/900675 [01:47<00:35, 6210.11it/s] 76%|███████▌  | 683107/900675 [01:48<00:34, 6297.94it/s] 76%|███████▌  | 683805/900675 [01:48<00:33, 6494.76it/s] 76%|███████▌  | 684527/900675 [01:48<00:32, 6704.80it/s] 76%|███████▌  | 685200/900675 [01:48<00:33, 6466.14it/s] 76%|███████▌  | 685850/900675 [01:48<00:33, 6388.91it/s] 76%|███████▌  | 686534/900675 [01:48<00:32, 6518.30it/s] 76%|███████▋  | 687188/900675 [01:48<00:35, 6087.86it/s] 76%|███████▋  | 687863/900675 [01:48<00:33, 6273.27it/s] 76%|███████▋  | 688498/900675 [01:48<00:33, 6294.60it/s] 77%|███████▋  | 689132/900675 [01:48<00:33, 6294.93it/s] 77%|███████▋  | 689765/900675 [01:49<00:34, 6159.56it/s] 77%|███████▋  | 690384/900675 [01:49<00:34, 6130.28it/s] 77%|███████▋  | 690999/900675 [01:49<00:35, 5974.70it/s] 77%|███████▋  | 691599/900675 [01:49<00:35, 5814.58it/s] 77%|███████▋  | 692183/900675 [01:49<00:35, 5800.00it/s] 77%|███████▋  | 692765/900675 [01:49<00:36, 5768.07it/s] 77%|███████▋  | 693495/900675 [01:49<00:33, 6211.18it/s] 77%|███████▋  | 694118/900675 [01:49<00:33, 6078.38it/s] 77%|███████▋  | 694745/900675 [01:49<00:33, 6132.86it/s] 77%|███████▋  | 695360/900675 [01:50<00:34, 5971.41it/s] 77%|███████▋  | 696015/900675 [01:50<00:33, 6135.12it/s] 77%|███████▋  | 696631/900675 [01:50<00:33, 6089.65it/s] 77%|███████▋  | 697242/900675 [01:50<00:33, 6066.91it/s] 77%|███████▋  | 697962/900675 [01:50<00:31, 6398.57it/s] 78%|███████▊  | 698604/900675 [01:50<00:32, 6277.76it/s] 78%|███████▊  | 699233/900675 [01:50<00:33, 5975.53it/s] 78%|███████▊  | 699934/900675 [01:50<00:32, 6256.29it/s] 78%|███████▊  | 700564/900675 [01:50<00:32, 6237.15it/s] 78%|███████▊  | 701273/900675 [01:50<00:30, 6478.22it/s] 78%|███████▊  | 701985/900675 [01:51<00:29, 6665.91it/s] 78%|███████▊  | 702654/900675 [01:51<00:30, 6415.60it/s] 78%|███████▊  | 703322/900675 [01:51<00:30, 6487.85it/s] 78%|███████▊  | 703974/900675 [01:51<00:30, 6381.88it/s] 78%|███████▊  | 704615/900675 [01:51<00:30, 6371.41it/s] 78%|███████▊  | 705254/900675 [01:51<00:32, 6029.78it/s] 78%|███████▊  | 705862/900675 [01:51<00:33, 5792.78it/s] 78%|███████▊  | 706472/900675 [01:51<00:33, 5875.09it/s] 79%|███████▊  | 707092/900675 [01:51<00:32, 5967.49it/s] 79%|███████▊  | 707692/900675 [01:52<00:32, 5914.23it/s] 79%|███████▊  | 708399/900675 [01:52<00:30, 6247.30it/s] 79%|███████▊  | 709044/900675 [01:52<00:30, 6304.52it/s] 79%|███████▉  | 709743/900675 [01:52<00:29, 6505.65it/s] 79%|███████▉  | 710412/900675 [01:52<00:29, 6551.31it/s] 79%|███████▉  | 711069/900675 [01:52<00:29, 6491.18it/s] 79%|███████▉  | 711744/900675 [01:52<00:28, 6552.76it/s] 79%|███████▉  | 712400/900675 [01:52<00:29, 6484.04it/s] 79%|███████▉  | 713103/900675 [01:52<00:28, 6643.79it/s] 79%|███████▉  | 713769/900675 [01:52<00:28, 6626.98it/s] 79%|███████▉  | 714433/900675 [01:53<00:29, 6403.86it/s] 79%|███████▉  | 715103/900675 [01:53<00:28, 6489.20it/s] 79%|███████▉  | 715754/900675 [01:53<00:29, 6357.72it/s] 80%|███████▉  | 716445/900675 [01:53<00:28, 6511.59it/s] 80%|███████▉  | 717098/900675 [01:53<00:28, 6477.54it/s] 80%|███████▉  | 717747/900675 [01:53<00:28, 6328.74it/s] 80%|███████▉  | 718382/900675 [01:53<00:29, 6145.16it/s] 80%|███████▉  | 718999/900675 [01:53<00:29, 6136.98it/s] 80%|███████▉  | 719614/900675 [01:53<00:30, 5938.43it/s] 80%|███████▉  | 720210/900675 [01:53<00:30, 5936.57it/s] 80%|████████  | 720834/900675 [01:54<00:29, 6018.62it/s] 80%|████████  | 721500/900675 [01:54<00:28, 6206.31it/s] 80%|████████  | 722210/900675 [01:54<00:27, 6464.60it/s] 80%|████████  | 722858/900675 [01:54<00:28, 6199.39it/s] 80%|████████  | 723493/900675 [01:54<00:28, 6241.39it/s] 80%|████████  | 724145/900675 [01:54<00:27, 6322.04it/s] 80%|████████  | 724779/900675 [01:54<00:28, 6154.43it/s] 81%|████████  | 725397/900675 [01:54<00:28, 6091.03it/s] 81%|████████  | 726010/900675 [01:54<00:28, 6101.49it/s] 81%|████████  | 726622/900675 [01:55<00:28, 6042.18it/s] 81%|████████  | 727317/900675 [01:55<00:27, 6305.22it/s] 81%|████████  | 728032/900675 [01:55<00:26, 6548.40it/s] 81%|████████  | 728688/900675 [01:55<00:28, 6110.13it/s] 81%|████████  | 729306/900675 [01:55<00:28, 5911.15it/s] 81%|████████  | 729966/900675 [01:55<00:27, 6102.10it/s] 81%|████████  | 730582/900675 [01:55<00:28, 6064.59it/s] 81%|████████  | 731196/900675 [01:55<00:27, 6086.08it/s] 81%|████████▏ | 731864/900675 [01:55<00:26, 6255.11it/s] 81%|████████▏ | 732492/900675 [01:55<00:27, 6188.35it/s] 81%|████████▏ | 733140/900675 [01:56<00:26, 6267.67it/s] 81%|████████▏ | 733977/900675 [01:56<00:24, 6887.80it/s] 82%|████████▏ | 734668/900675 [01:56<00:24, 6814.61it/s] 82%|████████▏ | 735351/900675 [01:56<00:26, 6207.71it/s] 82%|████████▏ | 736037/900675 [01:56<00:25, 6384.34it/s] 82%|████████▏ | 736854/900675 [01:56<00:23, 6885.34it/s] 82%|████████▏ | 737552/900675 [01:56<00:24, 6759.41it/s] 82%|████████▏ | 738235/900675 [01:56<00:24, 6748.34it/s] 82%|████████▏ | 738915/900675 [01:56<00:24, 6496.15it/s] 82%|████████▏ | 739570/900675 [01:57<00:25, 6352.90it/s] 82%|████████▏ | 740209/900675 [01:57<00:25, 6209.79it/s] 82%|████████▏ | 740920/900675 [01:57<00:24, 6458.36it/s] 82%|████████▏ | 741569/900675 [01:57<00:25, 6232.96it/s] 82%|████████▏ | 742196/900675 [01:57<00:26, 6032.54it/s] 82%|████████▏ | 742869/900675 [01:57<00:25, 6224.59it/s] 83%|████████▎ | 743546/900675 [01:57<00:24, 6366.93it/s] 83%|████████▎ | 744186/900675 [01:57<00:26, 5959.15it/s] 83%|████████▎ | 744825/900675 [01:57<00:25, 6074.60it/s] 83%|████████▎ | 745464/900675 [01:57<00:25, 6164.41it/s] 83%|████████▎ | 746085/900675 [01:58<00:25, 6073.21it/s] 83%|████████▎ | 746696/900675 [01:58<00:25, 5990.45it/s] 83%|████████▎ | 747298/900675 [01:58<00:25, 5981.46it/s] 83%|████████▎ | 747898/900675 [01:58<00:26, 5856.27it/s] 83%|████████▎ | 748485/900675 [01:58<00:26, 5842.23it/s] 83%|████████▎ | 749189/900675 [01:58<00:24, 6190.71it/s] 83%|████████▎ | 749810/900675 [01:58<00:24, 6160.21it/s] 83%|████████▎ | 750460/900675 [01:58<00:24, 6255.24it/s] 83%|████████▎ | 751087/900675 [01:58<00:24, 6120.59it/s] 83%|████████▎ | 751701/900675 [01:59<00:24, 6108.87it/s] 84%|████████▎ | 752313/900675 [01:59<00:24, 5981.86it/s] 84%|████████▎ | 752996/900675 [01:59<00:23, 6226.35it/s] 84%|████████▎ | 753667/900675 [01:59<00:23, 6368.23it/s] 84%|████████▎ | 754306/900675 [01:59<00:23, 6317.89it/s] 84%|████████▍ | 754939/900675 [01:59<00:24, 6069.58it/s] 84%|████████▍ | 755549/900675 [01:59<00:24, 5900.77it/s] 84%|████████▍ | 756142/900675 [01:59<00:24, 5872.15it/s] 84%|████████▍ | 756749/900675 [01:59<00:24, 5927.80it/s] 84%|████████▍ | 757405/900675 [01:59<00:23, 6107.39it/s] 84%|████████▍ | 758037/900675 [02:00<00:23, 6166.64it/s] 84%|████████▍ | 758678/900675 [02:00<00:22, 6232.74it/s] 84%|████████▍ | 759335/900675 [02:00<00:22, 6330.68it/s] 84%|████████▍ | 760004/900675 [02:00<00:21, 6434.79it/s] 84%|████████▍ | 760761/900675 [02:00<00:20, 6770.51it/s] 85%|████████▍ | 761439/900675 [02:00<00:21, 6529.16it/s] 85%|████████▍ | 762095/900675 [02:00<00:21, 6534.48it/s] 85%|████████▍ | 762750/900675 [02:00<00:22, 6187.69it/s] 85%|████████▍ | 763374/900675 [02:00<00:22, 6032.66it/s] 85%|████████▍ | 763998/900675 [02:01<00:22, 6084.48it/s] 85%|████████▍ | 764643/900675 [02:01<00:21, 6186.47it/s] 85%|████████▍ | 765264/900675 [02:01<00:22, 6126.07it/s] 85%|████████▌ | 765879/900675 [02:01<00:22, 6055.14it/s] 85%|████████▌ | 766612/900675 [02:01<00:20, 6421.34it/s] 85%|████████▌ | 767256/900675 [02:01<00:21, 6336.78it/s] 85%|████████▌ | 767938/900675 [02:01<00:20, 6473.36it/s] 85%|████████▌ | 768587/900675 [02:01<00:21, 6247.72it/s] 85%|████████▌ | 769215/900675 [02:01<00:22, 5963.78it/s] 85%|████████▌ | 769815/900675 [02:01<00:22, 5887.69it/s] 86%|████████▌ | 770424/900675 [02:02<00:21, 5945.05it/s] 86%|████████▌ | 771021/900675 [02:02<00:21, 5949.49it/s] 86%|████████▌ | 771667/900675 [02:02<00:21, 6097.99it/s] 86%|████████▌ | 772295/900675 [02:02<00:20, 6143.26it/s] 86%|████████▌ | 772985/900675 [02:02<00:20, 6363.89it/s] 86%|████████▌ | 773640/900675 [02:02<00:19, 6412.57it/s] 86%|████████▌ | 774316/900675 [02:02<00:19, 6511.93it/s] 86%|████████▌ | 774968/900675 [02:02<00:20, 6134.19it/s] 86%|████████▌ | 775590/900675 [02:02<00:20, 6155.47it/s] 86%|████████▌ | 776209/900675 [02:02<00:20, 6053.97it/s] 86%|████████▌ | 776817/900675 [02:03<00:21, 5863.37it/s] 86%|████████▋ | 777498/900675 [02:03<00:20, 6131.19it/s] 86%|████████▋ | 778152/900675 [02:03<00:19, 6247.43it/s] 86%|████████▋ | 778815/900675 [02:03<00:19, 6357.20it/s] 87%|████████▋ | 779453/900675 [02:03<00:19, 6151.50it/s] 87%|████████▋ | 780071/900675 [02:03<00:20, 6004.50it/s] 87%|████████▋ | 780677/900675 [02:03<00:19, 6017.50it/s] 87%|████████▋ | 781309/900675 [02:03<00:19, 6105.36it/s] 87%|████████▋ | 782086/900675 [02:03<00:17, 6589.54it/s] 87%|████████▋ | 782747/900675 [02:04<00:19, 5906.11it/s] 87%|████████▋ | 783352/900675 [02:04<00:19, 5929.25it/s] 87%|████████▋ | 783956/900675 [02:04<00:20, 5829.71it/s] 87%|████████▋ | 784598/900675 [02:04<00:19, 5992.18it/s] 87%|████████▋ | 785204/900675 [02:04<00:19, 5977.04it/s] 87%|████████▋ | 785862/900675 [02:04<00:18, 6150.27it/s] 87%|████████▋ | 786610/900675 [02:04<00:17, 6538.05it/s] 87%|████████▋ | 787268/900675 [02:04<00:18, 6249.65it/s] 87%|████████▋ | 787964/900675 [02:04<00:17, 6446.48it/s] 88%|████████▊ | 788613/900675 [02:04<00:17, 6233.10it/s] 88%|████████▊ | 789241/900675 [02:05<00:17, 6195.00it/s] 88%|████████▊ | 789864/900675 [02:05<00:18, 6020.82it/s] 88%|████████▊ | 790566/900675 [02:05<00:17, 6304.60it/s] 88%|████████▊ | 791200/900675 [02:05<00:17, 6261.02it/s] 88%|████████▊ | 791829/900675 [02:05<00:18, 5880.30it/s] 88%|████████▊ | 792476/900675 [02:05<00:17, 6037.33it/s] 88%|████████▊ | 793149/900675 [02:05<00:17, 6233.59it/s] 88%|████████▊ | 793777/900675 [02:05<00:17, 5975.65it/s] 88%|████████▊ | 794380/900675 [02:05<00:17, 5952.12it/s] 88%|████████▊ | 795055/900675 [02:06<00:17, 6175.02it/s] 88%|████████▊ | 795769/900675 [02:06<00:16, 6451.53it/s] 88%|████████▊ | 796418/900675 [02:06<00:16, 6307.51it/s] 88%|████████▊ | 797089/900675 [02:06<00:16, 6420.48it/s] 89%|████████▊ | 797734/900675 [02:06<00:16, 6138.70it/s] 89%|████████▊ | 798414/900675 [02:06<00:16, 6326.61it/s] 89%|████████▊ | 799051/900675 [02:06<00:16, 6337.74it/s] 89%|████████▉ | 799688/900675 [02:06<00:16, 6284.18it/s] 89%|████████▉ | 800326/900675 [02:06<00:15, 6296.59it/s] 89%|████████▉ | 800957/900675 [02:06<00:16, 6191.65it/s] 89%|████████▉ | 801601/900675 [02:07<00:15, 6262.94it/s] 89%|████████▉ | 802229/900675 [02:07<00:15, 6256.98it/s] 89%|████████▉ | 802856/900675 [02:07<00:16, 5776.67it/s] 89%|████████▉ | 803599/900675 [02:07<00:15, 6236.90it/s] 89%|████████▉ | 804231/900675 [02:07<00:15, 6209.68it/s] 89%|████████▉ | 804894/900675 [02:07<00:15, 6329.01it/s] 89%|████████▉ | 805532/900675 [02:07<00:15, 6286.82it/s] 90%|████████▉ | 806164/900675 [02:07<00:15, 6288.35it/s] 90%|████████▉ | 806796/900675 [02:07<00:15, 6250.12it/s] 90%|████████▉ | 807423/900675 [02:08<00:15, 6187.41it/s] 90%|████████▉ | 808043/900675 [02:08<00:16, 5697.95it/s] 90%|████████▉ | 808621/900675 [02:08<00:16, 5720.54it/s] 90%|████████▉ | 809345/900675 [02:08<00:14, 6141.29it/s] 90%|████████▉ | 809966/900675 [02:08<00:14, 6152.83it/s] 90%|████████▉ | 810586/900675 [02:08<00:15, 5765.11it/s] 90%|█████████ | 811170/900675 [02:08<00:15, 5705.18it/s] 90%|█████████ | 811848/900675 [02:08<00:14, 6009.10it/s] 90%|█████████ | 812455/900675 [02:08<00:15, 5807.74it/s] 90%|█████████ | 813168/900675 [02:09<00:14, 6178.83it/s] 90%|█████████ | 813899/900675 [02:09<00:13, 6502.23it/s] 90%|█████████ | 814555/900675 [02:09<00:13, 6515.26it/s] 91%|█████████ | 815314/900675 [02:09<00:12, 6829.15it/s] 91%|█████████ | 816000/900675 [02:09<00:12, 6751.72it/s] 91%|█████████ | 816808/900675 [02:09<00:11, 7133.23it/s] 91%|█████████ | 817524/900675 [02:09<00:11, 7073.40it/s] 91%|█████████ | 818312/900675 [02:09<00:11, 7305.32it/s] 91%|█████████ | 819044/900675 [02:09<00:12, 6555.22it/s] 91%|█████████ | 819715/900675 [02:09<00:12, 6484.12it/s] 91%|█████████ | 820448/900675 [02:10<00:11, 6711.45it/s] 91%|█████████ | 821221/900675 [02:10<00:11, 6989.32it/s] 91%|█████████▏| 821928/900675 [02:10<00:12, 6282.71it/s] 91%|█████████▏| 822574/900675 [02:10<00:12, 6235.32it/s] 91%|█████████▏| 823262/900675 [02:10<00:12, 6412.15it/s] 91%|█████████▏| 823914/900675 [02:10<00:11, 6417.99it/s] 92%|█████████▏| 824563/900675 [02:10<00:12, 6153.34it/s] 92%|█████████▏| 825185/900675 [02:10<00:12, 5968.40it/s] 92%|█████████▏| 825787/900675 [02:10<00:12, 5891.83it/s] 92%|█████████▏| 826388/900675 [02:11<00:12, 5919.39it/s] 92%|█████████▏| 827068/900675 [02:11<00:11, 6168.36it/s] 92%|█████████▏| 827743/900675 [02:11<00:11, 6334.82it/s] 92%|█████████▏| 828379/900675 [02:11<00:11, 6274.94it/s] 92%|█████████▏| 829009/900675 [02:11<00:12, 5619.98it/s] 92%|█████████▏| 829606/900675 [02:11<00:12, 5712.97it/s] 92%|█████████▏| 830278/900675 [02:11<00:11, 5985.66it/s] 92%|█████████▏| 830886/900675 [02:11<00:11, 5859.58it/s] 92%|█████████▏| 831479/900675 [02:11<00:11, 5846.46it/s] 92%|█████████▏| 832111/900675 [02:11<00:11, 5977.98it/s] 92%|█████████▏| 832731/900675 [02:12<00:11, 6037.46it/s] 93%|█████████▎| 833364/900675 [02:12<00:10, 6119.96it/s] 93%|█████████▎| 833989/900675 [02:12<00:10, 6153.86it/s] 93%|█████████▎| 834606/900675 [02:12<00:10, 6134.77it/s] 93%|█████████▎| 835255/900675 [02:12<00:10, 6239.69it/s] 93%|█████████▎| 835887/900675 [02:12<00:10, 6262.74it/s] 93%|█████████▎| 836560/900675 [02:12<00:10, 6401.50it/s] 93%|█████████▎| 837201/900675 [02:12<00:09, 6386.77it/s] 93%|█████████▎| 837879/900675 [02:12<00:09, 6499.89it/s] 93%|█████████▎| 838530/900675 [02:13<00:09, 6413.89it/s] 93%|█████████▎| 839172/900675 [02:13<00:10, 6065.31it/s] 93%|█████████▎| 839818/900675 [02:13<00:09, 6174.82it/s] 93%|█████████▎| 840585/900675 [02:13<00:09, 6606.55it/s] 93%|█████████▎| 841250/900675 [02:13<00:09, 6414.49it/s] 93%|█████████▎| 841895/900675 [02:13<00:09, 6302.93it/s] 94%|█████████▎| 842528/900675 [02:13<00:09, 6131.63it/s] 94%|█████████▎| 843144/900675 [02:13<00:09, 5843.78it/s] 94%|█████████▎| 843732/900675 [02:13<00:10, 5666.33it/s] 94%|█████████▎| 844302/900675 [02:13<00:10, 5436.73it/s] 94%|█████████▍| 844876/900675 [02:14<00:10, 5517.24it/s] 94%|█████████▍| 845487/900675 [02:14<00:09, 5681.50it/s] 94%|█████████▍| 846100/900675 [02:14<00:09, 5803.04it/s] 94%|█████████▍| 846699/900675 [02:14<00:09, 5855.09it/s] 94%|█████████▍| 847356/900675 [02:14<00:08, 6064.50it/s] 94%|█████████▍| 847988/900675 [02:14<00:08, 6132.31it/s] 94%|█████████▍| 848603/900675 [02:14<00:08, 6123.73it/s] 94%|█████████▍| 849239/900675 [02:14<00:08, 6190.83it/s] 94%|█████████▍| 849884/900675 [02:14<00:08, 6263.69it/s] 94%|█████████▍| 850511/900675 [02:15<00:08, 6147.89it/s] 94%|█████████▍| 851127/900675 [02:15<00:08, 5937.35it/s] 95%|█████████▍| 851775/900675 [02:15<00:08, 6094.08it/s] 95%|█████████▍| 852387/900675 [02:15<00:07, 6063.50it/s] 95%|█████████▍| 852995/900675 [02:15<00:08, 5944.27it/s] 95%|█████████▍| 853602/900675 [02:15<00:07, 5980.76it/s] 95%|█████████▍| 854205/900675 [02:15<00:07, 5990.54it/s] 95%|█████████▍| 854858/900675 [02:15<00:07, 6148.59it/s] 95%|█████████▍| 855474/900675 [02:15<00:07, 6087.09it/s] 95%|█████████▌| 856114/900675 [02:15<00:07, 6175.73it/s] 95%|█████████▌| 856763/900675 [02:16<00:07, 6261.62it/s] 95%|█████████▌| 857421/900675 [02:16<00:06, 6354.29it/s] 95%|█████████▌| 858060/900675 [02:16<00:06, 6360.64it/s] 95%|█████████▌| 858697/900675 [02:16<00:06, 6109.84it/s] 95%|█████████▌| 859311/900675 [02:16<00:06, 6030.96it/s] 95%|█████████▌| 859947/900675 [02:16<00:06, 6123.56it/s] 96%|█████████▌| 860801/900675 [02:16<00:05, 6820.61it/s] 96%|█████████▌| 861486/900675 [02:16<00:06, 6507.25it/s] 96%|█████████▌| 862142/900675 [02:16<00:06, 6407.69it/s] 96%|█████████▌| 862859/900675 [02:16<00:05, 6625.97it/s] 96%|█████████▌| 863525/900675 [02:17<00:05, 6490.20it/s] 96%|█████████▌| 864177/900675 [02:17<00:05, 6459.79it/s] 96%|█████████▌| 864825/900675 [02:17<00:05, 6336.65it/s] 96%|█████████▌| 865461/900675 [02:17<00:05, 6306.20it/s] 96%|█████████▌| 866093/900675 [02:17<00:05, 5956.88it/s] 96%|█████████▌| 866795/900675 [02:17<00:05, 6255.63it/s] 96%|█████████▋| 867426/900675 [02:17<00:05, 6182.57it/s] 96%|█████████▋| 868053/900675 [02:17<00:05, 6207.40it/s] 96%|█████████▋| 868677/900675 [02:17<00:05, 6126.96it/s] 97%|█████████▋| 869303/900675 [02:18<00:05, 6159.48it/s] 97%|█████████▋| 870001/900675 [02:18<00:04, 6389.97it/s] 97%|█████████▋| 870669/900675 [02:18<00:04, 6470.48it/s] 97%|█████████▋| 871432/900675 [02:18<00:04, 6812.04it/s] 97%|█████████▋| 872115/900675 [02:18<00:04, 6361.64it/s] 97%|█████████▋| 872758/900675 [02:18<00:04, 6105.80it/s] 97%|█████████▋| 873447/900675 [02:18<00:04, 6322.00it/s] 97%|█████████▋| 874165/900675 [02:18<00:04, 6564.41it/s] 97%|█████████▋| 874827/900675 [02:18<00:04, 6385.23it/s] 97%|█████████▋| 875479/900675 [02:18<00:03, 6423.77it/s] 97%|█████████▋| 876155/900675 [02:19<00:03, 6516.40it/s] 97%|█████████▋| 876810/900675 [02:19<00:03, 6219.41it/s] 97%|█████████▋| 877437/900675 [02:19<00:03, 6093.31it/s] 97%|█████████▋| 878050/900675 [02:19<00:03, 6023.32it/s] 98%|█████████▊| 878693/900675 [02:19<00:03, 6133.95it/s] 98%|█████████▊| 879326/900675 [02:19<00:03, 6183.24it/s] 98%|█████████▊| 879946/900675 [02:19<00:03, 5946.45it/s] 98%|█████████▊| 880578/900675 [02:19<00:03, 6053.45it/s] 98%|█████████▊| 881186/900675 [02:19<00:03, 5922.77it/s] 98%|█████████▊| 881781/900675 [02:20<00:03, 5819.18it/s] 98%|█████████▊| 882365/900675 [02:20<00:03, 5594.44it/s] 98%|█████████▊| 883047/900675 [02:20<00:02, 5941.33it/s] 98%|█████████▊| 883697/900675 [02:20<00:02, 6097.06it/s] 98%|█████████▊| 884334/900675 [02:20<00:02, 6172.65it/s] 98%|█████████▊| 884954/900675 [02:20<00:02, 5948.73it/s] 98%|█████████▊| 885672/900675 [02:20<00:02, 6302.05it/s] 98%|█████████▊| 886423/900675 [02:20<00:02, 6651.06it/s] 98%|█████████▊| 887092/900675 [02:20<00:02, 6466.01it/s] 99%|█████████▊| 887742/900675 [02:20<00:02, 6230.10it/s] 99%|█████████▊| 888369/900675 [02:21<00:01, 6194.34it/s] 99%|█████████▊| 888991/900675 [02:21<00:01, 6073.83it/s] 99%|█████████▉| 889668/900675 [02:21<00:01, 6268.73it/s] 99%|█████████▉| 890297/900675 [02:21<00:01, 6258.94it/s] 99%|█████████▉| 891017/900675 [02:21<00:01, 6523.39it/s] 99%|█████████▉| 891671/900675 [02:21<00:01, 6212.83it/s] 99%|█████████▉| 892297/900675 [02:21<00:01, 6202.08it/s] 99%|█████████▉| 892920/900675 [02:21<00:01, 6004.57it/s] 99%|█████████▉| 893538/900675 [02:21<00:01, 6052.83it/s] 99%|█████████▉| 894146/900675 [02:22<00:01, 5729.94it/s] 99%|█████████▉| 894724/900675 [02:22<00:01, 5731.71it/s] 99%|█████████▉| 895361/900675 [02:22<00:00, 5908.84it/s] 99%|█████████▉| 896044/900675 [02:22<00:00, 6175.72it/s]100%|█████████▉| 896665/900675 [02:22<00:00, 6149.16it/s]100%|█████████▉| 897340/900675 [02:22<00:00, 6318.35it/s]100%|█████████▉| 898028/900675 [02:22<00:00, 6475.67it/s]100%|█████████▉| 898684/900675 [02:22<00:00, 6498.79it/s]100%|█████████▉| 899335/900675 [02:22<00:00, 6462.40it/s]100%|█████████▉| 900010/900675 [02:22<00:00, 6544.64it/s]100%|█████████▉| 900666/900675 [02:23<00:00, 6412.50it/s]100%|██████████| 900675/900675 [02:23<00:00, 6295.44it/s]

gathering stats for n=1
  0%|          | 0/900675 [00:00<?, ?it/s]  0%|          | 1902/900675 [00:00<00:47, 19007.78it/s]  0%|          | 3956/900675 [00:00<00:45, 19898.76it/s]  1%|          | 6209/900675 [00:00<00:42, 21093.41it/s]  1%|          | 8319/900675 [00:00<00:44, 20125.44it/s]  1%|          | 10338/900675 [00:00<00:44, 20033.80it/s]  1%|▏         | 12346/900675 [00:00<00:44, 19878.02it/s]  2%|▏         | 14420/900675 [00:00<00:43, 20149.27it/s]  2%|▏         | 16438/900675 [00:00<00:44, 20043.02it/s]  2%|▏         | 18446/900675 [00:00<00:44, 20048.62it/s]  2%|▏         | 20577/900675 [00:01<00:43, 20433.86it/s]  3%|▎         | 22622/900675 [00:01<00:43, 20324.82it/s]  3%|▎         | 24902/900675 [00:01<00:41, 21068.11it/s]  3%|▎         | 27011/900675 [00:01<00:42, 20637.28it/s]  3%|▎         | 29078/900675 [00:01<00:42, 20628.52it/s]  3%|▎         | 31143/900675 [00:01<00:43, 19868.58it/s]  4%|▎         | 33137/900675 [00:01<00:44, 19637.01it/s]  4%|▍         | 35153/900675 [00:01<00:43, 19788.17it/s]  4%|▍         | 37136/900675 [00:01<00:43, 19701.41it/s]  4%|▍         | 39109/900675 [00:01<00:44, 19508.42it/s]  5%|▍         | 41157/900675 [00:02<00:43, 19793.10it/s]  5%|▍         | 43139/900675 [00:02<00:44, 19226.16it/s]  5%|▌         | 45169/900675 [00:02<00:43, 19536.50it/s]  5%|▌         | 47477/900675 [00:02<00:41, 20574.75it/s]  6%|▌         | 49780/900675 [00:02<00:39, 21299.40it/s]  6%|▌         | 51915/900675 [00:02<00:40, 20828.45it/s]  6%|▌         | 54046/900675 [00:02<00:40, 20968.69it/s]  6%|▌         | 56147/900675 [00:02<00:40, 20697.80it/s]  6%|▋         | 58220/900675 [00:02<00:41, 20067.05it/s]  7%|▋         | 60252/900675 [00:02<00:41, 20139.44it/s]  7%|▋         | 62434/900675 [00:03<00:40, 20628.41it/s]  7%|▋         | 64501/900675 [00:03<00:41, 20376.81it/s]  7%|▋         | 66607/900675 [00:03<00:40, 20574.58it/s]  8%|▊         | 68668/900675 [00:03<00:40, 20428.36it/s]  8%|▊         | 70724/900675 [00:03<00:40, 20462.90it/s]  8%|▊         | 72849/900675 [00:03<00:40, 20691.91it/s]  8%|▊         | 75160/900675 [00:03<00:38, 21409.05it/s]  9%|▊         | 77303/900675 [00:03<00:39, 21079.01it/s]  9%|▉         | 79413/900675 [00:03<00:40, 20363.49it/s]  9%|▉         | 81491/900675 [00:04<00:39, 20483.67it/s]  9%|▉         | 83578/900675 [00:04<00:39, 20590.73it/s] 10%|▉         | 85641/900675 [00:04<00:39, 20476.27it/s] 10%|▉         | 87760/900675 [00:04<00:39, 20684.90it/s] 10%|▉         | 89831/900675 [00:04<00:39, 20333.13it/s] 10%|█         | 91867/900675 [00:04<00:40, 20125.26it/s] 10%|█         | 93911/900675 [00:04<00:39, 20216.97it/s] 11%|█         | 95989/900675 [00:04<00:39, 20378.27it/s] 11%|█         | 98179/900675 [00:04<00:38, 20826.63it/s] 11%|█         | 100264/900675 [00:04<00:39, 20310.03it/s] 11%|█▏        | 102334/900675 [00:05<00:39, 20418.47it/s] 12%|█▏        | 104379/900675 [00:05<00:39, 20073.87it/s] 12%|█▏        | 106390/900675 [00:05<00:39, 19977.37it/s] 12%|█▏        | 108390/900675 [00:05<00:39, 19900.44it/s] 12%|█▏        | 110382/900675 [00:05<00:40, 19719.04it/s] 12%|█▏        | 112467/900675 [00:05<00:39, 20045.47it/s] 13%|█▎        | 114473/900675 [00:05<00:39, 19881.45it/s] 13%|█▎        | 116463/900675 [00:05<00:40, 19535.99it/s] 13%|█▎        | 118488/900675 [00:05<00:39, 19741.01it/s] 13%|█▎        | 120574/900675 [00:05<00:38, 20070.38it/s] 14%|█▎        | 122583/900675 [00:06<00:39, 19869.94it/s] 14%|█▍        | 124572/900675 [00:06<00:39, 19812.26it/s] 14%|█▍        | 126697/900675 [00:06<00:38, 20233.81it/s] 14%|█▍        | 128722/900675 [00:06<00:38, 19973.24it/s] 15%|█▍        | 130721/900675 [00:06<00:38, 19851.58it/s] 15%|█▍        | 132708/900675 [00:06<00:39, 19591.99it/s] 15%|█▍        | 134928/900675 [00:06<00:37, 20357.85it/s] 15%|█▌        | 137188/900675 [00:06<00:36, 21018.70it/s] 15%|█▌        | 139293/900675 [00:06<00:37, 20577.77it/s] 16%|█▌        | 141355/900675 [00:06<00:37, 20288.15it/s] 16%|█▌        | 143387/900675 [00:07<00:37, 20164.46it/s] 16%|█▌        | 145615/900675 [00:07<00:36, 20780.55it/s] 16%|█▋        | 147696/900675 [00:07<00:37, 20172.62it/s] 17%|█▋        | 149719/900675 [00:07<00:37, 20071.01it/s] 17%|█▋        | 151730/900675 [00:07<00:37, 19758.45it/s] 17%|█▋        | 153741/900675 [00:07<00:37, 19856.80it/s] 17%|█▋        | 155751/900675 [00:07<00:37, 19923.04it/s] 18%|█▊        | 157878/900675 [00:07<00:36, 20317.89it/s] 18%|█▊        | 159912/900675 [00:07<00:36, 20289.27it/s] 18%|█▊        | 161943/900675 [00:08<00:36, 20249.49it/s] 18%|█▊        | 163969/900675 [00:08<00:36, 20141.03it/s] 18%|█▊        | 166110/900675 [00:08<00:35, 20516.02it/s] 19%|█▊        | 168163/900675 [00:08<00:36, 20334.87it/s] 19%|█▉        | 170198/900675 [00:08<00:36, 20079.94it/s] 19%|█▉        | 172208/900675 [00:08<00:36, 19764.10it/s] 19%|█▉        | 174211/900675 [00:08<00:36, 19835.91it/s] 20%|█▉        | 176196/900675 [00:08<00:36, 19687.95it/s] 20%|█▉        | 178166/900675 [00:08<00:37, 19263.71it/s] 20%|██        | 180164/900675 [00:08<00:37, 19468.39it/s] 20%|██        | 182130/900675 [00:09<00:36, 19519.91it/s] 20%|██        | 184084/900675 [00:09<00:37, 19048.84it/s] 21%|██        | 186008/900675 [00:09<00:37, 19098.49it/s] 21%|██        | 187997/900675 [00:09<00:36, 19327.59it/s] 21%|██        | 190084/900675 [00:09<00:35, 19782.42it/s] 21%|██▏       | 192116/900675 [00:09<00:35, 19941.50it/s] 22%|██▏       | 194185/900675 [00:09<00:35, 20160.64it/s] 22%|██▏       | 196203/900675 [00:09<00:35, 19988.37it/s] 22%|██▏       | 198420/900675 [00:09<00:34, 20633.45it/s] 22%|██▏       | 200485/900675 [00:09<00:34, 20322.30it/s] 22%|██▏       | 202520/900675 [00:10<00:34, 20110.37it/s] 23%|██▎       | 204590/900675 [00:10<00:34, 20280.08it/s] 23%|██▎       | 206620/900675 [00:10<00:34, 20045.71it/s] 23%|██▎       | 208626/900675 [00:10<00:35, 19710.47it/s] 23%|██▎       | 210626/900675 [00:10<00:34, 19794.60it/s] 24%|██▎       | 212736/900675 [00:10<00:34, 20172.17it/s] 24%|██▍       | 214755/900675 [00:10<00:35, 19586.67it/s] 24%|██▍       | 216718/900675 [00:10<00:35, 19333.36it/s] 24%|██▍       | 218775/900675 [00:10<00:34, 19692.63it/s] 25%|██▍       | 220748/900675 [00:10<00:34, 19619.52it/s] 25%|██▍       | 222713/900675 [00:11<00:34, 19397.06it/s] 25%|██▍       | 224680/900675 [00:11<00:34, 19474.67it/s] 25%|██▌       | 226830/900675 [00:11<00:33, 20063.21it/s] 25%|██▌       | 228839/900675 [00:11<00:34, 19738.54it/s] 26%|██▌       | 230815/900675 [00:11<00:34, 19591.07it/s] 26%|██▌       | 232839/900675 [00:11<00:33, 19780.94it/s] 26%|██▌       | 235044/900675 [00:11<00:32, 20445.10it/s] 26%|██▋       | 237091/900675 [00:11<00:32, 20189.98it/s] 27%|██▋       | 239150/900675 [00:11<00:32, 20306.07it/s] 27%|██▋       | 241341/900675 [00:11<00:31, 20772.55it/s] 27%|██▋       | 243420/900675 [00:12<00:32, 20515.38it/s] 27%|██▋       | 245474/900675 [00:12<00:32, 20458.64it/s] 28%|██▊       | 247711/900675 [00:12<00:31, 21021.46it/s] 28%|██▊       | 249860/900675 [00:12<00:30, 21154.55it/s] 28%|██▊       | 251977/900675 [00:12<00:30, 20942.25it/s] 28%|██▊       | 254073/900675 [00:12<00:31, 20842.13it/s] 28%|██▊       | 256159/900675 [00:12<00:31, 20646.19it/s] 29%|██▊       | 258225/900675 [00:12<00:31, 20346.80it/s] 29%|██▉       | 260261/900675 [00:12<00:31, 20039.22it/s] 29%|██▉       | 262267/900675 [00:13<00:31, 20032.00it/s] 29%|██▉       | 264272/900675 [00:13<00:32, 19661.88it/s] 30%|██▉       | 266240/900675 [00:13<00:32, 19543.27it/s] 30%|██▉       | 268196/900675 [00:13<00:32, 19485.05it/s] 30%|███       | 270302/900675 [00:13<00:31, 19945.31it/s] 30%|███       | 272421/900675 [00:13<00:30, 20313.09it/s] 30%|███       | 274515/900675 [00:13<00:30, 20499.01it/s] 31%|███       | 276567/900675 [00:13<00:31, 19690.28it/s] 31%|███       | 278667/900675 [00:13<00:30, 20068.96it/s] 31%|███       | 280681/900675 [00:13<00:31, 19792.69it/s] 31%|███▏      | 282666/900675 [00:14<00:31, 19642.37it/s] 32%|███▏      | 284741/900675 [00:14<00:30, 19939.83it/s] 32%|███▏      | 286738/900675 [00:14<00:31, 19728.58it/s] 32%|███▏      | 288792/900675 [00:14<00:30, 19962.41it/s] 32%|███▏      | 290791/900675 [00:14<00:30, 19731.82it/s] 33%|███▎      | 292788/900675 [00:14<00:30, 19796.08it/s] 33%|███▎      | 294769/900675 [00:14<00:30, 19790.35it/s] 33%|███▎      | 296852/900675 [00:14<00:30, 20098.78it/s] 33%|███▎      | 298890/900675 [00:14<00:29, 20178.50it/s] 33%|███▎      | 300914/900675 [00:14<00:29, 20194.77it/s] 34%|███▎      | 302934/900675 [00:15<00:29, 20119.21it/s] 34%|███▍      | 305015/900675 [00:15<00:29, 20317.61it/s] 34%|███▍      | 307048/900675 [00:15<00:29, 19985.23it/s] 34%|███▍      | 309063/900675 [00:15<00:29, 20031.06it/s] 35%|███▍      | 311214/900675 [00:15<00:28, 20464.35it/s] 35%|███▍      | 313315/900675 [00:15<00:28, 20620.95it/s] 35%|███▌      | 315378/900675 [00:15<00:28, 20299.87it/s] 35%|███▌      | 317410/900675 [00:15<00:29, 20105.92it/s] 35%|███▌      | 319422/900675 [00:15<00:29, 19741.54it/s] 36%|███▌      | 321472/900675 [00:15<00:29, 19962.81it/s] 36%|███▌      | 323471/900675 [00:16<00:29, 19562.82it/s] 36%|███▌      | 325430/900675 [00:16<00:29, 19410.35it/s] 36%|███▋      | 327585/900675 [00:16<00:28, 20033.57it/s] 37%|███▋      | 329705/900675 [00:16<00:28, 20377.12it/s] 37%|███▋      | 331746/900675 [00:16<00:28, 20002.93it/s] 37%|███▋      | 333750/900675 [00:16<00:28, 19950.48it/s] 37%|███▋      | 335799/900675 [00:16<00:28, 20109.11it/s] 38%|███▊      | 337927/900675 [00:16<00:27, 20447.41it/s] 38%|███▊      | 339974/900675 [00:16<00:28, 20021.50it/s] 38%|███▊      | 341989/900675 [00:17<00:27, 20057.16it/s] 38%|███▊      | 344092/900675 [00:17<00:27, 20331.88it/s] 38%|███▊      | 346127/900675 [00:17<00:28, 19711.37it/s] 39%|███▊      | 348200/900675 [00:17<00:27, 20002.19it/s] 39%|███▉      | 350440/900675 [00:17<00:26, 20702.01it/s] 39%|███▉      | 352515/900675 [00:17<00:27, 19729.03it/s] 39%|███▉      | 354609/900675 [00:17<00:27, 20074.17it/s] 40%|███▉      | 356627/900675 [00:17<00:27, 19805.03it/s] 40%|███▉      | 358615/900675 [00:17<00:27, 19553.75it/s] 40%|████      | 360669/900675 [00:17<00:27, 19837.86it/s] 40%|████      | 362868/900675 [00:18<00:26, 20465.17it/s] 41%|████      | 365000/900675 [00:18<00:25, 20715.27it/s] 41%|████      | 367076/900675 [00:18<00:25, 20691.51it/s] 41%|████      | 369148/900675 [00:18<00:26, 20276.37it/s] 41%|████      | 371179/900675 [00:18<00:26, 20030.46it/s] 41%|████▏     | 373185/900675 [00:18<00:26, 19939.56it/s] 42%|████▏     | 375249/900675 [00:18<00:26, 20142.63it/s] 42%|████▏     | 377265/900675 [00:18<00:26, 19520.15it/s] 42%|████▏     | 379305/900675 [00:18<00:26, 19769.86it/s] 42%|████▏     | 381467/900675 [00:18<00:25, 20304.35it/s] 43%|████▎     | 383502/900675 [00:19<00:26, 19657.56it/s] 43%|████▎     | 385606/900675 [00:19<00:25, 20056.53it/s] 43%|████▎     | 387760/900675 [00:19<00:25, 20489.67it/s] 43%|████▎     | 389815/900675 [00:19<00:25, 20274.92it/s] 44%|████▎     | 391847/900675 [00:19<00:25, 19799.86it/s] 44%|████▎     | 393832/900675 [00:19<00:25, 19571.89it/s] 44%|████▍     | 395885/900675 [00:19<00:25, 19847.09it/s] 44%|████▍     | 397873/900675 [00:19<00:25, 19482.31it/s] 44%|████▍     | 400095/900675 [00:19<00:24, 20278.59it/s] 45%|████▍     | 402128/900675 [00:20<00:25, 19657.67it/s] 45%|████▍     | 404100/900675 [00:20<00:25, 19462.12it/s] 45%|████▌     | 406051/900675 [00:20<00:25, 19432.74it/s] 45%|████▌     | 407998/900675 [00:20<00:25, 19139.35it/s] 46%|████▌     | 410018/900675 [00:20<00:25, 19443.13it/s] 46%|████▌     | 412002/900675 [00:20<00:24, 19553.55it/s] 46%|████▌     | 413960/900675 [00:20<00:25, 18752.90it/s] 46%|████▌     | 415862/900675 [00:20<00:25, 18821.36it/s] 46%|████▋     | 417824/900675 [00:20<00:25, 19053.77it/s] 47%|████▋     | 419802/900675 [00:20<00:24, 19261.86it/s] 47%|████▋     | 421732/900675 [00:21<00:25, 19094.59it/s] 47%|████▋     | 423757/900675 [00:21<00:24, 19434.44it/s] 47%|████▋     | 425703/900675 [00:21<00:24, 19224.04it/s] 47%|████▋     | 427798/900675 [00:21<00:23, 19728.67it/s] 48%|████▊     | 429917/900675 [00:21<00:23, 20159.44it/s] 48%|████▊     | 431935/900675 [00:21<00:24, 19431.00it/s] 48%|████▊     | 434122/900675 [00:21<00:23, 20134.03it/s] 48%|████▊     | 436143/900675 [00:21<00:23, 19682.77it/s] 49%|████▊     | 438123/900675 [00:21<00:23, 19706.13it/s] 49%|████▉     | 440099/900675 [00:21<00:23, 19324.45it/s] 49%|████▉     | 442078/900675 [00:22<00:23, 19456.05it/s] 49%|████▉     | 444140/900675 [00:22<00:23, 19797.14it/s] 50%|████▉     | 446123/900675 [00:22<00:23, 19102.40it/s] 50%|████▉     | 448109/900675 [00:22<00:23, 19319.97it/s] 50%|████▉     | 450089/900675 [00:22<00:23, 19457.47it/s] 50%|█████     | 452039/900675 [00:22<00:23, 19419.58it/s] 50%|█████     | 453984/900675 [00:22<00:23, 19348.83it/s] 51%|█████     | 455921/900675 [00:22<00:23, 19225.91it/s] 51%|█████     | 458045/900675 [00:22<00:22, 19820.56it/s] 51%|█████     | 460031/900675 [00:22<00:22, 19831.56it/s] 51%|█████▏    | 462016/900675 [00:23<00:22, 19528.28it/s] 52%|█████▏    | 463971/900675 [00:23<00:22, 19397.06it/s] 52%|█████▏    | 466001/900675 [00:23<00:22, 19657.15it/s] 52%|█████▏    | 467976/900675 [00:23<00:21, 19680.60it/s] 52%|█████▏    | 469945/900675 [00:23<00:21, 19605.77it/s] 52%|█████▏    | 471919/900675 [00:23<00:21, 19638.83it/s] 53%|█████▎    | 473884/900675 [00:23<00:21, 19604.21it/s] 53%|█████▎    | 476081/900675 [00:23<00:20, 20307.50it/s] 53%|█████▎    | 478113/900675 [00:23<00:21, 19813.97it/s] 53%|█████▎    | 480150/900675 [00:24<00:21, 19975.21it/s] 54%|█████▎    | 482150/900675 [00:24<00:21, 19690.89it/s] 54%|█████▍    | 484144/900675 [00:24<00:21, 19763.84it/s] 54%|█████▍    | 486283/900675 [00:24<00:20, 20240.27it/s] 54%|█████▍    | 488309/900675 [00:24<00:20, 20231.15it/s] 54%|█████▍    | 490334/900675 [00:24<00:21, 19324.28it/s] 55%|█████▍    | 492295/900675 [00:24<00:21, 19403.17it/s] 55%|█████▍    | 494242/900675 [00:24<00:21, 19191.56it/s] 55%|█████▌    | 496250/900675 [00:24<00:20, 19449.05it/s] 55%|█████▌    | 498199/900675 [00:24<00:20, 19302.99it/s] 56%|█████▌    | 500304/900675 [00:25<00:20, 19816.45it/s] 56%|█████▌    | 502289/900675 [00:25<00:20, 19399.76it/s] 56%|█████▌    | 504233/900675 [00:25<00:20, 19194.61it/s] 56%|█████▌    | 506273/900675 [00:25<00:20, 19544.48it/s] 56%|█████▋    | 508231/900675 [00:25<00:20, 19017.38it/s] 57%|█████▋    | 510234/900675 [00:25<00:20, 19305.20it/s] 57%|█████▋    | 512243/900675 [00:25<00:19, 19534.62it/s] 57%|█████▋    | 514276/900675 [00:25<00:19, 19763.73it/s] 57%|█████▋    | 516255/900675 [00:25<00:19, 19625.12it/s] 58%|█████▊    | 518220/900675 [00:25<00:19, 19377.42it/s] 58%|█████▊    | 520160/900675 [00:26<00:19, 19351.09it/s] 58%|█████▊    | 522140/900675 [00:26<00:19, 19474.38it/s] 58%|█████▊    | 524089/900675 [00:26<00:19, 19220.05it/s] 58%|█████▊    | 526013/900675 [00:26<00:19, 19163.76it/s] 59%|█████▊    | 527931/900675 [00:26<00:19, 18958.81it/s] 59%|█████▉    | 529939/900675 [00:26<00:19, 19283.13it/s] 59%|█████▉    | 531869/900675 [00:26<00:19, 19173.67it/s] 59%|█████▉    | 533788/900675 [00:26<00:19, 19011.68it/s] 59%|█████▉    | 535692/900675 [00:26<00:19, 19014.42it/s] 60%|█████▉    | 537594/900675 [00:26<00:19, 18938.25it/s] 60%|█████▉    | 539595/900675 [00:27<00:18, 19250.76it/s] 60%|██████    | 541551/900675 [00:27<00:18, 19341.92it/s] 60%|██████    | 543605/900675 [00:27<00:18, 19694.73it/s] 61%|██████    | 545575/900675 [00:27<00:18, 19656.16it/s] 61%|██████    | 547568/900675 [00:27<00:17, 19737.51it/s] 61%|██████    | 549627/900675 [00:27<00:17, 19991.50it/s] 61%|██████    | 551660/900675 [00:27<00:17, 20085.72it/s] 61%|██████▏   | 553669/900675 [00:27<00:17, 19824.83it/s] 62%|██████▏   | 555715/900675 [00:27<00:17, 20007.07it/s] 62%|██████▏   | 557717/900675 [00:27<00:17, 19978.08it/s] 62%|██████▏   | 559716/900675 [00:28<00:17, 19554.90it/s] 62%|██████▏   | 561765/900675 [00:28<00:17, 19828.41it/s] 63%|██████▎   | 563767/900675 [00:28<00:16, 19884.68it/s] 63%|██████▎   | 565786/900675 [00:28<00:16, 19974.67it/s] 63%|██████▎   | 567785/900675 [00:28<00:16, 19814.45it/s] 63%|██████▎   | 569851/900675 [00:28<00:16, 20064.00it/s] 64%|██████▎   | 572098/900675 [00:28<00:15, 20776.33it/s] 64%|██████▎   | 574177/900675 [00:28<00:16, 20289.93it/s] 64%|██████▍   | 576210/900675 [00:28<00:16, 20043.01it/s] 64%|██████▍   | 578217/900675 [00:29<00:16, 19697.27it/s] 64%|██████▍   | 580308/900675 [00:29<00:15, 20043.62it/s] 65%|██████▍   | 582355/900675 [00:29<00:15, 20163.23it/s] 65%|██████▍   | 584660/900675 [00:29<00:15, 21012.02it/s] 65%|██████▌   | 586764/900675 [00:29<00:15, 20566.17it/s] 65%|██████▌   | 588825/900675 [00:29<00:15, 19969.77it/s] 66%|██████▌   | 590885/900675 [00:29<00:15, 20147.39it/s] 66%|██████▌   | 592905/900675 [00:29<00:15, 19699.19it/s] 66%|██████▌   | 594989/900675 [00:29<00:15, 20029.25it/s] 66%|██████▋   | 596997/900675 [00:29<00:15, 19867.64it/s] 67%|██████▋   | 598987/900675 [00:30<00:15, 19767.99it/s] 67%|██████▋   | 601016/900675 [00:30<00:15, 19919.84it/s] 67%|██████▋   | 603010/900675 [00:30<00:15, 19298.74it/s] 67%|██████▋   | 605074/900675 [00:30<00:15, 19681.87it/s] 67%|██████▋   | 607071/900675 [00:30<00:14, 19765.70it/s] 68%|██████▊   | 609051/900675 [00:30<00:15, 19431.57it/s] 68%|██████▊   | 611212/900675 [00:30<00:14, 20067.60it/s] 68%|██████▊   | 613229/900675 [00:30<00:14, 20087.84it/s] 68%|██████▊   | 615241/900675 [00:30<00:14, 19759.20it/s] 69%|██████▊   | 617392/900675 [00:30<00:13, 20269.74it/s] 69%|██████▉   | 619678/900675 [00:31<00:13, 21035.06it/s] 69%|██████▉   | 621785/900675 [00:31<00:13, 21043.53it/s] 69%|██████▉   | 623892/900675 [00:31<00:13, 20718.90it/s] 70%|██████▉   | 626010/900675 [00:31<00:13, 20854.73it/s] 70%|██████▉   | 628098/900675 [00:31<00:13, 20551.02it/s] 70%|██████▉   | 630156/900675 [00:31<00:13, 20213.01it/s] 70%|███████   | 632352/900675 [00:31<00:12, 20722.41it/s] 70%|███████   | 634528/900675 [00:31<00:12, 21026.45it/s] 71%|███████   | 636634/900675 [00:31<00:12, 20689.26it/s] 71%|███████   | 638736/900675 [00:31<00:12, 20777.07it/s] 71%|███████   | 640816/900675 [00:32<00:12, 20653.57it/s] 71%|███████▏  | 642914/900675 [00:32<00:12, 20747.07it/s] 72%|███████▏  | 644990/900675 [00:32<00:12, 19930.88it/s] 72%|███████▏  | 647046/900675 [00:32<00:12, 20112.16it/s] 72%|███████▏  | 649063/900675 [00:32<00:12, 19388.52it/s] 72%|███████▏  | 651010/900675 [00:32<00:13, 19078.78it/s] 73%|███████▎  | 653072/900675 [00:32<00:12, 19520.08it/s] 73%|███████▎  | 655105/900675 [00:32<00:12, 19748.26it/s] 73%|███████▎  | 657112/900675 [00:32<00:12, 19842.35it/s] 73%|███████▎  | 659209/900675 [00:33<00:11, 20169.19it/s] 73%|███████▎  | 661229/900675 [00:33<00:11, 20130.41it/s] 74%|███████▎  | 663245/900675 [00:33<00:11, 19936.97it/s] 74%|███████▍  | 665283/900675 [00:33<00:11, 20064.74it/s] 74%|███████▍  | 667301/900675 [00:33<00:11, 20096.16it/s] 74%|███████▍  | 669456/900675 [00:33<00:11, 20526.60it/s] 75%|███████▍  | 671542/900675 [00:33<00:11, 20614.93it/s] 75%|███████▍  | 673636/900675 [00:33<00:10, 20705.65it/s] 75%|███████▌  | 675718/900675 [00:33<00:10, 20739.37it/s] 75%|███████▌  | 677793/900675 [00:33<00:10, 20272.79it/s] 75%|███████▌  | 679823/900675 [00:34<00:11, 19856.52it/s] 76%|███████▌  | 681928/900675 [00:34<00:10, 20192.47it/s] 76%|███████▌  | 683966/900675 [00:34<00:10, 20243.19it/s] 76%|███████▌  | 686009/900675 [00:34<00:10, 20292.48it/s] 76%|███████▋  | 688040/900675 [00:34<00:10, 20030.11it/s] 77%|███████▋  | 690045/900675 [00:34<00:10, 19887.07it/s] 77%|███████▋  | 692035/900675 [00:34<00:10, 19320.66it/s] 77%|███████▋  | 693971/900675 [00:34<00:10, 19206.58it/s] 77%|███████▋  | 695939/900675 [00:34<00:10, 19344.29it/s] 77%|███████▋  | 697949/900675 [00:34<00:10, 19565.55it/s] 78%|███████▊  | 699908/900675 [00:35<00:10, 19443.93it/s] 78%|███████▊  | 702064/900675 [00:35<00:09, 20069.22it/s] 78%|███████▊  | 704117/900675 [00:35<00:09, 20205.78it/s] 78%|███████▊  | 706139/900675 [00:35<00:10, 19401.94it/s] 79%|███████▊  | 708117/900675 [00:35<00:09, 19510.35it/s] 79%|███████▉  | 710281/900675 [00:35<00:09, 20126.08it/s] 79%|███████▉  | 712321/900675 [00:35<00:09, 20206.31it/s] 79%|███████▉  | 714378/900675 [00:35<00:09, 20312.06it/s] 80%|███████▉  | 716429/900675 [00:35<00:09, 20369.68it/s] 80%|███████▉  | 718468/900675 [00:35<00:09, 19966.44it/s] 80%|███████▉  | 720468/900675 [00:36<00:09, 19526.95it/s] 80%|████████  | 722551/900675 [00:36<00:08, 19905.90it/s] 80%|████████  | 724546/900675 [00:36<00:08, 19783.86it/s] 81%|████████  | 726527/900675 [00:36<00:08, 19387.41it/s] 81%|████████  | 728581/900675 [00:36<00:08, 19717.52it/s] 81%|████████  | 730556/900675 [00:36<00:08, 19437.54it/s] 81%|████████▏ | 732566/900675 [00:36<00:08, 19631.16it/s] 82%|████████▏ | 734807/900675 [00:36<00:08, 20450.12it/s] 82%|████████▏ | 736855/900675 [00:36<00:08, 20322.20it/s] 82%|████████▏ | 738890/900675 [00:37<00:08, 20185.80it/s] 82%|████████▏ | 740911/900675 [00:37<00:07, 20139.04it/s] 82%|████████▏ | 742926/900675 [00:37<00:08, 19643.82it/s] 83%|████████▎ | 744897/900675 [00:37<00:07, 19656.61it/s] 83%|████████▎ | 746865/900675 [00:37<00:07, 19644.10it/s] 83%|████████▎ | 748831/900675 [00:37<00:07, 19291.71it/s] 83%|████████▎ | 750899/900675 [00:37<00:07, 19696.23it/s] 84%|████████▎ | 752871/900675 [00:37<00:07, 19477.59it/s] 84%|████████▍ | 754821/900675 [00:37<00:07, 19427.66it/s] 84%|████████▍ | 756766/900675 [00:37<00:07, 19060.60it/s] 84%|████████▍ | 758784/900675 [00:38<00:07, 19386.85it/s] 84%|████████▍ | 760905/900675 [00:38<00:07, 19921.65it/s] 85%|████████▍ | 762900/900675 [00:38<00:07, 19604.09it/s] 85%|████████▍ | 764863/900675 [00:38<00:06, 19513.15it/s] 85%|████████▌ | 766940/900675 [00:38<00:06, 19882.88it/s] 85%|████████▌ | 768931/900675 [00:38<00:06, 19508.24it/s] 86%|████████▌ | 770885/900675 [00:38<00:06, 19317.03it/s] 86%|████████▌ | 772899/900675 [00:38<00:06, 19555.57it/s] 86%|████████▌ | 774880/900675 [00:38<00:06, 19630.19it/s] 86%|████████▋ | 776845/900675 [00:38<00:06, 19347.24it/s] 86%|████████▋ | 778931/900675 [00:39<00:06, 19789.62it/s] 87%|████████▋ | 780912/900675 [00:39<00:06, 19423.06it/s] 87%|████████▋ | 782857/900675 [00:39<00:06, 19298.98it/s] 87%|████████▋ | 784789/900675 [00:39<00:06, 19240.71it/s] 87%|████████▋ | 786870/900675 [00:39<00:05, 19697.69it/s] 88%|████████▊ | 788842/900675 [00:39<00:05, 19557.56it/s] 88%|████████▊ | 790912/900675 [00:39<00:05, 19887.20it/s] 88%|████████▊ | 792902/900675 [00:39<00:05, 19676.04it/s] 88%|████████▊ | 794871/900675 [00:39<00:05, 19466.95it/s] 88%|████████▊ | 796833/900675 [00:39<00:05, 19507.45it/s] 89%|████████▊ | 798785/900675 [00:40<00:05, 19452.83it/s] 89%|████████▉ | 800793/900675 [00:40<00:05, 19638.24it/s] 89%|████████▉ | 802758/900675 [00:40<00:05, 19240.19it/s] 89%|████████▉ | 804823/900675 [00:40<00:04, 19653.93it/s] 90%|████████▉ | 806793/900675 [00:40<00:04, 19666.15it/s] 90%|████████▉ | 808762/900675 [00:40<00:04, 19025.12it/s] 90%|█████████ | 810670/900675 [00:40<00:04, 18953.58it/s] 90%|█████████ | 812581/900675 [00:40<00:04, 18998.33it/s] 90%|█████████ | 814764/900675 [00:40<00:04, 19827.52it/s] 91%|█████████ | 817075/900675 [00:41<00:04, 20791.38it/s] 91%|█████████ | 819158/900675 [00:41<00:03, 20527.78it/s] 91%|█████████ | 821326/900675 [00:41<00:03, 20867.56it/s] 91%|█████████▏| 823416/900675 [00:41<00:03, 20230.97it/s] 92%|█████████▏| 825445/900675 [00:41<00:03, 19533.33it/s] 92%|█████████▏| 827474/900675 [00:41<00:03, 19739.80it/s] 92%|█████████▏| 829455/900675 [00:41<00:03, 19119.97it/s] 92%|█████████▏| 831374/900675 [00:41<00:03, 19047.62it/s] 93%|█████████▎| 833448/900675 [00:41<00:03, 19536.47it/s] 93%|█████████▎| 835466/900675 [00:41<00:03, 19722.56it/s] 93%|█████████▎| 837518/900675 [00:42<00:03, 19954.54it/s] 93%|█████████▎| 839517/900675 [00:42<00:03, 19801.74it/s] 93%|█████████▎| 841603/900675 [00:42<00:02, 20109.63it/s] 94%|█████████▎| 843617/900675 [00:42<00:02, 19272.53it/s] 94%|█████████▍| 845553/900675 [00:42<00:02, 18808.41it/s] 94%|█████████▍| 847490/900675 [00:42<00:02, 18968.70it/s] 94%|█████████▍| 849457/900675 [00:42<00:02, 19172.31it/s] 95%|█████████▍| 851379/900675 [00:42<00:02, 19134.16it/s] 95%|█████████▍| 853296/900675 [00:42<00:02, 19080.32it/s] 95%|█████████▍| 855243/900675 [00:42<00:02, 19194.05it/s] 95%|█████████▌| 857298/900675 [00:43<00:02, 19595.83it/s] 95%|█████████▌| 859260/900675 [00:43<00:02, 19309.53it/s] 96%|█████████▌| 861421/900675 [00:43<00:01, 19985.87it/s] 96%|█████████▌| 863479/900675 [00:43<00:01, 20153.88it/s] 96%|█████████▌| 865497/900675 [00:43<00:01, 19948.75it/s] 96%|█████████▋| 867494/900675 [00:43<00:01, 19741.82it/s] 97%|█████████▋| 869470/900675 [00:43<00:01, 19565.50it/s] 97%|█████████▋| 871606/900675 [00:43<00:01, 20093.79it/s] 97%|█████████▋| 873618/900675 [00:43<00:01, 19827.99it/s] 97%|█████████▋| 875710/900675 [00:44<00:01, 20144.06it/s] 97%|█████████▋| 877727/900675 [00:44<00:01, 19899.35it/s] 98%|█████████▊| 879719/900675 [00:44<00:01, 19797.16it/s] 98%|█████████▊| 881700/900675 [00:44<00:00, 19337.38it/s] 98%|█████████▊| 883645/900675 [00:44<00:00, 19368.75it/s] 98%|█████████▊| 885590/900675 [00:44<00:00, 19390.71it/s] 99%|█████████▊| 887647/900675 [00:44<00:00, 19739.23it/s] 99%|█████████▉| 889623/900675 [00:44<00:00, 19728.11it/s] 99%|█████████▉| 891610/900675 [00:44<00:00, 19769.39it/s] 99%|█████████▉| 893588/900675 [00:44<00:00, 19334.73it/s] 99%|█████████▉| 895524/900675 [00:45<00:00, 18971.88it/s]100%|█████████▉| 897570/900675 [00:45<00:00, 19403.88it/s]100%|█████████▉| 899685/900675 [00:45<00:00, 19915.22it/s]100%|██████████| 900675/900675 [00:45<00:00, 19890.74it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 16.54it/s]2022-02-25 10:47:52 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(430640, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=430640, bias=False)
  )
)
2022-02-25 10:47:52 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-25 10:47:52 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-25 10:47:52 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-02-25 10:47:52 | INFO | fairseq_cli.train | num. shared model params: 239,401,984 (num. trained: 239,401,984)
2022-02-25 10:47:52 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-25 10:47:52 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.5/valid
2022-02-25 10:47:52 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-25 10:47:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 10:47:52 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-02-25 10:47:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-25 10:47:52 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-25 10:47:52 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-02-25 10:47:52 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_last.pt
2022-02-25 10:47:52 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_last.pt
2022-02-25 10:47:52 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-25 10:47:53 | INFO | fairseq.data.data_utils | loaded 900,675 examples from: data-bin/wikitext-103-raw-size-0.5/train
2022-02-25 10:47:53 | INFO | fairseq.trainer | begin training epoch 1
2022-02-25 10:47:53 | INFO | fairseq_cli.train | Start iterating over samples

2022-02-25 10:48:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-25 10:48:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-25 10:48:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 10:48:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-25 10:49:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-25 10:59:56 | INFO | train_inner | epoch 001:    105 / 788 loss=17.588, ppl=196993, wps=10177.4, ups=0.16, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.311, loss_scale=4, train_wall=718, gb_free=4, wall=723
2022-02-25 11:10:39 | INFO | train_inner | epoch 001:    205 / 788 loss=15.154, ppl=36464.3, wps=10181.9, ups=0.16, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.656, loss_scale=4, train_wall=639, gb_free=4, wall=1367
2022-02-25 11:21:23 | INFO | train_inner | epoch 001:    305 / 788 loss=12.925, ppl=7774.97, wps=10186.1, ups=0.16, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.139, loss_scale=4, train_wall=639, gb_free=4, wall=2010
2022-02-25 11:32:06 | INFO | train_inner | epoch 001:    405 / 788 loss=11.271, ppl=2470.57, wps=10188.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.653, loss_scale=4, train_wall=638, gb_free=4, wall=2654
2022-02-25 11:42:49 | INFO | train_inner | epoch 001:    505 / 788 loss=10.597, ppl=1549.17, wps=10188.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.495, loss_scale=4, train_wall=638, gb_free=4, wall=3297
2022-02-25 11:53:32 | INFO | train_inner | epoch 001:    605 / 788 loss=10.274, ppl=1238.47, wps=10187.3, ups=0.16, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.549, loss_scale=8, train_wall=638, gb_free=4, wall=3940
2022-02-25 12:04:16 | INFO | train_inner | epoch 001:    705 / 788 loss=9.996, ppl=1021.04, wps=10182, ups=0.16, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.607, loss_scale=8, train_wall=639, gb_free=4, wall=4584
2022-02-25 12:13:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 12:13:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.605 | ppl 778.82 | wps 23478.4 | wpb 2034.1 | bsz 4 | num_updates 783
2022-02-25 12:13:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 783 updates
2022-02-25 12:13:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 12:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 12:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 1 @ 783 updates, score 9.605) (writing took 6.76111720316112 seconds)
2022-02-25 12:13:23 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-25 12:13:23 | INFO | train | epoch 001 | loss 12.252 | ppl 4879.28 | wps 10152.3 | ups 0.16 | wpb 65497.3 | bsz 127.9 | num_updates 783 | lr 9.79554e-05 | gnorm 1.149 | loss_scale 8 | train_wall 5076 | gb_free 4 | wall 5131
2022-02-25 12:13:23 | INFO | fairseq.trainer | begin training epoch 2
2022-02-25 12:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 12:15:13 | INFO | train_inner | epoch 002:     17 / 788 loss=9.764, ppl=869.25, wps=9935.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.703, loss_scale=8, train_wall=636, gb_free=4, wall=5240
2022-02-25 12:25:56 | INFO | train_inner | epoch 002:    117 / 788 loss=9.532, ppl=740.17, wps=10185.5, ups=0.16, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.787, loss_scale=8, train_wall=639, gb_free=4, wall=5884
2022-02-25 12:36:40 | INFO | train_inner | epoch 002:    217 / 788 loss=9.334, ppl=645.33, wps=10185.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.844, loss_scale=8, train_wall=639, gb_free=4, wall=6527
2022-02-25 12:47:23 | INFO | train_inner | epoch 002:    317 / 788 loss=9.171, ppl=576.37, wps=10185.2, ups=0.16, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.883, loss_scale=16, train_wall=639, gb_free=4, wall=7171
2022-02-25 12:58:07 | INFO | train_inner | epoch 002:    417 / 788 loss=9.015, ppl=517.35, wps=10183.5, ups=0.16, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.879, loss_scale=16, train_wall=639, gb_free=4, wall=7814
2022-02-25 13:08:50 | INFO | train_inner | epoch 002:    517 / 788 loss=8.878, ppl=470.42, wps=10189.3, ups=0.16, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.904, loss_scale=16, train_wall=638, gb_free=4, wall=8457
2022-02-25 13:19:33 | INFO | train_inner | epoch 002:    617 / 788 loss=8.747, ppl=429.54, wps=10187.1, ups=0.16, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.924, loss_scale=16, train_wall=638, gb_free=4, wall=9101
2022-02-25 13:30:16 | INFO | train_inner | epoch 002:    717 / 788 loss=8.62, ppl=393.46, wps=10187, ups=0.16, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.918, loss_scale=16, train_wall=638, gb_free=4, wall=9744
2022-02-25 13:37:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 13:37:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.325 | ppl 320.65 | wps 23596.6 | wpb 2034.1 | bsz 4 | num_updates 1571 | best_loss 8.325
2022-02-25 13:37:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1571 updates
2022-02-25 13:37:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 13:38:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 13:38:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 2 @ 1571 updates, score 8.325) (writing took 6.193599785910919 seconds)
2022-02-25 13:38:06 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-25 13:38:06 | INFO | train | epoch 002 | loss 9.009 | ppl 515.11 | wps 10155.2 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 1571 | lr 0.000196436 | gnorm 0.875 | loss_scale 32 | train_wall 5029 | gb_free 4 | wall 10213
2022-02-25 13:38:06 | INFO | fairseq.trainer | begin training epoch 3
2022-02-25 13:38:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 13:41:12 | INFO | train_inner | epoch 003:     29 / 788 loss=8.491, ppl=359.74, wps=9945.6, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.889, loss_scale=32, train_wall=636, gb_free=4, wall=10400
2022-02-25 13:51:56 | INFO | train_inner | epoch 003:    129 / 788 loss=8.346, ppl=325.34, wps=10189.2, ups=0.16, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.916, loss_scale=32, train_wall=638, gb_free=4, wall=11043
2022-02-25 14:02:39 | INFO | train_inner | epoch 003:    229 / 788 loss=8.259, ppl=306.26, wps=10185.7, ups=0.16, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.899, loss_scale=32, train_wall=639, gb_free=4, wall=11687
2022-02-25 14:08:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 14:13:29 | INFO | train_inner | epoch 003:    330 / 788 loss=8.157, ppl=285.48, wps=10085.7, ups=0.15, wpb=65519.3, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.891, loss_scale=16, train_wall=645, gb_free=4, wall=12336
2022-02-25 14:24:12 | INFO | train_inner | epoch 003:    430 / 788 loss=8.062, ppl=267.19, wps=10188.6, ups=0.16, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.861, loss_scale=16, train_wall=638, gb_free=4, wall=12979
2022-02-25 14:34:55 | INFO | train_inner | epoch 003:    530 / 788 loss=7.995, ppl=255.07, wps=10185.9, ups=0.16, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.852, loss_scale=16, train_wall=639, gb_free=4, wall=13623
2022-02-25 14:45:38 | INFO | train_inner | epoch 003:    630 / 788 loss=7.908, ppl=240.15, wps=10188.9, ups=0.16, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.83, loss_scale=16, train_wall=638, gb_free=4, wall=14266
2022-02-25 14:56:22 | INFO | train_inner | epoch 003:    730 / 788 loss=7.828, ppl=227.3, wps=10187.2, ups=0.16, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.808, loss_scale=16, train_wall=638, gb_free=4, wall=14909
2022-02-25 15:02:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 15:02:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.608 | ppl 195.03 | wps 23587.9 | wpb 2034.1 | bsz 4 | num_updates 2358 | best_loss 7.608
2022-02-25 15:02:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2358 updates
2022-02-25 15:02:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 15:02:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 15:02:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 3 @ 2358 updates, score 7.608) (writing took 6.223728715907782 seconds)
2022-02-25 15:02:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-25 15:02:47 | INFO | train | epoch 003 | loss 8.069 | ppl 268.47 | wps 10143.4 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 2358 | lr 0.000294791 | gnorm 0.863 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 15295
2022-02-25 15:02:47 | INFO | fairseq.trainer | begin training epoch 4
2022-02-25 15:02:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 15:07:18 | INFO | train_inner | epoch 004:     42 / 788 loss=7.727, ppl=211.81, wps=9946.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.826, loss_scale=32, train_wall=636, gb_free=4, wall=15565
2022-02-25 15:18:01 | INFO | train_inner | epoch 004:    142 / 788 loss=7.622, ppl=196.94, wps=10188.5, ups=0.16, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.782, loss_scale=32, train_wall=638, gb_free=4, wall=16209
2022-02-25 15:28:44 | INFO | train_inner | epoch 004:    242 / 788 loss=7.56, ppl=188.64, wps=10186.6, ups=0.16, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.788, loss_scale=32, train_wall=638, gb_free=4, wall=16852
2022-02-25 15:35:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 15:39:34 | INFO | train_inner | epoch 004:    343 / 788 loss=7.506, ppl=181.75, wps=10082.2, ups=0.15, wpb=65520.6, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.785, loss_scale=16, train_wall=645, gb_free=4, wall=17502
2022-02-25 15:50:17 | INFO | train_inner | epoch 004:    443 / 788 loss=7.462, ppl=176.36, wps=10188.6, ups=0.16, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.763, loss_scale=16, train_wall=638, gb_free=4, wall=18145
2022-02-25 16:01:01 | INFO | train_inner | epoch 004:    543 / 788 loss=7.404, ppl=169.34, wps=10187.8, ups=0.16, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.743, loss_scale=16, train_wall=638, gb_free=4, wall=18788
2022-02-25 16:11:44 | INFO | train_inner | epoch 004:    643 / 788 loss=7.348, ppl=162.88, wps=10187.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.745, loss_scale=16, train_wall=638, gb_free=4, wall=19432
2022-02-25 16:22:27 | INFO | train_inner | epoch 004:    743 / 788 loss=7.307, ppl=158.32, wps=10187.5, ups=0.16, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.754, loss_scale=16, train_wall=638, gb_free=4, wall=20075
2022-02-25 16:27:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 16:27:23 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.154 | ppl 142.43 | wps 23568.6 | wpb 2034.1 | bsz 4 | num_updates 3145 | best_loss 7.154
2022-02-25 16:27:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3145 updates
2022-02-25 16:27:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 16:27:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 16:27:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 4 @ 3145 updates, score 7.154) (writing took 6.2435624280478805 seconds)
2022-02-25 16:27:29 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-25 16:27:29 | INFO | train | epoch 004 | loss 7.459 | ppl 176 | wps 10142.9 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 3145 | lr 0.000393146 | gnorm 0.766 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 20377
2022-02-25 16:27:29 | INFO | fairseq.trainer | begin training epoch 5
2022-02-25 16:27:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 16:31:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 16:33:30 | INFO | train_inner | epoch 005:     56 / 788 loss=7.192, ppl=146.25, wps=9849.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.717, loss_scale=16, train_wall=642, gb_free=4, wall=20737
2022-02-25 16:44:13 | INFO | train_inner | epoch 005:    156 / 788 loss=7.132, ppl=140.3, wps=10189, ups=0.16, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.721, loss_scale=16, train_wall=638, gb_free=4, wall=21381
2022-02-25 16:54:56 | INFO | train_inner | epoch 005:    256 / 788 loss=7.087, ppl=135.93, wps=10185.1, ups=0.16, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.742, loss_scale=16, train_wall=639, gb_free=4, wall=22024
2022-02-25 17:05:40 | INFO | train_inner | epoch 005:    356 / 788 loss=7.071, ppl=134.48, wps=10187.5, ups=0.16, wpb=65519.3, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.689, loss_scale=16, train_wall=638, gb_free=4, wall=22667
2022-02-25 17:16:23 | INFO | train_inner | epoch 005:    456 / 788 loss=7.033, ppl=130.97, wps=10187.1, ups=0.16, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.683, loss_scale=16, train_wall=638, gb_free=4, wall=23310
2022-02-25 17:27:06 | INFO | train_inner | epoch 005:    556 / 788 loss=7.006, ppl=128.54, wps=10186.3, ups=0.16, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.699, loss_scale=32, train_wall=639, gb_free=4, wall=23954
2022-02-25 17:30:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 17:37:56 | INFO | train_inner | epoch 005:    657 / 788 loss=6.98, ppl=126.27, wps=10086.9, ups=0.15, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.667, loss_scale=16, train_wall=645, gb_free=4, wall=24604
2022-02-25 17:48:39 | INFO | train_inner | epoch 005:    757 / 788 loss=6.944, ppl=123.15, wps=10185.2, ups=0.16, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.676, loss_scale=16, train_wall=638, gb_free=4, wall=25247
2022-02-25 17:51:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 17:52:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.853 | ppl 115.59 | wps 23577.7 | wpb 2034.1 | bsz 4 | num_updates 3931 | best_loss 6.853
2022-02-25 17:52:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3931 updates
2022-02-25 17:52:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 17:52:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 17:52:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 5 @ 3931 updates, score 6.853) (writing took 6.230763836065307 seconds)
2022-02-25 17:52:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-25 17:52:11 | INFO | train | epoch 005 | loss 7.038 | ppl 131.45 | wps 10130.1 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 3931 | lr 0.000491377 | gnorm 0.696 | loss_scale 16 | train_wall 5028 | gb_free 4 | wall 25459
2022-02-25 17:52:11 | INFO | fairseq.trainer | begin training epoch 6
2022-02-25 17:52:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 17:59:35 | INFO | train_inner | epoch 006:     69 / 788 loss=6.822, ppl=113.17, wps=9947.4, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.663, loss_scale=16, train_wall=636, gb_free=4, wall=25903
2022-02-25 18:10:18 | INFO | train_inner | epoch 006:    169 / 788 loss=6.776, ppl=109.58, wps=10189.8, ups=0.16, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.66, loss_scale=16, train_wall=638, gb_free=4, wall=26546
2022-02-25 18:21:02 | INFO | train_inner | epoch 006:    269 / 788 loss=6.751, ppl=107.75, wps=10189.9, ups=0.16, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.628, loss_scale=16, train_wall=638, gb_free=4, wall=27189
2022-02-25 18:28:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 18:31:51 | INFO | train_inner | epoch 006:    370 / 788 loss=6.755, ppl=107.97, wps=10089.8, ups=0.15, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.623, loss_scale=16, train_wall=645, gb_free=4, wall=27839
2022-02-25 18:42:34 | INFO | train_inner | epoch 006:    470 / 788 loss=6.721, ppl=105.51, wps=10191.5, ups=0.16, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.604, loss_scale=16, train_wall=638, gb_free=4, wall=28482
2022-02-25 18:53:17 | INFO | train_inner | epoch 006:    570 / 788 loss=6.711, ppl=104.79, wps=10193.9, ups=0.16, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.603, loss_scale=16, train_wall=638, gb_free=4, wall=29125
2022-02-25 19:04:00 | INFO | train_inner | epoch 006:    670 / 788 loss=6.692, ppl=103.41, wps=10192.2, ups=0.16, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.604, loss_scale=16, train_wall=638, gb_free=4, wall=29768
2022-02-25 19:14:43 | INFO | train_inner | epoch 006:    770 / 788 loss=6.683, ppl=102.72, wps=10191.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.58, loss_scale=16, train_wall=638, gb_free=4, wall=30411
2022-02-25 19:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 19:16:45 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.64 | ppl 99.77 | wps 23523.2 | wpb 2034.1 | bsz 4 | num_updates 4718 | best_loss 6.64
2022-02-25 19:16:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4718 updates
2022-02-25 19:16:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 19:16:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 19:16:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 6 @ 4718 updates, score 6.64) (writing took 6.271024754038081 seconds)
2022-02-25 19:16:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-25 19:16:51 | INFO | train | epoch 006 | loss 6.73 | ppl 106.17 | wps 10146.9 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 4718 | lr 0.000460385 | gnorm 0.617 | loss_scale 16 | train_wall 5026 | gb_free 4 | wall 30539
2022-02-25 19:16:52 | INFO | fairseq.trainer | begin training epoch 7
2022-02-25 19:16:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 19:25:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 19:25:45 | INFO | train_inner | epoch 007:     83 / 788 loss=6.531, ppl=92.5, wps=9852.2, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.574, loss_scale=16, train_wall=642, gb_free=4, wall=31073
2022-02-25 19:36:28 | INFO | train_inner | epoch 007:    183 / 788 loss=6.516, ppl=91.53, wps=10189.9, ups=0.16, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.567, loss_scale=16, train_wall=638, gb_free=4, wall=31716
2022-02-25 19:47:11 | INFO | train_inner | epoch 007:    283 / 788 loss=6.505, ppl=90.79, wps=10193.7, ups=0.16, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.577, loss_scale=16, train_wall=638, gb_free=4, wall=32359
2022-02-25 19:57:54 | INFO | train_inner | epoch 007:    383 / 788 loss=6.507, ppl=90.95, wps=10192.5, ups=0.16, wpb=65534.7, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.57, loss_scale=16, train_wall=638, gb_free=4, wall=33002
2022-02-25 20:08:37 | INFO | train_inner | epoch 007:    483 / 788 loss=6.511, ppl=91.22, wps=10191.4, ups=0.16, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.568, loss_scale=16, train_wall=638, gb_free=4, wall=33645
2022-02-25 20:19:20 | INFO | train_inner | epoch 007:    583 / 788 loss=6.49, ppl=89.91, wps=10193.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.543, loss_scale=16, train_wall=638, gb_free=4, wall=34288
2022-02-25 20:23:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 20:30:10 | INFO | train_inner | epoch 007:    684 / 788 loss=6.483, ppl=89.45, wps=10090.5, ups=0.15, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.559, loss_scale=16, train_wall=645, gb_free=4, wall=34937
2022-02-25 20:40:53 | INFO | train_inner | epoch 007:    784 / 788 loss=6.482, ppl=89.37, wps=10191.6, ups=0.16, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.531, loss_scale=16, train_wall=638, gb_free=4, wall=35580
2022-02-25 20:41:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 20:41:25 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.51 | ppl 91.12 | wps 23600.6 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 6.51
2022-02-25 20:41:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5504 updates
2022-02-25 20:41:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 20:41:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 20:41:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 7 @ 5504 updates, score 6.51) (writing took 6.1510410469491035 seconds)
2022-02-25 20:41:31 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-25 20:41:31 | INFO | train | epoch 007 | loss 6.499 | ppl 90.47 | wps 10135.2 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 5504 | lr 0.000426246 | gnorm 0.562 | loss_scale 16 | train_wall 5025 | gb_free 4 | wall 35618
2022-02-25 20:41:31 | INFO | fairseq.trainer | begin training epoch 8
2022-02-25 20:41:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 20:51:48 | INFO | train_inner | epoch 008:     96 / 788 loss=6.323, ppl=80.05, wps=9950.6, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=5600, lr=0.000422577, gnorm=0.539, loss_scale=16, train_wall=635, gb_free=4, wall=36236
2022-02-25 21:02:31 | INFO | train_inner | epoch 008:    196 / 788 loss=6.329, ppl=80.37, wps=10193.6, ups=0.16, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.565, loss_scale=16, train_wall=638, gb_free=4, wall=36879
2022-02-25 21:13:14 | INFO | train_inner | epoch 008:    296 / 788 loss=6.324, ppl=80.1, wps=10191.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.553, loss_scale=16, train_wall=638, gb_free=4, wall=37522
2022-02-25 21:20:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 21:24:04 | INFO | train_inner | epoch 008:    397 / 788 loss=6.348, ppl=81.43, wps=10090.4, ups=0.15, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.54, loss_scale=16, train_wall=645, gb_free=4, wall=38171
2022-02-25 21:34:47 | INFO | train_inner | epoch 008:    497 / 788 loss=6.338, ppl=80.9, wps=10192.1, ups=0.16, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.535, loss_scale=16, train_wall=638, gb_free=4, wall=38814
2022-02-25 21:45:30 | INFO | train_inner | epoch 008:    597 / 788 loss=6.342, ppl=81.14, wps=10190.5, ups=0.16, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.54, loss_scale=16, train_wall=638, gb_free=4, wall=39457
2022-02-25 21:56:13 | INFO | train_inner | epoch 008:    697 / 788 loss=6.357, ppl=81.98, wps=10191.8, ups=0.16, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.538, loss_scale=16, train_wall=638, gb_free=4, wall=40100
2022-02-25 22:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 22:06:04 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.414 | ppl 85.27 | wps 23595.6 | wpb 2034.1 | bsz 4 | num_updates 6291 | best_loss 6.414
2022-02-25 22:06:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6291 updates
2022-02-25 22:06:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 22:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 22:06:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 8 @ 6291 updates, score 6.414) (writing took 6.15711013507098 seconds)
2022-02-25 22:06:11 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-25 22:06:11 | INFO | train | epoch 008 | loss 6.337 | ppl 80.83 | wps 10147.5 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 6291 | lr 0.000398694 | gnorm 0.544 | loss_scale 16 | train_wall 5026 | gb_free 4 | wall 40698
2022-02-25 22:06:11 | INFO | fairseq.trainer | begin training epoch 9
2022-02-25 22:06:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 22:07:09 | INFO | train_inner | epoch 009:      9 / 788 loss=6.324, ppl=80.12, wps=9948.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.542, loss_scale=16, train_wall=636, gb_free=4, wall=40756
2022-02-25 22:15:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 22:17:58 | INFO | train_inner | epoch 009:    110 / 788 loss=6.178, ppl=72.4, wps=10091.1, ups=0.15, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.536, loss_scale=16, train_wall=645, gb_free=4, wall=41406
2022-02-25 22:28:41 | INFO | train_inner | epoch 009:    210 / 788 loss=6.188, ppl=72.9, wps=10191.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.534, loss_scale=16, train_wall=638, gb_free=4, wall=42049
2022-02-25 22:39:24 | INFO | train_inner | epoch 009:    310 / 788 loss=6.212, ppl=74.14, wps=10192.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.535, loss_scale=16, train_wall=638, gb_free=4, wall=42692
2022-02-25 22:50:07 | INFO | train_inner | epoch 009:    410 / 788 loss=6.223, ppl=74.72, wps=10191.6, ups=0.16, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.533, loss_scale=16, train_wall=638, gb_free=4, wall=43335
2022-02-25 23:00:50 | INFO | train_inner | epoch 009:    510 / 788 loss=6.223, ppl=74.72, wps=10192, ups=0.16, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.534, loss_scale=16, train_wall=638, gb_free=4, wall=43978
2022-02-25 23:11:33 | INFO | train_inner | epoch 009:    610 / 788 loss=6.23, ppl=75.07, wps=10191.5, ups=0.16, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.549, loss_scale=32, train_wall=638, gb_free=4, wall=44621
2022-02-25 23:13:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-25 23:22:23 | INFO | train_inner | epoch 009:    711 / 788 loss=6.236, ppl=75.39, wps=10089.9, ups=0.15, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.515, loss_scale=16, train_wall=645, gb_free=4, wall=45270
2022-02-25 23:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-25 23:30:44 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.356 | ppl 81.94 | wps 23555.7 | wpb 2034.1 | bsz 4 | num_updates 7077 | best_loss 6.356
2022-02-25 23:30:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7077 updates
2022-02-25 23:30:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 23:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-25 23:30:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 9 @ 7077 updates, score 6.356) (writing took 6.127207946963608 seconds)
2022-02-25 23:30:50 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-25 23:30:50 | INFO | train | epoch 009 | loss 6.214 | ppl 74.23 | wps 10135 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 7077 | lr 0.000375903 | gnorm 0.535 | loss_scale 16 | train_wall 5026 | gb_free 4 | wall 45778
2022-02-25 23:30:50 | INFO | fairseq.trainer | begin training epoch 10
2022-02-25 23:30:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-25 23:33:18 | INFO | train_inner | epoch 010:     23 / 788 loss=6.188, ppl=72.89, wps=9951.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.538, loss_scale=16, train_wall=635, gb_free=4, wall=45926
2022-02-25 23:44:01 | INFO | train_inner | epoch 010:    123 / 788 loss=6.078, ppl=67.55, wps=10190.7, ups=0.16, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.517, loss_scale=16, train_wall=638, gb_free=4, wall=46569
2022-02-25 23:54:44 | INFO | train_inner | epoch 010:    223 / 788 loss=6.106, ppl=68.9, wps=10190.4, ups=0.16, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.536, loss_scale=16, train_wall=638, gb_free=4, wall=47212
2022-02-26 00:05:28 | INFO | train_inner | epoch 010:    323 / 788 loss=6.126, ppl=69.82, wps=10190.5, ups=0.16, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.532, loss_scale=16, train_wall=638, gb_free=4, wall=47855
2022-02-26 00:10:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 00:16:17 | INFO | train_inner | epoch 010:    424 / 788 loss=6.122, ppl=69.65, wps=10090.2, ups=0.15, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.523, loss_scale=16, train_wall=645, gb_free=4, wall=48505
2022-02-26 00:27:00 | INFO | train_inner | epoch 010:    524 / 788 loss=6.128, ppl=69.93, wps=10191.8, ups=0.16, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.525, loss_scale=16, train_wall=638, gb_free=4, wall=49148
2022-02-26 00:37:43 | INFO | train_inner | epoch 010:    624 / 788 loss=6.127, ppl=69.9, wps=10191.8, ups=0.16, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.546, loss_scale=16, train_wall=638, gb_free=4, wall=49791
2022-02-26 00:48:26 | INFO | train_inner | epoch 010:    724 / 788 loss=6.135, ppl=70.26, wps=10193.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.505, loss_scale=16, train_wall=638, gb_free=4, wall=50433
2022-02-26 00:55:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 00:55:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.309 | ppl 79.28 | wps 23577.2 | wpb 2034.1 | bsz 4 | num_updates 7864 | best_loss 6.309
2022-02-26 00:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 7864 updates
2022-02-26 00:55:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 00:55:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 00:55:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 10 @ 7864 updates, score 6.309) (writing took 6.216119923861697 seconds)
2022-02-26 00:55:30 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-26 00:55:30 | INFO | train | epoch 010 | loss 6.117 | ppl 69.4 | wps 10147.4 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 7864 | lr 0.000356597 | gnorm 0.527 | loss_scale 16 | train_wall 5026 | gb_free 4 | wall 50857
2022-02-26 00:55:30 | INFO | fairseq.trainer | begin training epoch 11
2022-02-26 00:55:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 00:59:22 | INFO | train_inner | epoch 011:     36 / 788 loss=6.074, ppl=67.38, wps=9951.5, ups=0.15, wpb=65248, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.525, loss_scale=16, train_wall=635, gb_free=4, wall=51089
2022-02-26 01:07:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 01:10:11 | INFO | train_inner | epoch 011:    137 / 788 loss=5.992, ppl=63.64, wps=10092.1, ups=0.15, wpb=65520.6, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.532, loss_scale=16, train_wall=644, gb_free=4, wall=51738
2022-02-26 01:20:54 | INFO | train_inner | epoch 011:    237 / 788 loss=6.013, ppl=64.59, wps=10189.2, ups=0.16, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.53, loss_scale=16, train_wall=638, gb_free=4, wall=52381
2022-02-26 01:31:37 | INFO | train_inner | epoch 011:    337 / 788 loss=6.029, ppl=65.28, wps=10192, ups=0.16, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.537, loss_scale=16, train_wall=638, gb_free=4, wall=53025
2022-02-26 01:42:20 | INFO | train_inner | epoch 011:    437 / 788 loss=6.044, ppl=65.97, wps=10190.3, ups=0.16, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.515, loss_scale=16, train_wall=638, gb_free=4, wall=53668
2022-02-26 01:53:03 | INFO | train_inner | epoch 011:    537 / 788 loss=6.061, ppl=66.76, wps=10191.1, ups=0.16, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.533, loss_scale=16, train_wall=638, gb_free=4, wall=54311
2022-02-26 02:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 02:03:53 | INFO | train_inner | epoch 011:    638 / 788 loss=6.065, ppl=66.95, wps=10091.8, ups=0.15, wpb=65534.7, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.529, loss_scale=16, train_wall=644, gb_free=4, wall=54960
2022-02-26 02:14:36 | INFO | train_inner | epoch 011:    738 / 788 loss=6.062, ppl=66.81, wps=10191.2, ups=0.16, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.525, loss_scale=16, train_wall=638, gb_free=4, wall=55603
2022-02-26 02:19:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 02:20:04 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.274 | ppl 77.37 | wps 23595.9 | wpb 2034.1 | bsz 4 | num_updates 8650 | best_loss 6.274
2022-02-26 02:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 8650 updates
2022-02-26 02:20:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 02:20:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 02:20:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 11 @ 8650 updates, score 6.274) (writing took 6.108270274009556 seconds)
2022-02-26 02:20:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-26 02:20:10 | INFO | train | epoch 011 | loss 6.037 | ppl 65.66 | wps 10134.5 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 8650 | lr 0.00034001 | gnorm 0.526 | loss_scale 16 | train_wall 5026 | gb_free 4 | wall 55937
2022-02-26 02:20:10 | INFO | fairseq.trainer | begin training epoch 12
2022-02-26 02:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 02:25:31 | INFO | train_inner | epoch 012:     50 / 788 loss=5.997, ppl=63.88, wps=9950.2, ups=0.15, wpb=65248, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.525, loss_scale=16, train_wall=635, gb_free=4, wall=56259
2022-02-26 02:36:14 | INFO | train_inner | epoch 012:    150 / 788 loss=5.927, ppl=60.84, wps=10191.6, ups=0.16, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.511, loss_scale=16, train_wall=638, gb_free=4, wall=56902
2022-02-26 02:46:57 | INFO | train_inner | epoch 012:    250 / 788 loss=5.945, ppl=61.6, wps=10191.6, ups=0.16, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.555, loss_scale=16, train_wall=638, gb_free=4, wall=57545
2022-02-26 02:57:40 | INFO | train_inner | epoch 012:    350 / 788 loss=5.964, ppl=62.42, wps=10192.4, ups=0.16, wpb=65520.6, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.534, loss_scale=16, train_wall=638, gb_free=4, wall=58188
2022-02-26 03:01:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 03:08:30 | INFO | train_inner | epoch 012:    451 / 788 loss=5.989, ppl=63.52, wps=10090.8, ups=0.15, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.522, loss_scale=16, train_wall=645, gb_free=4, wall=58837
2022-02-26 03:19:13 | INFO | train_inner | epoch 012:    551 / 788 loss=5.981, ppl=63.16, wps=10193.5, ups=0.16, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.54, loss_scale=16, train_wall=638, gb_free=4, wall=59480
2022-02-26 03:29:56 | INFO | train_inner | epoch 012:    651 / 788 loss=5.989, ppl=63.51, wps=10191.3, ups=0.16, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.511, loss_scale=16, train_wall=638, gb_free=4, wall=60123
2022-02-26 03:40:39 | INFO | train_inner | epoch 012:    751 / 788 loss=6.002, ppl=64.08, wps=10192.7, ups=0.16, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.538, loss_scale=16, train_wall=638, gb_free=4, wall=60766
2022-02-26 03:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 03:44:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.254 | ppl 76.3 | wps 23583.1 | wpb 2034.1 | bsz 4 | num_updates 9437 | best_loss 6.254
2022-02-26 03:44:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 9437 updates
2022-02-26 03:44:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 03:44:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 03:44:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 12 @ 9437 updates, score 6.254) (writing took 6.131409089080989 seconds)
2022-02-26 03:44:49 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-26 03:44:49 | INFO | train | epoch 012 | loss 5.97 | ppl 62.68 | wps 10148.1 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 9437 | lr 0.000325524 | gnorm 0.532 | loss_scale 16 | train_wall 5026 | gb_free 4 | wall 61017
2022-02-26 03:44:49 | INFO | fairseq.trainer | begin training epoch 13
2022-02-26 03:44:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 03:51:34 | INFO | train_inner | epoch 013:     63 / 788 loss=5.91, ppl=60.14, wps=9950.8, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.54, loss_scale=16, train_wall=635, gb_free=4, wall=61422
2022-02-26 03:57:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 04:02:24 | INFO | train_inner | epoch 013:    164 / 788 loss=5.868, ppl=58.4, wps=10091, ups=0.15, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.525, loss_scale=16, train_wall=645, gb_free=4, wall=62071
2022-02-26 04:13:07 | INFO | train_inner | epoch 013:    264 / 788 loss=5.893, ppl=59.43, wps=10192.2, ups=0.16, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.525, loss_scale=16, train_wall=638, gb_free=4, wall=62714
2022-02-26 04:23:50 | INFO | train_inner | epoch 013:    364 / 788 loss=5.906, ppl=59.96, wps=10191.5, ups=0.16, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.525, loss_scale=16, train_wall=638, gb_free=4, wall=63357
2022-02-26 04:34:33 | INFO | train_inner | epoch 013:    464 / 788 loss=5.926, ppl=60.81, wps=10191.8, ups=0.16, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.522, loss_scale=16, train_wall=638, gb_free=4, wall=64000
2022-02-26 04:45:16 | INFO | train_inner | epoch 013:    564 / 788 loss=5.931, ppl=61.02, wps=10191.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.521, loss_scale=16, train_wall=638, gb_free=4, wall=64643
2022-02-26 04:46:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 04:56:05 | INFO | train_inner | epoch 013:    665 / 788 loss=5.937, ppl=61.28, wps=10091.9, ups=0.15, wpb=65534.7, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.532, loss_scale=8, train_wall=644, gb_free=4, wall=65293
2022-02-26 05:06:48 | INFO | train_inner | epoch 013:    765 / 788 loss=5.957, ppl=62.13, wps=10197, ups=0.16, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.526, loss_scale=8, train_wall=638, gb_free=4, wall=65935
2022-02-26 05:09:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 05:09:22 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.231 | ppl 75.12 | wps 23544.6 | wpb 2034.1 | bsz 4 | num_updates 10223 | best_loss 6.231
2022-02-26 05:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 10223 updates
2022-02-26 05:09:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 05:09:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 05:09:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 13 @ 10223 updates, score 6.231) (writing took 6.141396300168708 seconds)
2022-02-26 05:09:28 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 05:09:28 | INFO | train | epoch 013 | loss 5.913 | ppl 60.27 | wps 10135.7 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 10223 | lr 0.00031276 | gnorm 0.525 | loss_scale 8 | train_wall 5025 | gb_free 4 | wall 66096
2022-02-26 05:09:28 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 05:09:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 05:17:43 | INFO | train_inner | epoch 014:     77 / 788 loss=5.837, ppl=57.18, wps=9955, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=10300, lr=0.000311588, gnorm=0.532, loss_scale=8, train_wall=635, gb_free=4, wall=66591
2022-02-26 05:28:26 | INFO | train_inner | epoch 014:    177 / 788 loss=5.821, ppl=56.54, wps=10197.1, ups=0.16, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.52, loss_scale=8, train_wall=638, gb_free=4, wall=67234
2022-02-26 05:39:09 | INFO | train_inner | epoch 014:    277 / 788 loss=5.838, ppl=57.18, wps=10194.1, ups=0.16, wpb=65534.7, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.53, loss_scale=8, train_wall=638, gb_free=4, wall=67876
2022-02-26 05:48:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 05:49:58 | INFO | train_inner | epoch 014:    378 / 788 loss=5.858, ppl=58, wps=10093.4, ups=0.15, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.54, loss_scale=8, train_wall=644, gb_free=4, wall=68526
2022-02-26 06:00:41 | INFO | train_inner | epoch 014:    478 / 788 loss=5.872, ppl=58.57, wps=10196.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.521, loss_scale=8, train_wall=638, gb_free=4, wall=69168
2022-02-26 06:11:24 | INFO | train_inner | epoch 014:    578 / 788 loss=5.881, ppl=58.95, wps=10195.3, ups=0.16, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.532, loss_scale=8, train_wall=638, gb_free=4, wall=69811
2022-02-26 06:22:06 | INFO | train_inner | epoch 014:    678 / 788 loss=5.906, ppl=59.95, wps=10197.9, ups=0.16, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.571, loss_scale=8, train_wall=638, gb_free=4, wall=70454
2022-02-26 06:32:49 | INFO | train_inner | epoch 014:    778 / 788 loss=5.916, ppl=60.39, wps=10194.6, ups=0.16, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.539, loss_scale=8, train_wall=638, gb_free=4, wall=71097
2022-02-26 06:33:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 06:34:00 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.206 | ppl 73.84 | wps 23559.4 | wpb 2034.1 | bsz 4 | num_updates 11010 | best_loss 6.206
2022-02-26 06:34:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 11010 updates
2022-02-26 06:34:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 06:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 06:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 14 @ 11010 updates, score 6.206) (writing took 6.152287901146337 seconds)
2022-02-26 06:34:06 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 06:34:06 | INFO | train | epoch 014 | loss 5.864 | ppl 58.22 | wps 10151.7 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 11010 | lr 0.000301374 | gnorm 0.536 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 71173
2022-02-26 06:34:06 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 06:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 06:43:45 | INFO | train_inner | epoch 015:     90 / 788 loss=5.766, ppl=54.42, wps=9953.9, ups=0.15, wpb=65248, bsz=127.4, num_updates=11100, lr=0.00030015, gnorm=0.54, loss_scale=16, train_wall=635, gb_free=4, wall=71752
2022-02-26 06:54:28 | INFO | train_inner | epoch 015:    190 / 788 loss=5.783, ppl=55.05, wps=10189.2, ups=0.16, wpb=65520.6, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.537, loss_scale=16, train_wall=638, gb_free=4, wall=72395
2022-02-26 07:05:11 | INFO | train_inner | epoch 015:    290 / 788 loss=5.802, ppl=55.79, wps=10192.7, ups=0.16, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.522, loss_scale=16, train_wall=638, gb_free=4, wall=73038
2022-02-26 07:15:53 | INFO | train_inner | epoch 015:    390 / 788 loss=5.817, ppl=56.36, wps=10194, ups=0.16, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.558, loss_scale=16, train_wall=638, gb_free=4, wall=73681
2022-02-26 07:26:36 | INFO | train_inner | epoch 015:    490 / 788 loss=5.832, ppl=56.96, wps=10192.1, ups=0.16, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.535, loss_scale=16, train_wall=638, gb_free=4, wall=74324
2022-02-26 07:34:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 07:37:26 | INFO | train_inner | epoch 015:    591 / 788 loss=5.83, ppl=56.9, wps=10094.9, ups=0.15, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.532, loss_scale=8, train_wall=644, gb_free=4, wall=74973
2022-02-26 07:48:08 | INFO | train_inner | epoch 015:    691 / 788 loss=5.864, ppl=58.24, wps=10195.7, ups=0.16, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.539, loss_scale=8, train_wall=638, gb_free=4, wall=75616
2022-02-26 07:58:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 07:58:38 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.204 | ppl 73.73 | wps 23569.1 | wpb 2034.1 | bsz 4 | num_updates 11797 | best_loss 6.204
2022-02-26 07:58:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 11797 updates
2022-02-26 07:58:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 07:58:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 07:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 15 @ 11797 updates, score 6.204) (writing took 6.150955182965845 seconds)
2022-02-26 07:58:44 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 07:58:44 | INFO | train | epoch 015 | loss 5.82 | ppl 56.48 | wps 10149.8 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 11797 | lr 0.000291148 | gnorm 0.539 | loss_scale 8 | train_wall 5025 | gb_free 4 | wall 76252
2022-02-26 07:58:44 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 07:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 07:59:04 | INFO | train_inner | epoch 016:      3 / 788 loss=5.868, ppl=58.41, wps=9955.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.551, loss_scale=8, train_wall=635, gb_free=4, wall=76271
2022-02-26 08:09:47 | INFO | train_inner | epoch 016:    103 / 788 loss=5.72, ppl=52.71, wps=10196.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.56, loss_scale=8, train_wall=638, gb_free=4, wall=76914
2022-02-26 08:20:29 | INFO | train_inner | epoch 016:    203 / 788 loss=5.743, ppl=53.57, wps=10196.4, ups=0.16, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.54, loss_scale=8, train_wall=638, gb_free=4, wall=77557
2022-02-26 08:31:12 | INFO | train_inner | epoch 016:    303 / 788 loss=5.758, ppl=54.12, wps=10195.1, ups=0.16, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.55, loss_scale=16, train_wall=638, gb_free=4, wall=78200
2022-02-26 08:40:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 08:42:01 | INFO | train_inner | epoch 016:    404 / 788 loss=5.78, ppl=54.97, wps=10093.2, ups=0.15, wpb=65520.6, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.55, loss_scale=8, train_wall=644, gb_free=4, wall=78849
2022-02-26 08:52:44 | INFO | train_inner | epoch 016:    504 / 788 loss=5.797, ppl=55.58, wps=10197.2, ups=0.16, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.578, loss_scale=8, train_wall=638, gb_free=4, wall=79492
2022-02-26 09:03:27 | INFO | train_inner | epoch 016:    604 / 788 loss=5.807, ppl=55.97, wps=10195, ups=0.16, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.553, loss_scale=8, train_wall=638, gb_free=4, wall=80134
2022-02-26 09:14:10 | INFO | train_inner | epoch 016:    704 / 788 loss=5.824, ppl=56.66, wps=10194.3, ups=0.16, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.534, loss_scale=8, train_wall=638, gb_free=4, wall=80777
2022-02-26 09:23:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 09:23:16 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.188 | ppl 72.91 | wps 23583 | wpb 2034.1 | bsz 4 | num_updates 12584 | best_loss 6.188
2022-02-26 09:23:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 12584 updates
2022-02-26 09:23:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 09:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 09:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 16 @ 12584 updates, score 6.188) (writing took 6.118738038931042 seconds)
2022-02-26 09:23:22 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 09:23:22 | INFO | train | epoch 016 | loss 5.781 | ppl 54.97 | wps 10151.7 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 12584 | lr 0.000281897 | gnorm 0.551 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 81330
2022-02-26 09:23:22 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 09:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 09:25:05 | INFO | train_inner | epoch 017:     16 / 788 loss=5.795, ppl=55.51, wps=9955.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.544, loss_scale=8, train_wall=635, gb_free=4, wall=81433
2022-02-26 09:35:48 | INFO | train_inner | epoch 017:    116 / 788 loss=5.676, ppl=51.13, wps=10196.6, ups=0.16, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.549, loss_scale=8, train_wall=638, gb_free=4, wall=82075
2022-02-26 09:46:31 | INFO | train_inner | epoch 017:    216 / 788 loss=5.717, ppl=52.59, wps=10191.3, ups=0.16, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.549, loss_scale=16, train_wall=638, gb_free=4, wall=82718
2022-02-26 09:47:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 09:57:20 | INFO | train_inner | epoch 017:    317 / 788 loss=5.746, ppl=53.66, wps=10094.3, ups=0.15, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.556, loss_scale=8, train_wall=644, gb_free=4, wall=83368
2022-02-26 10:08:03 | INFO | train_inner | epoch 017:    417 / 788 loss=5.746, ppl=53.67, wps=10196, ups=0.16, wpb=65534.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.545, loss_scale=8, train_wall=638, gb_free=4, wall=84010
2022-02-26 10:18:45 | INFO | train_inner | epoch 017:    517 / 788 loss=5.759, ppl=54.14, wps=10195.9, ups=0.16, wpb=65520.6, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.542, loss_scale=8, train_wall=638, gb_free=4, wall=84653
2022-02-26 10:29:28 | INFO | train_inner | epoch 017:    617 / 788 loss=5.776, ppl=54.81, wps=10197.8, ups=0.16, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.538, loss_scale=8, train_wall=638, gb_free=4, wall=85296
2022-02-26 10:40:11 | INFO | train_inner | epoch 017:    717 / 788 loss=5.788, ppl=55.24, wps=10196.8, ups=0.16, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.536, loss_scale=8, train_wall=638, gb_free=4, wall=85938
2022-02-26 10:43:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 10:47:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 10:47:54 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.18 | ppl 72.51 | wps 23594.2 | wpb 2034.1 | bsz 4 | num_updates 13370 | best_loss 6.18
2022-02-26 10:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 13370 updates
2022-02-26 10:47:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 10:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 10:48:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 17 @ 13370 updates, score 6.18) (writing took 6.276739441091195 seconds)
2022-02-26 10:48:00 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 10:48:00 | INFO | train | epoch 017 | loss 5.746 | ppl 53.65 | wps 10138.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 13370 | lr 0.000273485 | gnorm 0.546 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 86407
2022-02-26 10:48:00 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 10:48:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 10:51:13 | INFO | train_inner | epoch 018:     30 / 788 loss=5.744, ppl=53.59, wps=9856.3, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.559, loss_scale=8, train_wall=641, gb_free=4, wall=86600
2022-02-26 11:01:56 | INFO | train_inner | epoch 018:    130 / 788 loss=5.653, ppl=50.3, wps=10195.4, ups=0.16, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.534, loss_scale=8, train_wall=638, gb_free=4, wall=87243
2022-02-26 11:12:38 | INFO | train_inner | epoch 018:    230 / 788 loss=5.677, ppl=51.16, wps=10195.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.542, loss_scale=8, train_wall=638, gb_free=4, wall=87886
2022-02-26 11:23:21 | INFO | train_inner | epoch 018:    330 / 788 loss=5.7, ppl=51.99, wps=10195.4, ups=0.16, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.549, loss_scale=8, train_wall=638, gb_free=4, wall=88529
2022-02-26 11:34:04 | INFO | train_inner | epoch 018:    430 / 788 loss=5.724, ppl=52.85, wps=10196.2, ups=0.16, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.543, loss_scale=8, train_wall=638, gb_free=4, wall=89171
2022-02-26 11:42:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 11:44:53 | INFO | train_inner | epoch 018:    531 / 788 loss=5.737, ppl=53.34, wps=10092, ups=0.15, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.538, loss_scale=8, train_wall=644, gb_free=4, wall=89821
2022-02-26 11:55:36 | INFO | train_inner | epoch 018:    631 / 788 loss=5.745, ppl=53.62, wps=10195.2, ups=0.16, wpb=65520.6, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.549, loss_scale=8, train_wall=638, gb_free=4, wall=90464
2022-02-26 12:06:19 | INFO | train_inner | epoch 018:    731 / 788 loss=5.756, ppl=54.04, wps=10196.4, ups=0.16, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.537, loss_scale=8, train_wall=638, gb_free=4, wall=91106
2022-02-26 12:12:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 12:12:31 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.181 | ppl 72.53 | wps 23582.4 | wpb 2034.1 | bsz 4 | num_updates 14157 | best_loss 6.18
2022-02-26 12:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 14157 updates
2022-02-26 12:12:31 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 12:12:31 | INFO | train | epoch 018 | loss 5.715 | ppl 52.52 | wps 10163.6 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 14157 | lr 0.000265775 | gnorm 0.544 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 91479
2022-02-26 12:12:32 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 12:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 12:17:08 | INFO | train_inner | epoch 019:     43 / 788 loss=5.707, ppl=52.23, wps=10051, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.546, loss_scale=8, train_wall=635, gb_free=4, wall=91755
2022-02-26 12:27:51 | INFO | train_inner | epoch 019:    143 / 788 loss=5.635, ppl=49.68, wps=10196.1, ups=0.16, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.566, loss_scale=8, train_wall=638, gb_free=4, wall=92398
2022-02-26 12:38:33 | INFO | train_inner | epoch 019:    243 / 788 loss=5.648, ppl=50.13, wps=10196.8, ups=0.16, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.556, loss_scale=16, train_wall=638, gb_free=4, wall=93041
2022-02-26 12:44:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 12:49:23 | INFO | train_inner | epoch 019:    344 / 788 loss=5.666, ppl=50.76, wps=10092.3, ups=0.15, wpb=65519.3, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.559, loss_scale=8, train_wall=644, gb_free=4, wall=93690
2022-02-26 13:00:05 | INFO | train_inner | epoch 019:    444 / 788 loss=5.691, ppl=51.65, wps=10195.4, ups=0.16, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.536, loss_scale=8, train_wall=638, gb_free=4, wall=94333
2022-02-26 13:10:48 | INFO | train_inner | epoch 019:    544 / 788 loss=5.704, ppl=52.13, wps=10196.3, ups=0.16, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.543, loss_scale=8, train_wall=638, gb_free=4, wall=94976
2022-02-26 13:21:31 | INFO | train_inner | epoch 019:    644 / 788 loss=5.718, ppl=52.64, wps=10197, ups=0.16, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.549, loss_scale=8, train_wall=638, gb_free=4, wall=95618
2022-02-26 13:32:14 | INFO | train_inner | epoch 019:    744 / 788 loss=5.743, ppl=53.54, wps=10196.5, ups=0.16, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.557, loss_scale=8, train_wall=638, gb_free=4, wall=96261
2022-02-26 13:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 13:37:03 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.172 | ppl 72.11 | wps 23543.6 | wpb 2034.1 | bsz 4 | num_updates 14944 | best_loss 6.172
2022-02-26 13:37:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 14944 updates
2022-02-26 13:37:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 13:37:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 13:37:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 19 @ 14944 updates, score 6.172) (writing took 6.297605267958716 seconds)
2022-02-26 13:37:09 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 13:37:09 | INFO | train | epoch 019 | loss 5.685 | ppl 51.46 | wps 10151.8 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 14944 | lr 0.000258682 | gnorm 0.55 | loss_scale 8 | train_wall 5023 | gb_free 4 | wall 96557
2022-02-26 13:37:09 | INFO | fairseq.trainer | begin training epoch 20
2022-02-26 13:37:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 13:43:09 | INFO | train_inner | epoch 020:     56 / 788 loss=5.656, ppl=50.42, wps=9950.6, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.546, loss_scale=16, train_wall=635, gb_free=4, wall=96917
2022-02-26 13:43:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 13:53:58 | INFO | train_inner | epoch 020:    157 / 788 loss=5.611, ppl=48.88, wps=10095.9, ups=0.15, wpb=65520.6, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.548, loss_scale=8, train_wall=644, gb_free=4, wall=97566
2022-02-26 14:04:41 | INFO | train_inner | epoch 020:    257 / 788 loss=5.636, ppl=49.72, wps=10193, ups=0.16, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.567, loss_scale=8, train_wall=638, gb_free=4, wall=98209
2022-02-26 14:15:24 | INFO | train_inner | epoch 020:    357 / 788 loss=5.647, ppl=50.1, wps=10194.6, ups=0.16, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.554, loss_scale=8, train_wall=638, gb_free=4, wall=98852
2022-02-26 14:26:07 | INFO | train_inner | epoch 020:    457 / 788 loss=5.66, ppl=50.55, wps=10196.5, ups=0.16, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.574, loss_scale=8, train_wall=638, gb_free=4, wall=99494
2022-02-26 14:36:50 | INFO | train_inner | epoch 020:    557 / 788 loss=5.679, ppl=51.24, wps=10194.4, ups=0.16, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.561, loss_scale=8, train_wall=638, gb_free=4, wall=100137
2022-02-26 14:41:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 14:47:39 | INFO | train_inner | epoch 020:    658 / 788 loss=5.698, ppl=51.9, wps=10093.3, ups=0.15, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.566, loss_scale=8, train_wall=644, gb_free=4, wall=100787
2022-02-26 14:58:22 | INFO | train_inner | epoch 020:    758 / 788 loss=5.711, ppl=52.39, wps=10194.9, ups=0.16, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.572, loss_scale=8, train_wall=638, gb_free=4, wall=101429
2022-02-26 15:01:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 15:01:41 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.168 | ppl 71.92 | wps 23581.7 | wpb 2034.1 | bsz 4 | num_updates 15730 | best_loss 6.168
2022-02-26 15:01:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 15730 updates
2022-02-26 15:01:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 15:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 15:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 20 @ 15730 updates, score 6.168) (writing took 6.24091334710829 seconds)
2022-02-26 15:01:47 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-26 15:01:47 | INFO | train | epoch 020 | loss 5.659 | ppl 50.54 | wps 10137.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 15730 | lr 0.000252136 | gnorm 0.561 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 101635
2022-02-26 15:01:47 | INFO | fairseq.trainer | begin training epoch 21
2022-02-26 15:01:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 15:09:18 | INFO | train_inner | epoch 021:     70 / 788 loss=5.607, ppl=48.72, wps=9950.5, ups=0.15, wpb=65248, bsz=127.4, num_updates=15800, lr=0.000251577, gnorm=0.559, loss_scale=8, train_wall=635, gb_free=4, wall=102085
2022-02-26 15:20:00 | INFO | train_inner | epoch 021:    170 / 788 loss=5.579, ppl=47.81, wps=10194.6, ups=0.16, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.553, loss_scale=8, train_wall=638, gb_free=4, wall=102728
2022-02-26 15:30:43 | INFO | train_inner | epoch 021:    270 / 788 loss=5.618, ppl=49.11, wps=10194.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.562, loss_scale=8, train_wall=638, gb_free=4, wall=103371
2022-02-26 15:41:26 | INFO | train_inner | epoch 021:    370 / 788 loss=5.625, ppl=49.36, wps=10193.8, ups=0.16, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.547, loss_scale=16, train_wall=638, gb_free=4, wall=104014
2022-02-26 15:52:09 | INFO | train_inner | epoch 021:    470 / 788 loss=5.646, ppl=50.08, wps=10193.4, ups=0.16, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.561, loss_scale=16, train_wall=638, gb_free=4, wall=104657
2022-02-26 15:55:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:02:58 | INFO | train_inner | epoch 021:    571 / 788 loss=5.657, ppl=50.46, wps=10093.9, ups=0.15, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.552, loss_scale=8, train_wall=644, gb_free=4, wall=105306
2022-02-26 16:13:41 | INFO | train_inner | epoch 021:    671 / 788 loss=5.671, ppl=50.97, wps=10197.2, ups=0.16, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.555, loss_scale=8, train_wall=638, gb_free=4, wall=105949
2022-02-26 16:24:23 | INFO | train_inner | epoch 021:    771 / 788 loss=5.693, ppl=51.74, wps=10197.6, ups=0.16, wpb=65520.6, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.588, loss_scale=8, train_wall=638, gb_free=4, wall=106591
2022-02-26 16:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 16:26:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.162 | ppl 71.63 | wps 23580.3 | wpb 2034.1 | bsz 4 | num_updates 16517 | best_loss 6.162
2022-02-26 16:26:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 16517 updates
2022-02-26 16:26:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 16:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 16:26:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 21 @ 16517 updates, score 6.162) (writing took 6.202126953983679 seconds)
2022-02-26 16:26:25 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-26 16:26:25 | INFO | train | epoch 021 | loss 5.636 | ppl 49.72 | wps 10150.7 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 16517 | lr 0.000246056 | gnorm 0.56 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 106713
2022-02-26 16:26:25 | INFO | fairseq.trainer | begin training epoch 22
2022-02-26 16:26:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 16:35:19 | INFO | train_inner | epoch 022:     83 / 788 loss=5.577, ppl=47.74, wps=9952.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=16600, lr=0.00024544, gnorm=0.544, loss_scale=8, train_wall=635, gb_free=4, wall=107247
2022-02-26 16:46:02 | INFO | train_inner | epoch 022:    183 / 788 loss=5.569, ppl=47.46, wps=10196.6, ups=0.16, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.567, loss_scale=8, train_wall=638, gb_free=4, wall=107889
2022-02-26 16:50:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 16:56:51 | INFO | train_inner | epoch 022:    284 / 788 loss=5.585, ppl=48.01, wps=10095.5, ups=0.15, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.554, loss_scale=8, train_wall=644, gb_free=4, wall=108539
2022-02-26 17:07:34 | INFO | train_inner | epoch 022:    384 / 788 loss=5.601, ppl=48.55, wps=10194.8, ups=0.16, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.556, loss_scale=8, train_wall=638, gb_free=4, wall=109181
2022-02-26 17:18:17 | INFO | train_inner | epoch 022:    484 / 788 loss=5.615, ppl=49, wps=10195.1, ups=0.16, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.576, loss_scale=8, train_wall=638, gb_free=4, wall=109824
2022-02-26 17:28:59 | INFO | train_inner | epoch 022:    584 / 788 loss=5.652, ppl=50.29, wps=10195.9, ups=0.16, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.574, loss_scale=8, train_wall=638, gb_free=4, wall=110467
2022-02-26 17:39:42 | INFO | train_inner | epoch 022:    684 / 788 loss=5.651, ppl=50.25, wps=10195.7, ups=0.16, wpb=65520.6, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.543, loss_scale=8, train_wall=638, gb_free=4, wall=111110
2022-02-26 17:50:25 | INFO | train_inner | epoch 022:    784 / 788 loss=5.665, ppl=50.72, wps=10193.8, ups=0.16, wpb=65534.7, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.575, loss_scale=16, train_wall=638, gb_free=4, wall=111752
2022-02-26 17:50:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 17:50:57 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.155 | ppl 71.26 | wps 23569.6 | wpb 2034.1 | bsz 4 | num_updates 17304 | best_loss 6.155
2022-02-26 17:50:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 17304 updates
2022-02-26 17:50:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 17:51:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 17:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 22 @ 17304 updates, score 6.155) (writing took 6.244392198976129 seconds)
2022-02-26 17:51:03 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-26 17:51:03 | INFO | train | epoch 022 | loss 5.613 | ppl 48.95 | wps 10151 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 17304 | lr 0.000240396 | gnorm 0.562 | loss_scale 16 | train_wall 5024 | gb_free 4 | wall 111791
2022-02-26 17:51:03 | INFO | fairseq.trainer | begin training epoch 23
2022-02-26 17:51:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 18:01:21 | INFO | train_inner | epoch 023:     96 / 788 loss=5.531, ppl=46.24, wps=9947.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=17400, lr=0.000239732, gnorm=0.56, loss_scale=16, train_wall=635, gb_free=4, wall=112408
2022-02-26 18:02:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 18:12:10 | INFO | train_inner | epoch 023:    197 / 788 loss=5.549, ppl=46.82, wps=10092.8, ups=0.15, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.561, loss_scale=8, train_wall=644, gb_free=4, wall=113058
2022-02-26 18:22:53 | INFO | train_inner | epoch 023:    297 / 788 loss=5.569, ppl=47.46, wps=10195.7, ups=0.16, wpb=65534.7, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.573, loss_scale=8, train_wall=638, gb_free=4, wall=113701
2022-02-26 18:33:36 | INFO | train_inner | epoch 023:    397 / 788 loss=5.598, ppl=48.45, wps=10194.9, ups=0.16, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.575, loss_scale=8, train_wall=638, gb_free=4, wall=114343
2022-02-26 18:44:19 | INFO | train_inner | epoch 023:    497 / 788 loss=5.595, ppl=48.33, wps=10193.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.566, loss_scale=8, train_wall=638, gb_free=4, wall=114986
2022-02-26 18:55:02 | INFO | train_inner | epoch 023:    597 / 788 loss=5.625, ppl=49.35, wps=10193.2, ups=0.16, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.584, loss_scale=8, train_wall=638, gb_free=4, wall=115629
2022-02-26 19:05:45 | INFO | train_inner | epoch 023:    697 / 788 loss=5.638, ppl=49.81, wps=10189.4, ups=0.16, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.567, loss_scale=16, train_wall=638, gb_free=4, wall=116272
2022-02-26 19:06:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 19:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:36 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.159 | ppl 71.43 | wps 23573.6 | wpb 2034.1 | bsz 4 | num_updates 18090 | best_loss 6.155
2022-02-26 19:15:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 18090 updates
2022-02-26 19:15:36 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-26 19:15:36 | INFO | train | epoch 023 | loss 5.593 | ppl 48.25 | wps 10148.8 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 18090 | lr 0.000235115 | gnorm 0.57 | loss_scale 8 | train_wall 5025 | gb_free 4 | wall 116863
2022-02-26 19:15:36 | INFO | fairseq.trainer | begin training epoch 24
2022-02-26 19:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:16:40 | INFO | train_inner | epoch 024:     10 / 788 loss=5.629, ppl=49.48, wps=9952.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.576, loss_scale=8, train_wall=641, gb_free=4, wall=116928
2022-02-26 19:27:23 | INFO | train_inner | epoch 024:    110 / 788 loss=5.513, ppl=45.65, wps=10196.2, ups=0.16, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.588, loss_scale=8, train_wall=638, gb_free=4, wall=117571
2022-02-26 19:38:06 | INFO | train_inner | epoch 024:    210 / 788 loss=5.534, ppl=46.32, wps=10196.5, ups=0.16, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.566, loss_scale=8, train_wall=638, gb_free=4, wall=118213
2022-02-26 19:48:48 | INFO | train_inner | epoch 024:    310 / 788 loss=5.55, ppl=46.86, wps=10198.5, ups=0.16, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.575, loss_scale=8, train_wall=638, gb_free=4, wall=118856
2022-02-26 19:59:31 | INFO | train_inner | epoch 024:    410 / 788 loss=5.573, ppl=47.62, wps=10198, ups=0.16, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.554, loss_scale=8, train_wall=638, gb_free=4, wall=119499
2022-02-26 20:10:14 | INFO | train_inner | epoch 024:    510 / 788 loss=5.593, ppl=48.27, wps=10192, ups=0.16, wpb=65520.6, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.57, loss_scale=16, train_wall=638, gb_free=4, wall=120141
2022-02-26 20:14:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 20:21:03 | INFO | train_inner | epoch 024:    611 / 788 loss=5.604, ppl=48.64, wps=10096.7, ups=0.15, wpb=65534.7, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.582, loss_scale=8, train_wall=644, gb_free=4, wall=120790
2022-02-26 20:31:46 | INFO | train_inner | epoch 024:    711 / 788 loss=5.61, ppl=48.83, wps=10198.9, ups=0.16, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.56, loss_scale=8, train_wall=638, gb_free=4, wall=121433
2022-02-26 20:39:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 20:40:07 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.151 | ppl 71.05 | wps 23553.9 | wpb 2034.1 | bsz 4 | num_updates 18877 | best_loss 6.151
2022-02-26 20:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 18877 updates
2022-02-26 20:40:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 20:40:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-26 20:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 24 @ 18877 updates, score 6.151) (writing took 6.107826037099585 seconds)
2022-02-26 20:40:13 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-26 20:40:13 | INFO | train | epoch 024 | loss 5.574 | ppl 47.62 | wps 10153 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 18877 | lr 0.000230162 | gnorm 0.57 | loss_scale 8 | train_wall 5023 | gb_free 4 | wall 121940
2022-02-26 20:40:13 | INFO | fairseq.trainer | begin training epoch 25
2022-02-26 20:40:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 20:42:41 | INFO | train_inner | epoch 025:     23 / 788 loss=5.599, ppl=48.46, wps=9955.9, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.57, loss_scale=8, train_wall=635, gb_free=4, wall=122088
2022-02-26 20:53:24 | INFO | train_inner | epoch 025:    123 / 788 loss=5.494, ppl=45.05, wps=10194.5, ups=0.16, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.564, loss_scale=8, train_wall=638, gb_free=4, wall=122731
2022-02-26 21:04:07 | INFO | train_inner | epoch 025:    223 / 788 loss=5.518, ppl=45.83, wps=10195, ups=0.16, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.577, loss_scale=8, train_wall=638, gb_free=4, wall=123374
2022-02-26 21:14:50 | INFO | train_inner | epoch 025:    323 / 788 loss=5.539, ppl=46.5, wps=10192.4, ups=0.16, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.585, loss_scale=16, train_wall=638, gb_free=4, wall=124017
2022-02-26 21:18:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 21:25:39 | INFO | train_inner | epoch 025:    424 / 788 loss=5.565, ppl=47.35, wps=10092.9, ups=0.15, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.568, loss_scale=8, train_wall=644, gb_free=4, wall=124666
2022-02-26 21:36:22 | INFO | train_inner | epoch 025:    524 / 788 loss=5.577, ppl=47.75, wps=10193.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.56, loss_scale=8, train_wall=638, gb_free=4, wall=125309
2022-02-26 21:47:05 | INFO | train_inner | epoch 025:    624 / 788 loss=5.579, ppl=47.79, wps=10195.4, ups=0.16, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.566, loss_scale=8, train_wall=638, gb_free=4, wall=125952
2022-02-26 21:57:47 | INFO | train_inner | epoch 025:    724 / 788 loss=5.602, ppl=48.58, wps=10194.6, ups=0.16, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.565, loss_scale=8, train_wall=638, gb_free=4, wall=126595
2022-02-26 22:04:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 22:04:45 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.156 | ppl 71.29 | wps 23535.1 | wpb 2034.1 | bsz 4 | num_updates 19664 | best_loss 6.151
2022-02-26 22:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 19664 updates
2022-02-26 22:04:45 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-26 22:04:45 | INFO | train | epoch 025 | loss 5.556 | ppl 47.05 | wps 10162.3 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 19664 | lr 0.000225509 | gnorm 0.569 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 127013
2022-02-26 22:04:45 | INFO | fairseq.trainer | begin training epoch 26
2022-02-26 22:04:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 22:08:37 | INFO | train_inner | epoch 026:     36 / 788 loss=5.554, ppl=47, wps=10045.4, ups=0.15, wpb=65218.6, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.567, loss_scale=8, train_wall=635, gb_free=4, wall=127244
2022-02-26 22:19:20 | INFO | train_inner | epoch 026:    136 / 788 loss=5.485, ppl=44.79, wps=10190.7, ups=0.16, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.565, loss_scale=16, train_wall=638, gb_free=4, wall=127887
2022-02-26 22:26:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 22:30:09 | INFO | train_inner | epoch 026:    237 / 788 loss=5.499, ppl=45.22, wps=10091.9, ups=0.15, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.566, loss_scale=8, train_wall=644, gb_free=4, wall=128537
2022-02-26 22:40:52 | INFO | train_inner | epoch 026:    337 / 788 loss=5.531, ppl=46.23, wps=10194.8, ups=0.16, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.589, loss_scale=8, train_wall=638, gb_free=4, wall=129180
2022-02-26 22:51:35 | INFO | train_inner | epoch 026:    437 / 788 loss=5.536, ppl=46.41, wps=10195.3, ups=0.16, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.564, loss_scale=8, train_wall=638, gb_free=4, wall=129822
2022-02-26 23:02:18 | INFO | train_inner | epoch 026:    537 / 788 loss=5.56, ppl=47.17, wps=10195.5, ups=0.16, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.571, loss_scale=8, train_wall=638, gb_free=4, wall=130465
2022-02-26 23:13:00 | INFO | train_inner | epoch 026:    637 / 788 loss=5.588, ppl=48.11, wps=10197, ups=0.16, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.578, loss_scale=8, train_wall=638, gb_free=4, wall=131108
2022-02-26 23:23:43 | INFO | train_inner | epoch 026:    737 / 788 loss=5.582, ppl=47.89, wps=10194.9, ups=0.16, wpb=65534.7, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.579, loss_scale=16, train_wall=638, gb_free=4, wall=131751
2022-02-26 23:24:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 23:29:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 23:29:17 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.158 | ppl 71.42 | wps 23568 | wpb 2034.1 | bsz 4 | num_updates 20450 | best_loss 6.151
2022-02-26 23:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 20450 updates
2022-02-26 23:29:17 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-26 23:29:17 | INFO | train | epoch 026 | loss 5.54 | ppl 46.53 | wps 10149.8 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 20450 | lr 0.000221133 | gnorm 0.573 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 132085
2022-02-26 23:29:17 | INFO | fairseq.trainer | begin training epoch 27
2022-02-26 23:29:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 23:34:39 | INFO | train_inner | epoch 027:     50 / 788 loss=5.535, ppl=46.38, wps=9950.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=20500, lr=0.000220863, gnorm=0.583, loss_scale=8, train_wall=641, gb_free=4, wall=132406
2022-02-26 23:45:22 | INFO | train_inner | epoch 027:    150 / 788 loss=5.468, ppl=44.25, wps=10193.4, ups=0.16, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.569, loss_scale=8, train_wall=638, gb_free=4, wall=133049
2022-02-26 23:56:05 | INFO | train_inner | epoch 027:    250 / 788 loss=5.486, ppl=44.8, wps=10192.7, ups=0.16, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.584, loss_scale=8, train_wall=638, gb_free=4, wall=133692
2022-02-27 00:06:48 | INFO | train_inner | epoch 027:    350 / 788 loss=5.501, ppl=45.28, wps=10194.2, ups=0.16, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.584, loss_scale=8, train_wall=638, gb_free=4, wall=134335
2022-02-27 00:17:30 | INFO | train_inner | epoch 027:    450 / 788 loss=5.525, ppl=46.05, wps=10195.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.571, loss_scale=8, train_wall=638, gb_free=4, wall=134978
2022-02-27 00:21:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 00:28:19 | INFO | train_inner | epoch 027:    551 / 788 loss=5.545, ppl=46.69, wps=10095.2, ups=0.15, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.574, loss_scale=8, train_wall=644, gb_free=4, wall=135627
2022-02-27 00:39:02 | INFO | train_inner | epoch 027:    651 / 788 loss=5.563, ppl=47.27, wps=10195.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.584, loss_scale=8, train_wall=638, gb_free=4, wall=136270
2022-02-27 00:49:45 | INFO | train_inner | epoch 027:    751 / 788 loss=5.582, ppl=47.9, wps=10193.7, ups=0.16, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.567, loss_scale=8, train_wall=638, gb_free=4, wall=136913
2022-02-27 00:53:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 00:53:49 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.158 | ppl 71.4 | wps 23573 | wpb 2034.1 | bsz 4 | num_updates 21237 | best_loss 6.151
2022-02-27 00:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 21237 updates
2022-02-27 00:53:49 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-27 00:53:49 | INFO | train | epoch 027 | loss 5.524 | ppl 46.02 | wps 10162.8 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 21237 | lr 0.000216997 | gnorm 0.577 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 137157
2022-02-27 00:53:49 | INFO | fairseq.trainer | begin training epoch 28
2022-02-27 00:53:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 01:00:34 | INFO | train_inner | epoch 028:     63 / 788 loss=5.495, ppl=45.08, wps=10048.7, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=21300, lr=0.000216676, gnorm=0.598, loss_scale=8, train_wall=635, gb_free=4, wall=137562
2022-02-27 01:11:17 | INFO | train_inner | epoch 028:    163 / 788 loss=5.463, ppl=44.12, wps=10194, ups=0.16, wpb=65534.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.566, loss_scale=8, train_wall=638, gb_free=4, wall=138205
2022-02-27 01:22:00 | INFO | train_inner | epoch 028:    263 / 788 loss=5.476, ppl=44.52, wps=10192.5, ups=0.16, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.585, loss_scale=16, train_wall=638, gb_free=4, wall=138848
2022-02-27 01:24:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 01:32:49 | INFO | train_inner | epoch 028:    364 / 788 loss=5.49, ppl=44.94, wps=10094.8, ups=0.15, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.585, loss_scale=8, train_wall=644, gb_free=4, wall=139497
2022-02-27 01:43:32 | INFO | train_inner | epoch 028:    464 / 788 loss=5.52, ppl=45.88, wps=10196.5, ups=0.16, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.596, loss_scale=8, train_wall=638, gb_free=4, wall=140140
2022-02-27 01:54:15 | INFO | train_inner | epoch 028:    564 / 788 loss=5.523, ppl=45.98, wps=10196.4, ups=0.16, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.598, loss_scale=8, train_wall=638, gb_free=4, wall=140782
2022-02-27 02:04:58 | INFO | train_inner | epoch 028:    664 / 788 loss=5.554, ppl=47, wps=10196.9, ups=0.16, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.571, loss_scale=8, train_wall=638, gb_free=4, wall=141425
2022-02-27 02:15:40 | INFO | train_inner | epoch 028:    764 / 788 loss=5.571, ppl=47.53, wps=10196.8, ups=0.16, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.573, loss_scale=8, train_wall=638, gb_free=4, wall=142068
2022-02-27 02:18:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 02:18:21 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.156 | ppl 71.32 | wps 23540.2 | wpb 2034.1 | bsz 4 | num_updates 22024 | best_loss 6.151
2022-02-27 02:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 22024 updates
2022-02-27 02:18:21 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-27 02:18:21 | INFO | train | epoch 028 | loss 5.509 | ppl 45.55 | wps 10163.9 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 22024 | lr 0.000213085 | gnorm 0.584 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 142229
2022-02-27 02:18:21 | INFO | fairseq.trainer | begin training epoch 29
2022-02-27 02:18:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 02:23:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 02:26:36 | INFO | train_inner | epoch 029:     77 / 788 loss=5.463, ppl=44.1, wps=9950.2, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22100, lr=0.000212718, gnorm=0.58, loss_scale=8, train_wall=641, gb_free=4, wall=142724
2022-02-27 02:37:19 | INFO | train_inner | epoch 029:    177 / 788 loss=5.45, ppl=43.72, wps=10195.4, ups=0.16, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.592, loss_scale=8, train_wall=638, gb_free=4, wall=143366
2022-02-27 02:48:02 | INFO | train_inner | epoch 029:    277 / 788 loss=5.478, ppl=44.57, wps=10197.2, ups=0.16, wpb=65534.7, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.606, loss_scale=8, train_wall=638, gb_free=4, wall=144009
2022-02-27 02:58:44 | INFO | train_inner | epoch 029:    377 / 788 loss=5.481, ppl=44.67, wps=10193, ups=0.16, wpb=65520.6, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.581, loss_scale=8, train_wall=638, gb_free=4, wall=144652
2022-02-27 03:09:27 | INFO | train_inner | epoch 029:    477 / 788 loss=5.506, ppl=45.45, wps=10195, ups=0.16, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.606, loss_scale=8, train_wall=638, gb_free=4, wall=145295
2022-02-27 03:20:10 | INFO | train_inner | epoch 029:    577 / 788 loss=5.515, ppl=45.73, wps=10193.8, ups=0.16, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.595, loss_scale=16, train_wall=638, gb_free=4, wall=145938
2022-02-27 03:30:53 | INFO | train_inner | epoch 029:    677 / 788 loss=5.542, ppl=46.58, wps=10191.2, ups=0.16, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.574, loss_scale=16, train_wall=638, gb_free=4, wall=146581
2022-02-27 03:36:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 03:41:42 | INFO | train_inner | epoch 029:    778 / 788 loss=5.542, ppl=46.59, wps=10093.1, ups=0.15, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.588, loss_scale=8, train_wall=644, gb_free=4, wall=147230
2022-02-27 03:42:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 03:42:53 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.156 | ppl 71.33 | wps 23544.9 | wpb 2034.1 | bsz 4 | num_updates 22810 | best_loss 6.151
2022-02-27 03:42:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 22810 updates
2022-02-27 03:42:53 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-27 03:42:53 | INFO | train | epoch 029 | loss 5.495 | ppl 45.11 | wps 10149.6 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 22810 | lr 0.000209381 | gnorm 0.59 | loss_scale 8 | train_wall 5024 | gb_free 4 | wall 147301
2022-02-27 03:42:53 | INFO | fairseq.trainer | begin training epoch 30
2022-02-27 03:42:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 03:52:32 | INFO | train_inner | epoch 030:     90 / 788 loss=5.419, ppl=42.78, wps=10049.7, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=22900, lr=0.000208969, gnorm=0.578, loss_scale=8, train_wall=635, gb_free=4, wall=147879
2022-02-27 04:03:14 | INFO | train_inner | epoch 030:    190 / 788 loss=5.446, ppl=43.59, wps=10195.5, ups=0.16, wpb=65520.6, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.585, loss_scale=8, train_wall=638, gb_free=4, wall=148522
2022-02-27 04:13:57 | INFO | train_inner | epoch 030:    290 / 788 loss=5.465, ppl=44.16, wps=10195.4, ups=0.16, wpb=65534.7, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.591, loss_scale=8, train_wall=638, gb_free=4, wall=149165
2022-02-27 04:24:40 | INFO | train_inner | epoch 030:    390 / 788 loss=5.473, ppl=44.4, wps=10196, ups=0.16, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.588, loss_scale=8, train_wall=638, gb_free=4, wall=149807
2022-02-27 04:33:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 04:35:29 | INFO | train_inner | epoch 030:    491 / 788 loss=5.492, ppl=44.99, wps=10095.9, ups=0.15, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.606, loss_scale=8, train_wall=644, gb_free=4, wall=150457
2022-02-27 04:46:12 | INFO | train_inner | epoch 030:    591 / 788 loss=5.51, ppl=45.58, wps=10196.4, ups=0.16, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.577, loss_scale=8, train_wall=638, gb_free=4, wall=151099
2022-02-27 04:56:55 | INFO | train_inner | epoch 030:    691 / 788 loss=5.53, ppl=46.2, wps=10196.2, ups=0.16, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.605, loss_scale=8, train_wall=638, gb_free=4, wall=151742
2022-02-27 05:07:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 05:07:24 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.152 | ppl 71.09 | wps 23572.5 | wpb 2034.1 | bsz 4 | num_updates 23597 | best_loss 6.151
2022-02-27 05:07:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 23597 updates
2022-02-27 05:07:25 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-27 05:07:25 | INFO | train | epoch 030 | loss 5.483 | ppl 44.72 | wps 10164.3 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 23597 | lr 0.00020586 | gnorm 0.589 | loss_scale 8 | train_wall 5023 | gb_free 4 | wall 152372
2022-02-27 05:07:25 | INFO | fairseq.trainer | begin training epoch 31
2022-02-27 05:07:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 05:07:44 | INFO | train_inner | epoch 031:      3 / 788 loss=5.531, ppl=46.24, wps=10048.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.584, loss_scale=8, train_wall=635, gb_free=4, wall=152391
2022-02-27 05:18:27 | INFO | train_inner | epoch 031:    103 / 788 loss=5.404, ppl=42.35, wps=10196.2, ups=0.16, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.581, loss_scale=8, train_wall=638, gb_free=4, wall=153034
2022-02-27 05:29:09 | INFO | train_inner | epoch 031:    203 / 788 loss=5.425, ppl=42.98, wps=10197.1, ups=0.16, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.594, loss_scale=16, train_wall=638, gb_free=4, wall=153677
2022-02-27 05:33:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 05:39:59 | INFO | train_inner | epoch 031:    304 / 788 loss=5.455, ppl=43.86, wps=10093.3, ups=0.15, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.593, loss_scale=8, train_wall=644, gb_free=4, wall=154326
2022-02-27 05:50:41 | INFO | train_inner | epoch 031:    404 / 788 loss=5.46, ppl=44.03, wps=10198, ups=0.16, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.605, loss_scale=8, train_wall=638, gb_free=4, wall=154969
2022-02-27 06:01:24 | INFO | train_inner | epoch 031:    504 / 788 loss=5.491, ppl=44.98, wps=10196.4, ups=0.16, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.583, loss_scale=8, train_wall=638, gb_free=4, wall=155612
2022-02-27 06:12:07 | INFO | train_inner | epoch 031:    604 / 788 loss=5.501, ppl=45.27, wps=10195.5, ups=0.16, wpb=65519.3, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.594, loss_scale=8, train_wall=638, gb_free=4, wall=156254
2022-02-27 06:22:49 | INFO | train_inner | epoch 031:    704 / 788 loss=5.512, ppl=45.63, wps=10196.2, ups=0.16, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.582, loss_scale=8, train_wall=638, gb_free=4, wall=156897
2022-02-27 06:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 06:31:56 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.148 | ppl 70.91 | wps 23615.7 | wpb 2034.1 | bsz 4 | num_updates 24384 | best_loss 6.148
2022-02-27 06:31:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 24384 updates
2022-02-27 06:31:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-27 06:32:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt
2022-02-27 06:32:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.5-jelinek_0.036_0.004_0.96_#2/checkpoint_best.pt (epoch 31 @ 24384 updates, score 6.148) (writing took 6.306548385182396 seconds)
2022-02-27 06:32:02 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-27 06:32:02 | INFO | train | epoch 031 | loss 5.47 | ppl 44.32 | wps 10151.8 | ups 0.15 | wpb 65497.5 | bsz 127.9 | num_updates 24384 | lr 0.00020251 | gnorm 0.59 | loss_scale 16 | train_wall 5023 | gb_free 4 | wall 157450
2022-02-27 06:32:02 | INFO | fairseq.trainer | begin training epoch 32
2022-02-27 06:32:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 06:33:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 06:33:51 | INFO | train_inner | epoch 032:     17 / 788 loss=5.499, ppl=45.23, wps=9855.5, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.591, loss_scale=8, train_wall=642, gb_free=4, wall=157559
2022-02-27 06:44:34 | INFO | train_inner | epoch 032:    117 / 788 loss=5.393, ppl=42.03, wps=10192.3, ups=0.16, wpb=65534.7, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.591, loss_scale=8, train_wall=638, gb_free=4, wall=158202
2022-02-27 06:55:17 | INFO | train_inner | epoch 032:    217 / 788 loss=5.43, ppl=43.12, wps=10194.1, ups=0.16, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.585, loss_scale=8, train_wall=638, gb_free=4, wall=158845
2022-02-27 07:06:00 | INFO | train_inner | epoch 032:    317 / 788 loss=5.43, ppl=43.12, wps=10192.6, ups=0.16, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.612, loss_scale=8, train_wall=638, gb_free=4, wall=159488
2022-02-27 07:16:43 | INFO | train_inner | epoch 032:    417 / 788 loss=5.464, ppl=44.13, wps=10194.2, ups=0.16, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.59, loss_scale=8, train_wall=638, gb_free=4, wall=160131
2022-02-27 07:27:26 | INFO | train_inner | epoch 032:    517 / 788 loss=5.476, ppl=44.52, wps=10194.2, ups=0.16, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.592, loss_scale=8, train_wall=638, gb_free=4, wall=160774
2022-02-27 07:38:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 07:38:16 | INFO | train_inner | epoch 032:    618 / 788 loss=5.484, ppl=44.75, wps=10080.7, ups=0.15, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.59, loss_scale=8, train_wall=645, gb_free=4, wall=161424
2022-02-27 07:48:59 | INFO | train_inner | epoch 032:    718 / 788 loss=5.509, ppl=45.52, wps=10193.1, ups=0.16, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.623, loss_scale=8, train_wall=638, gb_free=4, wall=162067
2022-02-27 07:56:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 07:56:35 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.155 | ppl 71.24 | wps 23547.3 | wpb 2034.1 | bsz 4 | num_updates 25170 | best_loss 6.148
2022-02-27 07:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 25170 updates
2022-02-27 07:56:35 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-27 07:56:35 | INFO | train | epoch 032 | loss 5.458 | ppl 43.97 | wps 10147.2 | ups 0.15 | wpb 65497.4 | bsz 127.9 | num_updates 25170 | lr 0.000199323 | gnorm 0.596 | loss_scale 8 | train_wall 5025 | gb_free 4 | wall 162523
2022-02-27 07:56:36 | INFO | fairseq.trainer | begin training epoch 33
2022-02-27 07:56:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 07:59:48 | INFO | train_inner | epoch 033:     30 / 788 loss=5.466, ppl=44.19, wps=10046.8, ups=0.15, wpb=65233.9, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.586, loss_scale=8, train_wall=635, gb_free=4, wall=162716
2022-02-27 08:10:31 | INFO | train_inner | epoch 033:    130 / 788 loss=5.381, ppl=41.67, wps=10196.7, ups=0.16, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.59, loss_scale=8, train_wall=638, gb_free=4, wall=163359
2022-02-27 08:21:14 | INFO | train_inner | epoch 033:    230 / 788 loss=5.41, ppl=42.52, wps=10197.9, ups=0.16, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.615, loss_scale=8, train_wall=638, gb_free=4, wall=164001
2022-02-27 08:31:56 | INFO | train_inner | epoch 033:    330 / 788 loss=5.432, ppl=43.18, wps=10197.6, ups=0.16, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.598, loss_scale=8, train_wall=638, gb_free=4, wall=164644
2022-02-27 08:33:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 08:42:46 | INFO | train_inner | epoch 033:    431 / 788 loss=5.44, ppl=43.41, wps=10096.5, ups=0.15, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.609, loss_scale=8, train_wall=644, gb_free=4, wall=165293
2022-02-27 08:53:28 | INFO | train_inner | epoch 033:    531 / 788 loss=5.467, ppl=44.23, wps=10196.7, ups=0.16, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.592, loss_scale=8, train_wall=638, gb_free=4, wall=165936
2022-02-27 09:04:11 | INFO | train_inner | epoch 033:    631 / 788 loss=5.492, ppl=44.99, wps=10196.9, ups=0.16, wpb=65519.3, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.597, loss_scale=8, train_wall=638, gb_free=4, wall=166578
2022-02-27 09:14:54 | INFO | train_inner | epoch 033:    731 / 788 loss=5.499, ppl=45.23, wps=10193.8, ups=0.16, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.595, loss_scale=8, train_wall=638, gb_free=4, wall=167221
2022-02-27 09:20:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-27 09:21:06 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.153 | ppl 71.16 | wps 23518.8 | wpb 2034.1 | bsz 4 | num_updates 25957 | best_loss 6.148
2022-02-27 09:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 25957 updates
2022-02-27 09:21:06 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-27 09:21:06 | INFO | train | epoch 033 | loss 5.448 | ppl 43.64 | wps 10165.1 | ups 0.16 | wpb 65497.5 | bsz 127.9 | num_updates 25957 | lr 0.000196279 | gnorm 0.601 | loss_scale 8 | train_wall 5023 | gb_free 4 | wall 167594
2022-02-27 09:21:06 | INFO | fairseq.trainer | begin training epoch 34
2022-02-27 09:21:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-27 09:25:43 | INFO | train_inner | epoch 034:     43 / 788 loss=5.446, ppl=43.58, wps=10049, ups=0.15, wpb=65249.3, bsz=127.4, num_updates=26000, lr=0.000196116, gnorm=0.599, loss_scale=8, train_wall=635, gb_free=4, wall=167871
2022-02-27 09:36:26 | INFO | train_inner | epoch 034:    143 / 788 loss=5.373, ppl=41.45, wps=10193.2, ups=0.16, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.589, loss_scale=16, train_wall=638, gb_free=4, wall=168514
2022-02-27 09:37:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 09:47:15 | INFO | train_inner | epoch 034:    244 / 788 loss=5.403, ppl=42.3, wps=10091.3, ups=0.15, wpb=65534.7, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.609, loss_scale=8, train_wall=644, gb_free=4, wall=169163
2022-02-27 09:57:58 | INFO | train_inner | epoch 034:    344 / 788 loss=5.416, ppl=42.7, wps=10195.3, ups=0.16, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.618, loss_scale=8, train_wall=638, gb_free=4, wall=169806
2022-02-27 10:08:41 | INFO | train_inner | epoch 034:    444 / 788 loss=5.444, ppl=43.53, wps=10195.5, ups=0.16, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.592, loss_scale=8, train_wall=638, gb_free=4, wall=170449
2022-02-27 10:19:24 | INFO | train_inner | epoch 034:    544 / 788 loss=5.463, ppl=44.1, wps=10197, ups=0.16, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.613, loss_scale=8, train_wall=638, gb_free=4, wall=171091
2022-02-27 10:30:06 | INFO | train_inner | epoch 034:    644 / 788 loss=5.475, ppl=44.49, wps=10196.9, ups=0.16, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.611, loss_scale=8, train_wall=638, gb_free=4, wall=171734
2022-02-27 10:38:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-27 10:40:56 | INFO | train_inner | epoch 034:    745 / 788 loss=5.487, ppl=44.84, wps=10093.8, ups=0.15, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.603, loss_scale=8, train_wall=644, gb_free=4, wall=172383
User defined signal 2
